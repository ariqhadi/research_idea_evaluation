{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "890e29b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "ideas_file = 'sample_ideas/agentic_ai_for_idea_generation_ideas.json'\n",
    "\n",
    "with open(ideas_file, 'r') as f:\n",
    "    ideas_data = json.load(f)\n",
    "\n",
    "for ideas in ideas_data['ideas']:\n",
    "    for idea, items in ideas.items():\n",
    "        print(idea)\n",
    "        for item, i in items.items():\n",
    "            print(f\"- {item}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "7f575aaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langgraph.graph import END, StateGraph, START\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langgraph.graph import MessagesState\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "# from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "from typing import Literal\n",
    "from dotenv import load_dotenv\n",
    "import json\n",
    "import os \n",
    "\n",
    "load_dotenv()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "e13a1677",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "llm = ChatOpenAI(\n",
    "    temperature=0,  # Set the temperature for the model's responses\n",
    "    model_name=\"gpt-5-nano\",  # Specify the model name\n",
    "    openai_api_key=os.getenv(\"OPENAI_API_KEY\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb13e2a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash-lite\",\n",
    "                             google_api_key=os.getenv(\"GOOGLE_API_KEY\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "cd02d825",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python is a high-level, versatile programming language renowned for its simple, readable syntax that enables rapid development for everything from web applications to data science.\n"
     ]
    }
   ],
   "source": [
    "## Testing LLM Call\n",
    "\n",
    "# \n",
    "messages = [\n",
    "    SystemMessage(content=\"You are a helpful assistant\"),\n",
    "    HumanMessage(content=\"Tell me about Python programming in one sentence\")\n",
    "]\n",
    "response = llm.invoke(messages)\n",
    "print(response.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6965b7c",
   "metadata": {},
   "source": [
    "## Retrieve Paper with SemanticScholar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7b750afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "\n",
    "class getReferencePaper():\n",
    "    def __init__(self):\n",
    "        self.api_key = os.getenv(\"SEMANTIC_SCHOLAR_API_KEY\")\n",
    "\n",
    "\n",
    "    def query_search(self, query):\n",
    "        url=\"https://api.semanticscholar.org/graph/v1/paper/search/\"\n",
    "        \n",
    "        query_params = {\n",
    "            \"query\": query,\n",
    "            \"fields\": \"title,citationCount,tldr,url,publicationTypes,publicationDate,openAccessPdf,abstract\",\n",
    "            \"year\": \"2020-2025\",\n",
    "            \"limit\": 50,\n",
    "            \"sort\": \"relevance\",\n",
    "            \"minCitationCount\": 10\n",
    "        }\n",
    "        headers = {\"x-api-key\": self.api_key}\n",
    "        response = requests.get(url, params=query_params, headers=headers).json()\n",
    "        \n",
    "        return response\n",
    "\n",
    "    def PaperDetails(self, paper_id, fields=\"title,year,abstract,authors,citationCount,venue,citations,references,tldr\"):\n",
    "        \n",
    "        url = \"https://api.semanticscholar.org/graph/v1/paper/\"\n",
    "        \n",
    "        paper_data_query_params = {\"fields\": fields}\n",
    "        headers = {\"x-api-key\": self.api_key}\n",
    "        response = requests.get(\n",
    "            url = url + paper_id, params=paper_data_query_params, headers=headers\n",
    "        )\n",
    "        \n",
    "        return response.json()\n",
    "    \n",
    "    @staticmethod\n",
    "    def prepare_papers_for_llm(list_of_papers):\n",
    "        unique_papers = {}\n",
    "\n",
    "        for query_string, query_data in list_of_papers.items():\n",
    "            for paper in query_data.get('data', []):\n",
    "                paper_id = paper.get('paperId')\n",
    "                if paper_id and paper_id not in unique_papers:  # Skips if paper_id is None\n",
    "                    paper_str = f\"\"\"Paper ID: {paper_id}\n",
    "                                    Title: {paper.get('title')}\n",
    "                                    Abstract: {paper.get('abstract')}\n",
    "                                \"\"\"\n",
    "                    unique_papers[paper_id] = paper_str\n",
    "                    \n",
    "        paper_list = list(unique_papers.values())\n",
    "                \n",
    "        papers_for_llm = \"\\n\\n---\\n\\n\".join(paper_list)\n",
    "        return papers_for_llm\n",
    "\n",
    " \n",
    "# query = 'Computing Machinery and Intelligence'\n",
    "\n",
    "# search_paper = getReferencePaper()\n",
    "# search_paper_response = search_paper.query_search(query)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c92d15a9",
   "metadata": {},
   "source": [
    "## Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e3b096",
   "metadata": {},
   "source": [
    "### Agent 1: Idea Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cb69f2e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, List, Dict\n",
    "from pydantic import BaseModel, Field, field_validator\n",
    "\n",
    "\n",
    "class IdeaParser(BaseModel):\n",
    "    \"\"\"\n",
    "    Agent 1: Parse Idea from user input into structured format.\n",
    "    1. Extract ideas from the user's input.\n",
    "    \"\"\"\n",
    "    research_question: str = Field(\n",
    "        description=\"The main research question in one conscise sentence\"\n",
    "    )\n",
    "    \n",
    "    problem_domain: str = Field(\n",
    "        description=\"The specific problem domain or area of interest related to the research question. (e.g., natural language processing, computer vision, etc.)\"\n",
    "    )\n",
    "    \n",
    "    methodology_keywords: List[str] = Field(\n",
    "        description=\"A list of technical keywords mentioned in the methods\"\n",
    "    )\n",
    "    \n",
    "    key_concepts: List[str] = Field(\n",
    "        description=\"A list of key concepts or theories relevant to the research question and problem domain.\"\n",
    "    )\n",
    "    \n",
    "    existing_methods: List[str] = Field(\n",
    "        description=\"A list of existing methods or approaches mentioned in the user's input that are relevant to the research question and problem domain.\"\n",
    "    )\n",
    "    \n",
    "    claimed_novelty: List[str] = Field(\n",
    "        description=\"A list of claimed novel aspects or contributions inferred from the user's input.\"\n",
    "    )\n",
    "    \n",
    "    \n",
    "    \n",
    "    @field_validator('key_concepts')\n",
    "    @classmethod\n",
    "    def validate_key_concepts_counts(cls, v):\n",
    "        \"\"\"Ensure that there are a reasonable number of key concepts extracted\"\"\"\n",
    "        if len(v) < 3 or len(v) > 15:\n",
    "            raise ValueError('At least 3 and at most 15 key concepts are required.')\n",
    "        return v\n",
    "\n",
    "    def to_dict(self) -> Dict:\n",
    "        return self.model_dump()\n",
    "    \n",
    "    def to_summary(self) -> str:\n",
    "        return f\"\"\"Research Question: {self.research_question}\\n\n",
    "                    Problem Domain: {self.problem_domain}\\n\n",
    "                    Key Concepts: {', '.join(self.key_concepts)}\\n\n",
    "                    Claimed Novelty: {', '.join(self.claimed_novelty)}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6b5727ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage\n",
    "\n",
    "idea_parser_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"\n",
    "            You are a research analysis assistant that has a deep understanding of extracting structured information from research proposals.\n",
    "\n",
    "            INPUT:\n",
    "            You will receive a research idea description with these fields:\n",
    "            - Problem: The research problem being addressed\n",
    "            - Existing Methods: Current approaches and their limitations\n",
    "            - Motivation: Why this research is needed\n",
    "            - Proposed Method: The new approach being proposed\n",
    "            - Experiment Plan: How the approach will be evaluated\n",
    "\n",
    "            YOUR TASK:\n",
    "            Extract and structure the key information needed for finding similar work.\n",
    "\n",
    "            OUTPUT REQUIREMENTS:\n",
    "            Return ONLY valid JSON with NO additional text, markdown formatting, or code blocks.\n",
    "            Do not include ```json or ``` markers.\n",
    "            Your entire response must be parseable by JSON.parse().\n",
    "\n",
    "            CRITICAL: Be precise and specific in extraction. Extract actual technical terms, not generic descriptions.\n",
    "\n",
    "            OUTPUT SCHEMA:\n",
    "            {{\n",
    "            \"research_question\": \"string - The main research question in one concise sentence\",\n",
    "            \"problem_domain\": \"string - The specific research area/field (e.g., 'natural language processing', 'computer vision')\",\n",
    "            \"methodology_keywords\": [\n",
    "                \"string - Specific technical methods mentioned (e.g., 'reinforcement learning', 'transformer architecture')\"\n",
    "            ],\n",
    "            \"key_concepts\": [\n",
    "                \"string - Core concepts and techniques (e.g., 'prompt optimization', 'context window management')\"\n",
    "            ],\n",
    "            \"existing_methods_mentioned\": [\n",
    "                \"string - Baseline methods or prior work explicitly mentioned\"\n",
    "            ],\n",
    "            \"claimed_novelty_aspects\": [\n",
    "                \"string - What the proposal claims is novel (extract from Motivation and Proposed Method)\"\n",
    "            ]\n",
    "            }}\n",
    "\n",
    "            EXTRACTION RULES:\n",
    "            1. Be specific: Extract \"transformer architecture\" not \"neural network\"\n",
    "            2. Preserve technical terms exactly as written\n",
    "            3. For methodology_keywords: Include only actionable technical terms\n",
    "            4. For key_concepts: Include 5-8 most important concepts\n",
    "            5. For claimed_novelty_aspects: Extract 2-4 specific novel claims\n",
    "            6. If a field has no relevant information, use empty array [] or empty string \"\"\n",
    "\n",
    "            RESEARCH IDEA:\n",
    "            {research_idea}\n",
    "\n",
    "            OUTPUT (valid JSON only):\n",
    "            \"\"\",\n",
    "        ),\n",
    "        # MessagesPlaceholder(variable_name=\"messages\"),  # Placeholder for dynamic messages\n",
    "    ])\n",
    "\n",
    "\n",
    "# Create the writer agent by binding the prompt template and language model\n",
    "idea_parser_agent = idea_parser_prompt |  llm.with_structured_output(IdeaParser)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a0f58820",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_idea_parser(state: MessagesState):\n",
    "    user_message = state[\"messages\"][-1]  # HumanMessage\n",
    "    \n",
    "    # Invoke the agent chain\n",
    "    response = idea_parser_agent.invoke({\n",
    "        \"research_idea\": user_message.content\n",
    "    })\n",
    "    # response is now an IdeaParser object\n",
    "    \n",
    "    return {\"messages\": [AIMessage(content=json.dumps(response.to_dict(), indent=2))]}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8941980b",
   "metadata": {},
   "source": [
    "### Agent 2: Literature Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d9626e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QueryGenerator(BaseModel):\n",
    "    \"\"\"\n",
    "    Agent 2: Generate Search Queries from parsed research idea.\n",
    "    1. Create effective search queries to find related work.\n",
    "    \"\"\"\n",
    "    query_string: str = Field(\n",
    "        description=\"Search query to be passed to the API\"\n",
    "    )\n",
    "    \n",
    "    rationale: str = Field(\n",
    "        description=\"Rationale for why this query will find relevant papers\"\n",
    "    )\n",
    "    \n",
    "    priority_concept: str = Field(\n",
    "    description=\"The most important concept to focus on in the search\"\n",
    "    )\n",
    "    \n",
    "    @field_validator('query_string')\n",
    "    @classmethod\n",
    "    def validate_query_string_length(cls, v):\n",
    "        \"\"\"Ensure that the query is not too long\"\"\"\n",
    "        if len(v.split()) > 8:\n",
    "            raise ValueError('Query string must be less than 8 words.')\n",
    "        return v\n",
    "\n",
    "    def to_dict(self) -> Dict:\n",
    "        return self.model_dump()\n",
    "    \n",
    "class QueryGeneratorOutput(BaseModel):\n",
    "    \"\"\"Multiple search queries\"\"\"\n",
    "    queries: List[QueryGenerator] = Field(\n",
    "        description=\"List of 5 diverse search queries\"\n",
    "    )\n",
    "    \n",
    "    def to_dict(self) -> Dict:\n",
    "        return {\n",
    "            \"queries\": [q.to_dict() for q in self.queries]\n",
    "        }\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a683c990",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "query_generator_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"You are an experienced professor with an established search query strategy skill for academic literature databases.\n",
    "\n",
    "            CONTEXT:\n",
    "            You have a parsed research idea and need to generate optimal search queries for Semantic Scholar API.\n",
    "            Your queries will retrieve papers to assess the novelty of the proposed research.\n",
    "\n",
    "            PARSED RESEARCH IDEA:\n",
    "            {parsed_idea_json}\n",
    "\n",
    "\n",
    "            YOUR TASK:\n",
    "            Generate 5 (five) diverse search queries that will find the most relevant existing work.\n",
    "\n",
    "\n",
    "            QUERY OPTIMIZATION RULES:\n",
    "            1. Keep queries SHORT: 2-6 words maximum for best results\n",
    "            2. Use technical terms, not natural language\n",
    "            3. Combine 2-3 concepts maximum per query\n",
    "            4. NO operators: Don't use \"AND\", \"OR\", \"-\", quotes, or \"site:\"\n",
    "            5. Prioritize precision over recall\n",
    "\n",
    "            OUTPUT REQUIREMENTS:\n",
    "            Return ONLY valid JSON with NO additional text or formatting.\n",
    "            Do not include ```json or ``` markers.\n",
    "\n",
    "            OUTPUT SCHEMA:\n",
    "            {{\n",
    "            \"queries\": [\n",
    "                {{\n",
    "                \"query_string\": \"string - The actual search query (2-6 words)\",\n",
    "                \"rationale\": \"string - Why this query will find relevant papers\",\n",
    "                \"priority_concepts\" : \"string - Top 3-5 concepts that should appear in similar papers\"\n",
    "                }}\n",
    "            ]\n",
    "            }}\n",
    "\n",
    "            EXAMPLES OF GOOD QUERIES:\n",
    "            - \"adaptive prompt dialogue coherence\"\n",
    "            - \"dynamic context management LLM\"\n",
    "            - \"conversational continuity language models\"\n",
    "            - \"iterative prompt optimization\"\n",
    "\n",
    "            EXAMPLES OF BAD QUERIES:\n",
    "            - \"papers about improving language model coherence\" (too natural language)\n",
    "            - \"dynamic AND adaptive OR iterative -static\" (operators not supported)\n",
    "            - \"comprehensive survey of prompt engineering techniques\" (too long/broad)\n",
    "\n",
    "            OUTPUT (valid JSON only):\"\"\",\n",
    "        ),\n",
    "        # MessagesPlaceholder(variable_name=\"messages\"),  # Placeholder for dynamic messages\n",
    "    ])\n",
    "\n",
    "\n",
    "query_generator_agent = query_generator_prompt |  llm.with_structured_output(QueryGeneratorOutput)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d42ce32d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_query_generator(state: MessagesState):\n",
    "    last_message = state[\"messages\"][-1]  # HumanMessage\n",
    "    \n",
    "    # Invoke the agent chain\n",
    "    response = query_generator_agent.invoke({\n",
    "        \"parsed_idea_json\":last_message.content #json.dumps(json.loads(last_message.content))\n",
    "    })\n",
    "    # response is now an IdeaParser object\n",
    "    \n",
    "    return {\"messages\": [AIMessage(content=json.dumps(response.to_dict(), indent=2))]}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "319927ef",
   "metadata": {},
   "source": [
    "#### AGENT 3 : search paper using semantic scholar api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a92d90fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_paper_search(state: MessagesState):\n",
    "    last_message = state[\"messages\"][-1]  # LastMessage\n",
    "    queries_json = json.loads(last_message.content)\n",
    "    all_search_results = {}\n",
    "    \n",
    "    search_paper = getReferencePaper()\n",
    "\n",
    "    \n",
    "    for search_query in queries_json['queries']:\n",
    "        query_string = search_query['query_string']\n",
    "        search_results = search_paper.query_search(query_string)\n",
    "        all_search_results[query_string] = search_results\n",
    "    \n",
    "    return {\"messages\": [AIMessage(content=json.dumps(all_search_results, indent=2))]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f8cfaeb",
   "metadata": {},
   "source": [
    "### Agent 4 : Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "04f52e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PaperAnalyzer(BaseModel):\n",
    "    \"\"\"\n",
    "    Agent 2: Generate Search Queries from parsed research idea.\n",
    "    1. Create effective search queries to find related work.\n",
    "    \"\"\"\n",
    "    paper_id: str = Field(\n",
    "        description=\"semantic scholar paper ID to be analyzed\"\n",
    "    )\n",
    "    \n",
    "    title: str = Field(\n",
    "        description=\"Title of the paper to be analyzed\"\n",
    "    )\n",
    "    \n",
    "    overlap_score: float = Field(\n",
    "        description=\"float 0.0-1.0 - overlap similarity with proposed idea\"\n",
    "    )\n",
    "    \n",
    "    methodology_overlap: float = Field(\n",
    "        description=\"score 0 - 1. Methodology overlap with the proposed idea (inferring from abstract and title)\"\n",
    "    )\n",
    "    \n",
    "    problem_overlap: float = Field(\n",
    "        description=\"score 0 - 1. Problem overlap with the proposed idea (inferring from abstract and title)\"\n",
    "    )\n",
    "    \n",
    "    domain_overlap: float = Field(\n",
    "        description=\"score 0 - 1. Domain overlap with the proposed idea (inferring from abstract and title)\"\n",
    "    )   \n",
    "    key_overlaps: List[str] = Field(\n",
    "        description=\"Specific overlapping aspects\"\n",
    "    )\n",
    "    \n",
    "    key_differences: List[str] = Field(\n",
    "        description=\"How proposed idea differs\"\n",
    "    )\n",
    "\n",
    "    def to_dict(self) -> Dict:\n",
    "        return self.model_dump()\n",
    "    \n",
    "class PaperAnalyzerOutput(BaseModel):\n",
    "    \"\"\"Multiple search queries\"\"\"\n",
    "    queries: List[PaperAnalyzer] = Field(\n",
    "        description=\"List of all analyzed papers\"\n",
    "    )\n",
    "    \n",
    "    def to_dict(self) -> Dict:\n",
    "        return {\n",
    "            \"papers\": [q.to_dict() for q in self.queries]\n",
    "        }\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "04b64098",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "prior_work_analysis_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"You are an experience researcher with years of expertise in academic literature review and analysis.\n",
    "\n",
    "                PROPOSED RESEARCH IDEA:\n",
    "                {research_idea}\n",
    "                \n",
    "                LIST OF RETRIEVED PAPERS:\n",
    "                {paper_list}\n",
    "                \n",
    "                YOUR TASK:\n",
    "                For each paper, assess the degree of overlap with the proposed research idea.\n",
    "                \n",
    "                ANALYSIS CRITERIA:\n",
    "                Methodology overlap: Do they use similar approaches?\n",
    "                Problem overlap: Do they address the same problem?\n",
    "                Domain overlap: Same application area?\n",
    "                Overall score: The average of the three overlap scores.\n",
    "                \n",
    "                OUTPUT REQUIREMENTS:\n",
    "                Return ONLY valid JSON with NO additional text.\n",
    "                Do not include ```json or ``` markers.\n",
    "                \n",
    "                OUTPUT SCHEMA:\n",
    "                {{\n",
    "                \"paper_analyses\": [\n",
    "                   {{\n",
    "                      \"paper_id\": \"string - Semantic Scholar paper ID\",\n",
    "                      \"title\": \"string\",\n",
    "                      \"overlap_score\": \"float 0.0-1.0 - Overall similarity\",\n",
    "                      \"methodology_overlap\": \"float 0.0-1.0\",\n",
    "                      \"problem_overlap\": \"float 0.0-1.0\", \n",
    "                      \"domain_overlap\": \"float 0.0-1.0\",\n",
    "                      \"key_overlaps\": [\n",
    "                        \"string - Specific overlapping aspects\"\n",
    "                      ],\n",
    "                      \"key_differences\": [\n",
    "                        \"string - How proposed idea differs\"\n",
    "                      ]\n",
    "                    }}\n",
    "                  ]\n",
    "                }}\n",
    "                \n",
    "                SCORING GUIDELINES:\n",
    "                overlap_score 0.8-1.0: Nearly identical approach\n",
    "                overlap_score 0.6-0.8: High similarity, incremental difference\n",
    "                overlap_score 0.4-0.6: Moderate similarity, related work\n",
    "                overlap_score 0.2-0.4: Tangentially related\n",
    "                overlap_score 0.0-0.2: Different approach, same domain\n",
    "                \n",
    "                Be precise and evidence-based. Cite specific aspects from paper titles/abstracts.\n",
    "                \n",
    "                OUTPUT (valid JSON only):\n",
    "                \n",
    "            \"\"\",\n",
    "        ),\n",
    "        # MessagesPlaceholder(variable_name=\"messages\"),  # Placeholder for dynamic messages\n",
    "    ])\n",
    "\n",
    "prior_work_analysis_agent = prior_work_analysis_prompt |  llm.with_structured_output(PaperAnalyzerOutput)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "67fb8285",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_prior_work_analysis(state: MessagesState):\n",
    "    last_message = state[\"messages\"][-1]  # HumanMessage\n",
    "    initial_user_input = state[\"messages\"][0]  # Research idea\n",
    "    \n",
    "    \n",
    "    list_of_papers = getReferencePaper.prepare_papers_for_llm(\n",
    "        json.loads(last_message.content)\n",
    "    )\n",
    "    \n",
    "    # Invoke the agent chain\n",
    "    response = prior_work_analysis_agent.invoke({\n",
    "        \"research_idea\":initial_user_input.content, #json.dumps(json.loads(last_message.content)),\n",
    "        \"paper_list\": list_of_papers\n",
    "    })\n",
    "    # response is now an IdeaParser object\n",
    "    \n",
    "    return {\"messages\": [AIMessage(content=json.dumps(response.to_dict(), indent=2))]}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "912274f2",
   "metadata": {},
   "source": [
    "### Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3afc5516",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x11d1dcb90>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "workflow = StateGraph(MessagesState)\n",
    "\n",
    "workflow.add_node(\"idea_parser\", call_idea_parser)\n",
    "workflow.add_node(\"search_query\", call_query_generator)\n",
    "workflow.add_node(\"search_paper\", call_paper_search)\n",
    "workflow.add_node(\"prior_work_analysis\", call_prior_work_analysis)\n",
    "\n",
    "workflow.add_edge(START, \"idea_parser\")\n",
    "workflow.add_edge(\"idea_parser\", \"search_query\")\n",
    "workflow.add_edge(\"search_query\", \"search_paper\")\n",
    "workflow.add_edge(\"search_paper\", \"prior_work_analysis\")\n",
    "workflow.add_edge(\"prior_work_analysis\", END)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "969dfdf5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "graph = workflow.compile()\n",
    "# graph.get_graph().draw_mermaid_png(output_file_path='story.png')\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "823bc0dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m[values]\u001b[0m {'messages': [HumanMessage(content=\"Dynamic Prompt Adaptation:\\n                Problem: Large Language Models (LLMs) often struggle with maintaining coherence over extended interactions or creative tasks, leading to thematic inconsistencies and reduced reader engagement.\\n                Existing Methods: Current methods often use fixed prompts or few-shot examples, which may not adapt to the evolving context of a conversation or creative narrative. Techniques such as Chain-of-Thought prompting are utilized, but they do not inherently address continuity and adaptability across interactions.\\n                Motivation: Human creative writing often involves iterative dialogue and adaptation to the flow of discussion. The style and context of interactions can shift dynamically based on prior exchanges. Dynamic adaptation mirrors how authors and conversationalists adjust based on audience feedback and shifting themes.\\n                Proposed Method: We propose Dynamic Prompt Adaptation, involving three phases: (1) Contextual Analysis - Analyze previous outputs and user prompts to extract key themes and tonal shifts, applying a prompting structure like 'Reflect on the previous topic of [theme] and build on it.' (2) Adaptive Prompt Generation - Using insights from the analysis, generate updated prompts that introduce new elements or clarify past responses, e.g., 'Continuing from your last thought on [theme], can you expand on how this might be represented in [new context]?' (3) Iterative Context Update - As the dialogue progresses, generate a synthesis of all prior interactions to maintain thematic coherence, prompting with 'Summarize the key points discussed so far to keep track of our narrative.'\\n                Experiment Plan: Test against static prompting strategies by evaluating engagement scores, coherence assessments, and user satisfaction in storytelling scenarios using standard text generation metrics such as BLEU and ROUGE. Incorporate user feedback on naturalness and adaptability during the interaction. Datasets could include the 'Story Cloze Test' dataset and user-generated dialogue interactions scraped from platforms like Reddit to assess conversational engagement.\", additional_kwargs={}, response_metadata={}, id='63ebf44d-2223-4537-afd7-c4eb3eec973e')]}\n",
      "\u001b[1m[updates]\u001b[0m {'idea_parser': {'messages': [AIMessage(content='{\\n  \"research_question\": \"How can dynamic prompt adaptation maintain thematic coherence and reader engagement across extended LLM conversations and creative narratives?\",\\n  \"problem_domain\": \"natural language processing and dialogue systems\",\\n  \"methodology_keywords\": [\\n    \"Dynamic Prompt Adaptation\",\\n    \"Contextual Analysis\",\\n    \"Adaptive Prompt Generation\",\\n    \"Iterative Context Update\",\\n    \"thematic coherence\",\\n    \"narrative continuity\",\\n    \"long-context dialogue\"\\n  ],\\n  \"key_concepts\": [\\n    \"Dynamic Prompt Adaptation\",\\n    \"Contextual Analysis of previous outputs and prompts\",\\n    \"Adaptive Prompt Generation\",\\n    \"Iterative Context Update\",\\n    \"thematic coherence\",\\n    \"audience feedback and shifting themes\",\\n    \"prompt templating for theme reflection\",\\n    \"synthesis of prior interactions\"\\n  ],\\n  \"existing_methods\": [\\n    \"fixed prompts\",\\n    \"few-shot examples\",\\n    \"Chain-of-Thought prompting\"\\n  ],\\n  \"claimed_novelty\": [\\n    \"Dynamic Prompt Adaptation to maintain coherence across extended interactions and creative tasks\",\\n    \"Three-phase framework: Contextual Analysis; Adaptive Prompt Generation; Iterative Context Update\",\\n    \"Iterative synthesis of all prior interactions to maintain thematic coherence\",\\n    \"Evaluation against static prompting strategies using BLEU and ROUGE on Story Cloze Test and Reddit dialogues\"\\n  ]\\n}', additional_kwargs={}, response_metadata={}, id='35ea71c3-1efb-4474-a8bb-b51930753e82')]}}\n",
      "\u001b[1m[values]\u001b[0m {'messages': [HumanMessage(content=\"Dynamic Prompt Adaptation:\\n                Problem: Large Language Models (LLMs) often struggle with maintaining coherence over extended interactions or creative tasks, leading to thematic inconsistencies and reduced reader engagement.\\n                Existing Methods: Current methods often use fixed prompts or few-shot examples, which may not adapt to the evolving context of a conversation or creative narrative. Techniques such as Chain-of-Thought prompting are utilized, but they do not inherently address continuity and adaptability across interactions.\\n                Motivation: Human creative writing often involves iterative dialogue and adaptation to the flow of discussion. The style and context of interactions can shift dynamically based on prior exchanges. Dynamic adaptation mirrors how authors and conversationalists adjust based on audience feedback and shifting themes.\\n                Proposed Method: We propose Dynamic Prompt Adaptation, involving three phases: (1) Contextual Analysis - Analyze previous outputs and user prompts to extract key themes and tonal shifts, applying a prompting structure like 'Reflect on the previous topic of [theme] and build on it.' (2) Adaptive Prompt Generation - Using insights from the analysis, generate updated prompts that introduce new elements or clarify past responses, e.g., 'Continuing from your last thought on [theme], can you expand on how this might be represented in [new context]?' (3) Iterative Context Update - As the dialogue progresses, generate a synthesis of all prior interactions to maintain thematic coherence, prompting with 'Summarize the key points discussed so far to keep track of our narrative.'\\n                Experiment Plan: Test against static prompting strategies by evaluating engagement scores, coherence assessments, and user satisfaction in storytelling scenarios using standard text generation metrics such as BLEU and ROUGE. Incorporate user feedback on naturalness and adaptability during the interaction. Datasets could include the 'Story Cloze Test' dataset and user-generated dialogue interactions scraped from platforms like Reddit to assess conversational engagement.\", additional_kwargs={}, response_metadata={}, id='63ebf44d-2223-4537-afd7-c4eb3eec973e'), AIMessage(content='{\\n  \"research_question\": \"How can dynamic prompt adaptation maintain thematic coherence and reader engagement across extended LLM conversations and creative narratives?\",\\n  \"problem_domain\": \"natural language processing and dialogue systems\",\\n  \"methodology_keywords\": [\\n    \"Dynamic Prompt Adaptation\",\\n    \"Contextual Analysis\",\\n    \"Adaptive Prompt Generation\",\\n    \"Iterative Context Update\",\\n    \"thematic coherence\",\\n    \"narrative continuity\",\\n    \"long-context dialogue\"\\n  ],\\n  \"key_concepts\": [\\n    \"Dynamic Prompt Adaptation\",\\n    \"Contextual Analysis of previous outputs and prompts\",\\n    \"Adaptive Prompt Generation\",\\n    \"Iterative Context Update\",\\n    \"thematic coherence\",\\n    \"audience feedback and shifting themes\",\\n    \"prompt templating for theme reflection\",\\n    \"synthesis of prior interactions\"\\n  ],\\n  \"existing_methods\": [\\n    \"fixed prompts\",\\n    \"few-shot examples\",\\n    \"Chain-of-Thought prompting\"\\n  ],\\n  \"claimed_novelty\": [\\n    \"Dynamic Prompt Adaptation to maintain coherence across extended interactions and creative tasks\",\\n    \"Three-phase framework: Contextual Analysis; Adaptive Prompt Generation; Iterative Context Update\",\\n    \"Iterative synthesis of all prior interactions to maintain thematic coherence\",\\n    \"Evaluation against static prompting strategies using BLEU and ROUGE on Story Cloze Test and Reddit dialogues\"\\n  ]\\n}', additional_kwargs={}, response_metadata={}, id='35ea71c3-1efb-4474-a8bb-b51930753e82')]}\n",
      "\u001b[1m[updates]\u001b[0m {'search_query': {'messages': [AIMessage(content='{\\n  \"queries\": [\\n    {\\n      \"query_string\": \"dynamic prompt adaptation coherence\",\\n      \"rationale\": \"Targets work on dynamic prompt adaptation to sustain thematic coherence across extended LLM conversations and narratives\",\\n      \"priority_concept\": \"Dynamic Prompt Adaptation\"\\n    },\\n    {\\n      \"query_string\": \"long context dialogue\",\\n      \"rationale\": \"Searches for literature on maintaining coherence in long-context dialogues and continuous narratives\",\\n      \"priority_concept\": \"Long-context dialogue\"\\n    },\\n    {\\n      \"query_string\": \"adaptive prompt generation narrative\",\\n      \"rationale\": \"Looks for studies on generating prompts adaptively to guide narrative tasks and creative outputs\",\\n      \"priority_concept\": \"Adaptive Prompt Generation\"\\n    },\\n    {\\n      \"query_string\": \"iterative context update\",\\n      \"rationale\": \"Finds work on iterative updates to context across turns for consistency and coherence\",\\n      \"priority_concept\": \"Iterative Context Update\"\\n    },\\n    {\\n      \"query_string\": \"thematic coherence prompting\",\\n      \"rationale\": \"Identifies prompting strategies that preserve or reflect themes within extended interactions\",\\n      \"priority_concept\": \"Thematic coherence\"\\n    }\\n  ]\\n}', additional_kwargs={}, response_metadata={}, id='44970bb0-ab27-4ee3-8006-a6ceeaa39143')]}}\n",
      "\u001b[1m[values]\u001b[0m {'messages': [HumanMessage(content=\"Dynamic Prompt Adaptation:\\n                Problem: Large Language Models (LLMs) often struggle with maintaining coherence over extended interactions or creative tasks, leading to thematic inconsistencies and reduced reader engagement.\\n                Existing Methods: Current methods often use fixed prompts or few-shot examples, which may not adapt to the evolving context of a conversation or creative narrative. Techniques such as Chain-of-Thought prompting are utilized, but they do not inherently address continuity and adaptability across interactions.\\n                Motivation: Human creative writing often involves iterative dialogue and adaptation to the flow of discussion. The style and context of interactions can shift dynamically based on prior exchanges. Dynamic adaptation mirrors how authors and conversationalists adjust based on audience feedback and shifting themes.\\n                Proposed Method: We propose Dynamic Prompt Adaptation, involving three phases: (1) Contextual Analysis - Analyze previous outputs and user prompts to extract key themes and tonal shifts, applying a prompting structure like 'Reflect on the previous topic of [theme] and build on it.' (2) Adaptive Prompt Generation - Using insights from the analysis, generate updated prompts that introduce new elements or clarify past responses, e.g., 'Continuing from your last thought on [theme], can you expand on how this might be represented in [new context]?' (3) Iterative Context Update - As the dialogue progresses, generate a synthesis of all prior interactions to maintain thematic coherence, prompting with 'Summarize the key points discussed so far to keep track of our narrative.'\\n                Experiment Plan: Test against static prompting strategies by evaluating engagement scores, coherence assessments, and user satisfaction in storytelling scenarios using standard text generation metrics such as BLEU and ROUGE. Incorporate user feedback on naturalness and adaptability during the interaction. Datasets could include the 'Story Cloze Test' dataset and user-generated dialogue interactions scraped from platforms like Reddit to assess conversational engagement.\", additional_kwargs={}, response_metadata={}, id='63ebf44d-2223-4537-afd7-c4eb3eec973e'), AIMessage(content='{\\n  \"research_question\": \"How can dynamic prompt adaptation maintain thematic coherence and reader engagement across extended LLM conversations and creative narratives?\",\\n  \"problem_domain\": \"natural language processing and dialogue systems\",\\n  \"methodology_keywords\": [\\n    \"Dynamic Prompt Adaptation\",\\n    \"Contextual Analysis\",\\n    \"Adaptive Prompt Generation\",\\n    \"Iterative Context Update\",\\n    \"thematic coherence\",\\n    \"narrative continuity\",\\n    \"long-context dialogue\"\\n  ],\\n  \"key_concepts\": [\\n    \"Dynamic Prompt Adaptation\",\\n    \"Contextual Analysis of previous outputs and prompts\",\\n    \"Adaptive Prompt Generation\",\\n    \"Iterative Context Update\",\\n    \"thematic coherence\",\\n    \"audience feedback and shifting themes\",\\n    \"prompt templating for theme reflection\",\\n    \"synthesis of prior interactions\"\\n  ],\\n  \"existing_methods\": [\\n    \"fixed prompts\",\\n    \"few-shot examples\",\\n    \"Chain-of-Thought prompting\"\\n  ],\\n  \"claimed_novelty\": [\\n    \"Dynamic Prompt Adaptation to maintain coherence across extended interactions and creative tasks\",\\n    \"Three-phase framework: Contextual Analysis; Adaptive Prompt Generation; Iterative Context Update\",\\n    \"Iterative synthesis of all prior interactions to maintain thematic coherence\",\\n    \"Evaluation against static prompting strategies using BLEU and ROUGE on Story Cloze Test and Reddit dialogues\"\\n  ]\\n}', additional_kwargs={}, response_metadata={}, id='35ea71c3-1efb-4474-a8bb-b51930753e82'), AIMessage(content='{\\n  \"queries\": [\\n    {\\n      \"query_string\": \"dynamic prompt adaptation coherence\",\\n      \"rationale\": \"Targets work on dynamic prompt adaptation to sustain thematic coherence across extended LLM conversations and narratives\",\\n      \"priority_concept\": \"Dynamic Prompt Adaptation\"\\n    },\\n    {\\n      \"query_string\": \"long context dialogue\",\\n      \"rationale\": \"Searches for literature on maintaining coherence in long-context dialogues and continuous narratives\",\\n      \"priority_concept\": \"Long-context dialogue\"\\n    },\\n    {\\n      \"query_string\": \"adaptive prompt generation narrative\",\\n      \"rationale\": \"Looks for studies on generating prompts adaptively to guide narrative tasks and creative outputs\",\\n      \"priority_concept\": \"Adaptive Prompt Generation\"\\n    },\\n    {\\n      \"query_string\": \"iterative context update\",\\n      \"rationale\": \"Finds work on iterative updates to context across turns for consistency and coherence\",\\n      \"priority_concept\": \"Iterative Context Update\"\\n    },\\n    {\\n      \"query_string\": \"thematic coherence prompting\",\\n      \"rationale\": \"Identifies prompting strategies that preserve or reflect themes within extended interactions\",\\n      \"priority_concept\": \"Thematic coherence\"\\n    }\\n  ]\\n}', additional_kwargs={}, response_metadata={}, id='44970bb0-ab27-4ee3-8006-a6ceeaa39143')]}\n",
      "\u001b[1m[updates]\u001b[0m {'search_paper': {'messages': [AIMessage(content='{\\n  \"dynamic prompt adaptation coherence\": {\\n    \"total\": 99,\\n    \"offset\": 0,\\n    \"next\": 50,\\n    \"data\": [\\n      {\\n        \"paperId\": \"246482d9758e93d0b349e2926996d887417174d8\",\\n        \"url\": \"https://www.semanticscholar.org/paper/246482d9758e93d0b349e2926996d887417174d8\",\\n        \"title\": \"DPCore: Dynamic Prompt Coreset for Continual Test-Time Adaptation\",\\n        \"citationCount\": 13,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2406.10737, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"DPCore is proposed, a method designed for robust performance across diverse domain change patterns while ensuring computational efficiency, and it is demonstrated that DPCore consistently outperforms various CTTA methods.\"\\n        },\\n        \"publicationTypes\": null,\\n        \"publicationDate\": \"2024-06-15\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2221081346\",\\n            \"name\": \"Yunbei Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2090707280\",\\n            \"name\": \"Akshay Mehra\"\\n          },\\n          {\\n            \"authorId\": \"2261749247\",\\n            \"name\": \"Shuaicheng Niu\"\\n          },\\n          {\\n            \"authorId\": \"4832622\",\\n            \"name\": \"Jihun Hamm\"\\n          }\\n        ],\\n        \"abstract\": \"Continual Test-Time Adaptation (CTTA) seeks to adapt source pre-trained models to continually changing, unseen target domains. While existing CTTA methods assume structured domain changes with uniform durations, real-world environments often exhibit dynamic patterns where domains recur with varying frequencies and durations. Current approaches, which adapt the same parameters across different domains, struggle in such dynamic conditions-they face convergence issues with brief domain exposures, risk forgetting previously learned knowledge, or misapplying it to irrelevant domains. To remedy this, we propose DPCore, a method designed for robust performance across diverse domain change patterns while ensuring computational efficiency. DPCore integrates three key components: Visual Prompt Adaptation for efficient domain alignment, a Prompt Coreset for knowledge preservation, and a Dynamic Update mechanism that intelligently adjusts existing prompts for similar domains while creating new ones for substantially different domains. Extensive experiments on four benchmarks demonstrate that DPCore consistently outperforms various CTTA methods, achieving state-of-the-art performance in both structured and dynamic settings while reducing trainable parameters by 99% and computation time by 64% compared to previous approaches.\"\\n      },\\n      {\\n        \"paperId\": \"6b533de65b3d2cee190415f5a2a2e6afe2ac7c78\",\\n        \"url\": \"https://www.semanticscholar.org/paper/6b533de65b3d2cee190415f5a2a2e6afe2ac7c78\",\\n        \"title\": \"Dynamic Adapter Meets Prompt Tuning: Parameter-Efficient Transfer Learning for Point Cloud Analysis\",\\n        \"citationCount\": 41,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://arxiv.org/pdf/2403.01439\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2403.01439, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper freezes the parameters of the default pretrained models and proposes the Dynamic Adapter, which generates a dynamic scale for each token, considering the token significance to the downstream task, and seamlessly integrates Dynamic Adapter with Prompt Tuning (DAPT) by constructing Internal Prompts.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-03-03\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2238201928\",\\n            \"name\": \"Xin Zhou\"\\n          },\\n          {\\n            \"authorId\": \"94882716\",\\n            \"name\": \"Dingkang Liang\"\\n          },\\n          {\\n            \"authorId\": \"2284804294\",\\n            \"name\": \"Wei Xu\"\\n          },\\n          {\\n            \"authorId\": \"2284636478\",\\n            \"name\": \"Xingkui Zhu\"\\n          },\\n          {\\n            \"authorId\": \"2290029091\",\\n            \"name\": \"Yihan Xu\"\\n          },\\n          {\\n            \"authorId\": \"2287848763\",\\n            \"name\": \"Zhikang Zou\"\\n          },\\n          {\\n            \"authorId\": \"2238115894\",\\n            \"name\": \"Xiang Bai\"\\n          }\\n        ],\\n        \"abstract\": \"Point cloud analysis has achieved outstanding performance by transferring point cloud pretrained models. However, existing methods for model adaptation usually update all model parameters, i.e., full fine-tuning paradigm, which is inefficient as it relies on high computational costs (e.g., training GPU memory) and massive storage space. In this paper, we aim to study parameter-efficient transfer learning for point cloud analysis with an ideal tradeoff between task performance and parameter efficiency. To achieve this goal, we freeze the parameters of the default pretrained models and then propose the Dynamic Adapter, which generates a dynamic scale for each token, considering the token significance to the downstream task. We further seamlessly integrate Dynamic Adapter with Prompt Tuning (DAPT) by constructing Internal Prompts, capturing the instance-specific features for interaction. Extensive experiments conducted on five challenging datasets demonstrate that the proposed DAPT achieves superior performance compared to the full fine-tuning counterparts while significantly reducing the trainable parameters and training GPU memory by 95% and 35%, respectively. Code is available at https://github.com/LMD0311/DAPT.\"\\n      },\\n      {\\n        \"paperId\": \"eacb61136023a2f30c5a0313f222d50e5f63ac9b\",\\n        \"url\": \"https://www.semanticscholar.org/paper/eacb61136023a2f30c5a0313f222d50e5f63ac9b\",\\n        \"title\": \"Domain Prompt Tuning via Meta Relabeling for Unsupervised Adversarial Adaptation\",\\n        \"citationCount\": 14,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {\\'tldr\\'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/TMM.2023.3272742?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/TMM.2023.3272742, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": null,\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": null,\\n        \"authors\": [\\n          {\\n            \"authorId\": \"1485402830\",\\n            \"name\": \"Xin Jin\"\\n          },\\n          {\\n            \"authorId\": \"40093162\",\\n            \"name\": \"Cuiling Lan\"\\n          },\\n          {\\n            \"authorId\": \"1634494276\",\\n            \"name\": \"Wenjun Zeng\"\\n          },\\n          {\\n            \"authorId\": \"31482866\",\\n            \"name\": \"Zhibo Chen\"\\n          }\\n        ],\\n        \"abstract\": \"Unsupervised adversarial domain adaptation (ADA) aims to learn domain-invariant features by confusing a domain discriminator. As training goes on, the feature distributions of source and target samples are increasingly aligned/indistinguishable. The discrimination capability of the domain discriminator w.r.t. those aligned samples deteriorates due to the domain label of each sample is still fixed all through the learning process, which thus cannot effectively further drive the feature learning. A recently proposed method named Re-enforceable Adversarial Domain Adaptation (RADA) [1] tend to re-energize the domain discriminator during the training by using dynamic domain labels. Specifically, RADA sets up a heuristic criterion and uses it to relabel the well aligned target domain samples as source domain samples on the fly. In our study, we identify a critical problem of RADA: it is a kind of heuristic domain data re-partition solution without explicitly serving the adaptation task itself, suggesting that the criteria of RADA on which sample should be relabeled is hard to decide. To address the problem, we revisit domain relabeling process from a perspective of prompt tuning, and introduce a meta-optimized learnable prompts into RADA to replace some hand-craft designs in dynamic relabeling process, which scheme is named as RADA-prompt. Particularly, we employ a module of meta-prompter, which learns to adaptively relabel the samples based on the objective of serving UDA task. To train the meta-prompter, we leverage a domain alignment measurement and a classification measurement as the meta optimization objective. Extensive experiments on multiple unsupervised domain adaptation benchmarks demonstrate the effectiveness and superiority of RADA-prompt, this scheme also achieves state-of-the-art performance.\"\\n      },\\n      {\\n        \"paperId\": \"165503c48e553a5559190ce74cda823f4e166b54\",\\n        \"url\": \"https://www.semanticscholar.org/paper/165503c48e553a5559190ce74cda823f4e166b54\",\\n        \"title\": \"Decorate the Newcomers: Visual Domain Prompt for Continual Test Time Adaptation\",\\n        \"citationCount\": 128,\\n        \"openAccessPdf\": {\\n          \"url\": \"http://arxiv.org/pdf/2212.04145\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2212.04145, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper proposes to learn an image-layer visual domain prompt for target domains while having the source model parameters frozen, and designs a homeostasis-based adaptation strategy to suppress domain-sensitive parameters in domain-invariant prompts to learn domain-shared knowledge more effectively.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2022-12-08\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2142286278\",\\n            \"name\": \"Yulu Gan\"\\n          },\\n          {\\n            \"authorId\": \"1387903470\",\\n            \"name\": \"Xianzheng Ma\"\\n          },\\n          {\\n            \"authorId\": \"9920529\",\\n            \"name\": \"Yihang Lou\"\\n          },\\n          {\\n            \"authorId\": \"145079192\",\\n            \"name\": \"Yan Bai\"\\n          },\\n          {\\n            \"authorId\": \"2115713503\",\\n            \"name\": \"Renrui Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2194622137\",\\n            \"name\": \"Nian Shi\"\\n          },\\n          {\\n            \"authorId\": \"2194260623\",\\n            \"name\": \"Lin Luo\"\\n          }\\n        ],\\n        \"abstract\": \"Continual Test-Time Adaptation (CTTA) aims to adapt the source model to continually changing unlabeled target domains without access to the source data. Existing methods mainly focus on model-based adaptation in a self-training manner, such as predicting pseudo labels for new domain datasets. Since pseudo labels are noisy and unreliable, these methods suffer from catastrophic forgetting and error accumulation when dealing with dynamic data distributions. Motivated by the prompt learning in NLP, in this paper, we propose to learn an image-layer visual domain prompt for target domains while having the source model parameters frozen. During testing, the changing target datasets can be adapted to the source model by reformulating the input data with the learned visual prompts. Specifically, we devise two types of prompts, i.e., domains-specific prompts and domains-agnostic prompts, to extract current domain knowledge and maintain the domain-shared knowledge in the continual adaptation. Furthermore, we design a homeostasis-based adaptation strategy to suppress domain-sensitive parameters in domain-invariant prompts to learn domain-shared knowledge more effectively. This transition from the model-dependent paradigm to the model-free one enables us to bypass the catastrophic forgetting and error accumulation problems. Experiments show that our proposed method achieves significant performance gains over state-of-the-art methods on four widely-used benchmarks, including CIFAR-10C, CIFAR-100C, ImageNet-C, and VLCS datasets.\"\\n      },\\n      {\\n        \"paperId\": \"5db3cfc974c42bfa2d9518a8910762790b516037\",\\n        \"url\": \"https://www.semanticscholar.org/paper/5db3cfc974c42bfa2d9518a8910762790b516037\",\\n        \"title\": \"Robust Test-Time Adaptation for Zero-Shot Prompt Tuning\",\\n        \"citationCount\": 12,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://ojs.aaai.org/index.php/AAAI/article/download/29611/31034\",\\n          \"status\": \"GOLD\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1609/aaai.v38i15.29611?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1609/aaai.v38i15.29611, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This study proposes robust test-time Adaptation for zeroshot Prompt tuning (ADAPROMPT), which alleviates model bias, adapts to data bias and mostly outperforms the state-of-the-art methods at a small time cost and demonstrates that ADAPROMPT hardly encounters any performance degradation on these datasets.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-03-24\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2229108213\",\\n            \"name\": \"Ding-Chu Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2149135777\",\\n            \"name\": \"Zhi Zhou\"\\n          },\\n          {\\n            \"authorId\": \"2276808245\",\\n            \"name\": \"Yufeng Li\"\\n          }\\n        ],\\n        \"abstract\": \"CLIP has demonstrated remarkable generalization across diverse downstream tasks. By aligning images and texts in a shared feature space, they enable zero-shot classification via hand-crafted prompts. However, recent studies have shown that hand-crafted prompts may be unsuitable in practical applications. Specifically, choosing an appropriate prompt for a given task requires accurate data and knowledge, which may not be obtainable in practical situations. An inappropriate prompt can result in poor performance. Moreover, if there is no training data, tuning prompts arbitrarily through unlabeled test data may lead to serious performance degradation when giving hand-crafted prompts. Our study reveals that the aforementioned problems are mainly due to the biases in testing data (Data Bias) and pre-trained CLIP model (Model Bias). The Data Bias makes it challenging to choose an appropriate prompt, while Model Bias renders some predictions inaccurate and biased, which leads to error accumulation. To address these biases, we propose robust test-time Adaptation for zeroshot Prompt tuning (ADAPROMPT). Specifically, we ensemble multiple prompts to avoid the worst-case results and dynamically tune prompts to adapt to Data Bias during testing. Furthermore, we adopt a confidence-aware buffer to store balanced and confident unlabeled test data to tune prompts in order to overcome Model Bias. Our extensive experiments on several benchmarks demonstrate that ADAPROMPT alleviates model bias, adapts to data bias and mostly outperforms the state-of-the-art methods at a small time cost. Moreover, our experimental results reveal that ADAPROMPT hardly encounters any performance degradation on these datasets.\"\\n      },\\n      {\\n        \"paperId\": \"4362edfd3907204cf1b7ec8e3c16c56db5cd14cf\",\\n        \"url\": \"https://www.semanticscholar.org/paper/4362edfd3907204cf1b7ec8e3c16c56db5cd14cf\",\\n        \"title\": \"Large Language Models-guided Dynamic Adaptation for Temporal Knowledge Graph Reasoning\",\\n        \"citationCount\": 30,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2405.14170, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A Large Language Models-guided Dynamic Adaptation (LLM-DA) method for reasoning on TKGs that harnesses the capabilities of LLMs to analyze historical data and extract temporal logical rules that unveil temporal patterns and facilitate interpretable reasoning.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-05-23\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2163833353\",\\n            \"name\": \"Jiapu Wang\"\\n          },\\n          {\\n            \"authorId\": \"2220259324\",\\n            \"name\": \"Kai Sun\"\\n          },\\n          {\\n            \"authorId\": \"2238130759\",\\n            \"name\": \"Linhao Luo\"\\n          },\\n          {\\n            \"authorId\": \"2302892265\",\\n            \"name\": \"Wei Wei\"\\n          },\\n          {\\n            \"authorId\": \"2140879677\",\\n            \"name\": \"Yongli Hu\"\\n          },\\n          {\\n            \"authorId\": \"2243282363\",\\n            \"name\": \"Alan Wee-Chung Liew\"\\n          },\\n          {\\n            \"authorId\": \"2294378361\",\\n            \"name\": \"Shirui Pan\"\\n          },\\n          {\\n            \"authorId\": \"2239088112\",\\n            \"name\": \"Baocai Yin\"\\n          }\\n        ],\\n        \"abstract\": \"Temporal Knowledge Graph Reasoning (TKGR) is the process of utilizing temporal information to capture complex relations within a Temporal Knowledge Graph (TKG) to infer new knowledge. Conventional methods in TKGR typically depend on deep learning algorithms or temporal logical rules. However, deep learning-based TKGRs often lack interpretability, whereas rule-based TKGRs struggle to effectively learn temporal rules that capture temporal patterns. Recently, Large Language Models (LLMs) have demonstrated extensive knowledge and remarkable proficiency in temporal reasoning. Consequently, the employment of LLMs for Temporal Knowledge Graph Reasoning (TKGR) has sparked increasing interest among researchers. Nonetheless, LLMs are known to function as black boxes, making it challenging to comprehend their reasoning process. Additionally, due to the resource-intensive nature of fine-tuning, promptly updating LLMs to integrate evolving knowledge within TKGs for reasoning is impractical. To address these challenges, in this paper, we propose a Large Language Models-guided Dynamic Adaptation (LLM-DA) method for reasoning on TKGs. Specifically, LLM-DA harnesses the capabilities of LLMs to analyze historical data and extract temporal logical rules. These rules unveil temporal patterns and facilitate interpretable reasoning. To account for the evolving nature of TKGs, a dynamic adaptation strategy is proposed to update the LLM-generated rules with the latest events. This ensures that the extracted rules always incorporate the most recent knowledge and better generalize to the predictions on future events. Experimental results show that without the need of fine-tuning, LLM-DA significantly improves the accuracy of reasoning over several common datasets, providing a robust framework for TKGR tasks.\"\\n      },\\n      {\\n        \"paperId\": \"759b5f58e58a76f79a7d845acd3169dc899d0ac2\",\\n        \"url\": \"https://www.semanticscholar.org/paper/759b5f58e58a76f79a7d845acd3169dc899d0ac2\",\\n        \"title\": \"Domain Adaptation via Prompt Learning\",\\n        \"citationCount\": 210,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://arxiv.org/pdf/2202.06687\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2202.06687, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This article introduces a novel prompt learning paradigm for UDA, named domain adaptation via prompt learning (DAPrompt), which outperforms previous methods on several cross-domain benchmarks but also is very efficient to train and easy to implement.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2022-02-14\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2130368185\",\\n            \"name\": \"Chunjiang Ge\"\\n          },\\n          {\\n            \"authorId\": null,\\n            \"name\": \"Rui Huang\"\\n          },\\n          {\\n            \"authorId\": \"2112817811\",\\n            \"name\": \"Mixue Xie\"\\n          },\\n          {\\n            \"authorId\": \"51451501\",\\n            \"name\": \"Zihang Lai\"\\n          },\\n          {\\n            \"authorId\": \"1760750\",\\n            \"name\": \"Shiji Song\"\\n          },\\n          {\\n            \"authorId\": null,\\n            \"name\": \"Shuang Li\"\\n          },\\n          {\\n            \"authorId\": \"2115218570\",\\n            \"name\": \"Gao Huang\"\\n          }\\n        ],\\n        \"abstract\": \"Unsupervised domain adaptation (UDA) aims to adapt models learned from a well-annotated source domain to a target domain, where only unlabeled samples are given. Current UDA approaches learn domain-invariant features by aligning source and target feature spaces through statistical discrepancy minimization or adversarial training. However, these constraints could lead to the distortion of semantic feature structures and loss of class discriminability. In this article, we introduce a novel prompt learning paradigm for UDA, named domain adaptation via prompt learning (DAPrompt). In contrast to prior works, our approach learns the underlying label distribution for target domain rather than aligning domains. The main idea is to embed domain information into prompts, a form of representation generated from natural language, which is then used to perform classification. This domain information is shared only by images from the same domain, thereby dynamically adapting the classifier according to each domain. By adopting this paradigm, we show that our model not only outperforms previous methods on several cross-domain benchmarks but also is very efficient to train and easy to implement.\"\\n      },\\n      {\\n        \"paperId\": \"b32861f83f5426ddb0b797cfe9a5ea80b0adc3cf\",\\n        \"url\": \"https://www.semanticscholar.org/paper/b32861f83f5426ddb0b797cfe9a5ea80b0adc3cf\",\\n        \"title\": \"Dynamic 3D imaging of cerebral blood flow in awake mice using self-supervised-learning-enhanced optical coherence Doppler tomography\",\\n        \"citationCount\": 13,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://www.nature.com/articles/s42003-023-04656-x.pdf\",\\n          \"status\": \"GOLD\",\\n          \"license\": \"CCBY\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC10030663, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"The 3D imaging platform presented provides a powerful tool to study dynamic changes in vessel diameters and morphology alongside CBFv networks in the brain of awake animals that can advance the understanding of the effects of drugs and disease conditions.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2023-03-21\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"1826992\",\\n            \"name\": \"Yingtian Pan\"\\n          },\\n          {\\n            \"authorId\": \"2449537\",\\n            \"name\": \"Kicheon Park\"\\n          },\\n          {\\n            \"authorId\": \"48115953\",\\n            \"name\": \"Jiaxiang Ren\"\\n          },\\n          {\\n            \"authorId\": \"2184066\",\\n            \"name\": \"N. Volkow\"\\n          },\\n          {\\n            \"authorId\": \"81281627\",\\n            \"name\": \"H. Ling\"\\n          },\\n          {\\n            \"authorId\": \"1898658\",\\n            \"name\": \"A. Koretsky\"\\n          },\\n          {\\n            \"authorId\": \"2024857\",\\n            \"name\": \"C. Du\"\\n          }\\n        ],\\n        \"abstract\": \"Cerebral blood flow (CBF) is widely used to assess brain function. However, most preclinical CBF studies have been performed under anesthesia, which confounds findings. High spatiotemporal-resolution CBF imaging of awake animals is challenging due to motion artifacts and background noise, particularly for Doppler-based flow imaging. Here, we report ultrahigh-resolution optical coherence Doppler tomography (\\\\u00b5ODT) for 3D imaging of CBF velocity (CBFv) dynamics in awake mice by developing self-supervised deep-learning for effective image denoising and motion-artifact removal. We compare cortical CBFv in awake vs. anesthetized mice and their dynamic responses in arteriolar, venular and capillary networks to acute cocaine (1\\\\u2009mg/kg, i.v .), a highly addictive drug associated with neurovascular toxicity. Compared with awake, isoflurane (2-2.5%) induces vasodilation and increases CBFv within 2-4\\\\u2009min, whereas dexmedetomidine (0.025\\\\u2009mg/kg, i.p .) does not change vessel diameters nor flow. Acute cocaine decreases CBFv to the same extent in dexmedetomidine and awake states, whereas decreases are larger under isoflurane, suggesting that isoflurane-induced vasodilation might have facilitated detection of cocaine-induced vasoconstriction. Awake mice after chronic cocaine show severe vasoconstriction, CBFv decreases and vascular adaptations with extended diving arteriolar/venular vessels that prioritize blood supply to deeper cortical capillaries. The 3D imaging platform we present provides a powerful tool to study dynamic changes in vessel diameters and morphology alongside CBFv networks in the brain of awake animals that can advance our understanding of the effects of drugs and disease conditions (ischemia, tumors, wound healing). An imaging platform with self-supervised deep learning allows for the imaging of cerebral blood flows under the effect of cocaine in awake mice using 3D ultrahigh-resolution optical coherence Doppler tomography.\"\\n      },\\n      {\\n        \"paperId\": \"83ac79bb8e8695fb3c3c024be74790d862adea74\",\\n        \"url\": \"https://www.semanticscholar.org/paper/83ac79bb8e8695fb3c3c024be74790d862adea74\",\\n        \"title\": \"TEMPO: Prompt-based Generative Pre-trained Transformer for Time Series Forecasting\",\\n        \"citationCount\": 199,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://arxiv.org/pdf/2310.04948\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2310.04948, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"TEMPO expands the capability for dynamically modeling real-world temporal phenomena from data within diverse domains by utilizing two essential inductive biases of the time series task for pre-trained models, and introducing the design of prompts to facilitate distribution adaptation in different types of time series.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2023-10-08\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"120783624\",\\n            \"name\": \"Defu Cao\"\\n          },\\n          {\\n            \"authorId\": \"2256985863\",\\n            \"name\": \"Furong Jia\"\\n          },\\n          {\\n            \"authorId\": \"2676352\",\\n            \"name\": \"Sercan \\\\u00d6. Arik\"\\n          },\\n          {\\n            \"authorId\": \"1945962\",\\n            \"name\": \"Tomas Pfister\"\\n          },\\n          {\\n            \"authorId\": \"2257061490\",\\n            \"name\": \"Yixiang Zheng\"\\n          },\\n          {\\n            \"authorId\": \"2256992266\",\\n            \"name\": \"Wen Ye\"\\n          },\\n          {\\n            \"authorId\": \"2257088730\",\\n            \"name\": \"Yan Liu\"\\n          }\\n        ],\\n        \"abstract\": \"The past decade has witnessed significant advances in time series modeling with deep learning. While achieving state-of-the-art results, the best-performing architectures vary highly across applications and domains. Meanwhile, for natural language processing, the Generative Pre-trained Transformer (GPT) has demonstrated impressive performance via training one general-purpose model across various textual datasets. It is intriguing to explore whether GPT-type architectures can be effective for time series, capturing the intrinsic dynamic attributes and leading to significant accuracy improvements. In this paper, we propose a novel framework, TEMPO, that can effectively learn time series representations. We focus on utilizing two essential inductive biases of the time series task for pre-trained models: (i) decomposition of the complex interaction between trend, seasonal and residual components; and (ii) introducing the design of prompts to facilitate distribution adaptation in different types of time series. TEMPO expands the capability for dynamically modeling real-world temporal phenomena from data within diverse domains. Our experiments demonstrate the superior performance of TEMPO over state-of-the-art methods on zero shot setting for a number of time series benchmark datasets. This performance gain is observed not only in scenarios involving previously unseen datasets but also in scenarios with multi-modal inputs. This compelling finding highlights TEMPO\\'s potential to constitute a foundational model-building framework.\"\\n      },\\n      {\\n        \"paperId\": \"e3bb7012edfaac357311eca07515d193b4cf26bb\",\\n        \"url\": \"https://www.semanticscholar.org/paper/e3bb7012edfaac357311eca07515d193b4cf26bb\",\\n        \"title\": \"Towards Dynamic Spatial-Temporal Graph Learning: A Decoupled Perspective\",\\n        \"citationCount\": 29,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://ojs.aaai.org/index.php/AAAI/article/download/28759/29459\",\\n          \"status\": \"GOLD\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1609/aaai.v38i8.28759?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1609/aaai.v38i8.28759, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A decoupled learning framework (DLF) is proposed in this paper, which consists of a spatial-temporal graph learning network (DSTG) with a specialized decoupling training strategy to enable prompt adaptation to the evolving distribution of the dynamic graph.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-03-24\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"9071547\",\\n            \"name\": \"Binwu Wang\"\\n          },\\n          {\\n            \"authorId\": \"2108814780\",\\n            \"name\": \"Pengkun Wang\"\\n          },\\n          {\\n            \"authorId\": \"2292591250\",\\n            \"name\": \"Yudong Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2108599981\",\\n            \"name\": \"Xu Wang\"\\n          },\\n          {\\n            \"authorId\": \"6231985\",\\n            \"name\": \"Zhengyang Zhou\"\\n          },\\n          {\\n            \"authorId\": \"2187108259\",\\n            \"name\": \"Lei Bai\"\\n          },\\n          {\\n            \"authorId\": \"46396284\",\\n            \"name\": \"Yang Wang\"\\n          }\\n        ],\\n        \"abstract\": \"With the progress of urban transportation systems, a significant amount of high-quality traffic data is continuously collected through streaming manners, which has propelled the prosperity of the field of spatial-temporal graph prediction. In this paper, rather than solely focusing on designing powerful models for static graphs, we shift our focus to spatial-temporal graph prediction in the dynamic scenario, which involves a continuously expanding and evolving underlying graph. To address inherent challenges, a decoupled learning framework (DLF) is proposed in this paper, which consists of a spatial-temporal graph learning network (DSTG) with a specialized decoupling training strategy. Incorporating inductive biases of time-series structures, DSTG can interpret time dependencies into latent trend and seasonal terms. To enable prompt adaptation to the evolving distribution of the dynamic graph, our decoupling training strategy is devised to iteratively update these two types of patterns. Specifically, for learning seasonal patterns, we conduct thorough training for the model using a long time series (e.g., three months of data). To enhance the learning ability of the model, we also introduce the masked auto-encoding mechanism. During this period, we frequently update trend patterns to expand new information from dynamic graphs. Considering both effectiveness and efficiency, we develop a subnet sampling strategy to select a few representative nodes for fine-tuning the weights of the model. These sampled nodes cover unseen patterns and previously learned patterns. Experiments on dynamic spatial-temporal graph datasets further demonstrate the competitive performance, superior efficiency, and strong scalability of the proposed framework.\"\\n      },\\n      {\\n        \"paperId\": \"650a24da1702beca7eb70011a26f1f3238efad4b\",\\n        \"url\": \"https://www.semanticscholar.org/paper/650a24da1702beca7eb70011a26f1f3238efad4b\",\\n        \"title\": \"Time-LlaMA: Adapting Large Language Models for Time Series Modeling via Dynamic Low-rank Adaptation\",\\n        \"citationCount\": 12,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.18653/v1/2025.acl-srw.90?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.18653/v1/2025.acl-srw.90, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"The proposed Time-LlaMA framework achieves the state-of-the-art (SOTA) performance and have potentials for wide industrial usages.\"\\n        },\\n        \"publicationTypes\": null,\\n        \"publicationDate\": null,\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2346152667\",\\n            \"name\": \"Juyuan Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2334602368\",\\n            \"name\": \"Jiechao Gao\"\\n          },\\n          {\\n            \"authorId\": \"2374971695\",\\n            \"name\": \"Wenwen Ouyang\"\\n          },\\n          {\\n            \"authorId\": \"2334718086\",\\n            \"name\": \"Wei Zhu\"\\n          },\\n          {\\n            \"authorId\": \"2321408906\",\\n            \"name\": \"Hui Yi Leong\"\\n          }\\n        ],\\n        \"abstract\": \"Time series modeling holds significant importance in many industrial applications and has been extensively studied. A series of recent studies have demonstrated that large language models (LLMs) possess robust pattern recognition and semantic understanding capabilities over time series data. However, the current literature have yet striked a high-quality balance between (a) effectively aligning the time series and natural language modalities and (b) keeping the inference efficiency for industrial deployment. To address the above issues, we now propose the Time-LlaMA framework. Time-LlaMA first converts the time series input into token embeddings through a linear tokenization mechanism. Second, the time series token embeddings are aligned with the text prompts. Third, to further adapt the large languag model (LLM) backbone for time series modeling, we have developed a dynamic low-rank adaptation technique (DynaLoRA). DynaLoRA dynamically chooses the most suitable LoRA modules at each layer of the Transformer backbone for each time series input, enhancing the model\\\\u2019s predictive capabilities. Our experimental re-sults on an extensive collection of challenging open and proprietary time series tasks confirm that our proposed method achieves the state-of-the-art (SOTA) performance and have potentials for wide industrial usages. 1\"\\n      },\\n      {\\n        \"paperId\": \"0609513205d94646c740fc219e79b01043e80ba6\",\\n        \"url\": \"https://www.semanticscholar.org/paper/0609513205d94646c740fc219e79b01043e80ba6\",\\n        \"title\": \"Domain-Agnostic Mutual Prompting for Unsupervised Domain Adaptation\",\\n        \"citationCount\": 42,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://arxiv.org/pdf/2403.02899\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2403.02899, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"Domain-Agnostic Mutual Prompting (DAMP) is proposed to exploit domain-invariant semantics by mutually aligning visual and textual embeddings to exploit domain-invariant semantics.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-03-05\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"82980465\",\\n            \"name\": \"Zhekai Du\"\\n          },\\n          {\\n            \"authorId\": \"2108482672\",\\n            \"name\": \"Xinyao Li\"\\n          },\\n          {\\n            \"authorId\": \"2211944242\",\\n            \"name\": \"Fengling Li\"\\n          },\\n          {\\n            \"authorId\": \"1655484744\",\\n            \"name\": \"Ke Lu\"\\n          },\\n          {\\n            \"authorId\": \"2268796475\",\\n            \"name\": \"Lei Zhu\"\\n          },\\n          {\\n            \"authorId\": \"2109058078\",\\n            \"name\": \"Jingjing Li\"\\n          }\\n        ],\\n        \"abstract\": \"Conventional Unsupervised Domain Adaptation (UDA) strives to minimize distribution discrepancy between do-mains, which neglects to harness rich semantics from data and struggles to handle complex domain shifts. A promising technique is to leverage the knowledge of large-scale pretrained vision-language models for more guided adaptation. Despite some endeavors, current methods often learn textual prompts to embed domain semantics for source and target domains separately and perform classification within each domain, limiting cross-domain knowledge transfer. Moreover, prompting only the language branch lacks flex-ibility to adapt both modalities dynamically. To bridge this gap, we propose Domain-Agnostic Mutual Prompting (DAMP) to exploit domain-invariant semantics by mutually aligning visual and textual embeddings. Specifically, the image contextual information is utilized to prompt the language branch in a domain-agnostic and instance-conditioned way. Meanwhile, visual prompts are im-posed based on the domain-agnostic textual prompt to elicit domain-invariant visual embeddings. These two branches of prompts are learned mutually with a cross-attention module and regularized with a semantic-consistency loss and an instance-discrimination contrastive loss. Experiments on three UDA benchmarks demonstrate the superiority of DAMP over state-of-the-art approaches 1.\"\\n      },\\n      {\\n        \"paperId\": \"3f46e1b49c7289eb944365e1bef7bab5b2c891e4\",\\n        \"url\": \"https://www.semanticscholar.org/paper/3f46e1b49c7289eb944365e1bef7bab5b2c891e4\",\\n        \"title\": \"DLTTA: Dynamic Learning Rate for Test-Time Adaptation on Cross-Domain Medical Images\",\\n        \"citationCount\": 35,\\n        \"openAccessPdf\": {\\n          \"url\": \"http://arxiv.org/pdf/2205.13723\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2205.13723, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A novel dynamic learning rate adjustment method for test-time adaptation, called DLTTA, which dynamically modulates the amount of weights update for each test image to account for the differences in their distribution shift.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2022-05-27\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2162926363\",\\n            \"name\": \"Hongzheng Yang\"\\n          },\\n          {\\n            \"authorId\": \"1390805683\",\\n            \"name\": \"Cheng Chen\"\\n          },\\n          {\\n            \"authorId\": \"2050138741\",\\n            \"name\": \"Meirui Jiang\"\\n          },\\n          {\\n            \"authorId\": \"51306676\",\\n            \"name\": \"Quande Liu\"\\n          },\\n          {\\n            \"authorId\": \"48981374\",\\n            \"name\": \"Jianfeng Cao\"\\n          },\\n          {\\n            \"authorId\": \"1714602\",\\n            \"name\": \"P. Heng\"\\n          },\\n          {\\n            \"authorId\": \"35647880\",\\n            \"name\": \"Q. Dou\"\\n          }\\n        ],\\n        \"abstract\": \"Test-time adaptation (TTA) has increasingly been an important topic to efficiently tackle the cross-domain distribution shift at test time for medical images from different institutions. Previous TTA methods have a common limitation of using a fixed learning rate for all the test samples. Such a practice would be sub-optimal for TTA, because test data may arrive sequentially therefore the scale of distribution shift would change frequently. To address this problem, we propose a novel dynamic learning rate adjustment method for test-time adaptation, called DLTTA, which dynamically modulates the amount of weights update for each test image to account for the differences in their distribution shift. Specifically, our DLTTA is equipped with a memory bank based estimation scheme to effectively measure the discrepancy of a given test sample. Based on this estimated discrepancy, a dynamic learning rate adjustment strategy is then developed to achieve a suitable degree of adaptation for each test sample. The effectiveness and general applicability of our DLTTA is extensively demonstrated on three tasks including retinal optical coherence tomography (OCT) segmentation, histopathological image classification, and prostate 3D MRI segmentation. Our method achieves effective and fast test-time adaptation with consistent performance improvement over current state-of-the-art test-time adaptation methods. Code is available at https://github.com/med-air/DLTTA.\"\\n      },\\n      {\\n        \"paperId\": \"54150c69b5e273cdec053c6c95f5ccc707d0c9af\",\\n        \"url\": \"https://www.semanticscholar.org/paper/54150c69b5e273cdec053c6c95f5ccc707d0c9af\",\\n        \"title\": \"Prompt Learning on Temporal Interaction Graphs\",\\n        \"citationCount\": 13,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2402.06326, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work introduces Temporal Interaction Graph Prompting (TIGPrompt), a versatile framework that seamlessly integrates with TIG models, bridging both the temporal and semantic gaps, and proposes a temporal prompt generator to offer temporally-aware prompts for different tasks.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-02-09\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2283939419\",\\n            \"name\": \"Xi Chen\"\\n          },\\n          {\\n            \"authorId\": \"2237941783\",\\n            \"name\": \"Siwei Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2212411539\",\\n            \"name\": \"Yun Xiong\"\\n          },\\n          {\\n            \"authorId\": \"2181344415\",\\n            \"name\": \"Xixi Wu\"\\n          },\\n          {\\n            \"authorId\": \"2283820229\",\\n            \"name\": \"Jiawei Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2265792034\",\\n            \"name\": \"Xiangguo Sun\"\\n          },\\n          {\\n            \"authorId\": \"2283767676\",\\n            \"name\": \"Yao Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2334050630\",\\n            \"name\": \"Feng Zhao\"\\n          },\\n          {\\n            \"authorId\": \"2283820192\",\\n            \"name\": \"Yulin Kang\"\\n          }\\n        ],\\n        \"abstract\": \"Temporal Interaction Graphs (TIGs) are widely utilized to represent real-world systems. To facilitate representation learning on TIGs, researchers have proposed a series of TIG models. However, these models are still facing two tough gaps between the pre-training and downstream predictions in their ``pre-train, predict\\'\\' training paradigm. First, the temporal discrepancy between the pre-training and inference data severely undermines the models\\' applicability in distant future predictions on the dynamically evolving data. Second, the semantic divergence between pretext and downstream tasks hinders their practical applications, as they struggle to align with their learning and prediction capabilities across application scenarios. Recently, the ``pre-train, prompt\\'\\' paradigm has emerged as a lightweight mechanism for model generalization. Applying this paradigm is a potential solution to solve the aforementioned challenges. However, the adaptation of this paradigm to TIGs is not straightforward. The application of prompting in static graph contexts falls short in temporal settings due to a lack of consideration for time-sensitive dynamics and a deficiency in expressive power. To address this issue, we introduce Temporal Interaction Graph Prompting (TIGPrompt), a versatile framework that seamlessly integrates with TIG models, bridging both the temporal and semantic gaps. In detail, we propose a temporal prompt generator to offer temporally-aware prompts for different tasks. These prompts stand out for their minimalistic design, relying solely on the tuning of the prompt generator with very little supervision data. To cater to varying computational resource demands, we propose an extended ``pre-train, prompt-based fine-tune\\'\\' paradigm, offering greater flexibility. Through extensive experiments, the TIGPrompt demonstrates the SOTA performance and remarkable efficiency advantages.\"\\n      },\\n      {\\n        \"paperId\": \"a042571caf46d9053d81441da72ede243f8b421f\",\\n        \"url\": \"https://www.semanticscholar.org/paper/a042571caf46d9053d81441da72ede243f8b421f\",\\n        \"title\": \"PURE: Prompt Evolution with Graph ODE for Out-of-distribution Fluid Dynamics Modeling\",\\n        \"citationCount\": 17,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.52202/079017-3333?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.52202/079017-3333, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"The core of the PURE is to learn time-evolving prompts using a graph ODE to adapt spatio-temporal forecasting models to different scenarios and minimize the mutual information between prompt embeddings and observation embeddings to enhance the robustness of the model to different distributions.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": null,\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2288389213\",\\n            \"name\": \"Hao Wu\"\\n          },\\n          {\\n            \"authorId\": \"2146410855\",\\n            \"name\": \"C. Wang\"\\n          },\\n          {\\n            \"authorId\": \"2217950059\",\\n            \"name\": \"Fan Xu\"\\n          },\\n          {\\n            \"authorId\": \"2345442589\",\\n            \"name\": \"Jinbao Xue\"\\n          },\\n          {\\n            \"authorId\": \"2257378991\",\\n            \"name\": \"Chong Chen\"\\n          },\\n          {\\n            \"authorId\": \"2238119871\",\\n            \"name\": \"Xian-Sheng Hua\"\\n          },\\n          {\\n            \"authorId\": \"2345411659\",\\n            \"name\": \"Xiao Luo\"\\n          }\\n        ],\\n        \"abstract\": \"This work studies the problem of out-of-distribution fluid dynamics modeling. Previous works usually design effective neural operators to learn from mesh-based data structures. However, in real-world applications, they would suffer from distribution shifts from the variance of system parameters and temporal evolution of the dynamical system. In this paper, we propose a novel approach named Prompt Evolution with Graph ODE (PURE) for out-of-distribution fluid dynamics modeling. The core of our PURE is to learn time-evolving prompts using a graph ODE to adapt spatio-temporal forecasting models to different scenarios. In particular, our PURE first learns from historical observations and system parameters in the frequency domain to explore multi-view context information, which could effectively initialize prompt embeddings. More importantly, we incorporate the interpolation of observation sequences into a graph ODE, which can capture the temporal evolution of prompt embeddings for model adaptation. These time-evolving prompt embeddings are then incorporated into basic forecasting models to overcome temporal distribution shifts. We also minimize the mutual information between prompt embeddings and observation embeddings to enhance the robustness of our model to different distributions. Extensive experiments on various benchmark datasets validate the superiority of the proposed PURE in comparison to various baselines. Our codes are available at https://github.com/easylearningscores/\"\\n      },\\n      {\\n        \"paperId\": \"0d0dbfb1b315a43216020abaf74d289456198219\",\\n        \"url\": \"https://www.semanticscholar.org/paper/0d0dbfb1b315a43216020abaf74d289456198219\",\\n        \"title\": \"MaPLe: Multi-modal Prompt Learning\",\\n        \"citationCount\": 816,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://arxiv.org/pdf/2210.03117\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2210.03117, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"The design promotes strong coupling between the vision-language prompts to ensure mutual synergy and discourages learning independent uni-modal solutions, and the effectiveness of the approach is evaluated on three representative tasks of generalization to novel classes, new target datasets and unseen domain shifts.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2022-10-06\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2175250687\",\\n            \"name\": \"Muhammad Uzair Khattak\"\\n          },\\n          {\\n            \"authorId\": \"2097712964\",\\n            \"name\": \"H. Rasheed\"\\n          },\\n          {\\n            \"authorId\": \"32437679\",\\n            \"name\": \"Muhammad Maaz\"\\n          },\\n          {\\n            \"authorId\": \"2111181927\",\\n            \"name\": \"Salman H. Khan\"\\n          },\\n          {\\n            \"authorId\": \"2358803\",\\n            \"name\": \"F. Khan\"\\n          }\\n        ],\\n        \"abstract\": \"Pre-trained vision-language (V-L) models such as CLIP have shown excellent generalization ability to downstream tasks. However, they are sensitive to the choice of input text prompts and require careful selection of prompt templates to perform well. Inspired by the Natural Language Processing (NLP) literature, recent CLIP adaptation approaches learn prompts as the textual inputs to fine-tune CLIP for downstream tasks. We note that using prompting to adapt representations in a single branch of CLIP (language or vision) is sub-optimal since it does not allow the flexibility to dynamically adjust both representation spaces on a downstream task. In this work, we propose Multi-modal Prompt Learning (MaPLe) for both vision and language branches to improve alignment between the vision and language representations. Our design promotes strong coupling between the vision-language prompts to ensure mutual synergy and discourages learning independent uni-modal solutions. Further, we learn separate prompts across different early stages to progressively model the stage-wise feature relationships to allow rich context learning. We evaluate the effectiveness of our approach on three representative tasks of generalization to novel classes, new target datasets and unseen domain shifts. Compared with the state-of-the-art method Co-CoOp, MaPLe exhibits favorable performance and achieves an absolute gain of 3.45% on novel classes and 2.72% on overall harmonic-mean, averaged over 11 diverse image recognition datasets. Our code and pre-trained models are available at https://github.com/muzairkhattak/multimodal-prompt-learning.\"\\n      },\\n      {\\n        \"paperId\": \"ceac663aa9213bfdb457deacba9e5ed9ddd7ae92\",\\n        \"url\": \"https://www.semanticscholar.org/paper/ceac663aa9213bfdb457deacba9e5ed9ddd7ae92\",\\n        \"title\": \"CLIPArTT: Adaptation of CLIP to New Domains at Test Time\",\\n        \"citationCount\": 10,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2405.00754, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This study introduces CLIP Adaptation duRing Test-Time (CLIPArTT), a fully test-time adaptation (TTA) approach for CLIP, which involves automatic text prompts construction during inference for their use as text supervision and pioneer the standardization of TTA benchmarks in the realm of VLMs.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-05-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2037886454\",\\n            \"name\": \"G. Hakim\"\\n          },\\n          {\\n            \"authorId\": \"2188345071\",\\n            \"name\": \"David Osowiechi\"\\n          },\\n          {\\n            \"authorId\": \"1380287805\",\\n            \"name\": \"Mehrdad Noori\"\\n          },\\n          {\\n            \"authorId\": \"2188346816\",\\n            \"name\": \"Milad Cheraghalikhani\"\\n          },\\n          {\\n            \"authorId\": \"108062243\",\\n            \"name\": \"Ali Bahri\"\\n          },\\n          {\\n            \"authorId\": \"2168705225\",\\n            \"name\": \"Moslem Yazdanpanah\"\\n          },\\n          {\\n            \"authorId\": \"144019647\",\\n            \"name\": \"Ismail Ben Ayed\"\\n          },\\n          {\\n            \"authorId\": \"2260340228\",\\n            \"name\": \"Christian Desrosiers\"\\n          }\\n        ],\\n        \"abstract\": \"Pre-trained vision-language models (VLMs), exemplified by CLIP, demonstrate remarkable adaptability across zero-shot classification tasks without additional training. However, their performance diminishes in the presence of domain shifts. In this study, we introduce CLIP Adaptation duRing Test-Time (CLIPArTT), a fully test-time adaptation (TTA) approach for CLIP, which involves automatic text prompts construction during inference for their use as text supervision. Our method employs a unique, minimally invasive text prompt tuning process, wherein multiple predicted classes are aggregated into a single new text prompt, used as pseudo label to re-classify inputs in a transductive manner. Additionally, we pioneer the standardization of TTA benchmarks (e.g., TENT) in the realm of VLMs. Our findings demonstrate that, without requiring additional transformations nor new trainable modules, CLIPArTT enhances performance dynamically across non-corrupted datasets such as CIFAR-100, corrupted datasets like CIFAR-100-C and ImageNet-C, alongside synthetic datasets such as VisDA-C. This research underscores the potential for improving VLMs\\' adaptability through novel test-time strategies, offering insights for robust performance across varied datasets and environments. The code can be found at: https://github.com/dosowiechi/CLIPArTT.git\"\\n      },\\n      {\\n        \"paperId\": \"b7e42f7bc9c3ef6bf9315736fa149dff82fccf8f\",\\n        \"url\": \"https://www.semanticscholar.org/paper/b7e42f7bc9c3ef6bf9315736fa149dff82fccf8f\",\\n        \"title\": \"Learning Domain-Aware Detection Head with Prompt Tuning\",\\n        \"citationCount\": 21,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://arxiv.org/pdf/2306.05718\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2306.05718, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A novel DAOD framework named Domain-Aware detection head with Prompt tuning (DA-Pro), which applies the learnable domain-adaptive prompt to generate the dynamic detection head for each domain.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2023-06-09\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2156607591\",\\n            \"name\": \"Haochen Li\"\\n          },\\n          {\\n            \"authorId\": \"2118404461\",\\n            \"name\": \"Rui Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2222738\",\\n            \"name\": \"Hantao Yao\"\\n          },\\n          {\\n            \"authorId\": \"2109332957\",\\n            \"name\": \"Xinkai Song\"\\n          },\\n          {\\n            \"authorId\": \"2232700\",\\n            \"name\": \"Yifan Hao\"\\n          },\\n          {\\n            \"authorId\": \"46317288\",\\n            \"name\": \"Yongwei Zhao\"\\n          },\\n          {\\n            \"authorId\": \"3353457\",\\n            \"name\": \"Ling Li\"\\n          },\\n          {\\n            \"authorId\": \"7377735\",\\n            \"name\": \"Yunji Chen\"\\n          }\\n        ],\\n        \"abstract\": \"Domain adaptive object detection (DAOD) aims to generalize detectors trained on an annotated source domain to an unlabelled target domain. However, existing methods focus on reducing the domain bias of the detection backbone by inferring a discriminative visual encoder, while ignoring the domain bias in the detection head. Inspired by the high generalization of vision-language models (VLMs), applying a VLM as the robust detection backbone following a domain-aware detection head is a reasonable way to learn the discriminative detector for each domain, rather than reducing the domain bias in traditional methods. To achieve the above issue, we thus propose a novel DAOD framework named Domain-Aware detection head with Prompt tuning (DA-Pro), which applies the learnable domain-adaptive prompt to generate the dynamic detection head for each domain. Formally, the domain-adaptive prompt consists of the domain-invariant tokens, domain-specific tokens, and the domain-related textual description along with the class label. Furthermore, two constraints between the source and target domains are applied to ensure that the domain-adaptive prompt can capture the domains-shared and domain-specific knowledge. A prompt ensemble strategy is also proposed to reduce the effect of prompt disturbance. Comprehensive experiments over multiple cross-domain adaptation tasks demonstrate that using the domain-adaptive prompt can produce an effectively domain-related detection head for boosting domain-adaptive object detection. Our code is available at https://github.com/Therock90421/DA-Pro.\"\\n      },\\n      {\\n        \"paperId\": \"27cffd2e86a8af951679b00f091b89400376d711\",\\n        \"url\": \"https://www.semanticscholar.org/paper/27cffd2e86a8af951679b00f091b89400376d711\",\\n        \"title\": \"Hybrid Prompt-Driven Large Language Model for Robust State-of-Charge Estimation of Multitype Li-ion Batteries\",\\n        \"citationCount\": 12,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {\\'tldr\\'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/TTE.2024.3391938?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/TTE.2024.3391938, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": null,\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2025-02-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2034349220\",\\n            \"name\": \"Chong Bian\"\\n          },\\n          {\\n            \"authorId\": \"2186400636\",\\n            \"name\": \"Xue Han\"\\n          },\\n          {\\n            \"authorId\": \"2284894497\",\\n            \"name\": \"Zhiyu Duan\"\\n          },\\n          {\\n            \"authorId\": \"2237426629\",\\n            \"name\": \"Chao Deng\"\\n          },\\n          {\\n            \"authorId\": \"2269234236\",\\n            \"name\": \"Shunkun Yang\"\\n          },\\n          {\\n            \"authorId\": \"2144086553\",\\n            \"name\": \"Junlan Feng\"\\n          }\\n        ],\\n        \"abstract\": \"State-of-charge (SOC) estimation is critical for reliable operation of Li-ion batteries (LIBs). However, the distinct electrochemical characteristics coupled with harsh low-temperature environments make a single estimator struggle to robustly estimate the volatile SOC of multitype LIBs. To address these issues, this article proposes a hard-soft hybrid prompt learning method to unleash the potential of a pretrained large language model (LLM) for SOC estimation. A textual encoder is introduced to convert LIB measurements into hard text prompts for language modeling, naturally eliciting the pretrained LLM to capture the intrarelations of measured values over time and their interrelations with contextual semantics for accurate estimates. A side adapter network is constructed to reparameterize model adaptation towards different LIB tasks into optimizations within a low-dimensional subspace, strengthening the estimation generalization of the pretrained LLM in a parameter-efficient manner. A knowledge infusion mechanism is designed to encapsulate task-specific information as soft prompt vectors for model integration along forward propagation, dynamically conditioning the hidden states inside the pretrained LLM to enhance the estimation robustness against SOC volatilities. Extensive experiments verify that the hybrid prompt-driven LLM can simultaneously perform estimations for multitype LIBs under diverse operations and sub-zero temperatures with superior accuracy, generalization, and robustness.\"\\n      },\\n      {\\n        \"paperId\": \"564812800caef63f1c83dda51f71c23f4e14cd00\",\\n        \"url\": \"https://www.semanticscholar.org/paper/564812800caef63f1c83dda51f71c23f4e14cd00\",\\n        \"title\": \"Choriocapillaris Impairment Is Associated With Delayed Rod-Mediated Dark Adaptation in Age-Related Macular Degeneration\",\\n        \"citationCount\": 26,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://doi.org/10.1167/iovs.64.12.41\",\\n          \"status\": \"GOLD\",\\n          \"license\": \"CCBYNCND\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC10540875, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"Choriocapillaris FD% were associated with AMD severity and with impaired vision, especially RMDA, which is a causal factor for high-risk soft drusen formation.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2023-09-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"52465212\",\\n            \"name\": \"Deepayan Kar\"\\n          },\\n          {\\n            \"authorId\": \"6550345\",\\n            \"name\": \"G. Corradetti\"\\n          },\\n          {\\n            \"authorId\": \"40654031\",\\n            \"name\": \"Thomas A. Swain\"\\n          },\\n          {\\n            \"authorId\": \"6894914\",\\n            \"name\": \"Mark E. Clark\"\\n          },\\n          {\\n            \"authorId\": \"144347384\",\\n            \"name\": \"G. McGwin\"\\n          },\\n          {\\n            \"authorId\": \"144561682\",\\n            \"name\": \"C. Owsley\"\\n          },\\n          {\\n            \"authorId\": \"1420016526\",\\n            \"name\": \"S. Sadda\"\\n          },\\n          {\\n            \"authorId\": \"4477577\",\\n            \"name\": \"C. Curcio\"\\n          }\\n        ],\\n        \"abstract\": \"Purpose Progress toward treatment and prevention of age-related macular degeneration (AMD) requires imaging end points that relate to vision. We investigated choriocapillaris flow signal deficits (FD%) and visual function in eyes of individuals aged \\\\u226560 years, with and without AMD. Methods One eye of each participant in the baseline visit of the Alabama Study on Early Age-Related Macular Degeneration 2 (ALSTAR2; NCT04112667) was studied. AMD presence and severity was determined using the Age-Related Eye Disease Study (AREDS) grading system. FD% was quantified using macular spectral domain optical coherence tomography angiography (OCTA) scans. Vision tests included rod-mediated dark adaptation (RMDA), best-corrected visual acuity, and contrast sensitivity (photopic and mesopic), and microperimetric light sensitivity (scotopic, mesopic, and photopic). Presence of subretinal drusenoid deposits (SDD) was determined using multimodal imaging. Results In 410 study eyes of 410 participants (mean [SD] age = 71.7 years [5.9]), FD% was higher in early AMD (mean [SD] = 54.0% [5.5], N = 122) and intermediate AMD (59.8% [7.4], N = 92), compared to normal (52.1% [5.3], N = 196) eyes. Among visual functions evaluated, RMDA showed the strongest association with FD% (r = 0.35, P < 0.0001), followed by contrast sensitivity (r = \\\\u22120.22, P < 0.0001). Eyes with SDD had worse FD% (58.3% [7.4], N = 87), compared to eyes without SDD (53.4% [6.0], N = 323, P = < 0.0001). Conclusions Choriocapillaris FD% were associated with AMD severity and with impaired vision, especially RMDA. Reduced metabolic transport and exchange across the choriocapillaris-Bruch\\'s membrane retinal pigment epithelium (RPE) complex, a causal factor for high-risk soft drusen formation, also may impair photoreceptor sustenance from the circulation. This includes retinoid resupply, essential to dynamic rod function.\"\\n      },\\n      {\\n        \"paperId\": \"5f3adb1a1099169733a268c7597af130f200a5d7\",\\n        \"url\": \"https://www.semanticscholar.org/paper/5f3adb1a1099169733a268c7597af130f200a5d7\",\\n        \"title\": \"DA-LSTM: A Dynamic Drift-Adaptive Learning Framework for Interval Load Forecasting with LSTM Networks\",\\n        \"citationCount\": 53,\\n        \"openAccessPdf\": {\\n          \"url\": \"http://arxiv.org/pdf/2305.08767\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2305.08767, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A dynamic drift-adaptive Long Short-Term Memory (DA-LSTM) framework that can improve the performance of load forecasting models without requiring a drift threshold setting is proposed.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2023-05-15\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2047342626\",\\n            \"name\": \"Firas Bayram\"\\n          },\\n          {\\n            \"authorId\": \"2128949725\",\\n            \"name\": \"Phil Aupke\"\\n          },\\n          {\\n            \"authorId\": \"1991147\",\\n            \"name\": \"Bestoun S. Ahmed\"\\n          },\\n          {\\n            \"authorId\": \"10281628\",\\n            \"name\": \"A. Kassler\"\\n          },\\n          {\\n            \"authorId\": \"152623862\",\\n            \"name\": \"A. Theocharis\"\\n          },\\n          {\\n            \"authorId\": \"2086342190\",\\n            \"name\": \"Jonas Forsman\"\\n          }\\n        ],\\n        \"abstract\": \"Load forecasting is a crucial topic in energy management systems (EMS) due to its vital role in optimizing energy scheduling and enabling more flexible and intelligent power grid systems. As a result, these systems allow power utility companies to respond promptly to demands in the electricity market. Deep learning (DL) models have been commonly employed in load forecasting problems supported by adaptation mechanisms to cope with the changing pattern of consumption by customers, known as concept drift. A drift magnitude threshold should be defined to design change detection methods to identify drifts. While the drift magnitude in load forecasting problems can vary significantly over time, existing literature often assumes a fixed drift magnitude threshold, which should be dynamically adjusted rather than fixed during system evolution. To address this gap, in this paper, we propose a dynamic drift-adaptive Long Short-Term Memory (DA-LSTM) framework that can improve the performance of load forecasting models without requiring a drift threshold setting. We integrate several strategies into the framework based on active and passive adaptation approaches. To evaluate DA-LSTM in real-life settings, we thoroughly analyze the proposed framework and deploy it in a real-world problem through a cloud-based environment. Efficiency is evaluated in terms of the prediction performance of each approach and computational cost. The experiments show performance improvements on multiple evaluation metrics achieved by our framework compared to baseline methods from the literature. Finally, we present a trade-off analysis between prediction performance and computational costs.\"\\n      },\\n      {\\n        \"paperId\": \"8feb33300c04fffa050e0dca59c3fdcafc920a3b\",\\n        \"url\": \"https://www.semanticscholar.org/paper/8feb33300c04fffa050e0dca59c3fdcafc920a3b\",\\n        \"title\": \"FlowZero: Zero-Shot Text-to-Video Synthesis with LLM-Driven Dynamic Scene Syntax\",\\n        \"citationCount\": 20,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2311.15813, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"By using spatio-temporal syntaxes to guide the diffusion process, FlowZero achieves improvement in zero-shot video synthesis, generating coherent videos with vivid motion.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2023-11-27\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2247164321\",\\n            \"name\": \"Yu Lu\"\\n          },\\n          {\\n            \"authorId\": \"2948393\",\\n            \"name\": \"Linchao Zhu\"\\n          },\\n          {\\n            \"authorId\": \"3446334\",\\n            \"name\": \"Hehe Fan\"\\n          },\\n          {\\n            \"authorId\": \"2257587812\",\\n            \"name\": \"Yi Yang\"\\n          }\\n        ],\\n        \"abstract\": \"Text-to-video (T2V) generation is a rapidly growing research area that aims to translate the scenes, objects, and actions within complex video text into a sequence of coherent visual frames. We present FlowZero, a novel framework that combines Large Language Models (LLMs) with image diffusion models to generate temporally-coherent videos. FlowZero uses LLMs to understand complex spatio-temporal dynamics from text, where LLMs can generate a comprehensive dynamic scene syntax (DSS) containing scene descriptions, object layouts, and background motion patterns. These elements in DSS are then used to guide the image diffusion model for video generation with smooth object motions and frame-to-frame coherence. Moreover, FlowZero incorporates an iterative self-refinement process, enhancing the alignment between the spatio-temporal layouts and the textual prompts for the videos. To enhance global coherence, we propose enriching the initial noise of each frame with motion dynamics to control the background movement and camera motion adaptively. By using spatio-temporal syntaxes to guide the diffusion process, FlowZero achieves improvement in zero-shot video synthesis, generating coherent videos with vivid motion.\"\\n      },\\n      {\\n        \"paperId\": \"d1e202d981f03d17ebaa61f941d4366e4db39578\",\\n        \"url\": \"https://www.semanticscholar.org/paper/d1e202d981f03d17ebaa61f941d4366e4db39578\",\\n        \"title\": \"Accelerating Chain-of-Thought Reasoning: When Goal-Gradient Importance Meets Dynamic Skipping\",\\n        \"citationCount\": 10,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2505.08392, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work proposes Adaptive GoGI-Skip, a novel framework learning dynamic CoT compression via supervised fine-tuning that achieves substantial efficiency gains and significantly outperforms existing baselines by preserving accuracy even at high effective compression rates, advancing the state of the art in the CoT reasoning efficiency-accuracy trade-off.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2025-05-13\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2293320862\",\\n            \"name\": \"Zhuang Ren\"\\n          },\\n          {\\n            \"authorId\": \"2279253652\",\\n            \"name\": \"Ben Wang\"\\n          },\\n          {\\n            \"authorId\": \"2282384934\",\\n            \"name\": \"Shuifa Sun\"\\n          }\\n        ],\\n        \"abstract\": \"Large Language Models leverage Chain-of-Thought (CoT) prompting for complex tasks, but their reasoning traces are often excessively verbose and inefficient, leading to significant computational costs and latency. Current CoT compression techniques typically rely on generic importance metrics and static compression rates, which may inadvertently remove functionally critical tokens or fail to adapt to varying reasoning complexity. To overcome these limitations, we propose Adaptive GoGI-Skip, a novel framework learning dynamic CoT compression via supervised fine-tuning. This approach introduces two synergistic innovations: (1) Goal-Gradient Importance (GoGI), a novel metric accurately identifying functionally relevant tokens by measuring the gradient influence of their intermediate representations on the final answer loss, and (2) Adaptive Dynamic Skipping (ADS), a mechanism dynamically regulating the compression rate based on runtime model uncertainty while ensuring local coherence through an adaptive N-token constraint. To our knowledge, this is the first work unifying a goal-oriented, gradient-based importance metric with dynamic, uncertainty-aware skipping for CoT compression. Trained on compressed MATH data, Adaptive GoGI-Skip demonstrates strong cross-domain generalization across diverse reasoning benchmarks including AIME, GPQA, and GSM8K. It achieves substantial efficiency gains - reducing CoT token counts by over 45% on average and delivering 1.6-2.0 times inference speedups - while maintaining high reasoning accuracy. Notably, it significantly outperforms existing baselines by preserving accuracy even at high effective compression rates, advancing the state of the art in the CoT reasoning efficiency-accuracy trade-off.\"\\n      },\\n      {\\n        \"paperId\": \"1cf87d8c98636f88a076f7d245e69b819c6e556b\",\\n        \"url\": \"https://www.semanticscholar.org/paper/1cf87d8c98636f88a076f7d245e69b819c6e556b\",\\n        \"title\": \"FineCLIPER: Multi-modal Fine-grained CLIP for Dynamic Facial Expression Recognition with AdaptERs\",\\n        \"citationCount\": 34,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://arxiv.org/pdf/2407.02157\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2407.02157, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A novel framework, named Multi-modal Fine-grained CLIP for DFER with AdaptERs (FineCLIPER), incorporating the following novel designs, which achieves SOTA performance on the DFEW, FERV39k, and MAFW datasets in both supervised and zero-shot settings with few tunable parameters.\"\\n        },\\n        \"publicationTypes\": [\\n          \"Book\",\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-07-02\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2271664660\",\\n            \"name\": \"Haodong Chen\"\\n          },\\n          {\\n            \"authorId\": \"2296857672\",\\n            \"name\": \"Haojian Huang\"\\n          },\\n          {\\n            \"authorId\": \"2309753429\",\\n            \"name\": \"Junhao Dong\"\\n          },\\n          {\\n            \"authorId\": \"2333836576\",\\n            \"name\": \"Mingzhe Zheng\"\\n          },\\n          {\\n            \"authorId\": \"2301156673\",\\n            \"name\": \"Dian Shao\"\\n          }\\n        ],\\n        \"abstract\": \"Dynamic Facial Expression Recognition (DFER) is crucial for understanding human behavior. However, current methods exhibit limited performance mainly due to the insufficient utilization of facial dynamics, and the ambiguity of expression semantics, etc. To this end, we propose a novel framework, named Multi-modal Fine-grained CLIP for DFER with AdaptERs (FineCLIPER), incorporating the following novel designs: 1) To better distinguish between similar facial expressions, we extend the class labels to textual descriptions from both positive and negative aspects, and obtain supervision by calculating the cross-modal similarity based on the CLIP model; 2) Our FineCLIPER adopts a hierarchical manner to effectively mine useful cues from DFE videos. Specifically, besides directly embedding video frames as input (low semantic level), we propose to extract the face segmentation masks and landmarks based on each frame (middle semantic level) and utilize the Multi-modal Large Language Model (MLLM) to further generate detailed descriptions of facial changes across frames with designed prompts (high semantic level). Additionally, we also adopt Parameter-Efficient Fine-Tuning (PEFT) to enable efficient adaptation of large pre-trained models (i.e., CLIP) for this task. Our FineCLIPER achieves SOTA performance on the DFEW, FERV39k, and MAFW datasets in both supervised and zero-shot settings with few tunable parameters. Project page: https://haroldchen19.github.io/FineCLIPER-Page/\"\\n      },\\n      {\\n        \"paperId\": \"7af0f6f5113b69644c3a232a5bee61a1bd15df25\",\\n        \"url\": \"https://www.semanticscholar.org/paper/7af0f6f5113b69644c3a232a5bee61a1bd15df25\",\\n        \"title\": \"Machine learning based estimation of dynamic balance and gait adaptability in persons with neurological diseases using inertial sensors\",\\n        \"citationCount\": 17,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://www.nature.com/articles/s41598-023-35744-x.pdf\",\\n          \"status\": \"GOLD\",\\n          \"license\": \"CCBY\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC10224964, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"Results confirm that steady-state walking measurements provide information about dynamic balance and gait adaptability and can help clinicians identify important features to improve upon during rehabilitation.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2023-05-27\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2123025486\",\\n            \"name\": \"Piergiuseppe Liuzzi\"\\n          },\\n          {\\n            \"authorId\": \"4873550\",\\n            \"name\": \"I. Carpinella\"\\n          },\\n          {\\n            \"authorId\": \"51909015\",\\n            \"name\": \"D. Anastasi\"\\n          },\\n          {\\n            \"authorId\": \"5116603\",\\n            \"name\": \"E. Gervasoni\"\\n          },\\n          {\\n            \"authorId\": \"4840695\",\\n            \"name\": \"T. Lencioni\"\\n          },\\n          {\\n            \"authorId\": \"34764126\",\\n            \"name\": \"R. Bertoni\"\\n          },\\n          {\\n            \"authorId\": \"1798096\",\\n            \"name\": \"M. Carrozza\"\\n          },\\n          {\\n            \"authorId\": \"2687643\",\\n            \"name\": \"D. Cattaneo\"\\n          },\\n          {\\n            \"authorId\": \"1713849\",\\n            \"name\": \"M. Ferrarin\"\\n          },\\n          {\\n            \"authorId\": \"1758858\",\\n            \"name\": \"A. Mannini\"\\n          }\\n        ],\\n        \"abstract\": \"Poor dynamic balance and impaired gait adaptation to different contexts are hallmarks of people with neurological disorders (PwND), leading to difficulties in daily life and increased fall risk. Frequent assessment of dynamic balance and gait adaptability is therefore essential for monitoring the evolution of these impairments and/or the long-term effects of rehabilitation. The modified dynamic gait index (mDGI) is a validated clinical test specifically devoted to evaluating gait facets in clinical settings under a physiotherapist\\\\u2019s supervision. The need of a clinical environment, consequently, limits the number of assessments. Wearable sensors are increasingly used to measure balance and locomotion in real-world contexts and may permit an increase in monitoring frequency. This study aims to provide a preliminary test of this opportunity by using nested cross-validated machine learning regressors to predict the mDGI scores of 95 PwND via inertial signals collected from short steady-state walking bouts derived from the 6-minute walk test. Four different models were compared, one for each pathology (multiple sclerosis, Parkinson\\\\u2019s disease, and stroke) and one for the pooled multipathological cohort. Model explanations were computed on the best-performing solution; the model trained on the multipathological cohort yielded a median (interquartile range) absolute test error of 3.58 (5.38) points. In total, 76% of the predictions were within the mDGI\\\\u2019s minimal detectable change of 5 points. These results confirm that steady-state walking measurements provide information about dynamic balance and gait adaptability and can help clinicians identify important features to improve upon during rehabilitation. Future developments will include training of the method using short steady-state walking bouts in real-world settings, analysing the feasibility of this solution to intensify performance monitoring, providing prompt detection of worsening/improvements, and complementing clinical assessments.\"\\n      },\\n      {\\n        \"paperId\": \"088623b50102baf2cf84ea126d558b574e267967\",\\n        \"url\": \"https://www.semanticscholar.org/paper/088623b50102baf2cf84ea126d558b574e267967\",\\n        \"title\": \"Towards Robust and Efficient Cloud-Edge Elastic Model Adaptation via Selective Entropy Distillation\",\\n        \"citationCount\": 12,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2402.17316, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A Cloud-Edge Elastic Model Adaptation (CEMA) paradigm in which the edge models only need to perform forward propagation and the edge models can be adapted online and to reduce the communication burden is established.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-02-27\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"1816749481\",\\n            \"name\": \"Yaofo Chen\"\\n          },\\n          {\\n            \"authorId\": \"1411039233\",\\n            \"name\": \"Shuaicheng Niu\"\\n          },\\n          {\\n            \"authorId\": \"2157422974\",\\n            \"name\": \"Shoukai Xu\"\\n          },\\n          {\\n            \"authorId\": \"2287873264\",\\n            \"name\": \"Hengjie Song\"\\n          },\\n          {\\n            \"authorId\": \"2288039852\",\\n            \"name\": \"Yaowei Wang\"\\n          },\\n          {\\n            \"authorId\": \"2287854562\",\\n            \"name\": \"Mingkui Tan\"\\n          }\\n        ],\\n        \"abstract\": \"The conventional deep learning paradigm often involves training a deep model on a server and then deploying the model or its distilled ones to resource-limited edge devices. Usually, the models shall remain fixed once deployed (at least for some period) due to the potential high cost of model adaptation for both the server and edge sides. However, in many real-world scenarios, the test environments may change dynamically (known as distribution shifts), which often results in degraded performance. Thus, one has to adapt the edge models promptly to attain promising performance. Moreover, with the increasing data collected at the edge, this paradigm also fails to further adapt the cloud model for better performance. To address these, we encounter two primary challenges: 1) the edge model has limited computation power and may only support forward propagation; 2) the data transmission budget between cloud and edge devices is limited in latency-sensitive scenarios. In this paper, we establish a Cloud-Edge Elastic Model Adaptation (CEMA) paradigm in which the edge models only need to perform forward propagation and the edge models can be adapted online. In our CEMA, to reduce the communication burden, we devise two criteria to exclude unnecessary samples from uploading to the cloud, i.e., dynamic unreliable and low-informative sample exclusion. Based on the uploaded samples, we update and distribute the affine parameters of normalization layers by distilling from the stronger foundation model to the edge model with a sample replay strategy. Extensive experimental results on ImageNet-C and ImageNet-R verify the effectiveness of our CEMA.\"\\n      },\\n      {\\n        \"paperId\": \"5006e5be4c3cb1c4af84e4a2717ef886b3a22464\",\\n        \"url\": \"https://www.semanticscholar.org/paper/5006e5be4c3cb1c4af84e4a2717ef886b3a22464\",\\n        \"title\": \"DART: Dual-Modal Adaptive Online Prompting and Knowledge Retention for Test-Time Adaptation\",\\n        \"citationCount\": 12,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://ojs.aaai.org/index.php/AAAI/article/download/29320/30490\",\\n          \"status\": \"GOLD\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1609/aaai.v38i13.29320?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1609/aaai.v38i13.29320, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"To increase the learning capacity, DART captures knowledge from each test sample by learning class-specific text prompts and instance-level image prompts, and utilizes dual-modal knowledge retention prompts to adaptively retain the acquired knowledge, thereby enhancing the predictions on subsequent test samples.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-03-24\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2293575785\",\\n            \"name\": \"Zichen Liu\"\\n          },\\n          {\\n            \"authorId\": \"2261902040\",\\n            \"name\": \"Hongbo Sun\"\\n          },\\n          {\\n            \"authorId\": \"2267159976\",\\n            \"name\": \"Yuxin Peng\"\\n          },\\n          {\\n            \"authorId\": \"2261798088\",\\n            \"name\": \"Jiahuan Zhou\"\\n          }\\n        ],\\n        \"abstract\": \"As an up-and-coming area, CLIP-based pre-trained vision-language models can readily facilitate downstream tasks through the zero-shot or few-shot fine-tuning manners. However, they still face critical challenges in test-time generalization due to the shifts between the training and test data distributions, hindering the further improvement of the performance. To address this crucial problem, the latest works have introduced Test-Time Adaptation (TTA) techniques to CLIP which dynamically learn text prompts using only test samples. However, their limited learning capacity due to the overlook of visual modality information, and the underutilization of knowledge in previously seen test samples result in reduced performance. In this paper, we propose a novel Dual-modal Adaptive online prompting and knowledge ReTention method called DART to overcome these challenges. To increase the learning capacity, DART captures knowledge from each test sample by learning class-specific text prompts and instance-level image prompts. Additionally, to fully leverage the knowledge from previously seen test samples, DART utilizes dual-modal knowledge retention prompts to adaptively retain the acquired knowledge, thereby enhancing the predictions on subsequent test samples. Extensive experiments on various large-scale benchmarks demonstrate the effectiveness of our proposed DART against state-of-the-art methods.\"\\n      },\\n      {\\n        \"paperId\": \"5076bbbf831a92174c9cc1b347bd0584560435fc\",\\n        \"url\": \"https://www.semanticscholar.org/paper/5076bbbf831a92174c9cc1b347bd0584560435fc\",\\n        \"title\": \"Adaptive-Solver Framework for Dynamic Strategy Selection in Large Language Model Reasoning\",\\n        \"citationCount\": 10,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://arxiv.org/pdf/2310.01446\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2310.01446, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"An Adaptive-Solver (AS) framework that dynamically adapts solving strategies to suit various problems, enabling the flexible allocation of test-time computational resources and can help reduce computational consumption and improve performance.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2023-10-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2256299370\",\\n            \"name\": \"Jianpeng Zhou\"\\n          },\\n          {\\n            \"authorId\": \"81970097\",\\n            \"name\": \"Wanjun Zhong\"\\n          },\\n          {\\n            \"authorId\": \"2214155529\",\\n            \"name\": \"Yanlin Wang\"\\n          },\\n          {\\n            \"authorId\": \"2110182921\",\\n            \"name\": \"Jiahai Wang\"\\n          }\\n        ],\\n        \"abstract\": \"Large Language Models (LLMs) demonstrate impressive ability in handling reasoning tasks. However, unlike humans who can instinctively adapt their problem-solving strategies to the complexity of task, most LLM-based methods adopt a one-size-fits-all approach. These methods employ consistent models, sample sizes, prompting methods and levels of problem decomposition, regardless of the problem complexity. The inflexibility of these methods can bring unnecessary computational overhead or sub-optimal performance. To address this limitation, we introduce an Adaptive-Solver (AS) framework tha dynamically adapts solving strategies to suit various problems, enabling the flexible allocation of test-time computational resources. The framework functions with two primary modules. The initial evaluation module assesses the reliability of the current solution using answer consistency. If the solution is deemed unreliable, the subsequent adaptation module comes into play. Within this module, various types of adaptation strategies are employed collaboratively. Through such dynamic and multi-faceted adaptations, our framework can help reduce computational consumption and improve performance. Experimental results from complex reasoning benchmarks reveal that our method can significantly reduce API costs (up to 85%) while maintaining original performance. Alternatively, it achieves up to 4.5% higher accuracy compared to the baselines at the same cost. The code and dataset are available at https://github.com/john1226966735/Adaptive-Solver.\"\\n      },\\n      {\\n        \"paperId\": \"f4460f3e44e4d035830ab2c676c06e193e51d203\",\\n        \"url\": \"https://www.semanticscholar.org/paper/f4460f3e44e4d035830ab2c676c06e193e51d203\",\\n        \"title\": \"Intrinsic signal optoretinography of dark adaptation kinetics\",\\n        \"citationCount\": 13,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://www.nature.com/articles/s41598-022-06562-4.pdf\",\\n          \"status\": \"GOLD\",\\n          \"license\": \"CCBY\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2112.07838, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"Functional OCT of dark adaptation kinetics promises an objective method for rapid ORG assessment of physiological integrity of retinal photoreceptors.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2021-12-15\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"82743798\",\\n            \"name\": \"Tae-Hoon Kim\"\\n          },\\n          {\\n            \"authorId\": \"2111184482\",\\n            \"name\": \"Jie Ding\"\\n          },\\n          {\\n            \"authorId\": \"8073475\",\\n            \"name\": \"Xincheng Yao\"\\n          }\\n        ],\\n        \"abstract\": \"Delayed dark adaptation due to impaired rod photoreceptor homeostasis has been reported as the earliest symptom of eye diseases such as age-related macular degeneration, diabetic retinopathy, and retinitis pigmentosa. Objective measurement of dark adaptation can facilitate early diagnosis to enable prompt intervention to prevent vision loss. However, there is a lack of noninvasive methods capable of spatiotemporal monitoring of photoreceptor changes during dark adaptation. Here we demonstrate functional optical coherence tomography (OCT) for in vivo intrinsic signal optoretinography (ORG) of dark adaptation kinetics in the C57BL/6J mouse retina. Functional OCT revealed a shortening of the outer retina, a rearrangement of the cone and rod photoreceptor interdigitation zone, and a reduction in intrinsic signal amplitude at the photoreceptor inner segment ellipsoid (ISe). A strong positive correlation between the outer retinal shortening and ISe intensity reduction was also confirmed. Functional OCT of dark adaptation kinetics promises an objective method for rapid ORG assessment of physiological integrity of retinal photoreceptors.\"\\n      },\\n      {\\n        \"paperId\": \"9fe222cb8464e8157b3654ab96ef719331dd2357\",\\n        \"url\": \"https://www.semanticscholar.org/paper/9fe222cb8464e8157b3654ab96ef719331dd2357\",\\n        \"title\": \"Meta-Learning the Difference: Preparing Large Language Models for Efficient Adaptation\",\\n        \"citationCount\": 20,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl_a_00517/2059907/tacl_a_00517.pdf\",\\n          \"status\": \"GOLD\",\\n          \"license\": \"CCBY\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2207.03509, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"Experiments on few-shot dialogue completion, low-resource abstractive summarization, and multi-domain language modeling show improvements in adaptation time and performance over direct finetuning or preparation via domain-adaptive pretraining.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2022-07-07\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"26563401\",\\n            \"name\": \"Zejiang Hou\"\\n          },\\n          {\\n            \"authorId\": \"143733211\",\\n            \"name\": \"Julian Salazar\"\\n          },\\n          {\\n            \"authorId\": \"1402376936\",\\n            \"name\": \"George Polovets\"\\n          }\\n        ],\\n        \"abstract\": \"Abstract Large pretrained language models (PLMs) are often domain- or task-adapted via finetuning or prompting. Finetuning requires modifying all of the parameters and having enough data to avoid overfitting while prompting requires no training and few examples but limits performance. Instead, we prepare PLMs for data- and parameter-efficient adaptation by learning to learn the difference between general and adapted PLMs. This difference is expressed in terms of model weights and sublayer structure through our proposed dynamic low-rank reparameterization and learned architecture controller. Experiments on few-shot dialogue completion, low-resource abstractive summarization, and multi-domain language modeling show improvements in adaptation time and performance over direct finetuning or preparation via domain-adaptive pretraining. Ablations show our task-adaptive reparameterization (TARP) and model search (TAMS) components individually improve on other parameter-efficient transfer like adapters and structure-learning methods like learned sparsification.\"\\n      },\\n      {\\n        \"paperId\": \"792bdea700be87488c5fa135ecef27b94f0d5c31\",\\n        \"url\": \"https://www.semanticscholar.org/paper/792bdea700be87488c5fa135ecef27b94f0d5c31\",\\n        \"title\": \"Sparse-Based Domain Adaptation Network for OCTA Image Super-Resolution Reconstruction\",\\n        \"citationCount\": 11,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://arxiv.org/pdf/2207.11882\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2207.11882, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A multi-level super-resolution model is proposed for the fully-supervised reconstruction of the synthetic data, guiding the reconstructing of the realistic LR images through a generative-adversarial strategy that allows the synthetic and realism LR images to be unified in the feature domain.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2022-07-25\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"47412750\",\\n            \"name\": \"Huaying Hao\"\\n          },\\n          {\\n            \"authorId\": \"2216350318\",\\n            \"name\": \"C. Xu\"\\n          },\\n          {\\n            \"authorId\": \"2109982918\",\\n            \"name\": \"Dan Zhang\"\\n          },\\n          {\\n            \"authorId\": \"151482895\",\\n            \"name\": \"Qifeng Yan\"\\n          },\\n          {\\n            \"authorId\": \"2116222083\",\\n            \"name\": \"Jiong Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2119033864\",\\n            \"name\": \"Yue Liu\"\\n          },\\n          {\\n            \"authorId\": \"1956017\",\\n            \"name\": \"Yitian Zhao\"\\n          }\\n        ],\\n        \"abstract\": \"Retinal Optical Coherence Tomography Angiography (OCTA) with high-resolution is important for the quantification and analysis of retinal vasculature. However, the resolution of OCTA images is inversely proportional to the field of view at the same sampling frequency, which is not conducive to clinicians for analyzing larger vascular areas. In this paper, we propose a novel <bold>S</bold>parse-based domain <bold>A</bold>daptation <bold>S</bold>uper-<bold>R</bold>esolution network (SASR) for the reconstruction of realistic <inline-formula><tex-math notation=\\\\\"LaTeX\\\\\">$6\\\\\\\\times \\\\\\\\text{6}{\\\\\\\\rm{mm}}^{2}$</tex-math></inline-formula>/low-resolution (LR) OCTA images to high-resolution (HR) representations. To be more specific, we first perform a simple degradation of the <inline-formula><tex-math notation=\\\\\"LaTeX\\\\\">$3\\\\\\\\times \\\\\\\\text{3}\\\\\\\\,{\\\\\\\\rm{mm}}^{2}$</tex-math></inline-formula>/high-resolution (HR) image to obtain the synthetic LR image. An efficient registration method is then employed to register the synthetic LR with its corresponding <inline-formula><tex-math notation=\\\\\"LaTeX\\\\\">$3\\\\\\\\times \\\\\\\\text{3}\\\\\\\\,{\\\\\\\\rm{mm}}^{2}$</tex-math></inline-formula> image region within the <inline-formula><tex-math notation=\\\\\"LaTeX\\\\\">$6\\\\\\\\times \\\\\\\\text{6}\\\\\\\\,{\\\\\\\\rm{mm}}^{2}$</tex-math></inline-formula> image to obtain the cropped realistic LR image. We then propose a multi-level super-resolution model for the fully-supervised reconstruction of the synthetic data, guiding the reconstruction of the realistic LR images through a generative-adversarial strategy that allows the synthetic and realistic LR images to be unified in the feature domain. Finally, a novel sparse edge-aware loss is designed to dynamically optimize the vessel edge structure. Extensive experiments on two OCTA sets have shown that our method performs better than state-of-the-art super-resolution reconstruction methods. In addition, we have investigated the performance of the reconstruction results on retina structure segmentations, which further validate the effectiveness of our approach.\"\\n      },\\n      {\\n        \"paperId\": \"3836a5ca7b7884b2333299b65f3436d210587f07\",\\n        \"url\": \"https://www.semanticscholar.org/paper/3836a5ca7b7884b2333299b65f3436d210587f07\",\\n        \"title\": \"Dynamics of Smallholder Farmers\\\\u2019 Livelihood Adaptation Decision-Making in Central Ethiopia\",\\n        \"citationCount\": 22,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://www.mdpi.com/2071-1050/12/11/4526/pdf?version=1591863876\",\\n          \"status\": \"GOLD\",\\n          \"license\": \"CCBY\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.3390/su12114526?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.3390/su12114526, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": null,\\n        \"publicationTypes\": null,\\n        \"publicationDate\": \"2020-06-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"13422136\",\\n            \"name\": \"D. Etana\"\\n          },\\n          {\\n            \"authorId\": \"4266116\",\\n            \"name\": \"D. Snelder\"\\n          },\\n          {\\n            \"authorId\": \"2051704940\",\\n            \"name\": \"C. V. van Wesenbeeck\"\\n          },\\n          {\\n            \"authorId\": \"3606491\",\\n            \"name\": \"T. de Cock Buning\"\\n          }\\n        ],\\n        \"abstract\": \"In previous studies mainly focusing on determinants of adaptation, evidence of the dynamic process of adaptation decision-making is negligible. The objective of this study was to investigate the effects of socio-cultural factors, changes in household characteristics, and climate variables on the transition from non-use to use of adaptation strategies. The study integrated primary data collected from households with secondary rainfall and temperature data. The quantitative and qualitative data were analysed using a dynamic random-effects probit model and a thematic approach, respectively. The result shows strong evidence of path dependence in which use of a strategy during the previous year significantly increases its current use. Climate-related risk perception and factual knowledge may not necessarily prompt adaptation action, whereas access to financial resources and farming-related trainings were consistent positive predictors of farmers\\\\u2019 adaptation decisions. The findings entail that economic capacity and the associated intrinsic motivation help few farmers to utilise robust and contesting adaptation strategies. For most households, economic problems and the consequent fatalistic attitude and risk-avoidance behaviour induce either non-use or use of responsive and accommodating strategies aimed at ensuring survival. Path dependence in non-use of adaptation strategies and sub-optimal adaptation actions demand effective institutional supports to address the behavioural and economic barriers of these households in order to build overall community resilience.\"\\n      },\\n      {\\n        \"paperId\": \"fdb97e3f5fa700fa19c3274f4a35834adb214c23\",\\n        \"url\": \"https://www.semanticscholar.org/paper/fdb97e3f5fa700fa19c3274f4a35834adb214c23\",\\n        \"title\": \"\\\\u201cThe emotions were like a roller-coaster\\\\u201d: a qualitative analysis of e-diary data on healthcare worker resilience and adaptation during the COVID-19 outbreak in Singapore\",\\n        \"citationCount\": 12,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://human-resources-health.biomedcentral.com/counter/pdf/10.1186/s12960-022-00756-7\",\\n          \"status\": \"GOLD\",\\n          \"license\": \"CCBY\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC9285872, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"What characterises risk, vulnerability and resilient responses of HCWs during the early stages of the outbreak in Singapore is explored and the Loads\\\\u2013Levers\\\\u2013Lifts model of resilience is used to guide analysis to inform intervention designs.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Review\"\\n        ],\\n        \"publicationDate\": \"2022-07-15\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2014928933\",\\n            \"name\": \"Alyssa Yenyi Chan\"\\n          },\\n          {\\n            \"authorId\": \"1491848786\",\\n            \"name\": \"Celene Ting\"\\n          },\\n          {\\n            \"authorId\": \"6374024\",\\n            \"name\": \"L. G. Chan\"\\n          },\\n          {\\n            \"authorId\": \"6130448\",\\n            \"name\": \"Z. Hildon\"\\n          }\\n        ],\\n        \"abstract\": \"Background Uncertainties related to COVID-19 have strained the mental health of healthcare workers (HCWs) worldwide. Gaining the ability to adapt and thrive under pressure will be key to addressing this. We explore what characterises risk, vulnerability and resilient responses of HCWs during the early stages of the outbreak in Singapore. Methods We undertook qualitative theory-guided thematic analysis of e-diary entries from HCWs who navigated the outbreak from June\\\\u2013August 2020. Data were extracted from a subset of an online survey of n \\\\u2009=\\\\u20093616 participants collected across 9 institutions, including restructured hospitals, hospices and affiliated primary care partners. Results N \\\\u2009=\\\\u2009663 or 18% submitted qualitative journal entries included for analyses. All professional cadres, local as well as foreign HCWs participated. Themes are reported according to the Loads\\\\u2013Levers\\\\u2013Lifts model of resilience and highlighted in italics. The model assumes that resilience is a dynamic process. Key factors threatening mental health (loading) risk included a notable rise in anxiety, the effects of being separated from loved ones, and experiencing heightened emotions and emotional overload . Bad situations were made worse, prompting vulnerable outcomes when HCWs experienced stigma in the community and effects of \\\\u201cpublic paranoia\\\\u201d; or under conditions where HCWs ended up feeling like a prisoner with little control or choice when either confined to staff accommodation or placed on quarantine/Stay Home Notices. Those with strife in their place of residence also described already difficult situations at work being aggravated by home life. Protection (lifts) came from being able to muster a sense of optimism about the future or feeling grateful for the pace of life slowing down and having the space to reprioritise. In contrast, when risk factors were present , balancing these in the direction of resilient outcomes was achieved by choosing to re-direct stress into positive narratives, drawing on inner agency, uptake of therapeutic activities , social support as well as faith and prayer and drawing comfort from religious community among other factors. Conclusion The Loads\\\\u2013Levers\\\\u2013Lifts model is used to guide analysis to inform intervention designs. Levers promoting resilience through targeting therapies, workplace policies and awareness campaigns accounting for identified loads are proposed.\"\\n      },\\n      {\\n        \"paperId\": \"1a031fead8b57e4c24b04ba79b15b12d4eca583b\",\\n        \"url\": \"https://www.semanticscholar.org/paper/1a031fead8b57e4c24b04ba79b15b12d4eca583b\",\\n        \"title\": \"The Geometry of Prompting: Unveiling Distinct Mechanisms of Task Adaptation in Language Models\",\\n        \"citationCount\": 10,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2502.08009, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work investigates how different prompting methods affect the geometry of representations in decoder-only language models and reveals that various prompting techniques, while achieving similar performance, operate through distinct representational mechanisms for task adaptation.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2025-02-11\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2345003776\",\\n            \"name\": \"Artem Kirsanov\"\\n          },\\n          {\\n            \"authorId\": \"2276206718\",\\n            \"name\": \"Chi-Ning Chou\"\\n          },\\n          {\\n            \"authorId\": \"2345694335\",\\n            \"name\": \"Kyunghyun Cho\"\\n          },\\n          {\\n            \"authorId\": \"2267869180\",\\n            \"name\": \"SueYeon Chung\"\\n          }\\n        ],\\n        \"abstract\": \"Decoder-only language models have the ability to dynamically switch between various computational tasks based on input prompts. Despite many successful applications of prompting, there is very limited understanding of the internal mechanism behind such flexibility. In this work, we investigate how different prompting methods affect the geometry of representations in these models. Employing a framework grounded in statistical physics, we reveal that various prompting techniques, while achieving similar performance, operate through distinct representational mechanisms for task adaptation. Our analysis highlights the critical role of input distribution samples and label semantics in few-shot in-context learning. We also demonstrate evidence of synergistic and interfering interactions between different tasks on the representational level. Our work contributes to the theoretical understanding of large language models and lays the groundwork for developing more effective, representation-aware prompting strategies.\"\\n      },\\n      {\\n        \"paperId\": \"9e0d9c5d858aac35a51b88e0807ace1c3369f863\",\\n        \"url\": \"https://www.semanticscholar.org/paper/9e0d9c5d858aac35a51b88e0807ace1c3369f863\",\\n        \"title\": \"Quantification of intrinsic optical signals in the outer human retina using optical coherence tomography\",\\n        \"citationCount\": 10,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://onlinelibrary.wiley.com/doi/pdfdirect/10.1111/nyas.14721\",\\n          \"status\": \"HYBRID\",\\n          \"license\": \"CCBYNC\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC9299665, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"The presented approach allowed for assess to dynamic changes in the outer retina in response to light and the change in the subretinal space occurring in the context of light adaptation could be measured using a standard OCT platform and a dedicated signal model.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2021-12-10\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"47182275\",\\n            \"name\": \"A. Messner\"\\n          },\\n          {\\n            \"authorId\": \"9535158\",\\n            \"name\": \"V. Aranha dos Santos\"\\n          },\\n          {\\n            \"authorId\": \"51902564\",\\n            \"name\": \"Hannes Stegmann\"\\n          },\\n          {\\n            \"authorId\": \"2494261\",\\n            \"name\": \"S. Puchner\"\\n          },\\n          {\\n            \"authorId\": \"7533266\",\\n            \"name\": \"D. Schmidl\"\\n          },\\n          {\\n            \"authorId\": \"2084180\",\\n            \"name\": \"R. Leitgeb\"\\n          },\\n          {\\n            \"authorId\": \"144820338\",\\n            \"name\": \"L. Schmetterer\"\\n          },\\n          {\\n            \"authorId\": \"4052780\",\\n            \"name\": \"R. Werkmeister\"\\n          }\\n        ],\\n        \"abstract\": \"Intrinsic optical signals constitute a noninvasive biomarker promising the objective assessment of retinal photoreceptor function. We employed a commercial optical coherence tomography (OCT) system and an OCT signal model for evaluation of optical path length (OPL) changes in the temporal outer retina of five healthy subjects during light adaptation. Data were acquired at 30 time points, in ambient light and during long duration stimulation with white light, and analyzed, employing a signal model based on the sum of seven Gaussian curves corresponding to all relevant anatomical structures of the outer retina. During light stimulation, mean OPL between rod outer segment tips (ROST) and the retinal pigment epithelium (RPE) decreased by 21.4 \\\\u00b1 3.5%. Further, OPL between the external\\\\u2010limiting membrane (ELM) and the RPE decreased by 5.2 \\\\u00b1 0.9% versus baseline, while OPL between ELM and ROST showed an initial decrease by 2.1 \\\\u00b1 1.6% versus baseline and, thereafter, increased by 2.8 \\\\u00b1 2.1% versus baseline. Thus, the presented approach allowed for assess to dynamic changes in the outer retina in response to light. The change in the subretinal space occurring in the context of light adaptation could be measured using a standard OCT platform and a dedicated signal model.\"\\n      },\\n      {\\n        \"paperId\": \"0681da799c86c6a92bb0e31f4471656958e60d4f\",\\n        \"url\": \"https://www.semanticscholar.org/paper/0681da799c86c6a92bb0e31f4471656958e60d4f\",\\n        \"title\": \"Teacher adaptation to flexible learning environments\",\\n        \"citationCount\": 43,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {\\'abstract\\'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1007/s10984-019-09302-0?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1007/s10984-019-09302-0, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": null,\\n        \"publicationTypes\": null,\\n        \"publicationDate\": \"2020-07-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2647966\",\\n            \"name\": \"Craig Deed\"\\n          },\\n          {\\n            \"authorId\": \"1713923\",\\n            \"name\": \"D. Blake\"\\n          },\\n          {\\n            \"authorId\": \"50070183\",\\n            \"name\": \"Joanne Henriksen\"\\n          },\\n          {\\n            \"authorId\": \"6355499\",\\n            \"name\": \"A. Mooney\"\\n          },\\n          {\\n            \"authorId\": \"8245143\",\\n            \"name\": \"V. Prain\"\\n          },\\n          {\\n            \"authorId\": \"8348550\",\\n            \"name\": \"R. Tytler\"\\n          },\\n          {\\n            \"authorId\": \"115009297\",\\n            \"name\": \"Tina Zitzlaff\"\\n          },\\n          {\\n            \"authorId\": \"48003546\",\\n            \"name\": \"M. Edwards\"\\n          },\\n          {\\n            \"authorId\": \"117230810\",\\n            \"name\": \"S. Emery\"\\n          },\\n          {\\n            \"authorId\": \"47751037\",\\n            \"name\": \"T. Muir\"\\n          },\\n          {\\n            \"authorId\": \"71944724\",\\n            \"name\": \"K. Swabey\"\\n          },\\n          {\\n            \"authorId\": \"1586673162\",\\n            \"name\": \"Damon P. Thomas\"\\n          },\\n          {\\n            \"authorId\": \"51467279\",\\n            \"name\": \"Cathleen Farrelly\"\\n          },\\n          {\\n            \"authorId\": \"83362649\",\\n            \"name\": \"Valerie Lovejoy\"\\n          },\\n          {\\n            \"authorId\": \"87961076\",\\n            \"name\": \"N. Meyers\"\\n          },\\n          {\\n            \"authorId\": \"113948066\",\\n            \"name\": \"Doug Fingland\"\\n          }\\n        ],\\n        \"abstract\": null\\n      },\\n      {\\n        \"paperId\": \"f7175048d39ed1bceea572b872b72633167fc79d\",\\n        \"url\": \"https://www.semanticscholar.org/paper/f7175048d39ed1bceea572b872b72633167fc79d\",\\n        \"title\": \"Diff-Tracker: Text-to-Image Diffusion Models are Unsupervised Trackers\",\\n        \"citationCount\": 17,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2407.08394, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"Diff-Tracker, a novel approach for the challenging unsupervised visual tracking task leveraging the pre-trained text-to-image diffusion model, designs an initial prompt learner to enable the diffusion model to recognize the tracking target by learning a prompt representing the target.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-07-11\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2309491187\",\\n            \"name\": \"Zhengbo Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2288264614\",\\n            \"name\": \"Li Xu\"\\n          },\\n          {\\n            \"authorId\": \"2067913944\",\\n            \"name\": \"Duo Peng\"\\n          },\\n          {\\n            \"authorId\": \"2265553215\",\\n            \"name\": \"Hossein Rahmani\"\\n          },\\n          {\\n            \"authorId\": \"2309177751\",\\n            \"name\": \"Jun Liu\"\\n          }\\n        ],\\n        \"abstract\": \"We introduce Diff-Tracker, a novel approach for the challenging unsupervised visual tracking task leveraging the pre-trained text-to-image diffusion model. Our main idea is to leverage the rich knowledge encapsulated within the pre-trained diffusion model, such as the understanding of image semantics and structural information, to address unsupervised visual tracking. To this end, we design an initial prompt learner to enable the diffusion model to recognize the tracking target by learning a prompt representing the target. Furthermore, to facilitate dynamic adaptation of the prompt to the target\\'s movements, we propose an online prompt updater. Extensive experiments on five benchmark datasets demonstrate the effectiveness of our proposed method, which also achieves state-of-the-art performance.\"\\n      },\\n      {\\n        \"paperId\": \"cd652dfc58befc0cdf5d36b024aa89ec58e25e53\",\\n        \"url\": \"https://www.semanticscholar.org/paper/cd652dfc58befc0cdf5d36b024aa89ec58e25e53\",\\n        \"title\": \"Exploring a Structural Basis for Delayed Rod-Mediated Dark Adaptation in Age-Related Macular Degeneration Via Deep Learning\",\\n        \"citationCount\": 25,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://doi.org/10.1167/tvst.9.2.62\",\\n          \"status\": \"GOLD\",\\n          \"license\": \"CCBYNCND\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC7745629, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A novel framework for imaging biomarker discovery using deep learning is reported and its ability to identify and localize a previously undescribed biomarker in retinal imaging is demonstrated, strengthening the rationale for RMDA as an outcome measure in early AMD clinical trials.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2020-01-28\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"33556235\",\\n            \"name\": \"Aaron Y. Lee\"\\n          },\\n          {\\n            \"authorId\": \"1390581576\",\\n            \"name\": \"Cecilia S. Lee\"\\n          },\\n          {\\n            \"authorId\": \"1768174127\",\\n            \"name\": \"Marian Blazes\"\\n          },\\n          {\\n            \"authorId\": \"34629227\",\\n            \"name\": \"J. Owen\"\\n          },\\n          {\\n            \"authorId\": \"10732089\",\\n            \"name\": \"Y. Bagdasarova\"\\n          },\\n          {\\n            \"authorId\": \"2109036097\",\\n            \"name\": \"Yue Wu\"\\n          },\\n          {\\n            \"authorId\": \"1471465327\",\\n            \"name\": \"Ted Spaide\"\\n          },\\n          {\\n            \"authorId\": \"11022483\",\\n            \"name\": \"R. Yanagihara\"\\n          },\\n          {\\n            \"authorId\": \"47702381\",\\n            \"name\": \"Y. Kihara\"\\n          },\\n          {\\n            \"authorId\": \"6894914\",\\n            \"name\": \"Mark E. Clark\"\\n          },\\n          {\\n            \"authorId\": \"48882557\",\\n            \"name\": \"M. Kwon\"\\n          },\\n          {\\n            \"authorId\": \"144561682\",\\n            \"name\": \"C. Owsley\"\\n          },\\n          {\\n            \"authorId\": \"4477577\",\\n            \"name\": \"C. Curcio\"\\n          }\\n        ],\\n        \"abstract\": \"Purpose Delayed rod-mediated dark adaptation (RMDA) is a functional biomarker for incipient age-related macular degeneration (AMD). We used anatomically restricted spectral domain optical coherence tomography (SD-OCT) imaging data to localize de novo imaging features associated with and to test hypotheses about delayed RMDA. Methods Rod intercept time (RIT) was measured in participants with and without AMD at 5 degrees from the fovea, and macular SD-OCT images were obtained. A deep learning model was trained with anatomically restricted information using a single representative B-scan through the fovea of each eye. Mean-occlusion masking was utilized to isolate the relevant imaging features. Results The model identified hyporeflective outer retinal bands on macular SD-OCT associated with delayed RMDA. The validation mean standard error (MSE) registered to the foveal B-scan localized the lowest error to 0.5 mm temporal to the fovea center, within an overall low-error region across the rod-free zone and adjoining parafovea. Mean absolute error (MAE) on the test set was 4.71 minutes (8.8% of the dynamic range). Conclusions We report a novel framework for imaging biomarker discovery using deep learning and demonstrate its ability to identify and localize a previously undescribed biomarker in retinal imaging. The hyporeflective outer retinal bands in central macula on SD-OCT demonstrate a structural basis for dysfunctional rod vision that correlates to published histopathologic findings. Translational Relevance This agnostic approach to anatomic biomarker discovery strengthens the rationale for RMDA as an outcome measure in early AMD clinical trials, and also expands the utility of deep learning beyond automated diagnosis to fundamental discovery.\"\\n      },\\n      {\\n        \"paperId\": \"9752a2e33d76d3b2fd91c0285b21d8a6db5bcef4\",\\n        \"url\": \"https://www.semanticscholar.org/paper/9752a2e33d76d3b2fd91c0285b21d8a6db5bcef4\",\\n        \"title\": \"Effective Adaptation in Multi-Task Co-Training for Unified Autonomous Driving\",\\n        \"citationCount\": 46,\\n        \"openAccessPdf\": {\\n          \"url\": \"http://arxiv.org/pdf/2209.08953\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2209.08953, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A simple yet effective pretrain-adapt-finetune paradigm for general multi-task training, where the off-the-shelf pretrained models can be effectively adapted without increasing the training overhead is proposed.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2022-09-19\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"51291599\",\\n            \"name\": \"Xiwen Liang\"\\n          },\\n          {\\n            \"authorId\": \"51255576\",\\n            \"name\": \"Yangxin Wu\"\\n          },\\n          {\\n            \"authorId\": \"47180442\",\\n            \"name\": \"Jianhua Han\"\\n          },\\n          {\\n            \"authorId\": \"2143534132\",\\n            \"name\": \"Hang Xu\"\\n          },\\n          {\\n            \"authorId\": \"1691522\",\\n            \"name\": \"Chunjing Xu\"\\n          },\\n          {\\n            \"authorId\": \"2153397698\",\\n            \"name\": \"Xiaodan Liang\"\\n          }\\n        ],\\n        \"abstract\": \"Aiming towards a holistic understanding of multiple downstream tasks simultaneously, there is a need for extracting features with better transferability. Though many latest self-supervised pre-training methods have achieved impressive performance on various vision tasks under the prevailing pretrain-finetune paradigm, their generalization capacity to multi-task learning scenarios is yet to be explored. In this paper, we extensively investigate the transfer performance of various types of self-supervised methods, e.g., MoCo and SimCLR, on three downstream tasks, including semantic segmentation, drivable area segmentation, and traffic object detection, on the large-scale driving dataset BDD100K. We surprisingly find that their performances are sub-optimal or even lag far behind the single-task baseline, which may be due to the distinctions of training objectives and architectural design lied in the pretrain-finetune paradigm. To overcome this dilemma as well as avoid redesigning the resource-intensive pre-training stage, we propose a simple yet effective pretrain-adapt-finetune paradigm for general multi-task training, where the off-the-shelf pretrained models can be effectively adapted without increasing the training overhead. During the adapt stage, we utilize learnable multi-scale adapters to dynamically adjust the pretrained model weights supervised by multi-task objectives while leaving the pretrained knowledge untouched. Furthermore, we regard the vision-language pre-training model CLIP as a strong complement to the pretrain-adapt-finetune paradigm and propose a novel adapter named LV-Adapter, which incorporates language priors in the multi-task model via task-specific prompting and alignment between visual and textual features.\"\\n      },\\n      {\\n        \"paperId\": \"4001af48eb49bf367a2c0428fd66feec6db54180\",\\n        \"url\": \"https://www.semanticscholar.org/paper/4001af48eb49bf367a2c0428fd66feec6db54180\",\\n        \"title\": \"MEVG: Multi-event Video Generation with Text-to-Video Models\",\\n        \"citationCount\": 33,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2312.04086, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A last frame-aware diffusion process to preserve visual coherence between consecutive videos where each video consists of different events by initializing the latent and simultaneously adjusting noise in the latent to enhance the motion dynamic in a generated video.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2023-12-07\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2162960534\",\\n            \"name\": \"Gyeongrok Oh\"\\n          },\\n          {\\n            \"authorId\": \"2272756256\",\\n            \"name\": \"Jaehwan Jeong\"\\n          },\\n          {\\n            \"authorId\": \"2311572749\",\\n            \"name\": \"Sieun Kim\"\\n          },\\n          {\\n            \"authorId\": \"145965455\",\\n            \"name\": \"Wonmin Byeon\"\\n          },\\n          {\\n            \"authorId\": \"2239053726\",\\n            \"name\": \"Jinkyu Kim\"\\n          },\\n          {\\n            \"authorId\": \"2311578473\",\\n            \"name\": \"Sungwoong Kim\"\\n          },\\n          {\\n            \"authorId\": \"2142668751\",\\n            \"name\": \"Sangpil Kim\"\\n          }\\n        ],\\n        \"abstract\": \"We introduce a novel diffusion-based video generation method, generating a video showing multiple events given multiple individual sentences from the user. Our method does not require a large-scale video dataset since our method uses a pre-trained diffusion-based text-to-video generative model without a fine-tuning process. Specifically, we propose a last frame-aware diffusion process to preserve visual coherence between consecutive videos where each video consists of different events by initializing the latent and simultaneously adjusting noise in the latent to enhance the motion dynamic in a generated video. Furthermore, we find that the iterative update of latent vectors by referring to all the preceding frames maintains the global appearance across the frames in a video clip. To handle dynamic text input for video generation, we utilize a novel prompt generator that transfers course text messages from the user into the multiple optimal prompts for the text-to-video diffusion model. Extensive experiments and user studies show that our proposed method is superior to other video-generative models in terms of temporal coherency of content and semantics. Video examples are available on our project page: https://kuai-lab.github.io/eccv2024mevg.\"\\n      },\\n      {\\n        \"paperId\": \"a6f2f252c88983965f60dfa8325afcd2eed03eb1\",\\n        \"url\": \"https://www.semanticscholar.org/paper/a6f2f252c88983965f60dfa8325afcd2eed03eb1\",\\n        \"title\": \"SAM-OCTA: A Fine-Tuning Strategy for Applying Foundation Model OCTA Image Segmentation Tasks\",\\n        \"citationCount\": 22,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://arxiv.org/pdf/2309.11758\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2309.11758, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"The low-rank adaptation technique is adopted for foundation model fine-tuning and corresponding prompt point generation strategies to process various segmentation tasks on OCTA datasets to achieve state-of-the-art performance metrics.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2023-09-21\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2243437545\",\\n            \"name\": \"Chengliang Wang\"\\n          },\\n          {\\n            \"authorId\": \"2243298334\",\\n            \"name\": \"Xinrun Chen\"\\n          },\\n          {\\n            \"authorId\": \"2216447644\",\\n            \"name\": \"Haojian Ning\"\\n          },\\n          {\\n            \"authorId\": \"48831702\",\\n            \"name\": \"Shiying Li\"\\n          }\\n        ],\\n        \"abstract\": \"In the analysis of optical coherence tomography angiography (OCTA) images, the operation of segmenting specific targets is necessary. Existing methods typically train on supervised datasets with limited samples (approximately a few hundred), which can lead to overfitting. To address this, the low-rank adaptation technique is adopted for foundation model fine-tuning and proposed corresponding prompt point generation strategies to process various segmentation tasks on OCTA datasets. This method is named SAM-OCTA and has been experimented on the publicly available OCTA-500 dataset. While achieving state-of-the-art performance metrics, this method accomplishes local vessel segmentation as well as effective artery-vein segmentation, which was not well-solved in previous works. The code is available at: https://github.com/ShellRedia/SAM-OCTA.\"\\n      },\\n      {\\n        \"paperId\": \"7cf6085c39c60cbc45cd06aaa70242069828fda9\",\\n        \"url\": \"https://www.semanticscholar.org/paper/7cf6085c39c60cbc45cd06aaa70242069828fda9\",\\n        \"title\": \"Prompt-Based Multi-Modal Image Segmentation\",\\n        \"citationCount\": 17,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work proposes a system that can generate image segmentations based on arbitrary prompts at test time with a transformer-based decoder that enables dense prediction and allows for dynamic adaptation to generalized queries involving affordances or properties.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": null,\\n        \"authors\": [\\n          {\\n            \"authorId\": \"73235537\",\\n            \"name\": \"Timo L\\\\u00fcddecke\"\\n          },\\n          {\\n            \"authorId\": \"1746183\",\\n            \"name\": \"Alexander S. Ecker\"\\n          }\\n        ],\\n        \"abstract\": null\\n      },\\n      {\\n        \"paperId\": \"2d0b030d314a5aa8feaa03695e8471270130bdf9\",\\n        \"url\": \"https://www.semanticscholar.org/paper/2d0b030d314a5aa8feaa03695e8471270130bdf9\",\\n        \"title\": \"Imagine while Reasoning in Space: Multimodal Visualization-of-Thought\",\\n        \"citationCount\": 90,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2501.07542, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work proposes a new reasoning paradigm, Multimodal Visualization-of-Thought (MVoT), which enables visual thinking in MLLMs by generating image visualizations of their reasoning traces and introduces token discrepancy loss into autoregressive MLLMs to ensure high-quality visualization.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2025-01-13\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2155795167\",\\n            \"name\": \"Chengzu Li\"\\n          },\\n          {\\n            \"authorId\": \"51198241\",\\n            \"name\": \"Wenshan Wu\"\\n          },\\n          {\\n            \"authorId\": \"2339967968\",\\n            \"name\": \"Huanyu Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2258547658\",\\n            \"name\": \"Yan Xia\"\\n          },\\n          {\\n            \"authorId\": \"2273419590\",\\n            \"name\": \"Shaoguang Mao\"\\n          },\\n          {\\n            \"authorId\": \"2294850817\",\\n            \"name\": \"Li Dong\"\\n          },\\n          {\\n            \"authorId\": \"2339667880\",\\n            \"name\": \"Ivan Vuli\\'c\"\\n          },\\n          {\\n            \"authorId\": \"2249539478\",\\n            \"name\": \"Furu Wei\"\\n          }\\n        ],\\n        \"abstract\": \"Chain-of-Thought (CoT) prompting has proven highly effective for enhancing complex reasoning in Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs). Yet, it struggles in complex spatial reasoning tasks. Nonetheless, human cognition extends beyond language alone, enabling the remarkable capability to think in both words and images. Inspired by this mechanism, we propose a new reasoning paradigm, Multimodal Visualization-of-Thought (MVoT). It enables visual thinking in MLLMs by generating image visualizations of their reasoning traces. To ensure high-quality visualization, we introduce token discrepancy loss into autoregressive MLLMs. This innovation significantly improves both visual coherence and fidelity. We validate this approach through several dynamic spatial reasoning tasks. Experimental results reveal that MVoT demonstrates competitive performance across tasks. Moreover, it exhibits robust and reliable improvements in the most challenging scenarios where CoT fails. Ultimately, MVoT establishes new possibilities for complex reasoning tasks where visual thinking can effectively complement verbal reasoning.\"\\n      },\\n      {\\n        \"paperId\": \"06cb9838ae2e386360aaacbf50775e164a741bd9\",\\n        \"url\": \"https://www.semanticscholar.org/paper/06cb9838ae2e386360aaacbf50775e164a741bd9\",\\n        \"title\": \"AdaCLIP: Adapting CLIP with Hybrid Learnable Prompts for Zero-Shot Anomaly Detection\",\\n        \"citationCount\": 108,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2407.15795, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This study introduces AdaCLIP for the ZSAD task, leveraging a pre-trained vision-language model (VLM), CLIP, and incorporates learnable prompts into CLIP and optimizes them through training on auxiliary annotated anomaly detection data.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-07-22\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2165889312\",\\n            \"name\": \"Yunkang Cao\"\\n          },\\n          {\\n            \"authorId\": \"2281792059\",\\n            \"name\": \"Jiangning Zhang\"\\n          },\\n          {\\n            \"authorId\": \"1788584596\",\\n            \"name\": \"Luca Frittoli\"\\n          },\\n          {\\n            \"authorId\": \"2257750846\",\\n            \"name\": \"Yuqi Cheng\"\\n          },\\n          {\\n            \"authorId\": \"2257204312\",\\n            \"name\": \"Weiming Shen\"\\n          },\\n          {\\n            \"authorId\": \"2319130369\",\\n            \"name\": \"Giacomo Boracchi\"\\n          }\\n        ],\\n        \"abstract\": \"Zero-shot anomaly detection (ZSAD) targets the identification of anomalies within images from arbitrary novel categories. This study introduces AdaCLIP for the ZSAD task, leveraging a pre-trained vision-language model (VLM), CLIP. AdaCLIP incorporates learnable prompts into CLIP and optimizes them through training on auxiliary annotated anomaly detection data. Two types of learnable prompts are proposed: static and dynamic. Static prompts are shared across all images, serving to preliminarily adapt CLIP for ZSAD. In contrast, dynamic prompts are generated for each test image, providing CLIP with dynamic adaptation capabilities. The combination of static and dynamic prompts is referred to as hybrid prompts, and yields enhanced ZSAD performance. Extensive experiments conducted across 14 real-world anomaly detection datasets from industrial and medical domains indicate that AdaCLIP outperforms other ZSAD methods and can generalize better to different categories and even domains. Finally, our analysis highlights the importance of diverse auxiliary data and optimized prompts for enhanced generalization capacity. Code is available at https://github.com/caoyunkang/AdaCLIP.\"\\n      },\\n      {\\n        \"paperId\": \"8da668b01beb344624e8c47ee5b2110da0f339dd\",\\n        \"url\": \"https://www.semanticscholar.org/paper/8da668b01beb344624e8c47ee5b2110da0f339dd\",\\n        \"title\": \"VideoJAM: Joint Appearance-Motion Representations for Enhanced Motion Generation in Video Models\",\\n        \"citationCount\": 42,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2502.02492, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"VideoJAM is a novel framework that instills an effective motion prior to video generators, by encouraging the model to learn a joint appearance-motion representation, and introduces Inner-Guidance, a mechanism that steers the generation toward coherent motion by leveraging the model\\'s own evolving motion prediction as a dynamic guidance signal.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2025-02-04\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2038268012\",\\n            \"name\": \"Hila Chefer\"\\n          },\\n          {\\n            \"authorId\": \"88622696\",\\n            \"name\": \"Uriel Singer\"\\n          },\\n          {\\n            \"authorId\": \"2266842423\",\\n            \"name\": \"Amit Zohar\"\\n          },\\n          {\\n            \"authorId\": \"2044194129\",\\n            \"name\": \"Yuval Kirstain\"\\n          },\\n          {\\n            \"authorId\": \"33964593\",\\n            \"name\": \"Adam Polyak\"\\n          },\\n          {\\n            \"authorId\": \"2188620\",\\n            \"name\": \"Yaniv Taigman\"\\n          },\\n          {\\n            \"authorId\": \"2336960746\",\\n            \"name\": \"Lior Wolf\"\\n          },\\n          {\\n            \"authorId\": \"2086827528\",\\n            \"name\": \"Shelly Sheynin\"\\n          }\\n        ],\\n        \"abstract\": \"Despite tremendous recent progress, generative video models still struggle to capture real-world motion, dynamics, and physics. We show that this limitation arises from the conventional pixel reconstruction objective, which biases models toward appearance fidelity at the expense of motion coherence. To address this, we introduce VideoJAM, a novel framework that instills an effective motion prior to video generators, by encouraging the model to learn a joint appearance-motion representation. VideoJAM is composed of two complementary units. During training, we extend the objective to predict both the generated pixels and their corresponding motion from a single learned representation. During inference, we introduce Inner-Guidance, a mechanism that steers the generation toward coherent motion by leveraging the model\\'s own evolving motion prediction as a dynamic guidance signal. Notably, our framework can be applied to any video model with minimal adaptations, requiring no modifications to the training data or scaling of the model. VideoJAM achieves state-of-the-art performance in motion coherence, surpassing highly competitive proprietary models while also enhancing the perceived visual quality of the generations. These findings emphasize that appearance and motion can be complementary and, when effectively integrated, enhance both the visual quality and the coherence of video generation. Project website: https://hila-chefer.github.io/videojam-paper.github.io/\"\\n      },\\n      {\\n        \"paperId\": \"6851fb7196c27fdec6b787d4196f0a1c1c279e96\",\\n        \"url\": \"https://www.semanticscholar.org/paper/6851fb7196c27fdec6b787d4196f0a1c1c279e96\",\\n        \"title\": \"DialogGen: Multi-modal Interactive Dialogue System for Multi-turn Text-to-Image Generation\",\\n        \"citationCount\": 18,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2403.08857, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"The Multi-modal Dialogue Benchmark (DialogBen) is introduced, a comprehensive bilingual benchmark designed to assess the ability of MLLMs to generate accurate and coherent multi-modal content that supports image editing and contains two evaluation metrics to measure the model\\'s ability to switch modalities and the coherence of the output images.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-03-13\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2162225343\",\\n            \"name\": \"Minbin Huang\"\\n          },\\n          {\\n            \"authorId\": \"2072953799\",\\n            \"name\": \"Yanxin Long\"\\n          },\\n          {\\n            \"authorId\": \"2291203760\",\\n            \"name\": \"Xinchi Deng\"\\n          },\\n          {\\n            \"authorId\": \"2291135101\",\\n            \"name\": \"Ruihang Chu\"\\n          },\\n          {\\n            \"authorId\": \"2291439620\",\\n            \"name\": \"Jiangfeng Xiong\"\\n          },\\n          {\\n            \"authorId\": \"2291389227\",\\n            \"name\": \"Xiaodan Liang\"\\n          },\\n          {\\n            \"authorId\": \"2290858486\",\\n            \"name\": \"Hong Cheng\"\\n          },\\n          {\\n            \"authorId\": \"2268722917\",\\n            \"name\": \"Qinglin Lu\"\\n          },\\n          {\\n            \"authorId\": \"2269146248\",\\n            \"name\": \"Wei Liu\"\\n          }\\n        ],\\n        \"abstract\": \"Text-to-image (T2I) generation models have significantly advanced in recent years. However, effective interaction with these models is challenging for average users due to the need for specialized prompt engineering knowledge and the inability to perform multi-turn image generation, hindering a dynamic and iterative creation process. Recent attempts have tried to equip Multi-modal Large Language Models (MLLMs) with T2I models to bring the user\\'s natural language instructions into reality. Hence, the output modality of MLLMs is extended, and the multi-turn generation quality of T2I models is enhanced thanks to the strong multi-modal comprehension ability of MLLMs. However, many of these works face challenges in identifying correct output modalities and generating coherent images accordingly as the number of output modalities increases and the conversations go deeper. Therefore, we propose DialogGen, an effective pipeline to align off-the-shelf MLLMs and T2I models to build a Multi-modal Interactive Dialogue System (MIDS) for multi-turn Text-to-Image generation. It is composed of drawing prompt alignment, careful training data curation, and error correction. Moreover, as the field of MIDS flourishes, comprehensive benchmarks are urgently needed to evaluate MIDS fairly in terms of output modality correctness and multi-modal output coherence. To address this issue, we introduce the Multi-modal Dialogue Benchmark (DialogBen), a comprehensive bilingual benchmark designed to assess the ability of MLLMs to generate accurate and coherent multi-modal content that supports image editing. It contains two evaluation metrics to measure the model\\'s ability to switch modalities and the coherence of the output images. Our extensive experiments on DialogBen and user study demonstrate the effectiveness of DialogGen compared with other State-of-the-Art models.\"\\n      },\\n      {\\n        \"paperId\": \"fff1e9eb32f8591aea3f2db54035465093381b89\",\\n        \"url\": \"https://www.semanticscholar.org/paper/fff1e9eb32f8591aea3f2db54035465093381b89\",\\n        \"title\": \"Selective Prompting Tuning for Personalized Conversations with LLMs\",\\n        \"citationCount\": 13,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2406.18187, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work initializes a set of soft prompts and uses a trainable dense retriever to adaptively select suitable soft prompts for LLMs according to different input contexts, where the prompt retriever is dynamically updated through feedback from the LLMs.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-06-26\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2274015208\",\\n            \"name\": \"Qiushi Huang\"\\n          },\\n          {\\n            \"authorId\": \"2110814131\",\\n            \"name\": \"Xubo Liu\"\\n          },\\n          {\\n            \"authorId\": \"2273320275\",\\n            \"name\": \"Tom Ko\"\\n          },\\n          {\\n            \"authorId\": \"49814531\",\\n            \"name\": \"Boyong Wu\"\\n          },\\n          {\\n            \"authorId\": \"2239051433\",\\n            \"name\": \"Wenwu Wang\"\\n          },\\n          {\\n            \"authorId\": \"2273525536\",\\n            \"name\": \"Yu Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2189113746\",\\n            \"name\": \"Lilian Tang\"\\n          }\\n        ],\\n        \"abstract\": \"In conversational AI, personalizing dialogues with persona profiles and contextual understanding is essential. Despite large language models\\' (LLMs) improved response coherence, effective persona integration remains a challenge. In this work, we first study two common approaches for personalizing LLMs: textual prompting and direct fine-tuning. We observed that textual prompting often struggles to yield responses that are similar to the ground truths in datasets, while direct fine-tuning tends to produce repetitive or overly generic replies. To alleviate those issues, we propose \\\\\\\\textbf{S}elective \\\\\\\\textbf{P}rompt \\\\\\\\textbf{T}uning (SPT), which softly prompts LLMs for personalized conversations in a selective way. Concretely, SPT initializes a set of soft prompts and uses a trainable dense retriever to adaptively select suitable soft prompts for LLMs according to different input contexts, where the prompt retriever is dynamically updated through feedback from the LLMs. Additionally, we propose context-prompt contrastive learning and prompt fusion learning to encourage the SPT to enhance the diversity of personalized conversations. Experiments on the CONVAI2 dataset demonstrate that SPT significantly enhances response diversity by up to 90\\\\\\\\%, along with improvements in other critical performance indicators. Those results highlight the efficacy of SPT in fostering engaging and personalized dialogue generation. The SPT model code (https://github.com/hqsiswiliam/SPT) is publicly available for further exploration.\"\\n      },\\n      {\\n        \"paperId\": \"5139b7a2744749517163dcfdccf13588aab287bf\",\\n        \"url\": \"https://www.semanticscholar.org/paper/5139b7a2744749517163dcfdccf13588aab287bf\",\\n        \"title\": \"Transformer-Squared: Self-adaptive LLMs\",\\n        \"citationCount\": 10,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2501.06252, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"Transformer-Squared is introduced, a novel self-adaptation framework that adapts LLMs for unseen tasks in real-time by selectively adjusting only the singular components of their weight matrices, and demonstrates versatility across different LLM architectures and modalities, including vision-language tasks.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2025-01-09\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2326487319\",\\n            \"name\": \"Qi Sun\"\\n          },\\n          {\\n            \"authorId\": \"2326294214\",\\n            \"name\": \"Edoardo Cetin\"\\n          },\\n          {\\n            \"authorId\": \"2326422916\",\\n            \"name\": \"Yujin Tang\"\\n          }\\n        ],\\n        \"abstract\": \"Self-adaptive large language models (LLMs) aim to solve the challenges posed by traditional fine-tuning methods, which are often computationally intensive and static in their ability to handle diverse tasks. We introduce Transformer-Squared, a novel self-adaptation framework that adapts LLMs for unseen tasks in real-time by selectively adjusting only the singular components of their weight matrices. During inference, Transformer-Squared employs a two-pass mechanism: first, a dispatch system identifies the task properties, and then task-specific \\'expert\\' vectors, trained using reinforcement learning, are dynamically mixed to obtain targeted behavior for the incoming prompt. Our method consistently outperforms ubiquitous approaches such as LoRA, with fewer parameters and greater efficiency. Furthermore, Transformer-Squared demonstrates versatility across different LLM architectures and modalities, including vision-language tasks. Transformer-Squared represents a significant leap forward, offering a scalable, efficient solution for enhancing the adaptability and task-specific performance of LLMs, paving the way for truly dynamic, self-organizing AI systems.\"\\n      },\\n      {\\n        \"paperId\": \"77711615485303032fc878d495943139ab2558e1\",\\n        \"url\": \"https://www.semanticscholar.org/paper/77711615485303032fc878d495943139ab2558e1\",\\n        \"title\": \"Vivid-ZOO: Multi-View Video Generation with Diffusion Model\",\\n        \"citationCount\": 30,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2406.08659, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A novel diffusion-based pipeline is proposed that generates high-quality multi-view videos centered around a dynamic 3D object from text, exhibiting vivid motions, temporal coherence, and multi-view consistency, given a variety of text prompts.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-06-12\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2287925633\",\\n            \"name\": \"Bing Li\"\\n          },\\n          {\\n            \"authorId\": \"2306182343\",\\n            \"name\": \"Cheng Zheng\"\\n          },\\n          {\\n            \"authorId\": \"2306084700\",\\n            \"name\": \"Wenxuan Zhu\"\\n          },\\n          {\\n            \"authorId\": \"2005711751\",\\n            \"name\": \"Jinjie Mai\"\\n          },\\n          {\\n            \"authorId\": \"2129438190\",\\n            \"name\": \"Biao Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2262444458\",\\n            \"name\": \"Peter Wonka\"\\n          },\\n          {\\n            \"authorId\": \"2288557868\",\\n            \"name\": \"Bernard Ghanem\"\\n          }\\n        ],\\n        \"abstract\": \"While diffusion models have shown impressive performance in 2D image/video generation, diffusion-based Text-to-Multi-view-Video (T2MVid) generation remains underexplored. The new challenges posed by T2MVid generation lie in the lack of massive captioned multi-view videos and the complexity of modeling such multi-dimensional distribution. To this end, we propose a novel diffusion-based pipeline that generates high-quality multi-view videos centered around a dynamic 3D object from text. Specifically, we factor the T2MVid problem into viewpoint-space and time components. Such factorization allows us to combine and reuse layers of advanced pre-trained multi-view image and 2D video diffusion models to ensure multi-view consistency as well as temporal coherence for the generated multi-view videos, largely reducing the training cost. We further introduce alignment modules to align the latent spaces of layers from the pre-trained multi-view and the 2D video diffusion models, addressing the reused layers\\' incompatibility that arises from the domain gap between 2D and multi-view data. In support of this and future research, we further contribute a captioned multi-view video dataset. Experimental results demonstrate that our method generates high-quality multi-view videos, exhibiting vivid motions, temporal coherence, and multi-view consistency, given a variety of text prompts.\"\\n      },\\n      {\\n        \"paperId\": \"4be040c953d8580f0127105924a8bbf139c4804d\",\\n        \"url\": \"https://www.semanticscholar.org/paper/4be040c953d8580f0127105924a8bbf139c4804d\",\\n        \"title\": \"Unbounded: A Generative Infinite Game of Character Life Simulation\",\\n        \"citationCount\": 11,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2410.18975, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A specialized, distilled large language model (LLM) that dynamically generates game mechanics, narratives, and character interactions in real-time, and a new dynamic regional image prompt Adapter (IP-Adapter) for vision models that ensures consistent yet flexible visual generation of a character across multiple environments.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-10-24\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2108961694\",\\n            \"name\": \"Jialu Li\"\\n          },\\n          {\\n            \"authorId\": \"2167749913\",\\n            \"name\": \"Yuanzhen Li\"\\n          },\\n          {\\n            \"authorId\": \"2248172496\",\\n            \"name\": \"Neal Wadhwa\"\\n          },\\n          {\\n            \"authorId\": \"1782328\",\\n            \"name\": \"Y. Pritch\"\\n          },\\n          {\\n            \"authorId\": \"2248055731\",\\n            \"name\": \"David E. Jacobs\"\\n          },\\n          {\\n            \"authorId\": \"2248163830\",\\n            \"name\": \"Michael Rubinstein\"\\n          },\\n          {\\n            \"authorId\": \"2276608813\",\\n            \"name\": \"Mohit Bansal\"\\n          },\\n          {\\n            \"authorId\": \"2248173214\",\\n            \"name\": \"Nataniel Ruiz\"\\n          }\\n        ],\\n        \"abstract\": \"We introduce the concept of a generative infinite game, a video game that transcends the traditional boundaries of finite, hard-coded systems by using generative models. Inspired by James P. Carse\\'s distinction between finite and infinite games, we leverage recent advances in generative AI to create Unbounded: a game of character life simulation that is fully encapsulated in generative models. Specifically, Unbounded draws inspiration from sandbox life simulations and allows you to interact with your autonomous virtual character in a virtual world by feeding, playing with and guiding it - with open-ended mechanics generated by an LLM, some of which can be emergent. In order to develop Unbounded, we propose technical innovations in both the LLM and visual generation domains. Specifically, we present: (1) a specialized, distilled large language model (LLM) that dynamically generates game mechanics, narratives, and character interactions in real-time, and (2) a new dynamic regional image prompt Adapter (IP-Adapter) for vision models that ensures consistent yet flexible visual generation of a character across multiple environments. We evaluate our system through both qualitative and quantitative analysis, showing significant improvements in character life simulation, user instruction following, narrative coherence, and visual consistency for both characters and the environments compared to traditional related approaches.\"\\n      }\\n    ]\\n  },\\n  \"long context dialogue\": {\\n    \"message\": \"Too Many Requests. Please wait and try again or apply for a key for higher rate limits. https://www.semanticscholar.org/product/api#api-key-form\",\\n    \"code\": \"429\"\\n  },\\n  \"adaptive prompt generation narrative\": {\\n    \"total\": 410,\\n    \"offset\": 0,\\n    \"next\": 50,\\n    \"data\": [\\n      {\\n        \"paperId\": \"ef399ed62fcc00e73b02f286012080351652693c\",\\n        \"url\": \"https://www.semanticscholar.org/paper/ef399ed62fcc00e73b02f286012080351652693c\",\\n        \"title\": \"Prompt-prompted Adaptive Structured Pruning for Efficient LLM Generation\",\\n        \"citationCount\": 15,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2404.01365, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A novel training-free and calibration-free method that selects unique FF experts at the sequence level for efficient generation across a plethora of LLMs with different non-ReLU activation functions, which is possible due to a critical observation that many trained LLMs naturally produce highly structured FF activation patterns within a sequence, which the authors call flocking.\"\\n        },\\n        \"publicationTypes\": null,\\n        \"publicationDate\": \"2024-04-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2171964328\",\\n            \"name\": \"Harry Dong\"\\n          },\\n          {\\n            \"authorId\": \"2282555057\",\\n            \"name\": \"Beidi Chen\"\\n          },\\n          {\\n            \"authorId\": \"2284063779\",\\n            \"name\": \"Yuejie Chi\"\\n          }\\n        ],\\n        \"abstract\": \"With the development of transformer-based large language models (LLMs), they have been applied to many fields due to their remarkable utility, but this comes at a considerable computational cost at deployment. Fortunately, some methods such as pruning or constructing a mixture of experts (MoE) aim at exploiting sparsity in transformer feedforward (FF) blocks to gain boosts in speed and reduction in memory requirements. However, these techniques can be very costly and inflexible in practice, as they often require training or are restricted to specific types of architectures. To address this, we introduce GRIFFIN, a novel training-free and calibration-free method that selects unique FF experts at the sequence level for efficient generation across a plethora of LLMs with different non-ReLU activation functions. This is possible due to a critical observation that many trained LLMs naturally produce highly structured FF activation patterns within a sequence, which we call flocking. Despite our method\\'s simplicity, we show with 50% of the FF parameters, GRIFFIN maintains the original model\\'s performance with little to no degradation on a variety of classification and generation tasks, all while improving latency (e.g. 1.29$\\\\\\\\times$ and 1.25$\\\\\\\\times$ speed-ups in Gemma 7B and Llama 2 13B, respectively, on an NVIDIA L40). Code is available at https://github.com/hdong920/GRIFFIN.\"\\n      },\\n      {\\n        \"paperId\": \"bde9fc752842a2a91c70933169fb8cee2b81f8b2\",\\n        \"url\": \"https://www.semanticscholar.org/paper/bde9fc752842a2a91c70933169fb8cee2b81f8b2\",\\n        \"title\": \"Prompt Expansion for Adaptive Text-to-Image Generation\",\\n        \"citationCount\": 23,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2312.16720, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A human evaluation study shows that images generated through Prompt Expansion are more aesthetically pleasing and diverse than those generated by baseline methods.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2023-12-27\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2070966889\",\\n            \"name\": \"Siddhartha Datta\"\\n          },\\n          {\\n            \"authorId\": \"2276608298\",\\n            \"name\": \"Alexander Ku\"\\n          },\\n          {\\n            \"authorId\": \"2275054270\",\\n            \"name\": \"Deepak Ramachandran\"\\n          },\\n          {\\n            \"authorId\": \"2276610768\",\\n            \"name\": \"Peter Anderson\"\\n          }\\n        ],\\n        \"abstract\": \"Text-to-image generation models are powerful but difficult to use. Users craft specific prompts to get better images, though the images can be repetitive. This paper proposes a Prompt Expansion framework that helps users generate high-quality, diverse images with less effort. The Prompt Expansion model takes a text query as input and outputs a set of expanded text prompts that are optimized such that when passed to a text-to-image model, generates a wider variety of appealing images. We conduct a human evaluation study that shows that images generated through Prompt Expansion are more aesthetically pleasing and diverse than those generated by baseline methods. Overall, this paper presents a novel and effective approach to improving the text-to-image generation experience.\"\\n      },\\n      {\\n        \"paperId\": \"c36cb4a41369369d837ea170397f7818d02150dd\",\\n        \"url\": \"https://www.semanticscholar.org/paper/c36cb4a41369369d837ea170397f7818d02150dd\",\\n        \"title\": \"Soft Prompt Generation for Domain Generalization\",\\n        \"citationCount\": 23,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2404.19286, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A simple yet efficient method for the Domain Generalization (DG) task, namely Soft Prompt Generation (SPG), which consists of a two-stage training phase and an inference phase, aiming to incorporate the generative model domain knowledge.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-04-30\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2274938328\",\\n            \"name\": \"Shuanghao Bai\"\\n          },\\n          {\\n            \"authorId\": \"2290728057\",\\n            \"name\": \"Yuedi Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2275025483\",\\n            \"name\": \"Wanqi Zhou\"\\n          },\\n          {\\n            \"authorId\": \"7778036\",\\n            \"name\": \"Zhirong Luan\"\\n          },\\n          {\\n            \"authorId\": \"2275030348\",\\n            \"name\": \"Badong Chen\"\\n          }\\n        ],\\n        \"abstract\": \"Large pre-trained vision language models (VLMs) have shown impressive zero-shot ability on downstream tasks with manually designed prompt. To further adapt VLMs to downstream tasks, soft prompt is proposed to replace manually designed prompt, which undergoes fine-tuning based on specific domain data. Prior prompt learning methods primarily learn a fixed prompt or residuled prompt from training samples. However, the learned prompts lack diversity and ignore information about unseen domains. In this paper, we reframe the prompt learning framework from a generative perspective and propose a simple yet efficient method for the Domain Generalization (DG) task, namely Soft Prompt Generation (SPG). Specifically, SPG consists of a two-stage training phase and an inference phase. During the training phase, we introduce soft prompt label for each domain, aiming to incorporate the generative model domain knowledge. During the inference phase, the generator of the generative model is employed to obtain instance-specific soft prompts for the unseen target domain. Extensive experiments on five domain generalization benchmarks of three DG tasks demonstrate that SPG achieves state-of-the-art performance. The code is available at https://github.com/renytek13/Soft-Prompt-Generation-with-CGAN.\"\\n      },\\n      {\\n        \"paperId\": \"1d954600f17a7c3f16aa726a5eaa902d6851e808\",\\n        \"url\": \"https://www.semanticscholar.org/paper/1d954600f17a7c3f16aa726a5eaa902d6851e808\",\\n        \"title\": \"SAFREE: Training-Free and Adaptive Guard for Safe Text-to-Image And Video Generation\",\\n        \"citationCount\": 50,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2410.12761, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work proposes SAFREE, a novel, training-free approach for safe T2I and T2V, that does not alter the model\\'s weights and incorporates a novel self-validating filtering mechanism that dynamically adjusts the denoising steps when applying the filtered embeddings.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-10-16\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"13563486\",\\n            \"name\": \"Jaehong Yoon\"\\n          },\\n          {\\n            \"authorId\": \"2164249715\",\\n            \"name\": \"Shoubin Yu\"\\n          },\\n          {\\n            \"authorId\": \"2061083016\",\\n            \"name\": \"Vaidehi Patil\"\\n          },\\n          {\\n            \"authorId\": \"2267311471\",\\n            \"name\": \"Huaxiu Yao\"\\n          },\\n          {\\n            \"authorId\": \"2276608813\",\\n            \"name\": \"Mohit Bansal\"\\n          }\\n        ],\\n        \"abstract\": \"Recent advances in diffusion models have significantly enhanced their ability to generate high-quality images and videos, but they have also increased the risk of producing unsafe content. Existing unlearning/editing-based methods for safe generation remove harmful concepts from models but face several challenges: (1) They cannot instantly remove harmful concepts without training. (2) Their safe generation capabilities depend on collected training data. (3) They alter model weights, risking degradation in quality for content unrelated to toxic concepts. To address these, we propose SAFREE, a novel, training-free approach for safe T2I and T2V, that does not alter the model\\'s weights. Specifically, we detect a subspace corresponding to a set of toxic concepts in the text embedding space and steer prompt embeddings away from this subspace, thereby filtering out harmful content while preserving intended semantics. To balance the trade-off between filtering toxicity and preserving safe concepts, SAFREE incorporates a novel self-validating filtering mechanism that dynamically adjusts the denoising steps when applying the filtered embeddings. Additionally, we incorporate adaptive re-attention mechanisms within the diffusion latent space to selectively diminish the influence of features related to toxic concepts at the pixel level. In the end, SAFREE ensures coherent safety checking, preserving the fidelity, quality, and safety of the output. SAFREE achieves SOTA performance in suppressing unsafe content in T2I generation compared to training-free baselines and effectively filters targeted concepts while maintaining high-quality images. It also shows competitive results against training-based methods. We extend SAFREE to various T2I backbones and T2V tasks, showcasing its flexibility and generalization. SAFREE provides a robust and adaptable safeguard for ensuring safe visual generation.\"\\n      },\\n      {\\n        \"paperId\": \"37e1bc43d22da21d6be616522e3fe217bf7f3d8e\",\\n        \"url\": \"https://www.semanticscholar.org/paper/37e1bc43d22da21d6be616522e3fe217bf7f3d8e\",\\n        \"title\": \"Adapting to Distribution Shift by Visual Domain Prompt Generation\",\\n        \"citationCount\": 19,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2405.02797, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work proposes a domain-aware contrastive loss and employ meta-learning to facilitate domain knowledge extraction and outperforms previous work on 5 large-scale benchmarks including WILDS and DomainNet.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-05-05\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"35793956\",\\n            \"name\": \"Zhixiang Chi\"\\n          },\\n          {\\n            \"authorId\": \"2300096585\",\\n            \"name\": \"Li Gu\"\\n          },\\n          {\\n            \"authorId\": \"2300089295\",\\n            \"name\": \"Tao Zhong\"\\n          },\\n          {\\n            \"authorId\": \"2277793919\",\\n            \"name\": \"Huan Liu\"\\n          },\\n          {\\n            \"authorId\": \"1787848\",\\n            \"name\": \"Yuanhao Yu\"\\n          },\\n          {\\n            \"authorId\": \"2277598061\",\\n            \"name\": \"Konstantinos N. Plataniotis\"\\n          },\\n          {\\n            \"authorId\": \"2277695392\",\\n            \"name\": \"Yang Wang\"\\n          }\\n        ],\\n        \"abstract\": \"In this paper, we aim to adapt a model at test-time using a few unlabeled data to address distribution shifts. To tackle the challenges of extracting domain knowledge from a limited amount of data, it is crucial to utilize correlated information from pre-trained backbones and source domains. Previous studies fail to utilize recent foundation models with strong out-of-distribution generalization. Additionally, domain-centric designs are not flavored in their works. Furthermore, they employ the process of modelling source domains and the process of learning to adapt independently into disjoint training stages. In this work, we propose an approach on top of the pre-computed features of the foundation model. Specifically, we build a knowledge bank to learn the transferable knowledge from source domains. Conditioned on few-shot target data, we introduce a domain prompt generator to condense the knowledge bank into a domain-specific prompt. The domain prompt then directs the visual features towards a particular domain via a guidance module. Moreover, we propose a domain-aware contrastive loss and employ meta-learning to facilitate domain knowledge extraction. Extensive experiments are conducted to validate the domain knowledge extraction. The proposed method outperforms previous work on 5 large-scale benchmarks including WILDS and DomainNet.\"\\n      },\\n      {\\n        \"paperId\": \"ac4dc5ff917141f30b53314f765d30fb4b83c8b7\",\\n        \"url\": \"https://www.semanticscholar.org/paper/ac4dc5ff917141f30b53314f765d30fb4b83c8b7\",\\n        \"title\": \"EviPrompt: A Training-Free Evidential Prompt Generation Method for Adapting Segment Anything Model in Medical Images\",\\n        \"citationCount\": 14,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {\\'tldr\\'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/TIP.2024.3482175?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/TIP.2024.3482175, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": null,\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-10-22\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2145205189\",\\n            \"name\": \"Yinsong Xu\"\\n          },\\n          {\\n            \"authorId\": \"2266873357\",\\n            \"name\": \"Jiaqi Tang\"\\n          },\\n          {\\n            \"authorId\": \"2266387679\",\\n            \"name\": \"Aidong Men\"\\n          },\\n          {\\n            \"authorId\": \"2266583142\",\\n            \"name\": \"Qingchao Chen\"\\n          }\\n        ],\\n        \"abstract\": \"Medical image segmentation is a critical task in clinical applications. Recently, the Segment Anything Model (SAM) has demonstrated potential for natural image segmentation. However, the requirement for expert labour to provide prompts, and the domain gap between natural and medical images pose significant obstacles in adapting SAM to medical images. To overcome these challenges, this paper introduces a novel prompt generation method named EviPrompt. The proposed method requires only a single reference image-annotation pair, making it a training-free solution that significantly reduces the need for extensive labelling and computational resources. First, prompts are automatically generated based on the similarity between features of the reference and target images, and evidential learning is introduced to improve reliability. Then, to mitigate the impact of the domain gap, committee voting and inference-guided in-context learning are employed, generating prompts primarily based on human prior knowledge and reducing reliance on extracted semantic information. EviPrompt represents an efficient and robust approach to medical image segmentation. We evaluate it across a broad range of tasks and modalities, confirming its efficacy. The source code is available at https://github.com/SPIresearch/EviPrompt.\"\\n      },\\n      {\\n        \"paperId\": \"6ae06f2bf66f19637adc695bc47256eeb1635b10\",\\n        \"url\": \"https://www.semanticscholar.org/paper/6ae06f2bf66f19637adc695bc47256eeb1635b10\",\\n        \"title\": \"A Structured Narrative Prompt for Prompting Narratives from Large Language Models: Sentiment Assessment of ChatGPT-Generated Narratives and Real Tweets\",\\n        \"citationCount\": 22,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://www.mdpi.com/1999-5903/15/12/375/pdf?version=1700746546\",\\n          \"status\": \"GOLD\",\\n          \"license\": \"CCBY\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.3390/fi15120375?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.3390/fi15120375, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A structured narrative prompt for sending queries to LLMs, an experiment with the narrative generation process using OpenAI\\'s ChatGPT, and an assessment of statistical significance in the sentiment prevalence of the generated narratives compared to real tweets indicate significant promise in the utilization of LLMs for helping to connect a simulated agent\\\\u2019s experiences with real people.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2023-11-23\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2253939386\",\\n            \"name\": \"Christopher J. Lynch\"\\n          },\\n          {\\n            \"authorId\": \"2222695010\",\\n            \"name\": \"Erik J. Jensen\"\\n          },\\n          {\\n            \"authorId\": \"2268131942\",\\n            \"name\": \"Virginia Zamponi\"\\n          },\\n          {\\n            \"authorId\": \"2059262010\",\\n            \"name\": \"Kevin O\\'Brien\"\\n          },\\n          {\\n            \"authorId\": \"144563993\",\\n            \"name\": \"Erika F. Frydenlund\"\\n          },\\n          {\\n            \"authorId\": \"1442241538\",\\n            \"name\": \"Ross J. Gore\"\\n          }\\n        ],\\n        \"abstract\": \"Large language models (LLMs) excel in providing natural language responses that sound authoritative, reflect knowledge of the context area, and can present from a range of varied perspectives. Agent-based models and simulations consist of simulated agents that interact within a simulated environment to explore societal, social, and ethical, among other, problems. Simulated agents generate large volumes of data and discerning useful and relevant content is an onerous task. LLMs can help in communicating agents\\\\u2019 perspectives on key life events by providing natural language narratives. However, these narratives should be factual, transparent, and reproducible. Therefore, we present a structured narrative prompt for sending queries to LLMs, we experiment with the narrative generation process using OpenAI\\\\u2019s ChatGPT, and we assess statistically significant differences across 11 Positive and Negative Affect Schedule (PANAS) sentiment levels between the generated narratives and real tweets using chi-squared tests and Fisher\\\\u2019s exact tests. The narrative prompt structure effectively yields narratives with the desired components from ChatGPT. In four out of forty-four categories, ChatGPT generated narratives which have sentiment scores that were not discernibly different, in terms of statistical significance (alpha level \\\\u03b1=0.05), from the sentiment expressed in real tweets. Three outcomes are provided: (1) a list of benefits and challenges for LLMs in narrative generation; (2) a structured prompt for requesting narratives of an LLM chatbot based on simulated agents\\\\u2019 information; (3) an assessment of statistical significance in the sentiment prevalence of the generated narratives compared to real tweets. This indicates significant promise in the utilization of LLMs for helping to connect a simulated agent\\\\u2019s experiences with real people.\"\\n      },\\n      {\\n        \"paperId\": \"a05b33072a8330413947eab0833ed2fd21de4963\",\\n        \"url\": \"https://www.semanticscholar.org/paper/a05b33072a8330413947eab0833ed2fd21de4963\",\\n        \"title\": \"Compound Text-Guided Prompt Tuning via Image-Adaptive Cues\",\\n        \"citationCount\": 12,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2312.06401, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work introduces text supervision to the optimization of prompts, which enables two benefits: 1) releasing the model reliance on the pre-defined category names during inference, thereby enabling more flexible prompt generation; 2) reducing the number of inputs to the text encoder, which decreases GPU memory consumption significantly.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2023-12-11\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2273557458\",\\n            \"name\": \"Hao Tan\"\\n          },\\n          {\\n            \"authorId\": \"2273559489\",\\n            \"name\": \"Jun Li\"\\n          },\\n          {\\n            \"authorId\": \"2118764798\",\\n            \"name\": \"Yizhuang Zhou\"\\n          },\\n          {\\n            \"authorId\": \"2273589717\",\\n            \"name\": \"Jun Wan\"\\n          },\\n          {\\n            \"authorId\": \"2113457514\",\\n            \"name\": \"Zhen Lei\"\\n          },\\n          {\\n            \"authorId\": \"2274088311\",\\n            \"name\": \"Xiangyu Zhang\"\\n          }\\n        ],\\n        \"abstract\": \"Vision-Language Models (VLMs) such as CLIP have demonstrated remarkable generalization capabilities to downstream tasks. However, existing prompt tuning based frameworks need to parallelize learnable textual inputs for all categories, suffering from massive GPU memory consumption when there is a large number of categories in the target dataset. Moreover, previous works require to include category names within prompts, exhibiting subpar performance when dealing with ambiguous category names. To address these shortcomings, we propose Compound Text-Guided Prompt Tuning (TGP-T) that significantly reduces resource demand while achieving superior performance. We introduce text supervision to the optimization of prompts, which enables two benefits: 1) releasing the model reliance on the pre-defined category names during inference, thereby enabling more flexible prompt generation; 2) reducing the number of inputs to the text encoder, which decreases GPU memory consumption significantly. Specifically, we found that compound text supervisions, i.e., category-wise and content-wise, is highly effective, since they provide inter-class separability and capture intra-class variations, respectively. Moreover, we condition the prompt generation on visual features through a module called Bonder, which facilitates the alignment between prompts and visual features. Extensive experiments on few-shot recognition and domain generalization demonstrate that TGP-T achieves superior performance with consistently lower training costs. It reduces GPU memory usage by 93% and attains a 2.5% performance gain on 16-shot ImageNet. The code is available at https://github.com/EricTan7/TGP-T.\"\\n      },\\n      {\\n        \"paperId\": \"b53dba04b2518ebed943daa9ab58f19af81e2012\",\\n        \"url\": \"https://www.semanticscholar.org/paper/b53dba04b2518ebed943daa9ab58f19af81e2012\",\\n        \"title\": \"Efficient Model Personalization in Federated Learning via Client-Specific Prompt Generation\",\\n        \"citationCount\": 98,\\n        \"openAccessPdf\": {\\n          \"url\": \"http://arxiv.org/pdf/2308.15367\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2308.15367, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work proposes a novel personalized FL framework of client-specific Prompt Generation (pFedPG), which learns to deploy a personalized prompt generator at the server for producing client- specific visual prompts that efficiently adapts frozen backbones to local data distributions.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2023-08-29\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"41015732\",\\n            \"name\": \"Fu-En Yang\"\\n          },\\n          {\\n            \"authorId\": \"48586406\",\\n            \"name\": \"Chien-Yi Wang\"\\n          },\\n          {\\n            \"authorId\": \"2218628989\",\\n            \"name\": \"Yu-Chiang Frank Wang\"\\n          }\\n        ],\\n        \"abstract\": \"Federated learning (FL) emerges as a decentralized learning framework which trains models from multiple distributed clients without sharing their data to preserve privacy. Recently, large-scale pre-trained models (e.g., Vision Transformer) have shown a strong capability of deriving robust representations. However, the data heterogeneity among clients, the limited computation resources, and the communication bandwidth restrict the deployment of large-scale models in FL frameworks. To leverage robust representations from large-scale models while enabling efficient model personalization for heterogeneous clients, we propose a novel personalized FL framework of client-specific Prompt Generation (pFedPG), which learns to deploy a personalized prompt generator at the server for producing client-specific visual prompts that efficiently adapts frozen backbones to local data distributions. Our proposed framework jointly optimizes the stages of personalized prompt adaptation locally and personalized prompt generation globally. The former aims to train visual prompts that adapt foundation models to each client, while the latter observes local optimization directions to generate personalized prompts for all clients. Through extensive experiments on benchmark datasets, we show that our pFedPG is favorable against state-of-the-art personalized FL methods under various types of data heterogeneity, allowing computation and communication efficient model personalization.\"\\n      },\\n      {\\n        \"paperId\": \"d92a423e09804595c8a2e241f890f5a24d326bb5\",\\n        \"url\": \"https://www.semanticscholar.org/paper/d92a423e09804595c8a2e241f890f5a24d326bb5\",\\n        \"title\": \"Prompt-based Code Completion via Multi-Retrieval Augmented Generation\",\\n        \"citationCount\": 17,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://arxiv.org/pdf/2405.07530\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2405.07530, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"ProCC, a code completion framework leveraging prompt engineering and the contextual multi-armed bandits algorithm to flexibly incorporate and adapt to multiple perspectives of code, is proposed.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-05-13\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2176503078\",\\n            \"name\": \"Hanzhuo Tan\"\\n          },\\n          {\\n            \"authorId\": \"2290488730\",\\n            \"name\": \"Qi Luo\"\\n          },\\n          {\\n            \"authorId\": \"51181043\",\\n            \"name\": \"Lingixao Jiang\"\\n          },\\n          {\\n            \"authorId\": \"2301156520\",\\n            \"name\": \"Zizheng Zhan\"\\n          },\\n          {\\n            \"authorId\": \"2174027344\",\\n            \"name\": \"Jing Li\"\\n          },\\n          {\\n            \"authorId\": \"1557360433\",\\n            \"name\": \"Haotian Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2290557105\",\\n            \"name\": \"Yuqun Zhang\"\\n          }\\n        ],\\n        \"abstract\": \"Automated code completion, aiming at generating subsequent tokens from unfinished code, has significantly benefited from recent progress in pre-trained Large Language Models (LLMs). However, these models often suffer from coherence issues and hallucinations when dealing with complex code logic or extrapolating beyond their training data. Existing Retrieval Augmented Generation (RAG) techniques partially address these issues by retrieving relevant code with a separate encoding model where the retrieved snippet serves as contextual reference for code completion. However, their retrieval scope is subject to a singular perspective defined by the encoding model, which largely overlooks the complexity and diversity inherent in code semantics. To address this limitation, we propose ProCC, a code completion framework leveraging prompt engineering and the contextual multi-armed bandits algorithm to flexibly incorporate and adapt to multiple perspectives of code. ProCC first employs a prompt-based multi-retriever system which crafts prompt templates to elicit LLM knowledge to understand code semantics with multiple retrieval perspectives. Then, it adopts the adaptive retrieval selection algorithm to incorporate code similarity into the decision-making process to determine the most suitable retrieval perspective for the LLM to complete the code. Experimental results demonstrate that ProCC outperforms a widely-studied code completion technique RepoCoder by 7.92% on the public benchmark CCEval, 3.19% in HumanEval-Infilling, 2.80% on our collected open-source benchmark suite, and 4.48% on the private-domain benchmark suite collected from Kuaishou Technology in terms of Exact Match. ProCC also allows augmenting fine-tuned techniques in a plug-and-play manner, yielding an averaged 6.5% improvement over the fine-tuned model.\"\\n      },\\n      {\\n        \"paperId\": \"47b2e5c3e7a907d2b65b3f9b918f83532d12f41a\",\\n        \"url\": \"https://www.semanticscholar.org/paper/47b2e5c3e7a907d2b65b3f9b918f83532d12f41a\",\\n        \"title\": \"Automatic counter-narrative generation for hate speech in Spanish\",\\n        \"citationCount\": 16,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"The article shows that the use of GPT-3 outperforms other models in generating non-offensive and informative counter-narratives, which some-times present compelling arguments.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": null,\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2229887611\",\\n            \"name\": \"Mar\\\\u00eda Estrella Vallecillo Rodrguez\"\\n          },\\n          {\\n            \"authorId\": \"1799967\",\\n            \"name\": \"Arturo Montejo-R\\\\u00e1ez\"\\n          },\\n          {\\n            \"authorId\": \"51183850\",\\n            \"name\": \"M. T. M. Valdivia\"\\n          }\\n        ],\\n        \"abstract\": null\\n      },\\n      {\\n        \"paperId\": \"bc9f1025246efeed568650934b6e183415aca279\",\\n        \"url\": \"https://www.semanticscholar.org/paper/bc9f1025246efeed568650934b6e183415aca279\",\\n        \"title\": \"PREADD: Prefix-Adaptive Decoding for Controlled Text Generation\",\\n        \"citationCount\": 29,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://arxiv.org/pdf/2307.03214\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2307.03214, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"The proposed Prefix-Adaptive Decoding (PREADD), a flexible method for controlled text generation, outperforms not only prompting baselines, but also an auxiliary-expert control method, by 12% or more in relative gain on the authors\\' main metrics for each task.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2023-07-06\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2221313264\",\\n            \"name\": \"Jonathan Pei\"\\n          },\\n          {\\n            \"authorId\": \"1410652795\",\\n            \"name\": \"Kevin Yang\"\\n          },\\n          {\\n            \"authorId\": \"38666915\",\\n            \"name\": \"D. Klein\"\\n          }\\n        ],\\n        \"abstract\": \"We propose Prefix-Adaptive Decoding (PREADD), a flexible method for controlled text generation. Unlike existing methods that use auxiliary expert models to control for attributes, PREADD does not require an external model, instead relying on linearly combining output logits from multiple prompts. Specifically, PREADD contrasts the output logits generated using a raw prompt against those generated using a prefix-prepended prompt, enabling both positive and negative control with respect to any attribute encapsulated by the prefix. We evaluate PREADD on three tasks -- toxic output mitigation, gender bias reduction, and sentiment control -- and find that PREADD outperforms not only prompting baselines, but also an auxiliary-expert control method, by 12% or more in relative gain on our main metrics for each task.\"\\n      },\\n      {\\n        \"paperId\": \"5a4d1add108f2e6d8ccd9f6ac94ab3e0335db540\",\\n        \"url\": \"https://www.semanticscholar.org/paper/5a4d1add108f2e6d8ccd9f6ac94ab3e0335db540\",\\n        \"title\": \"Agents\\' Room: Narrative Generation through Multi-step Collaboration\",\\n        \"citationCount\": 31,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2410.02603, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"It is shown that Agents\\' Room generates stories that are preferred by expert evaluators over those produced by baseline systems by leveraging collaboration and specialization to decompose the complex story writing task into tractable components.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-10-03\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2174667321\",\\n            \"name\": \"Fantine Huot\"\\n          },\\n          {\\n            \"authorId\": \"23181472\",\\n            \"name\": \"Reinald Kim Amplayo\"\\n          },\\n          {\\n            \"authorId\": \"52578817\",\\n            \"name\": \"J. Palomaki\"\\n          },\\n          {\\n            \"authorId\": \"1411178896\",\\n            \"name\": \"Alice Shoshana Jakobovits\"\\n          },\\n          {\\n            \"authorId\": \"2324056615\",\\n            \"name\": \"Elizabeth Clark\"\\n          },\\n          {\\n            \"authorId\": \"1747893\",\\n            \"name\": \"Mirella Lapata\"\\n          }\\n        ],\\n        \"abstract\": \"Writing compelling fiction is a multifaceted process combining elements such as crafting a plot, developing interesting characters, and using evocative language. While large language models (LLMs) show promise for story writing, they currently rely heavily on intricate prompting, which limits their use. We propose Agents\\' Room, a generation framework inspired by narrative theory, that decomposes narrative writing into subtasks tackled by specialized agents. To illustrate our method, we introduce Tell Me A Story, a high-quality dataset of complex writing prompts and human-written stories, and a novel evaluation framework designed specifically for assessing long narratives. We show that Agents\\' Room generates stories that are preferred by expert evaluators over those produced by baseline systems by leveraging collaboration and specialization to decompose the complex story writing task into tractable components. We provide extensive analysis with automated and human-based metrics of the generated output.\"\\n      },\\n      {\\n        \"paperId\": \"ff3edac95dcc82eae72cd25519e5dff075fc58be\",\\n        \"url\": \"https://www.semanticscholar.org/paper/ff3edac95dcc82eae72cd25519e5dff075fc58be\",\\n        \"title\": \"Retrieval Augmented Generation in Prompt-based Text-to-Speech Synthesis with Context-Aware Contrastive Language-Audio Pretraining\",\\n        \"citationCount\": 14,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2406.03714, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper adapt retrieval augmented generation (RAG) from LLMs to prompt-based TTS, and presents a Context-Aware Contrastive Language-Audio Pre-training (CA-CLAP) model to extract context-aware, style-related features.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-06-06\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2159562819\",\\n            \"name\": \"Jinlong Xue\"\\n          },\\n          {\\n            \"authorId\": \"2111214299\",\\n            \"name\": \"Yayue Deng\"\\n          },\\n          {\\n            \"authorId\": \"2261912430\",\\n            \"name\": \"Yingming Gao\"\\n          },\\n          {\\n            \"authorId\": \"2161324824\",\\n            \"name\": \"Ya Li\"\\n          }\\n        ],\\n        \"abstract\": \"Recent prompt-based text-to-speech (TTS) models can clone an unseen speaker using only a short speech prompt. They leverage a strong in-context ability to mimic the speech prompts, including speaker style, prosody, and emotion. Therefore, the selection of a speech prompt greatly influences the generated speech, akin to the importance of a prompt in large language models (LLMs). However, current prompt-based TTS models choose the speech prompt manually or simply at random. Hence, in this paper, we adapt retrieval augmented generation (RAG) from LLMs to prompt-based TTS. Unlike traditional RAG methods, we additionally consider contextual information during the retrieval process and present a Context-Aware Contrastive Language-Audio Pre-training (CA-CLAP) model to extract context-aware, style-related features. The objective and subjective evaluations demonstrate that our proposed RAG method outperforms baselines, and our CA-CLAP achieves better results than text-only retrieval methods.\"\\n      },\\n      {\\n        \"paperId\": \"3a6d9e0d5896491dbdb192ea1a9032e9940abe54\",\\n        \"url\": \"https://www.semanticscholar.org/paper/3a6d9e0d5896491dbdb192ea1a9032e9940abe54\",\\n        \"title\": \"Text-driven Prompt Generation for Vision-Language Models in Federated Learning\",\\n        \"citationCount\": 17,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://arxiv.org/pdf/2310.06123\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2310.06123, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work proposes Federated Text-driven Prompt Generation (FedTPG), which learns a unified prompt generation network across multiple remote clients in a scalable manner and is superior to existing federated prompt learning methods, that achieve overall better generalization on both seen and unseen classes.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2023-10-09\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2256983385\",\\n            \"name\": \"Chen Qiu\"\\n          },\\n          {\\n            \"authorId\": \"2257324808\",\\n            \"name\": \"Xingyu Li\"\\n          },\\n          {\\n            \"authorId\": \"29359383\",\\n            \"name\": \"Chaithanya Kumar Mummadi\"\\n          },\\n          {\\n            \"authorId\": \"144487556\",\\n            \"name\": \"M. Ganesh\"\\n          },\\n          {\\n            \"authorId\": \"2257091754\",\\n            \"name\": \"Zhenzhen Li\"\\n          },\\n          {\\n            \"authorId\": \"2257130661\",\\n            \"name\": \"Lu Peng\"\\n          },\\n          {\\n            \"authorId\": \"2257132255\",\\n            \"name\": \"Wan-Yi Lin\"\\n          }\\n        ],\\n        \"abstract\": \"Prompt learning for vision-language models, e.g., CoOp, has shown great success in adapting CLIP to different downstream tasks, making it a promising solution for federated learning due to computational reasons. Existing prompt learning techniques replace hand-crafted text prompts with learned vectors that offer improvements on seen classes, but struggle to generalize to unseen classes. Our work addresses this challenge by proposing Federated Text-driven Prompt Generation (FedTPG), which learns a unified prompt generation network across multiple remote clients in a scalable manner. The prompt generation network is conditioned on task-related text input, thus is context-aware, making it suitable to generalize for both seen and unseen classes. Our comprehensive empirical evaluations on nine diverse image classification datasets show that our method is superior to existing federated prompt learning methods, that achieve overall better generalization on both seen and unseen classes and is also generalizable to unseen datasets.\"\\n      },\\n      {\\n        \"paperId\": \"d6d6b2c88bee4004428435c7e070733709df71a6\",\\n        \"url\": \"https://www.semanticscholar.org/paper/d6d6b2c88bee4004428435c7e070733709df71a6\",\\n        \"title\": \"Socrates: Data Story Generation via Adaptive Machine-Guided Elicitation of User Feedback\",\\n        \"citationCount\": 16,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {\\'tldr\\'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/TVCG.2023.3327363?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/TVCG.2023.3327363, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": null,\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2023-11-03\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2153300135\",\\n            \"name\": \"Guande Wu\"\\n          },\\n          {\\n            \"authorId\": \"30518075\",\\n            \"name\": \"Shunan Guo\"\\n          },\\n          {\\n            \"authorId\": \"1890683\",\\n            \"name\": \"J. Hoffswell\"\\n          },\\n          {\\n            \"authorId\": \"51192588\",\\n            \"name\": \"G. Chan\"\\n          },\\n          {\\n            \"authorId\": \"2176092943\",\\n            \"name\": \"R. Rossi\"\\n          },\\n          {\\n            \"authorId\": \"2176108906\",\\n            \"name\": \"E. Koh\"\\n          }\\n        ],\\n        \"abstract\": \"Visual data stories can effectively convey insights from data, yet their creation often necessitates intricate data exploration, insight discovery, narrative organization, and customization to meet the communication objectives of the storyteller. Existing automated data storytelling techniques, however, tend to overlook the importance of user customization during the data story authoring process, limiting the system\\'s ability to create tailored narratives that reflect the user\\'s intentions. We present a novel data story generation workflow that leverages adaptive machine-guided elicitation of user feedback to customize the story. Our approach employs an adaptive plug-in module for existing story generation systems, which incorporates user feedback through interactive questioning based on the conversation history and dataset. This adaptability refines the system\\'s understanding of the user\\'s intentions, ensuring the final narrative aligns with their goals. We demonstrate the feasibility of our approach through the implementation of an interactive prototype: Socrates. Through a quantitative user study with 18 participants that compares our method to a state-of-the-art data story generation algorithm, we show that Socrates produces more relevant stories with a larger overlap of insights compared to human-generated stories. We also demonstrate the usability of Socrates via interviews with three data analysts and highlight areas of future work.\"\\n      },\\n      {\\n        \"paperId\": \"57fca8aba9760ed8001be7ad074554ee4515d258\",\\n        \"url\": \"https://www.semanticscholar.org/paper/57fca8aba9760ed8001be7ad074554ee4515d258\",\\n        \"title\": \"Adaptive Radiotherapy: Next-Generation Radiotherapy\",\\n        \"citationCount\": 54,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://www.mdpi.com/2072-6694/16/6/1206/pdf?version=1710899842\",\\n          \"status\": \"GOLD\",\\n          \"license\": \"CCBY\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC10968833, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"Adaptive radiotherapy (ART), a notable innovation that addresses anatomy changes and optimizes the therapeutic ratio, is introduced, highlighting the ongoing development of imaging technologies and AI and emphasizing their contribution to enhancing the applicability and effectiveness of ART.\"\\n        },\\n        \"publicationTypes\": [\\n          \"Review\",\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-03-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2283275224\",\\n            \"name\": \"Olga Dona Lemus\"\\n          },\\n          {\\n            \"authorId\": \"2251160990\",\\n            \"name\": \"Minsong Cao\"\\n          },\\n          {\\n            \"authorId\": \"2054380388\",\\n            \"name\": \"Bin Cai\"\\n          },\\n          {\\n            \"authorId\": \"2292572199\",\\n            \"name\": \"Michael Cummings\"\\n          },\\n          {\\n            \"authorId\": \"2248857176\",\\n            \"name\": \"Dandan Zheng\"\\n          }\\n        ],\\n        \"abstract\": \"Simple Summary Radiotherapy, a crucial cancer treatment, has evolved significantly over the years. Traditionally, treatment plans were based on initial scans used throughout the treatment course, accounting for changes in the patient\\\\u2019s anatomy by additional margins to targets. However, the field has moved towards decreasing margins with the advancement of delivery and targeting accuracy in order to decrease toxicity, and the increasing use of image guidance has illuminated patient anatomical changes such as organ deformation, weight loss, tumor shrinkage, and even biological changes that are unaccounted for by the conventional approach. Adaptive radiotherapy (ART) addresses this by adjusting treatment plans according to these changes. ART can be conducted in two ways: online (adjustments made during treatment sessions) and offline (adjustments made between treatment sessions). Advances in technology, especially in medical imaging (CT, MRI, and PET scans) and artificial intelligence, have made ART more feasible and efficient. ART offers more precise cancer treatment by adapting to changes in the patient\\\\u2019s body, leading to better outcomes with fewer side effects. Abstract Radiotherapy, a crucial technique in cancer therapy, has traditionally relied on the premise of largely unchanging patient anatomy during the treatment course and encompassing uncertainties by target margins. This review introduces adaptive radiotherapy (ART), a notable innovation that addresses anatomy changes and optimizes the therapeutic ratio. ART utilizes advanced imaging techniques such as CT, MRI, and PET to modify the treatment plan based on observed anatomical changes and even biological changes during the course of treatment. The narrative review provides a comprehensive guide on ART for healthcare professionals and trainees in radiation oncology and anyone else interested in the topic. The incorporation of artificial intelligence in ART has played a crucial role in improving effectiveness, particularly in contour segmentation, treatment planning, and quality assurance. This has expedited the process to render online ART feasible, lowered the burden for radiation oncology practitioners, and enhanced the precision of dynamically personalized treatment. Current technical and clinical progress on ART is discussed in this review, highlighting the ongoing development of imaging technologies and AI and emphasizing their contribution to enhancing the applicability and effectiveness of ART.\"\\n      },\\n      {\\n        \"paperId\": \"62d4fcfb2f6717d4eb562f4e51f37004fc5d7109\",\\n        \"url\": \"https://www.semanticscholar.org/paper/62d4fcfb2f6717d4eb562f4e51f37004fc5d7109\",\\n        \"title\": \"Adaptive Inference-Time Compute: LLMs Can Predict if They Can Do Better, Even Mid-Generation\",\\n        \"citationCount\": 47,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2410.02725, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A new generative self-evaluation scheme designed to adaptively reduce the number of generated samples while maintaining or even improving performance and enabling more efficient and scalable compute utilization during inference for LLMs is introduced.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-10-03\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2161717022\",\\n            \"name\": \"Rohin Manvi\"\\n          },\\n          {\\n            \"authorId\": \"2324336057\",\\n            \"name\": \"Anikait Singh\"\\n          },\\n          {\\n            \"authorId\": \"2269095529\",\\n            \"name\": \"Stefano Ermon\"\\n          }\\n        ],\\n        \"abstract\": \"Inference-time computation is a powerful paradigm to enhance the performance of large language models (LLMs), with Best-of-N sampling being a widely used technique. However, this method is computationally expensive, requiring both (1) an external reward model and (2) the generation of multiple samples. In this work, we introduce a new generative self-evaluation scheme designed to adaptively reduce the number of generated samples while maintaining or even improving performance. We use a generative reward model formulation, allowing the LLM to predict mid-generation the probability that restarting the generation will yield a better response. These predictions are obtained without an external reward model and can be used to decide whether or not to generate more samples, prune unpromising samples early on, or to pick the best sample. This capability is very inexpensive as it involves generating a single predefined token. Trained using a dataset constructed with real unfiltered LMSYS user prompts, Llama 3.1 8B\\'s win rate against GPT-4 on AlpacaEval increases from 21% to 34% with 16 samples and math performance on GSM8K improves from 84% to 91%. By sampling only when the LLM determines that it is beneficial to do so and adaptively adjusting temperature annealing, we demonstrate that 74% of the improvement from using 16 samples can be achieved with only 1.2 samples on average. We further demonstrate that 50-75% of samples can be pruned early in generation with minimal degradation in performance. Overall, our methods enable more efficient and scalable compute utilization during inference for LLMs.\"\\n      },\\n      {\\n        \"paperId\": \"c0e24f98323c7114b9229ac17b8b63581e3e5914\",\\n        \"url\": \"https://www.semanticscholar.org/paper/c0e24f98323c7114b9229ac17b8b63581e3e5914\",\\n        \"title\": \"Dynamic and Adaptive Feature Generation with LLM\",\\n        \"citationCount\": 29,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2406.03505, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This research introduces a novel approach adopting large language models and feature-generating prompts to address challenges of explainability, applicability, and inflexible strategy and proposes a dynamic and adaptive feature generation method that enhances the interpretability of the feature generation process.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-06-04\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2130031226\",\\n            \"name\": \"XinHao Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2108045855\",\\n            \"name\": \"Jinghan Zhang\"\\n          },\\n          {\\n            \"authorId\": \"1966492\",\\n            \"name\": \"Banafsheh Rekabdar\"\\n          },\\n          {\\n            \"authorId\": \"2145108199\",\\n            \"name\": \"Yuanchun Zhou\"\\n          },\\n          {\\n            \"authorId\": \"2301248160\",\\n            \"name\": \"Pengfei Wang\"\\n          },\\n          {\\n            \"authorId\": \"2293571072\",\\n            \"name\": \"Kunpeng Liu\"\\n          }\\n        ],\\n        \"abstract\": \"The representation of feature space is a crucial environment where data points get vectorized and embedded for subsequent modeling. Thus, the efficacy of machine learning (ML) algorithms is closely related to the quality of feature engineering. As one of the most important techniques, feature generation transforms raw data into an optimized feature space conducive to model training and further refines the space. Despite the advancements in automated feature engineering and feature generation, current methodologies often suffer from three fundamental issues: lack of explainability, limited applicability, and inflexible strategy. These shortcomings frequently hinder and limit the deployment of ML models across varied scenarios. Our research introduces a novel approach adopting large language models (LLMs) and feature-generating prompts to address these challenges. We propose a dynamic and adaptive feature generation method that enhances the interpretability of the feature generation process. Our approach broadens the applicability across various data types and tasks and offers advantages in terms of strategic flexibility. A broad range of experiments showcases that our approach is significantly superior to existing methods.\"\\n      },\\n      {\\n        \"paperId\": \"a314b4c385e9732794a48d1d34f637b13245c71d\",\\n        \"url\": \"https://www.semanticscholar.org/paper/a314b4c385e9732794a48d1d34f637b13245c71d\",\\n        \"title\": \"RetrievalQA: Assessing Adaptive Retrieval-Augmented Generation for Short-form Open-Domain Question Answering\",\\n        \"citationCount\": 21,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2402.16457, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work presents a benchmark, RetrievalQA, comprising 1,271 short-form questions covering new world and long-tail knowledge, and proposes Time-Aware Adaptive Retrieval (TA-ARE), a simple yet effective method that helps LLMs assess the necessity of retrieval without calibration or additional training.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-02-26\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2144370264\",\\n            \"name\": \"Zihan Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2257039084\",\\n            \"name\": \"Meng Fang\"\\n          },\\n          {\\n            \"authorId\": \"2261455899\",\\n            \"name\": \"Ling Chen\"\\n          }\\n        ],\\n        \"abstract\": \"Adaptive retrieval-augmented generation (ARAG) aims to dynamically determine the necessity of retrieval for queries instead of retrieving indiscriminately to enhance the efficiency and relevance of the sourced information. However, previous works largely overlook the evaluation of ARAG approaches, leading to their effectiveness being understudied. This work presents a benchmark, RetrievalQA, comprising 1,271 short-form questions covering new world and long-tail knowledge. The knowledge necessary to answer the questions is absent from LLMs; therefore, external information must be retrieved to answer correctly. This makes RetrievalQA a suitable testbed to evaluate existing ARAG methods. We observe that calibration-based methods heavily rely on threshold tuning, while vanilla prompting is inadequate for guiding LLMs to make reliable retrieval decisions. Based on our findings, we propose Time-Aware Adaptive Retrieval (TA-ARE), a simple yet effective method that helps LLMs assess the necessity of retrieval without calibration or additional training. The dataset and code will be available at https://github.com/hyintell/RetrievalQA\"\\n      },\\n      {\\n        \"paperId\": \"c958736c08a86816c588fd8fe12f39cec8a64bf8\",\\n        \"url\": \"https://www.semanticscholar.org/paper/c958736c08a86816c588fd8fe12f39cec8a64bf8\",\\n        \"title\": \"SGDM: An Adaptive Style-Guided Diffusion Model for Personalized Text to Image Generation\",\\n        \"citationCount\": 21,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {\\'tldr\\'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/TMM.2024.3399075?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/TMM.2024.3399075, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": null,\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": null,\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2300989129\",\\n            \"name\": \"Yifei Xu\"\\n          },\\n          {\\n            \"authorId\": \"2238891797\",\\n            \"name\": \"Xiaolong Xu\"\\n          },\\n          {\\n            \"authorId\": \"2112666114\",\\n            \"name\": \"Honghao Gao\"\\n          },\\n          {\\n            \"authorId\": \"2166050115\",\\n            \"name\": \"Fu Xiao\"\\n          }\\n        ],\\n        \"abstract\": \"The existing personalized text-to-image generation models face issues such as repeated training and insufficient generalization capabilities. We present an adaptive Style-Guided Diffusion Model (SGDM). When provided with a set of stylistically consistent images and prompts as inputs, SGDM can generate images that align with the prompts while maintaining style consistency with the input images. SGDM first extracts features from the input style image and then combines style features from different depths. Last, style features are injected into the noise generation process of the original Stable Diffusion (SD) model by the style-guided module we propose. This strategy fully leverages the generative and generalization capabilities of the pre-trained text-to-image model to ensure the accuracy of the generated image\\'s content. We present a dataset construction method suitable for style personalized generation tasks of this kind, enabling the trained model to generate stylized images adaptively instead of re-training for each style. We also present an evaluation metric, StySim, to measure the style similarity between two images, and this metric shows that the style personalization capability of SGDM is the best. And metrics such as FID, KID, and CLIPSIM indicate that SGDM maintains good performance in text-to-image generation.\"\\n      },\\n      {\\n        \"paperId\": \"cf56a7b28fb27279b1c94fb920b5722cf50c8852\",\\n        \"url\": \"https://www.semanticscholar.org/paper/cf56a7b28fb27279b1c94fb920b5722cf50c8852\",\\n        \"title\": \"AdvPrompter: Fast Adaptive Adversarial Prompting for LLMs\",\\n        \"citationCount\": 113,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2404.16873, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A novel method that uses another LLM, called AdvPrompter, to generate human-readable adversarial prompts in seconds, and shows that training on adversarial suffixes generated by AdvPrompter is a promising strategy for improving the robustness of LLMs to jailbreaking attacks.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-04-21\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"1443432623\",\\n            \"name\": \"Anselm Paulus\"\\n          },\\n          {\\n            \"authorId\": \"3461866\",\\n            \"name\": \"Arman Zharmagambetov\"\\n          },\\n          {\\n            \"authorId\": \"2298951327\",\\n            \"name\": \"Chuan Guo\"\\n          },\\n          {\\n            \"authorId\": \"2298758184\",\\n            \"name\": \"Brandon Amos\"\\n          },\\n          {\\n            \"authorId\": \"2253746559\",\\n            \"name\": \"Yuandong Tian\"\\n          }\\n        ],\\n        \"abstract\": \"Large Language Models (LLMs) are vulnerable to jailbreaking attacks that lead to generation of inappropriate or harmful content. Manual red-teaming requires a time-consuming search for adversarial prompts, whereas automatic adversarial prompt generation often leads to semantically meaningless attacks that do not scale well. In this paper, we present a novel method that uses another LLM, called AdvPrompter, to generate human-readable adversarial prompts in seconds. AdvPrompter, which is trained using an alternating optimization algorithm, generates suffixes that veil the input instruction without changing its meaning, such that the TargetLLM is lured to give a harmful response. Experimental results on popular open source TargetLLMs show highly competitive results on the AdvBench and HarmBench datasets, that also transfer to closed-source black-box LLMs. We also show that training on adversarial suffixes generated by AdvPrompter is a promising strategy for improving the robustness of LLMs to jailbreaking attacks.\"\\n      },\\n      {\\n        \"paperId\": \"1a41d449f2ed4051ac4c763fb8b1b07357eaf5f4\",\\n        \"url\": \"https://www.semanticscholar.org/paper/1a41d449f2ed4051ac4c763fb8b1b07357eaf5f4\",\\n        \"title\": \"WavLLM: Towards Robust and Adaptive Speech Large Language Model\",\\n        \"citationCount\": 104,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2404.00656, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"WavLLM is introduced, a robust and adaptive speech large language model with dual encoders, and a prompt-aware LoRA weight adapter, optimized by a two-stage curriculum learning approach, exhibiting robust generalization capabilities in executing complex tasks using CoT approach.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-03-31\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2277450543\",\\n            \"name\": \"Shujie Hu\"\\n          },\\n          {\\n            \"authorId\": \"2135918679\",\\n            \"name\": \"Long Zhou\"\\n          },\\n          {\\n            \"authorId\": \"2107983441\",\\n            \"name\": \"Shujie Liu\"\\n          },\\n          {\\n            \"authorId\": \"2107970655\",\\n            \"name\": \"Sanyuan Chen\"\\n          },\\n          {\\n            \"authorId\": \"2294360053\",\\n            \"name\": \"Hongkun Hao\"\\n          },\\n          {\\n            \"authorId\": \"2258308585\",\\n            \"name\": \"Jing Pan\"\\n          },\\n          {\\n            \"authorId\": \"2274190703\",\\n            \"name\": \"Xunying Liu\"\\n          },\\n          {\\n            \"authorId\": \"2280887661\",\\n            \"name\": \"Jinyu Li\"\\n          },\\n          {\\n            \"authorId\": \"9075412\",\\n            \"name\": \"S. Sivasankaran\"\\n          },\\n          {\\n            \"authorId\": \"2294832157\",\\n            \"name\": \"Linquan Liu\"\\n          },\\n          {\\n            \"authorId\": \"2277299355\",\\n            \"name\": \"Furu Wei\"\\n          }\\n        ],\\n        \"abstract\": \"The recent advancements in large language models (LLMs) have revolutionized the field of natural language processing, progressively broadening their scope to multimodal perception and generation. However, effectively integrating listening capabilities into LLMs poses significant challenges, particularly with respect to generalizing across varied contexts and executing complex auditory tasks. In this work, we introduce WavLLM, a robust and adaptive speech large language model with dual encoders, and a prompt-aware LoRA weight adapter, optimized by a two-stage curriculum learning approach. Leveraging dual encoders, we decouple different types of speech information, utilizing a Whisper encoder to process the semantic content of speech, and a WavLM encoder to capture the unique characteristics of the speaker\\'s identity. Within the curriculum learning framework, WavLLM first builds its foundational capabilities by optimizing on mixed elementary single tasks, followed by advanced multi-task training on more complex tasks such as combinations of the elementary tasks. To enhance the flexibility and adherence to different tasks and instructions, a prompt-aware LoRA weight adapter is introduced in the second advanced multi-task training stage. We validate the proposed model on universal speech benchmarks including tasks such as ASR, ST, SV, ER, and also apply it to specialized datasets like Gaokao English listening comprehension set for SQA, and speech Chain-of-Thought (CoT) evaluation set. Experiments demonstrate that the proposed model achieves state-of-the-art performance across a range of speech tasks on the same model size, exhibiting robust generalization capabilities in executing complex tasks using CoT approach. Furthermore, our model successfully completes Gaokao tasks without specialized training. The codes, models, audio, and Gaokao evaluation set can be accessed at \\\\\\\\url{aka.ms/wavllm}.\"\\n      },\\n      {\\n        \"paperId\": \"5cacb35e7bd86e64e5aca126e5011b64630007d8\",\\n        \"url\": \"https://www.semanticscholar.org/paper/5cacb35e7bd86e64e5aca126e5011b64630007d8\",\\n        \"title\": \"OmniAvatar: Efficient Audio-Driven Avatar Video Generation with Adaptive Body Animation\",\\n        \"citationCount\": 12,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2506.18866, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"OmniAvatar is introduced, an innovative audio-driven full-body video generation model that enhances human animation with improved lip-sync accuracy and natural movements, and introduces a pixel-wise multi-hierarchical audio embedding strategy to better capture audio features in the latent space.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2025-06-23\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2293272991\",\\n            \"name\": \"Qijun Gan\"\\n          },\\n          {\\n            \"authorId\": \"2294510159\",\\n            \"name\": \"Ruizi Yang\"\\n          },\\n          {\\n            \"authorId\": \"2314648986\",\\n            \"name\": \"Jianke Zhu\"\\n          },\\n          {\\n            \"authorId\": \"2370937157\",\\n            \"name\": \"Shaofei Xue\"\\n          },\\n          {\\n            \"authorId\": \"2370937932\",\\n            \"name\": \"Steven Hoi\"\\n          }\\n        ],\\n        \"abstract\": \"Significant progress has been made in audio-driven human animation, while most existing methods focus mainly on facial movements, limiting their ability to create full-body animations with natural synchronization and fluidity. They also struggle with precise prompt control for fine-grained generation. To tackle these challenges, we introduce OmniAvatar, an innovative audio-driven full-body video generation model that enhances human animation with improved lip-sync accuracy and natural movements. OmniAvatar introduces a pixel-wise multi-hierarchical audio embedding strategy to better capture audio features in the latent space, enhancing lip-syncing across diverse scenes. To preserve the capability for prompt-driven control of foundation models while effectively incorporating audio features, we employ a LoRA-based training approach. Extensive experiments show that OmniAvatar surpasses existing models in both facial and semi-body video generation, offering precise text-based control for creating videos in various domains, such as podcasts, human interactions, dynamic scenes, and singing. Our project page is https://omni-avatar.github.io/.\"\\n      },\\n      {\\n        \"paperId\": \"e8a897a12ce666a898582c3454a3e49acae40422\",\\n        \"url\": \"https://www.semanticscholar.org/paper/e8a897a12ce666a898582c3454a3e49acae40422\",\\n        \"title\": \"SceneCraft: Automating Interactive Narrative Scene Generation in Digital Games with Large Language Models\",\\n        \"citationCount\": 56,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://ojs.aaai.org/index.php/AIIDE/article/download/27504/27277\",\\n          \"status\": \"BRONZE\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1609/aiide.v19i1.27504?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1609/aiide.v19i1.27504, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper proposes SCENECRAFT, a narrative scene generation framework that automates NPC interaction crucial to unfolding plot events and demonstrates its effectiveness in creating narrative experiences based on creativity, adaptability, and alignment with intended author instructions.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2023-10-06\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2071942775\",\\n            \"name\": \"Vikram Kumaran\"\\n          },\\n          {\\n            \"authorId\": \"2256208513\",\\n            \"name\": \"Jonathan Rowe\"\\n          },\\n          {\\n            \"authorId\": \"9808011\",\\n            \"name\": \"Bradford W. Mott\"\\n          },\\n          {\\n            \"authorId\": \"2238471264\",\\n            \"name\": \"James C. Lester\"\\n          }\\n        ],\\n        \"abstract\": \"Creating engaging interactive story-based experiences dynamically responding to individual player choices poses significant challenges for narrative-centered games. Recent advances in pre-trained large language models (LLMs) have the potential to revolutionize procedural content generation for narrative-centered games. Historically, interactive narrative generation has specified pivotal events in the storyline, often utilizing planning-based approaches toward achieving narrative coherence and maintaining the story arc. However, manual authorship is typically used to create detail and variety in non-player character (NPC) interaction to specify and instantiate plot events. This paper proposes SCENECRAFT, a narrative scene generation framework that automates NPC interaction crucial to unfolding plot events. SCENECRAFT interprets natural language instructions about scene objectives, NPC traits, location, and narrative variations. It then employs large language models to generate game scenes aligned with authorial intent. It generates branching conversation paths that adapt to player choices while adhering to the author\\\\u2019s interaction goals. LLMs generate interaction scripts, semantically extract character emotions and gestures to align with the script, and convert dialogues into a game scripting language. The generated script can then be played utilizing an existing narrative-centered game framework. Through empirical evaluation using automated and human assessments, we demonstrate SCENECRAFT\\\\u2019s effectiveness in creating narrative experiences based on creativity, adaptability, and alignment with intended author instructions.\"\\n      },\\n      {\\n        \"paperId\": \"d599dc40c9cb8d6d76554ee7d21d20c22cc7cdb5\",\\n        \"url\": \"https://www.semanticscholar.org/paper/d599dc40c9cb8d6d76554ee7d21d20c22cc7cdb5\",\\n        \"title\": \"ConsistI2V: Enhancing Visual Consistency for Image-to-Video Generation\",\\n        \"citationCount\": 99,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2402.04324, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work proposes ConsistI2V, a diffusion-based method to enhance visual consistency for I2V generation that introduces spatiotemporal attention over the first frame to maintain spatial and motion consistency and noise initialization from the low-frequency band of the first frame to enhance layout consistency.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-02-06\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2268493042\",\\n            \"name\": \"Weiming Ren\"\\n          },\\n          {\\n            \"authorId\": \"2283183497\",\\n            \"name\": \"Harry Yang\"\\n          },\\n          {\\n            \"authorId\": \"2143853895\",\\n            \"name\": \"Ge Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2268683835\",\\n            \"name\": \"Cong Wei\"\\n          },\\n          {\\n            \"authorId\": \"2279346001\",\\n            \"name\": \"Xinrun Du\"\\n          },\\n          {\\n            \"authorId\": \"2283188391\",\\n            \"name\": \"Stephen W. Huang\"\\n          },\\n          {\\n            \"authorId\": \"2253811180\",\\n            \"name\": \"Wenhu Chen\"\\n          }\\n        ],\\n        \"abstract\": \"Image-to-video (I2V) generation aims to use the initial frame (alongside a text prompt) to create a video sequence. A grand challenge in I2V generation is to maintain visual consistency throughout the video: existing methods often struggle to preserve the integrity of the subject, background, and style from the first frame, as well as ensure a fluid and logical progression within the video narrative. To mitigate these issues, we propose ConsistI2V, a diffusion-based method to enhance visual consistency for I2V generation. Specifically, we introduce (1) spatiotemporal attention over the first frame to maintain spatial and motion consistency, (2) noise initialization from the low-frequency band of the first frame to enhance layout consistency. These two approaches enable ConsistI2V to generate highly consistent videos. We also extend the proposed approaches to show their potential to improve consistency in auto-regressive long video generation and camera motion control. To verify the effectiveness of our method, we propose I2V-Bench, a comprehensive evaluation benchmark for I2V generation. Our automatic and human evaluation results demonstrate the superiority of ConsistI2V over existing methods.\"\\n      },\\n      {\\n        \"paperId\": \"f00a8162648403f32c9dfb937b9220b234c82531\",\\n        \"url\": \"https://www.semanticscholar.org/paper/f00a8162648403f32c9dfb937b9220b234c82531\",\\n        \"title\": \"viz2viz: Prompt-driven stylized visualization generation using a diffusion model\",\\n        \"citationCount\": 13,\\n        \"openAccessPdf\": {\\n          \"url\": \"http://arxiv.org/pdf/2304.01919\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2304.01919, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": null\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2023-04-04\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2305625074\",\\n            \"name\": \"Jiaqi Wu\"\\n          },\\n          {\\n            \"authorId\": \"152836325\",\\n            \"name\": \"John Joon Young Chung\"\\n          },\\n          {\\n            \"authorId\": \"2630700\",\\n            \"name\": \"Eytan Adar\"\\n          }\\n        ],\\n        \"abstract\": \"Creating stylized visualization requires going beyond the limited, abstract, geometric marks produced by most tools. Rather, the designer builds stylized idioms where the marks are both transformed (e.g., photographs of candles instead of bars) and also synthesized into a \\'scene\\' that pushes the boundaries of traditional visualizations. To support this, we introduce viz2viz, a system for transforming visualizations with a textual prompt to a stylized form. The system follows a high-level recipe that leverages various generative methods to produce new visualizations that retain the properties of the original dataset. While the base recipe is consistent across many visualization types, we demonstrate how it can be specifically adapted to the creation of different visualization types (bar charts, area charts, pie charts, and network visualizations). Our approach introduces techniques for using different prompts for different marks (i.e., each bar can be something completely different) while still retaining image\\\\\"coherence.\\\\\"We conclude with an evaluation of the approach and discussion on extensions and limitations.\"\\n      },\\n      {\\n        \"paperId\": \"80b8d03f910bb252cb251f7f74ceaaf8f4c4aabe\",\\n        \"url\": \"https://www.semanticscholar.org/paper/80b8d03f910bb252cb251f7f74ceaaf8f4c4aabe\",\\n        \"title\": \"Adaptive Test Generation Using a Large Language Model\",\\n        \"citationCount\": 69,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://arxiv.org/pdf/2302.06527\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {\\'abstract\\'}. Paper or abstract available at https://api.unpaywall.org/v2/10.48550/arXiv.2302.06527?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.48550/arXiv.2302.06527, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"T EST P ILOT uses Codex, an off-the-shelf LLM, to automatically generate unit tests for a given program without requiring additional training or few-shot learning on examples of existing tests, and does not generate memorized tests.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": null,\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2257193192\",\\n            \"name\": \"Max Sch\\\\u00e4fer\"\\n          },\\n          {\\n            \"authorId\": \"2317114000\",\\n            \"name\": \"Sarah Nadi\"\\n          },\\n          {\\n            \"authorId\": \"46235560\",\\n            \"name\": \"A. Eghbali\"\\n          },\\n          {\\n            \"authorId\": \"2257181137\",\\n            \"name\": \"Frank Tip\"\\n          }\\n        ],\\n        \"abstract\": null\\n      },\\n      {\\n        \"paperId\": \"9038f40c43e7d62d8f1dc4819093083090911f7a\",\\n        \"url\": \"https://www.semanticscholar.org/paper/9038f40c43e7d62d8f1dc4819093083090911f7a\",\\n        \"title\": \"Novelty Controlled Paraphrase Generation with Retrieval Augmented Conditional Prompt Tuning\",\\n        \"citationCount\": 39,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://ojs.aaai.org/index.php/AAAI/article/download/21297/21046\",\\n          \"status\": \"GOLD\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2202.00535, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"The effectiveness of the proposed approaches for retaining the semantic content of the original text while inducing lexical novelty in the generation of paraphrase generation is demonstrated.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2022-02-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"123467107\",\\n            \"name\": \"Jishnu Ray Chowdhury\"\\n          },\\n          {\\n            \"authorId\": \"2152482425\",\\n            \"name\": \"Yong Zhuang\"\\n          },\\n          {\\n            \"authorId\": \"2152375661\",\\n            \"name\": \"Shuyi Wang\"\\n          }\\n        ],\\n        \"abstract\": \"Paraphrase generation is a fundamental and long-standing task in natural language processing. In this paper, we concentrate on two contributions to the task: (1) we propose Retrieval Augmented Prompt Tuning (RAPT) as a parameter-efficient method to adapt large pre-trained language models for paraphrase generation; (2) we propose Novelty Conditioned RAPT (NC-RAPT) as a simple model-agnostic method of using specialized prompt tokens for controlled paraphrase generation with varying levels of lexical novelty. By conducting extensive experiments on four datasets, we demonstrate the effectiveness of the proposed approaches for retaining the semantic content of the original text while inducing lexical novelty in the generation.\"\\n      },\\n      {\\n        \"paperId\": \"df19bd0bb788474a578fac33d6cff9867af3eead\",\\n        \"url\": \"https://www.semanticscholar.org/paper/df19bd0bb788474a578fac33d6cff9867af3eead\",\\n        \"title\": \"FABULA: Intelligence Report Generation Using Retrieval-Augmented Narrative Construction\",\\n        \"citationCount\": 16,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://dl.acm.org/doi/pdf/10.1145/3625007.3627505\",\\n          \"status\": \"BRONZE\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2310.13848, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A Retrieval Augmented Generation (RAG) approach to augment prompting of an autoregressive decoder by retrieving structured information asserted in a knowledge graph to generate targeted information based on a narrative plot model is introduced.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Book\"\\n        ],\\n        \"publicationDate\": \"2023-10-20\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"51092885\",\\n            \"name\": \"P. Ranade\"\\n          },\\n          {\\n            \"authorId\": \"2269936199\",\\n            \"name\": \"Anupam Joshi\"\\n          }\\n        ],\\n        \"abstract\": \"Narrative construction is the process of representing disparate event information into a logical plot structure that models an end to end story. Intelligence analysis is an example of a domain that can benefit tremendously from narrative construction techniques, particularly in aiding analysts during the largely manual and costly process of synthesizing event information into comprehensive intelligence reports. Manual intelligence report generation is often prone to challenges such as integrating dynamic event information, writing fine-grained queries, and closing information gaps. This motivates the development of a system that retrieves and represents critical aspects of events in a form that aids in automatic generation of intelligence reports. We introduce a Retrieval Augmented Generation (RAG) approach to augment prompting of an autoregressive decoder by retrieving structured information asserted in a knowledge graph to generate targeted information based on a narrative plot model. We apply our approach to the problem of neural intelligence report generation and introduce FABULA, framework to augment intelligence analysis workflows using RAG. An analyst can use FABULA to query an Event Plot Graph (EPG) to retrieve relevant event plot points, which can be used to augment prompting of a Large Language Model (LLM) during intelligence report generation. Our evaluation studies show that the plot points included in the generated intelligence reports have high semantic relevance, high coherency, and low data redundancy.\"\\n      },\\n      {\\n        \"paperId\": \"bb2ef694e8b5a99e1f7ceb014968b4d1dc2e122a\",\\n        \"url\": \"https://www.semanticscholar.org/paper/bb2ef694e8b5a99e1f7ceb014968b4d1dc2e122a\",\\n        \"title\": \"EIPE-text: Evaluation-Guided Iterative Plan Extraction for Long-Form Narrative Text Generation\",\\n        \"citationCount\": 11,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://arxiv.org/pdf/2310.08185\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2310.08185, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A new framework called Evaluation-guided Iterative Plan Extraction for long-form narrative text generation (EIPE-text), which extracts plans from the corpus of narratives and utilizes the extracted plans to construct a better planner.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2023-10-12\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2214585980\",\\n            \"name\": \"Wang You\"\\n          },\\n          {\\n            \"authorId\": \"51198241\",\\n            \"name\": \"Wenshan Wu\"\\n          },\\n          {\\n            \"authorId\": \"3887469\",\\n            \"name\": \"Yaobo Liang\"\\n          },\\n          {\\n            \"authorId\": \"35374367\",\\n            \"name\": \"Shaoguang Mao\"\\n          },\\n          {\\n            \"authorId\": \"2151101534\",\\n            \"name\": \"Chenfei Wu\"\\n          },\\n          {\\n            \"authorId\": \"2257345975\",\\n            \"name\": \"Maosong Cao\"\\n          },\\n          {\\n            \"authorId\": \"2257125576\",\\n            \"name\": \"Yuzhe Cai\"\\n          },\\n          {\\n            \"authorId\": \"2214448244\",\\n            \"name\": \"Yiduo Guo\"\\n          },\\n          {\\n            \"authorId\": \"2258547658\",\\n            \"name\": \"Yan Xia\"\\n          },\\n          {\\n            \"authorId\": \"2249539478\",\\n            \"name\": \"Furu Wei\"\\n          },\\n          {\\n            \"authorId\": \"2072609829\",\\n            \"name\": \"Nan Duan\"\\n          }\\n        ],\\n        \"abstract\": \"Plan-and-Write is a common hierarchical approach in long-form narrative text generation, which first creates a plan to guide the narrative writing. Following this approach, several studies rely on simply prompting large language models for planning, which often yields suboptimal results. In this paper, we propose a new framework called Evaluation-guided Iterative Plan Extraction for long-form narrative text generation (EIPE-text), which extracts plans from the corpus of narratives and utilizes the extracted plans to construct a better planner. EIPE-text has three stages: plan extraction, learning, and inference. In the plan extraction stage, it iteratively extracts and improves plans from the narrative corpus and constructs a plan corpus. We propose a question answer (QA) based evaluation mechanism to automatically evaluate the plans and generate detailed plan refinement instructions to guide the iterative improvement. In the learning stage, we build a better planner by fine-tuning with the plan corpus or in-context learning with examples in the plan corpus. Finally, we leverage a hierarchical approach to generate long-form narratives. We evaluate the effectiveness of EIPE-text in the domains of novels and storytelling. Both GPT-4-based evaluations and human evaluations demonstrate that our method can generate more coherent and relevant long-form narratives. Our code will be released in the future.\"\\n      },\\n      {\\n        \"paperId\": \"02747bc59ddf832b0b7d04a1f491d9fa519066fe\",\\n        \"url\": \"https://www.semanticscholar.org/paper/02747bc59ddf832b0b7d04a1f491d9fa519066fe\",\\n        \"title\": \"PANGeA: Procedural Artificial Narrative Using Generative AI for Turn-Based, Role-Playing Video Games\",\\n        \"citationCount\": 15,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://ojs.aaai.org/index.php/AIIDE/article/download/31876/34043\",\\n          \"status\": \"BRONZE\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1609/aiide.v20i1.31876?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1609/aiide.v20i1.31876, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"PANGeA can help game designers generate narrative-consistent content while leveraging LLMs of different sizes, suitable for various devices, and is supported by a novel validation system for handling free-form text input during game development and gameplay.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-11-15\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2286339849\",\\n            \"name\": \"Stephanie Buongiorno\"\\n          },\\n          {\\n            \"authorId\": \"2132867162\",\\n            \"name\": \"Lawrence J. Klinkert\"\\n          },\\n          {\\n            \"authorId\": \"2298971047\",\\n            \"name\": \"Zixin Zhuang\"\\n          },\\n          {\\n            \"authorId\": \"2298969170\",\\n            \"name\": \"Tanishq Chawla\"\\n          },\\n          {\\n            \"authorId\": \"2299097774\",\\n            \"name\": \"Corey Clark\"\\n          }\\n        ],\\n        \"abstract\": \"Large language models (LLMs) offer unprecedented flexibility in procedural generation, enabling the creation of dynamic video game storylines that evolve with user input. A critical aspect of realizing this potential is allowing players and developers to provide dynamic or free-form text to drive generation. Ingesting free-form text for a video game poses challenges, however, as it can prompt the LLM to generate content beyond the intended narrative scope. In response to this challenge, this research introduces Procedural Artificial Narrative using Generative AI (PANGeA) for leveraging LLMs to create narrative content for turn-based, role-playing games (RPGs). PANGeA is an approach comprised of components including a memory system, validation system, a Unity game engine plug-in, and a server with a RESTful interface that enables connecting PANGeA components with any game engine as well as accessing local and private LLMs. PANGeA procedurally generates level data like setting, key items, non-playable characters (NPCs)), and dialogue based on a set of configuration and design rules provided by the game designer. This process is supported by a novel validation system for handling free-form text input during game development and gameplay, which aligns LLM generation with the narrative. It does this by evoking the LLM\\'s capabilities to dynamically evaluate the text input against game rules that reinforce the designer\\'s initial criteria. To enrich player-NPC interactions, PANGeA uses the Big Five Personality model to shape NPC responses. To explore its broad application, PANGeA is evaluated across two studies. First, this research presents a narrative test scenario of the prototype game, Dark Shadows, which was developed using PANGeA within the Unity game engine. This is followed by an ablation study that tests PANGeA\\'s performance across 10 different role-playing game scenarios\\\\u2013from western to science fiction\\\\u2013and across three model sizes: Llama-3 (8B), GPT-3.5, and GPT-4. These evaluations demonstrate that PANGeA\\'s NPCs can hold dynamic, narrative-consistent conversations that, without the memory system, would exceed the LLM\\'s context length. In addition, the results demonstrate PANGeA\\'s validation system not only aligns LLM responses with the game narrative but also improves the performance of Llama-3 (8B), enabling it to perform comparably to large-scale foundational models like GPT-4. With the validation system, Llama-3 (8B)\\'s performance improved from 28% accuracy to 98%, and GPT-4\\'s from 71% to 99%. These findings indicate PANGeA can help game designers generate narrative-consistent content while leveraging LLMs of different sizes, suitable for various devices.\"\\n      },\\n      {\\n        \"paperId\": \"22a0bfac8cc0cb9c01123d8a898e3235ddcab269\",\\n        \"url\": \"https://www.semanticscholar.org/paper/22a0bfac8cc0cb9c01123d8a898e3235ddcab269\",\\n        \"title\": \"Weigh Your Own Words: Improving Hate Speech Counter Narrative Generation via Attention Regularization\",\\n        \"citationCount\": 11,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://arxiv.org/pdf/2309.02311\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2309.02311, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"Novel attention regularization methodologies to improve the generalization capabilities of Pretrained Transformer-based Language Models for counter narratives generation and paves the way for better and more flexible counter-speech generation models.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2023-09-05\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2161343118\",\\n            \"name\": \"Helena Bonaldi\"\\n          },\\n          {\\n            \"authorId\": \"1481857041\",\\n            \"name\": \"Giuseppe Attanasio\"\\n          },\\n          {\\n            \"authorId\": \"2101317501\",\\n            \"name\": \"Debora Nozza\"\\n          },\\n          {\\n            \"authorId\": \"1912357\",\\n            \"name\": \"Marco Guerini\"\\n          }\\n        ],\\n        \"abstract\": \"Recent computational approaches for combating online hate speech involve the automatic generation of counter narratives by adapting Pretrained Transformer-based Language Models (PLMs) with human-curated data. This process, however, can produce in-domain overfitting, resulting in models generating acceptable narratives only for hatred similar to training data, with little portability to other targets or to real-world toxic language. This paper introduces novel attention regularization methodologies to improve the generalization capabilities of PLMs for counter narratives generation. Overfitting to training-specific terms is then discouraged, resulting in more diverse and richer narratives. We experiment with two attention-based regularization techniques on a benchmark English dataset. Regularized models produce better counter narratives than state-of-the-art approaches in most cases, both in terms of automatic metrics and human evaluation, especially when hateful targets are not present in the training data. This work paves the way for better and more flexible counter-speech generation models, a task for which datasets are highly challenging to produce.\"\\n      },\\n      {\\n        \"paperId\": \"919e7ce11dd6d6605259b7d176d8cebaec9e7d06\",\\n        \"url\": \"https://www.semanticscholar.org/paper/919e7ce11dd6d6605259b7d176d8cebaec9e7d06\",\\n        \"title\": \"AI Illustrator: Translating Raw Descriptions into Images by Prompt-based Cross-Modal Generation\",\\n        \"citationCount\": 10,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://arxiv.org/pdf/2209.03160\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2209.03160, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A Prompt-based Cross-Modal Generation Framework (PCM-Frame) to leverage two powerful pre-trained models, including CLIP and StyleGAN, and conducts a user study to demonstrate its superiority over the competing methods of text-to-image translation with complicated semantics.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Book\"\\n        ],\\n        \"publicationDate\": \"2022-09-07\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"144987142\",\\n            \"name\": \"Y. Ma\"\\n          },\\n          {\\n            \"authorId\": \"46402216\",\\n            \"name\": \"Huan Yang\"\\n          },\\n          {\\n            \"authorId\": \"2127734772\",\\n            \"name\": \"Bei Liu\"\\n          },\\n          {\\n            \"authorId\": \"3247966\",\\n            \"name\": \"Jianlong Fu\"\\n          },\\n          {\\n            \"authorId\": \"2168547810\",\\n            \"name\": \"Jiaying Liu\"\\n          }\\n        ],\\n        \"abstract\": \"AI illustrator aims to automatically design visually appealing images for books to provoke rich thoughts and emotions. To achieve this goal, we propose a framework for translating raw descriptions with complex semantics into semantically corresponding images. The main challenge lies in the complexity of the semantics of raw descriptions, which may be hard to be visualized e.g., \\\\\"gloomy\\\\\" or \\\\\"Asian\\\\\"). It usually poses challenges for existing methods to handle such descriptions. To address this issue, we propose a Prompt-based Cross-Modal Generation Framework (PCM-Frame) to leverage two powerful pre-trained models, including CLIP and StyleGAN. Our framework consists of two components: a projection module from Text Embeddings to Image Embeddings based on prompts, and an adapted image generation module built on StyleGAN which takes Image Embeddings as inputs and is trained by combined semantic consistency losses. To bridge the gap between realistic images and illustration designs, we further adopt a stylization model as post-processing in our framework for better visual effects. Benefiting from the pre-trained models, our method can handle complex descriptions and does not require external paired data for training. Furthermore, we have built a benchmark that consists of 200 descriptions from literature books or online resources. We conduct a user study to demonstrate our superiority over the competing methods of text-to-image translation with complicated semantics.\"\\n      },\\n      {\\n        \"paperId\": \"8e37dc1215681aa153a51c07078ba8befd6a6e01\",\\n        \"url\": \"https://www.semanticscholar.org/paper/8e37dc1215681aa153a51c07078ba8befd6a6e01\",\\n        \"title\": \"AdaPlanner: Adaptive Planning from Feedback with Language Models\",\\n        \"citationCount\": 174,\\n        \"openAccessPdf\": {\\n          \"url\": \"http://arxiv.org/pdf/2305.16653\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2305.16653, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A closed-loop approach, AdaPlanner, is proposed, which allows the LLM agent to refine its self-generated plan adaptively in response to environmental feedback, and develops a code-style LLM prompt structure that facilitates plan generation across a variety of tasks, environments, and agent capabilities.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2023-05-26\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2118180896\",\\n            \"name\": \"Haotian Sun\"\\n          },\\n          {\\n            \"authorId\": \"8103389\",\\n            \"name\": \"Yuchen Zhuang\"\\n          },\\n          {\\n            \"authorId\": \"2865034\",\\n            \"name\": \"Lingkai Kong\"\\n          },\\n          {\\n            \"authorId\": \"2218437288\",\\n            \"name\": \"Bo Dai\"\\n          },\\n          {\\n            \"authorId\": \"145657504\",\\n            \"name\": \"Chao Zhang\"\\n          }\\n        ],\\n        \"abstract\": \"Large language models (LLMs) have recently demonstrated the potential in acting as autonomous agents for sequential decision-making tasks. However, most existing methods either take actions greedily without planning or rely on static plans that are not adaptable to environmental feedback. Consequently, the sequential decision-making performance of LLM agents degenerates with problem complexity and plan horizons increase. We propose a closed-loop approach, AdaPlanner, which allows the LLM agent to refine its self-generated plan adaptively in response to environmental feedback. In AdaPlanner, the LLM agent adaptively refines its plan from feedback with both in-plan and out-of-plan refinement strategies. To mitigate hallucination, we develop a code-style LLM prompt structure that facilitates plan generation across a variety of tasks, environments, and agent capabilities. Furthermore, we propose a skill discovery mechanism that leverages successful plans as few-shot exemplars, enabling the agent to plan and refine with fewer task demonstrations. Our experiments in the ALFWorld and MiniWoB++ environments demonstrate that AdaPlanner outperforms state-of-the-art baselines by 3.73% and 4.11% while utilizing 2x and 600x fewer samples, respectively.\"\\n      },\\n      {\\n        \"paperId\": \"3c23a892605e55f260f647234eb6b5108c84ab84\",\\n        \"url\": \"https://www.semanticscholar.org/paper/3c23a892605e55f260f647234eb6b5108c84ab84\",\\n        \"title\": \"Decoding Methods for Neural Narrative Generation\",\\n        \"citationCount\": 27,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://aclanthology.org/2021.gem-1.16.pdf\",\\n          \"status\": \"HYBRID\",\\n          \"license\": \"CCBY\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2010.07375, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work employs GPT-2 and performs ablations across nucleus sampling thresholds and diverse decoding hyperparameters and analyses results over multiple criteria with automatic and human evaluation, finding that nucleus sampling is generally best with thresholds between 0.7 and 0.9 and a maximum mutual information objective can improve the quality of generated stories.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2020-10-14\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"34989844\",\\n            \"name\": \"Alexandra DeLucia\"\\n          },\\n          {\\n            \"authorId\": \"49355602\",\\n            \"name\": \"Aaron Mueller\"\\n          },\\n          {\\n            \"authorId\": \"32551341\",\\n            \"name\": \"Xiang Lisa Li\"\\n          },\\n          {\\n            \"authorId\": \"2319137716\",\\n            \"name\": \"Jo\\\\u00e3o Sedoc\"\\n          }\\n        ],\\n        \"abstract\": \"Narrative generation is an open-ended NLP task in which a model generates a story given a prompt. The task is similar to neural response generation for chatbots; however, innovations in response generation are often not applied to narrative generation, despite the similarity between these tasks. We aim to bridge this gap by applying and evaluating advances in decoding methods for neural response generation to neural narrative generation. In particular, we employ GPT-2 and perform ablations across nucleus sampling thresholds and diverse decoding hyperparameters\\\\u2014specifically, maximum mutual information\\\\u2014analyzing results over multiple criteria with automatic and human evaluation. We find that (1) nucleus sampling is generally best with thresholds between 0.7 and 0.9; (2) a maximum mutual information objective can improve the quality of generated stories; and (3) established automatic metrics do not correlate well with human judgments of narrative quality on any qualitative metric.\"\\n      },\\n      {\\n        \"paperId\": \"08b85bce712168998004ee80ce4e475390413c74\",\\n        \"url\": \"https://www.semanticscholar.org/paper/08b85bce712168998004ee80ce4e475390413c74\",\\n        \"title\": \"A Prompt Pattern Catalog to Enhance Prompt Engineering with ChatGPT\",\\n        \"citationCount\": 1442,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2302.11382, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A catalog of prompt engineering techniques presented in pattern form that have been applied to solve common problems when conversing with LLMs to improve the outputs of LLM conversations is described.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2023-02-21\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2111231491\",\\n            \"name\": \"Jules White\"\\n          },\\n          {\\n            \"authorId\": \"31669889\",\\n            \"name\": \"Quchen Fu\"\\n          },\\n          {\\n            \"authorId\": \"2159003474\",\\n            \"name\": \"Sam Hays\"\\n          },\\n          {\\n            \"authorId\": \"2087444219\",\\n            \"name\": \"Michael Sandborn\"\\n          },\\n          {\\n            \"authorId\": \"2088026203\",\\n            \"name\": \"Carlos Olea\"\\n          },\\n          {\\n            \"authorId\": \"2180248126\",\\n            \"name\": \"Henry Gilbert\"\\n          },\\n          {\\n            \"authorId\": \"2065484415\",\\n            \"name\": \"Ashraf Elnashar\"\\n          },\\n          {\\n            \"authorId\": \"1405594772\",\\n            \"name\": \"Jesse Spencer-Smith\"\\n          },\\n          {\\n            \"authorId\": \"2064746796\",\\n            \"name\": \"Douglas C. Schmidt\"\\n          }\\n        ],\\n        \"abstract\": \"Prompt engineering is an increasingly important skill set needed to converse effectively with large language models (LLMs), such as ChatGPT. Prompts are instructions given to an LLM to enforce rules, automate processes, and ensure specific qualities (and quantities) of generated output. Prompts are also a form of programming that can customize the outputs and interactions with an LLM. This paper describes a catalog of prompt engineering techniques presented in pattern form that have been applied to solve common problems when conversing with LLMs. Prompt patterns are a knowledge transfer method analogous to software patterns since they provide reusable solutions to common problems faced in a particular context, i.e., output generation and interaction when working with LLMs. This paper provides the following contributions to research on prompt engineering that apply LLMs to automate software development tasks. First, it provides a framework for documenting patterns for structuring prompts to solve a range of problems so that they can be adapted to different domains. Second, it presents a catalog of patterns that have been applied successfully to improve the outputs of LLM conversations. Third, it explains how prompts can be built from multiple patterns and illustrates prompt patterns that benefit from combination with other prompt patterns.\"\\n      },\\n      {\\n        \"paperId\": \"177d8c5d0b172cb1da3f2694949acfd840485a4b\",\\n        \"url\": \"https://www.semanticscholar.org/paper/177d8c5d0b172cb1da3f2694949acfd840485a4b\",\\n        \"title\": \"Manage Real Time Power Imbalance with Renewable Energy: Fast Generation Dispatch or Adaptive Frequency Regulation?\",\\n        \"citationCount\": 11,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {\\'tldr\\'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/pesgm52003.2023.10252632?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/pesgm52003.2023.10252632, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": null,\\n        \"publicationTypes\": null,\\n        \"publicationDate\": \"2023-07-16\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"30562087\",\\n            \"name\": \"Ningchao Gao\"\\n          },\\n          {\\n            \"authorId\": \"145870971\",\\n            \"name\": \"X. Fang\"\\n          }\\n        ],\\n        \"abstract\": \"The carbon neutrality objective requires a large amount of renewable energy integrated into power systems. The rapid deployment of variable renewable energy (VRE), such as solar photovoltaic (PV) generation, increases the system realtime power imbalance because of the random variation and uncertainty of VRE power generation. To manage this, system operators have two potential options: The first is a fast real-time generation dispatch to schedule generation resources with a small time interval to promptly follow the load and renewable power change. The second is to procure adaptive frequency regulation services, such as secondary frequency regulation (SFR), based on the system variation and imbalance conditions to reduce the power imbalance and maintain the stable intra-interval frequency. This paper proposes an integrated alternating current optimal power flow-based generation scheduling and time domain simulation framework to investigate the economic and reliability perspectives of these two options. The impacts of the SFR requirements on the generation cost and frequency response performance are analyzed. In addition, the SFR provided by the PV generation is investigated. The uncertainty of the PV power output is considered using chance constraints to guarantee the real-time delivery of its frequency regulation services. The framework is tested in the IEEE 39-bus network integrated with a large-scale PV power plant. The simulation results demonstrate that the small dispatch interval does not necessarily improve the system frequency response and cannot maintain stable frequency in real time. An adaptive frequency regulation with a 5-minute economic dispatch interval is more appropriate and efficient to reduce the generation cost and improve the frequency response with renewable energy.\"\\n      },\\n      {\\n        \"paperId\": \"6b7f54a8451977a5d24ed1cf93a5ad71b32a9828\",\\n        \"url\": \"https://www.semanticscholar.org/paper/6b7f54a8451977a5d24ed1cf93a5ad71b32a9828\",\\n        \"title\": \"Adaptive In-Context Learning with Large Language Models for Bundle Generation\",\\n        \"citationCount\": 11,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2312.16262, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"An adaptive in-context learning paradigm is proposed, which allows LLMs to draw tailored lessons from related sessions as demonstrations, enhancing the performance on target sessions, and introduces a self-correction strategy promoting mutual improvements of the two tasks without supervision signals.\"\\n        },\\n        \"publicationTypes\": [\\n          \"Book\",\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2023-12-26\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2274082438\",\\n            \"name\": \"Zhu Sun\"\\n          },\\n          {\\n            \"authorId\": \"2273938691\",\\n            \"name\": \"Kaidong Feng\"\\n          },\\n          {\\n            \"authorId\": \"2276986037\",\\n            \"name\": \"Jie Yang\"\\n          },\\n          {\\n            \"authorId\": \"40507824\",\\n            \"name\": \"Xinghua Qu\"\\n          },\\n          {\\n            \"authorId\": \"2276798784\",\\n            \"name\": \"Hui Fang\"\\n          },\\n          {\\n            \"authorId\": \"8748397\",\\n            \"name\": \"Y. Ong\"\\n          },\\n          {\\n            \"authorId\": \"2276742821\",\\n            \"name\": \"Wenyuan Liu\"\\n          }\\n        ],\\n        \"abstract\": \"Most existing bundle generation approaches fall short in generating fixed-size bundles. Furthermore, they often neglect the underlying user intents reflected by the bundles in the generation process, resulting in less intelligible bundles. This paper addresses these limitations through the exploration of two interrelated tasks, i.e., personalized bundle generation and the underlying intent inference, based on different user sessions. Inspired by the reasoning capabilities of large language models (LLMs), we propose an adaptive in-context learning paradigm, which allows LLMs to draw tailored lessons from related sessions as demonstrations, enhancing the performance on target sessions. Specifically, we first employ retrieval augmented generation to identify nearest neighbor sessions, and then carefully design prompts to guide LLMs in executing both tasks on these neighbor sessions. To tackle reliability and hallucination challenges, we further introduce (1) a self-correction strategy promoting mutual improvements of the two tasks without supervision signals and (2) an auto-feedback mechanism for adaptive supervision based on the distinct mistakes made by LLMs on different neighbor sessions. Thereby, the target session can gain customized lessons for improved performance by observing the demonstrations of its neighbor sessions. Experiments on three real-world datasets demonstrate the effectiveness of our proposed method.\"\\n      },\\n      {\\n        \"paperId\": \"5b33c0ab87c532261435c93e38819185f0132d99\",\\n        \"url\": \"https://www.semanticscholar.org/paper/5b33c0ab87c532261435c93e38819185f0132d99\",\\n        \"title\": \"Text Prompt with Normality Guidance for Weakly Supervised Video Anomaly Detection\",\\n        \"citationCount\": 61,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://arxiv.org/pdf/2404.08531\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2404.08531, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A novel pseudo-label generation and self-training framework based on Text Prompt with Normality Guidance (TPWNG) for WSVAD is proposed and achieves state-of-the-art performance on two benchmark datasets, UCF-Crime and XD-Violence.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-04-12\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2296889795\",\\n            \"name\": \"Zhiwei Yang\"\\n          },\\n          {\\n            \"authorId\": \"2163063860\",\\n            \"name\": \"Jing Liu\"\\n          },\\n          {\\n            \"authorId\": \"2678268\",\\n            \"name\": \"Peng Wu\"\\n          }\\n        ],\\n        \"abstract\": \"Weakly supervised video anomaly detection (WSVAD) is a challenging task. Generating fine-grained pseudo-labels based on weak-label and then self-training a classifier is currently a promising solution. However, since the existing methods use only RGB visual modality and the utilization of category text information is neglected, thus limiting the generation of more accurate pseudo-labels and affecting the performance of self-training. Inspired by the manual labeling process based on the event description, in this paper, we propose a novel pseudo-label generation and self-training framework based on Text Prompt with Normality Guidance (TPWNG) for WSVAD. Our idea is to transfer the rich language-visual knowledge of the contrastive language-image pre-training (CLIP) model for aligning the video event description text and corresponding video frames to generate pseudo-labels. Specifically, We first fine-tune the CLIP for domain adaptation by designing two ranking losses and a distributional inconsistency loss. Further, we propose a learnable text prompt mechanism with the assist of a normality visual prompt to further improve the matching accuracy of video event description text and video frames. Then, we design a pseudo-label generation module based on the normality guidance to infer reliable frame-level pseudo-labels. Finally, we introduce a temporal context self-adaptive learning module to learn the temporal dependencies of different video events more flexibly and accurately. Extensive experiments show that our method achieves state-of-the-art performance on two benchmark datasets, UCF-Crime and XD-Violence, demonstrating the effectiveness of our proposed method.\"\\n      },\\n      {\\n        \"paperId\": \"9689acb6cb760e8bc21c16f368368b37dee977f9\",\\n        \"url\": \"https://www.semanticscholar.org/paper/9689acb6cb760e8bc21c16f368368b37dee977f9\",\\n        \"title\": \"Adaptive Machine Translation with Large Language Models\",\\n        \"citationCount\": 105,\\n        \"openAccessPdf\": {\\n          \"url\": \"http://arxiv.org/pdf/2301.13294\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2301.13294, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"It is observed that the translation quality with few-shot in-context learning can surpass that of strong encoder-decoder MT systems, especially for high-resource languages.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2023-01-30\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"9400076\",\\n            \"name\": \"Yasmin Moslem\"\\n          },\\n          {\\n            \"authorId\": \"1748844\",\\n            \"name\": \"Rejwanul Haque\"\\n          },\\n          {\\n            \"authorId\": \"144315616\",\\n            \"name\": \"Andy Way\"\\n          }\\n        ],\\n        \"abstract\": \"Consistency is a key requirement of high-quality translation. It is especially important to adhere to pre-approved terminology and adapt to corrected translations in domain-specific projects. Machine translation (MT) has achieved significant progress in the area of domain adaptation. However, real-time adaptation remains challenging. Large-scale language models (LLMs) have recently shown interesting capabilities of in-context learning, where they learn to replicate certain input-output text generation patterns, without further fine-tuning. By feeding an LLM at inference time with a prompt that consists of a list of translation pairs, it can then simulate the domain and style characteristics. This work aims to investigate how we can utilize in-context learning to improve real-time adaptive MT. Our extensive experiments show promising results at translation time. For example, GPT-3.5 can adapt to a set of in-domain sentence pairs and/or terminology while translating a new sentence. We observe that the translation quality with few-shot in-context learning can surpass that of strong encoder-decoder MT systems, especially for high-resource languages. Moreover, we investigate whether we can combine MT from strong encoder-decoder models with fuzzy matches, which can further improve translation quality, especially for less supported languages. We conduct our experiments across five diverse language pairs, namely English-to-Arabic (EN-AR), English-to-Chinese (EN-ZH), English-to-French (EN-FR), English-to-Kinyarwanda (EN-RW), and English-to-Spanish (EN-ES).\"\\n      },\\n      {\\n        \"paperId\": \"7665642af9e682e012bec045102a4d009421067c\",\\n        \"url\": \"https://www.semanticscholar.org/paper/7665642af9e682e012bec045102a4d009421067c\",\\n        \"title\": \"HumanGaussian: Text-Driven 3D Human Generation with Gaussian Splatting\",\\n        \"citationCount\": 97,\\n        \"openAccessPdf\": {\\n          \"url\": \"http://arxiv.org/pdf/2311.17061\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2311.17061, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"An efficient yet effective framework that generates high-quality 3D humans with fine-grained geometry and realistic appearance, based on the insight that 3D Gaussian Splatting is an efficient renderer with peri-odic Gaussian shrinkage or growing, where such adaptive density control can be naturally guided by intrinsic human structures.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2023-11-28\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2257708193\",\\n            \"name\": \"Xian Liu\"\\n          },\\n          {\\n            \"authorId\": \"2260342453\",\\n            \"name\": \"Xiaohang Zhan\"\\n          },\\n          {\\n            \"authorId\": \"1397711601\",\\n            \"name\": \"Jiaxiang Tang\"\\n          },\\n          {\\n            \"authorId\": \"2260340529\",\\n            \"name\": \"Ying Shan\"\\n          },\\n          {\\n            \"authorId\": \"2247995148\",\\n            \"name\": \"Gang Zeng\"\\n          },\\n          {\\n            \"authorId\": \"2258618427\",\\n            \"name\": \"Dahua Lin\"\\n          },\\n          {\\n            \"authorId\": \"2257370021\",\\n            \"name\": \"Xihui Liu\"\\n          },\\n          {\\n            \"authorId\": \"2249080787\",\\n            \"name\": \"Ziwei Liu\"\\n          }\\n        ],\\n        \"abstract\": \"Realistic 3D human generationfrom text prompts is a de-sirable yet challenging task. Existing methods optimize 3D representations like mesh or neural fields via score distil-lation sampling (SDS), which suffers from inadequate fine details or excessive training time. In this paper, we pro-pose an efficient yet effective framework, HumanGaussian, that generates high-quality 3D humans with fine-grained geometry and realistic appearance. Our key insight is that 3D Gaussian Splatting is an efficient renderer with peri-odic Gaussian shrinkage or growing, where such adaptive density control can be naturally guided by intrinsic human structures. Specifically, 1) we first propose a Structure-Aware SDS that simultaneously optimizes human appear-ance and geometry. The multi-modal score function from both RGB and depth space is leveraged to distill the Gaus-sian densification and pruning process. 2) Moreover, we devise an Annealed Negative Prompt Guidance by decom-posing SDS into a noisier generative score and a cleaner classifier score, which well addresses the over-saturation issue. The floating artifacts are further eliminated based on Gaussian size in a prune-only phase to enhance generation smoothness. Extensive experiments demonstrate the supe-rior efficiency and competitive quality of our framework, rendering vivid 3D humans under diverse scenarios.\"\\n      },\\n      {\\n        \"paperId\": \"5888f7aba5c601a668c290bf57addf79cc1518f1\",\\n        \"url\": \"https://www.semanticscholar.org/paper/5888f7aba5c601a668c290bf57addf79cc1518f1\",\\n        \"title\": \"ChatUniTest: a ChatGPT-based automated unit test generation tool\",\\n        \"citationCount\": 84,\\n        \"openAccessPdf\": {\\n          \"url\": \"http://arxiv.org/pdf/2305.04764\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {\\'abstract\\'}. Paper or abstract available at https://api.unpaywall.org/v2/10.48550/arXiv.2305.04764?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.48550/arXiv.2305.04764, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper introduces ChatUniTest, a ChatGPT-based automated unit test generation tool developed under the Generation-Validation-Repair framework that outperforms EvoSuite in branch and line coverage, surpasses AthenaTest and A3Test in focal method coverage, and generates assertions while utilizing mock objects and re\\\\ufb02ection to achieve test objectives.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": null,\\n        \"authors\": [\\n          {\\n            \"authorId\": \"1387638000\",\\n            \"name\": \"Zhuo-Qi Xie\"\\n          },\\n          {\\n            \"authorId\": \"2300176046\",\\n            \"name\": \"Yinghao Chen\"\\n          },\\n          {\\n            \"authorId\": \"2064478633\",\\n            \"name\": \"Chen Zhi\"\\n          },\\n          {\\n            \"authorId\": \"145590434\",\\n            \"name\": \"Shuiguang Deng\"\\n          },\\n          {\\n            \"authorId\": \"2116398505\",\\n            \"name\": \"Jianwei Yin\"\\n          }\\n        ],\\n        \"abstract\": null\\n      },\\n      {\\n        \"paperId\": \"107dce4ffbdd8d83b75492216646269d8c037ab5\",\\n        \"url\": \"https://www.semanticscholar.org/paper/107dce4ffbdd8d83b75492216646269d8c037ab5\",\\n        \"title\": \"The Future of Learning in the Age of Generative AI: Automated Question Generation and Assessment with Large Language Models\",\\n        \"citationCount\": 10,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2410.09576, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"The discussion underscores the potential of LLMs to replace costly, time-consuming human assessments when appropriately guided, showcasing their advanced understanding and reasoning capabilities in streamlining educational processes.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-10-12\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"1847858\",\\n            \"name\": \"Subhankar Maity\"\\n          },\\n          {\\n            \"authorId\": \"2144085844\",\\n            \"name\": \"Aniket Deroy\"\\n          }\\n        ],\\n        \"abstract\": \"In recent years, large language models (LLMs) and generative AI have revolutionized natural language processing (NLP), offering unprecedented capabilities in education. This chapter explores the transformative potential of LLMs in automated question generation and answer assessment. It begins by examining the mechanisms behind LLMs, emphasizing their ability to comprehend and generate human-like text. The chapter then discusses methodologies for creating diverse, contextually relevant questions, enhancing learning through tailored, adaptive strategies. Key prompting techniques, such as zero-shot and chain-of-thought prompting, are evaluated for their effectiveness in generating high-quality questions, including open-ended and multiple-choice formats in various languages. Advanced NLP methods like fine-tuning and prompt-tuning are explored for their role in generating task-specific questions, despite associated costs. The chapter also covers the human evaluation of generated questions, highlighting quality variations across different methods and areas for improvement. Furthermore, it delves into automated answer assessment, demonstrating how LLMs can accurately evaluate responses, provide constructive feedback, and identify nuanced understanding or misconceptions. Examples illustrate both successful assessments and areas needing improvement. The discussion underscores the potential of LLMs to replace costly, time-consuming human assessments when appropriately guided, showcasing their advanced understanding and reasoning capabilities in streamlining educational processes.\"\\n      },\\n      {\\n        \"paperId\": \"c0a526a448a5d0d6b17c5b08fc5920c2d22fcb74\",\\n        \"url\": \"https://www.semanticscholar.org/paper/c0a526a448a5d0d6b17c5b08fc5920c2d22fcb74\",\\n        \"title\": \"UN-SAM: Universal Prompt-Free Segmentation for Generalized Nuclei Images\",\\n        \"citationCount\": 32,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2402.16663, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work proposes the Universal prompt-free SAM framework for Nuclei segmentation (UN-SAM), by providing a fully automated solution with remarkable generalization capabilities, and devise a multi-scale Self-Prompt Generation (SPGen) module to revolutionize clinical workflow.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-02-26\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2117102192\",\\n            \"name\": \"Zhen Chen\"\\n          },\\n          {\\n            \"authorId\": \"2287826940\",\\n            \"name\": \"Qing Xu\"\\n          },\\n          {\\n            \"authorId\": \"2275332982\",\\n            \"name\": \"Xinyu Liu\"\\n          },\\n          {\\n            \"authorId\": \"2275284236\",\\n            \"name\": \"Yixuan Yuan\"\\n          }\\n        ],\\n        \"abstract\": \"In digital pathology, precise nuclei segmentation is pivotal yet challenged by the diversity of tissue types, staining protocols, and imaging conditions. Recently, the segment anything model (SAM) revealed overwhelming performance in natural scenarios and impressive adaptation to medical imaging. Despite these advantages, the reliance of labor-intensive manual annotation as segmentation prompts severely hinders their clinical applicability, especially for nuclei image analysis containing massive cells where dense manual prompts are impractical. To overcome the limitations of current SAM methods while retaining the advantages, we propose the Universal prompt-free SAM framework for Nuclei segmentation (UN-SAM), by providing a fully automated solution with remarkable generalization capabilities. Specifically, to eliminate the labor-intensive requirement of per-nuclei annotations for prompt, we devise a multi-scale Self-Prompt Generation (SPGen) module to revolutionize clinical workflow by automatically generating high-quality mask hints to guide the segmentation tasks. Moreover, to unleash the generalization capability of SAM across a variety of nuclei images, we devise a Domain-adaptive Tuning Encoder (DT-Encoder) to seamlessly harmonize visual features with domain-common and domain-specific knowledge, and further devise a Domain Query-enhanced Decoder (DQ-Decoder) by leveraging learnable domain queries for segmentation decoding in different nuclei domains. Extensive experiments prove that UN-SAM with exceptional performance surpasses state-of-the-arts in nuclei instance and semantic segmentation, especially the generalization capability in zero-shot scenarios. The source code is available at https://github.com/CUHK-AIM-Group/UN-SAM.\"\\n      },\\n      {\\n        \"paperId\": \"3c38d98583bdfc0d02b618d94394161f65d4dc96\",\\n        \"url\": \"https://www.semanticscholar.org/paper/3c38d98583bdfc0d02b618d94394161f65d4dc96\",\\n        \"title\": \"Adapting LLMs for Efficient Context Processing through Soft Prompt Compression\",\\n        \"citationCount\": 35,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://arxiv.org/pdf/2404.04997\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2404.04997, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This investigation presents an innovative framework that strategically tailors LLMs for streamlined context processing by harnessing the synergies among natural language summarization, soft prompt com- pression, and augmented utility preservation mechanisms.\"\\n        },\\n        \"publicationTypes\": [\\n          \"Book\",\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-04-07\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2295677749\",\\n            \"name\": \"Cangqing Wang\"\\n          },\\n          {\\n            \"authorId\": \"2295682977\",\\n            \"name\": \"Yutian Yang\"\\n          },\\n          {\\n            \"authorId\": \"2303231934\",\\n            \"name\": \"Ruisi Li\"\\n          },\\n          {\\n            \"authorId\": \"2295684775\",\\n            \"name\": \"Dan Sun\"\\n          },\\n          {\\n            \"authorId\": \"2295667065\",\\n            \"name\": \"Ruicong Cai\"\\n          },\\n          {\\n            \"authorId\": \"2295678151\",\\n            \"name\": \"Yuzhu Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2295659472\",\\n            \"name\": \"Chengqian Fu\"\\n          }\\n        ],\\n        \"abstract\": \"The rapid advancement of Large Language Models (LLMs) has inaugurated a transformative epoch in natural language processing, fostering unprecedented proficiency in text generation, comprehension, and contextual scrutiny. Nevertheless, effectively handling extensive contexts, crucial for myriad applications, poses a formidable obstacle owing to the intrinsic constraints of the models\\\\u2019 context window sizes and the computational burdens entailed by their operations. This investigation presents an innovative framework that strategically tailors LLMs for streamlined context processing by harnessing the synergies among natural language summarization, soft prompt com- pression, and augmented utility preservation mechanisms. Our methodology, dubbed SoftPromptComp, amalgamates natural language prompts extracted from summarization methodologies with dynamically generated soft prompts to forge a concise yet semantically robust depiction of protracted contexts. This depiction undergoes further refinement via a weighting mechanism optimizing information retention and utility for subsequent tasks. We substantiate that our framework markedly diminishes computational overhead and enhances LLMs\\\\u2019 efficacy across various benchmarks, while upholding or even augmenting the caliber of the produced content. By amalgamating soft prompt compression with sophisticated summarization, SoftPromptComp confronts the dual challenges of managing lengthy contexts and ensuring model scalability. Our findings point towards a propitious trajectory for augmenting LLMs\\\\u2019 applicability and efficiency, rendering them more versatile and pragmatic for real- world applications. This research enriches the ongoing discourse on optimizing language models, providing insights into the potency of soft prompts and summarization techniques as pivotal instruments for the forthcoming generation of NLP solutions.\"\\n      },\\n      {\\n        \"paperId\": \"836e3069a83f455f916114e7265e00187e511838\",\\n        \"url\": \"https://www.semanticscholar.org/paper/836e3069a83f455f916114e7265e00187e511838\",\\n        \"title\": \"Locally Differentially Private Document Generation Using Zero Shot Prompting\",\\n        \"citationCount\": 55,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2310.16111, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work proposes a locally differentially private mechanism called DP-Prompt, which leverages the power of pretrained large language models and zero-shot prompting to counter author de-anonymization attacks while minimizing the impact on downstream utility.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2023-10-24\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"1512255229\",\\n            \"name\": \"Saiteja Utpala\"\\n          },\\n          {\\n            \"authorId\": \"2261493078\",\\n            \"name\": \"Sara Hooker\"\\n          },\\n          {\\n            \"authorId\": \"2261697074\",\\n            \"name\": \"Pin Yu Chen\"\\n          }\\n        ],\\n        \"abstract\": \"Numerous studies have highlighted the privacy risks associated with pretrained large language models. In contrast, our research offers a unique perspective by demonstrating that pretrained large language models can effectively contribute to privacy preservation. We propose a locally differentially private mechanism called DP-Prompt, which leverages the power of pretrained large language models and zero-shot prompting to counter author de-anonymization attacks while minimizing the impact on downstream utility. When DP-Prompt is used with a powerful language model like ChatGPT (gpt-3.5), we observe a notable reduction in the success rate of de-anonymization attacks, showing that it surpasses existing approaches by a considerable margin despite its simpler design. For instance, in the case of the IMDB dataset, DP-Prompt (with ChatGPT) perfectly recovers the clean sentiment F1 score while achieving a 46\\\\\\\\% reduction in author identification F1 score against static attackers and a 26\\\\\\\\% reduction against adaptive attackers. We conduct extensive experiments across six open-source large language models, ranging up to 7 billion parameters, to analyze various effects of the privacy-utility tradeoff.\"\\n      },\\n      {\\n        \"paperId\": \"f087d0175346f143e190fc39f4ec4ceb5b3cc093\",\\n        \"url\": \"https://www.semanticscholar.org/paper/f087d0175346f143e190fc39f4ec4ceb5b3cc093\",\\n        \"title\": \"Fine-Grained Visual Prompt Learning of Vision-Language Models for Image Recognition\",\\n        \"citationCount\": 17,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1145/3581783.3612403?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/3581783.3612403, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"Fine-grained visual prompt is introduced into the image encoder of the vision-language model for focusing on the target object and conducting information interaction within the object, which facilitates generating discriminative visual features for image recognition.\"\\n        },\\n        \"publicationTypes\": [\\n          \"Book\",\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2023-10-26\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2261902040\",\\n            \"name\": \"Hongbo Sun\"\\n          },\\n          {\\n            \"authorId\": \"153003087\",\\n            \"name\": \"Xiangteng He\"\\n          },\\n          {\\n            \"authorId\": \"2261798088\",\\n            \"name\": \"Jiahuan Zhou\"\\n          },\\n          {\\n            \"authorId\": \"143753918\",\\n            \"name\": \"Yuxin Peng\"\\n          }\\n        ],\\n        \"abstract\": \"Large-scale pre-trained vision-language (VL) models have shown powerful generic representation capabilities for adapting to downstream tasks with limited training data, which are data-efficient solutions to various applications such as image recognition. In order to enhance the adaption performance, most existing methods attempt to introduce learnable vectors into the text prompt to generate adaptive classification weights for the class in the downstream task. However, they generally focus on the text side while neglecting adaptive visual feature generation on the image side, which is insufficient to fit the downstream task data. In this paper, we propose fine-grained visual prompt learning (FG-VPL) of vision-language models for image recognition with few training samples, and the main contributions are: (1) Fine-grained visual prompt is introduced into the image encoder of the vision-language model for focusing on the target object and conducting information interaction within the object, which facilitates generating discriminative visual features for image recognition. (2) A two-pathway adaptive recognition module is proposed to narrow the domain gap and utilize both the cross-modal knowledge of the vision-language model and the visual information of the few-sample training set for classifying images with the help of feature adapters. We conduct extensive experiments on 11 image recognition benchmark datasets under the few training samples setting, which demonstrate that our proposed approach can achieve state-of-the-art performance. The code is available at https://github.com/PKU-ICST-MIPL/FG-VPL_ACMMM2023.\"\\n      },\\n      {\\n        \"paperId\": \"06d8562831c32844285a691c5250d04726df3c61\",\\n        \"url\": \"https://www.semanticscholar.org/paper/06d8562831c32844285a691c5250d04726df3c61\",\\n        \"title\": \"A Systematic Survey of Prompt Engineering on Vision-Language Foundation Models\",\\n        \"citationCount\": 206,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://arxiv.org/pdf/2307.12980\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {\\'abstract\\'}. Paper or abstract available at https://api.unpaywall.org/v2/10.48550/arXiv.2307.12980?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.48550/arXiv.2307.12980, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper aims to provide a comprehensive survey of cutting-edge research in prompt engineering on three types of vision-language models: multimodal-to-text generation models, image-text matching models, and text- to-image generation models.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Review\"\\n        ],\\n        \"publicationDate\": null,\\n        \"authors\": [\\n          {\\n            \"authorId\": \"52203056\",\\n            \"name\": \"Jindong Gu\"\\n          },\\n          {\\n            \"authorId\": \"2223193538\",\\n            \"name\": \"Zhen Han\"\\n          },\\n          {\\n            \"authorId\": \"2116572341\",\\n            \"name\": \"Shuo Chen\"\\n          },\\n          {\\n            \"authorId\": \"1791052\",\\n            \"name\": \"Ahmad Beirami\"\\n          },\\n          {\\n            \"authorId\": \"2147293727\",\\n            \"name\": \"Bailan He\"\\n          },\\n          {\\n            \"authorId\": \"2143853643\",\\n            \"name\": \"Gengyuan Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2072387342\",\\n            \"name\": \"Ruotong Liao\"\\n          },\\n          {\\n            \"authorId\": \"2219078907\",\\n            \"name\": \"Yao Qin\"\\n          },\\n          {\\n            \"authorId\": \"1742501819\",\\n            \"name\": \"Volker Tresp\"\\n          },\\n          {\\n            \"authorId\": \"143635540\",\\n            \"name\": \"Philip H. S. Torr\"\\n          }\\n        ],\\n        \"abstract\": null\\n      },\\n      {\\n        \"paperId\": \"b013c9eb1284554ae696fba02bd4d7fc599890b6\",\\n        \"url\": \"https://www.semanticscholar.org/paper/b013c9eb1284554ae696fba02bd4d7fc599890b6\",\\n        \"title\": \"StyleCrafter: Enhancing Stylized Text-to-Video Generation with Style Adapter\",\\n        \"citationCount\": 34,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {\\'abstract\\'}. Paper or abstract available at https://api.unpaywall.org/v2/10.48550/arXiv.2312.00330?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.48550/arXiv.2312.00330, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"StyleCrafter is introduced, a generic method that enhances pre-trained T2V models with a style control adapter, enabling video generation in any style by providing a reference image, and designs a scale-adaptive fusion module to balance the influences of text-based content features and image-based style features.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": null,\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2269171464\",\\n            \"name\": \"Gongye Liu\"\\n          },\\n          {\\n            \"authorId\": \"2257035878\",\\n            \"name\": \"Menghan Xia\"\\n          },\\n          {\\n            \"authorId\": \"2257199953\",\\n            \"name\": \"Yong Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2149052351\",\\n            \"name\": \"Haoxin Chen\"\\n          },\\n          {\\n            \"authorId\": \"2087273800\",\\n            \"name\": \"Jinbo Xing\"\\n          },\\n          {\\n            \"authorId\": \"2253795356\",\\n            \"name\": \"Xintao Wang\"\\n          },\\n          {\\n            \"authorId\": \"3001727\",\\n            \"name\": \"Yujiu Yang\"\\n          },\\n          {\\n            \"authorId\": \"2257019659\",\\n            \"name\": \"Ying Shan\"\\n          }\\n        ],\\n        \"abstract\": null\\n      }\\n    ]\\n  },\\n  \"iterative context update\": {\\n    \"total\": 9050,\\n    \"offset\": 0,\\n    \"next\": 50,\\n    \"data\": [\\n      {\\n        \"paperId\": \"a0d2ea210c9bd21676605682a76cec1a4004320a\",\\n        \"url\": \"https://www.semanticscholar.org/paper/a0d2ea210c9bd21676605682a76cec1a4004320a\",\\n        \"title\": \"Iterative Context-Aware Graph Inference for Visual Dialog\",\\n        \"citationCount\": 51,\\n        \"openAccessPdf\": {\\n          \"url\": \"http://arxiv.org/pdf/2004.02194\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2004.02194, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work proposes a novel Context-Aware Graph (CAG) neural network, where each node in the graph corresponds to a joint semantic feature, including both object-based (visual) and history-related (textual) context representations.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2020-04-05\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"144713153\",\\n            \"name\": \"Dan Guo\"\\n          },\\n          {\\n            \"authorId\": \"46507139\",\\n            \"name\": \"Haibo Wang\"\\n          },\\n          {\\n            \"authorId\": \"5462268\",\\n            \"name\": \"Hanwang Zhang\"\\n          },\\n          {\\n            \"authorId\": \"143962510\",\\n            \"name\": \"Zhengjun Zha\"\\n          },\\n          {\\n            \"authorId\": \"47446553\",\\n            \"name\": \"Meng Wang\"\\n          }\\n        ],\\n        \"abstract\": \"Visual dialog is a challenging task that requires the comprehension of the semantic dependencies among implicit visual and textual contexts. This task can refer to the relation inference in a graphical model with sparse contexts and unknown graph structure (relation descriptor), and how to model the underlying context-aware relation inference is critical. To this end, we propose a novel Context-Aware Graph (CAG) neural network. Each node in the graph corresponds to a joint semantic feature, including both object-based (visual) and history-related (textual) context representations. The graph structure (relations in dialog) is iteratively updated using an adaptive top-K message passing mechanism. Specifically, in every message passing step, each node selects the most K relevant nodes, and only receives messages from them. Then, after the update, we impose graph attention on all the nodes to get the final graph embedding and infer the answer. In CAG, each node has dynamic relations in the graph (different related K neighbor nodes), and only the most relevant nodes are attributive to the context-aware relational graph inference. Experimental results on VisDial v0.9 and v1.0 datasets show that CAG outperforms comparative methods. Visualization results further validate the interpretability of our method.\"\\n      },\\n      {\\n        \"paperId\": \"697e176d66a17c0b24613b8513ab951dc4112c34\",\\n        \"url\": \"https://www.semanticscholar.org/paper/697e176d66a17c0b24613b8513ab951dc4112c34\",\\n        \"title\": \"Iterative Geometry Encoding Volume for Stereo Matching\",\\n        \"citationCount\": 280,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://arxiv.org/pdf/2303.06615\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2303.06615, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper proposes Iterative Geometry Encoding Volume (IGEV-Stereo), a new deep network architecture for stereo matching that builds a combined geometry encoding volume that encodes geometry and context information as well as local matching details, and iteratively indexes it to update the disparity map.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2023-03-12\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2158317969\",\\n            \"name\": \"Gangwei Xu\"\\n          },\\n          {\\n            \"authorId\": \"2107956900\",\\n            \"name\": \"Xianqi Wang\"\\n          },\\n          {\\n            \"authorId\": \"2117436225\",\\n            \"name\": \"Xiao-Hua Ding\"\\n          },\\n          {\\n            \"authorId\": \"2150441002\",\\n            \"name\": \"Xin Yang\"\\n          }\\n        ],\\n        \"abstract\": \"Recurrent All-Pairs Field Transforms (RAFT) has shown great potentials in matching tasks. However, all-pairs correlations lack non-local geometry knowledge and have difficulties tackling local ambiguities in ill-posed regions. In this paper, we propose Iterative Geometry Encoding Volume (IGEV-Stereo), a new deep network architecture for stereo matching. The proposed IGEV-Stereo builds a combined geometry encoding volume that encodes geometry and context information as well as local matching details, and iteratively indexes it to update the disparity map. To speed up the convergence, we exploit GEV to regress an accurate starting point for ConvGRUs iterations. Our IGEV-Stereo ranks 1st on KITTI 2015 and 2012 (Reflective) among all published methods and is the fastest among the top 10 methods. In addition, IGEV-Stereo has strong cross-dataset generalization as well as high inference efficiency. We also extend our IGEV to multi-view stereo (MVS), i.e. IGEV-MVS, which achieves competitive accuracy on DTU benchmark. Code is available at https://github.com/gangweiX/IGEV.\"\\n      },\\n      {\\n        \"paperId\": \"8c3a6d134aa3374ed8e1de2c47bcc73e3f9ee15e\",\\n        \"url\": \"https://www.semanticscholar.org/paper/8c3a6d134aa3374ed8e1de2c47bcc73e3f9ee15e\",\\n        \"title\": \"Proximal Stabilized Interior Point Methods and Low-Frequency-Update Preconditioning Techniques\",\\n        \"citationCount\": 13,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://link.springer.com/content/pdf/10.1007/s10957-023-02194-4.pdf\",\\n          \"status\": \"HYBRID\",\\n          \"license\": \"CCBY\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1007/s10957-023-02194-4?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1007/s10957-023-02194-4, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work considers Primal Dual Regularized Interior Point Methods in the framework of the Proximal Point Method, and shows experimentally that low-frequency-update of the preconditioners are allowed, paving the path for an alternative class of second order methods characterized by reduced computational effort.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2023-04-05\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"15569181\",\\n            \"name\": \"S. Cipolla\"\\n          },\\n          {\\n            \"authorId\": \"51043392\",\\n            \"name\": \"J. Gondzio\"\\n          }\\n        ],\\n        \"abstract\": \"In this work, in the context of Linear and convex Quadratic Programming, we consider Primal Dual Regularized Interior Point Methods (PDR-IPMs) in the framework of the Proximal Point Method. The resulting Proximal Stabilized IPM (PS-IPM) is strongly supported by theoretical results concerning convergence and the rate of convergence, and can handle degenerate problems. Moreover, in the second part of this work, we analyse the interactions between the regularization parameters and the computational footprint of the linear algebra routines used to solve the Newton linear systems. In particular, when these systems are solved using an iterative Krylov method, we are able to show\\\\u2014using a new rearrangement of the Schur complement which exploits regularization\\\\u2014that general purposes preconditioners remain attractive for a series of subsequent IPM iterations. Indeed, if on the one hand a series of theoretical results underpin the fact that the approach here presented allows a better re-use of such computed preconditioners, on the other, we show experimentally that such (re)computations are needed only in a fraction of the total IPM iterations. The resulting regularized second order methods, for which low-frequency-update of the preconditioners are allowed, pave the path for an alternative class of second order methods characterized by reduced computational effort.\"\\n      },\\n      {\\n        \"paperId\": \"8fd4d5762de7c6861c841bc54208ebeec76b6213\",\\n        \"url\": \"https://www.semanticscholar.org/paper/8fd4d5762de7c6861c841bc54208ebeec76b6213\",\\n        \"title\": \"IMENet: Joint 3D Semantic Scene Completion and 2D Semantic Segmentation through Iterative Mutual Enhancement\",\\n        \"citationCount\": 13,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://www.ijcai.org/proceedings/2021/0110.pdf\",\\n          \"status\": \"BRONZE\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2106.15413, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work argues that this sequential scheme does not ensure these two tasks fully benefit each other, and presents an Iterative Mutual Enhancement Network (IMENet) to solve them jointly, which interactively refines the two tasks at the late prediction stage.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2021-06-29\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"1490486462\",\\n            \"name\": \"Jie Li\"\\n          },\\n          {\\n            \"authorId\": \"1518268141\",\\n            \"name\": \"Laiyan Ding\"\\n          },\\n          {\\n            \"authorId\": \"1516136186\",\\n            \"name\": \"Rui Huang\"\\n          }\\n        ],\\n        \"abstract\": \"3D semantic scene completion and 2D semantic segmentation are two tightly correlated tasks that are both essential for indoor scene understanding, because they predict the same semantic classes, using positively correlated high-level features. Current methods use 2D features extracted from early-fused RGB-D images for 2D segmentation to improve 3D scene completion. We argue that this sequential scheme does not ensure these two tasks fully benefit each other, and present an Iterative Mutual Enhancement Network (IMENet) to solve them jointly, which interactively refines the two tasks at the late prediction stage. Specifically, two refinement modules are developed under a unified framework for the two tasks. The first is a 2D Deformable Context Pyramid (DCP) module, which receives the projection from the current 3D predictions to refine the 2D predictions. In turn, a 3D Deformable Depth Attention (DDA) module is proposed to leverage the reprojected results from 2D predictions to update the coarse 3D predictions. This iterative fusion happens to the stable high-level features of both tasks at a late stage. Extensive experiments on NYU and NYUCAD datasets verify the effectiveness of the proposed iterative late fusion scheme, and our approach outperforms the state of the art on both 3D semantic scene completion and 2D semantic segmentation.\"\\n      },\\n      {\\n        \"paperId\": \"be089bc2e9983223068b4eb0d67b4b2f5fc30321\",\\n        \"url\": \"https://www.semanticscholar.org/paper/be089bc2e9983223068b4eb0d67b4b2f5fc30321\",\\n        \"title\": \"Iterative Privileged Learning\",\\n        \"citationCount\": 17,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {\\'tldr\\'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/TNNLS.2018.2889906?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/TNNLS.2018.2889906, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": null,\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2020-08-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2116383294\",\\n            \"name\": \"Xue Li\"\\n          },\\n          {\\n            \"authorId\": \"145728792\",\\n            \"name\": \"Bo Du\"\\n          },\\n          {\\n            \"authorId\": \"2760404\",\\n            \"name\": \"Yipeng Zhang\"\\n          },\\n          {\\n            \"authorId\": null,\\n            \"name\": \"Chang Xu\"\\n          },\\n          {\\n            \"authorId\": \"143719920\",\\n            \"name\": \"D. Tao\"\\n          }\\n        ],\\n        \"abstract\": \"While in the learning using privileged information paradigm, privileged information may not be as informative as example features in the context of making accurate label predictions, it may be able to provide some effective comments (e.g., the values of the auxiliary function) like a human teacher on the efficacy of the learned model. In a departure from conventional static manipulations of privileged information within the support vector machine framework, this paper investigates iterative privileged learning within the context of gradient boosted decision trees (GBDTs). As the learned model evolves, the comments learned from privileged information to assess the model should also be actively upgraded instead of remaining static and passive. During the learning phase of the GBDT method, new DTs are discovered to enhance the performance of the model, and iteratively update the comments generated from the privileged information to accurately assess and coach the up-to-date model. The resulting objective function can be efficiently solved within the gradient boosting framework. Experimental results on real-world data sets demonstrate the benefits of studying privileged information in an iterative manner, as well as the effectiveness of the proposed algorithm.\"\\n      },\\n      {\\n        \"paperId\": \"22ae025a4f0b644369d05a3e13c0021f868a8372\",\\n        \"url\": \"https://www.semanticscholar.org/paper/22ae025a4f0b644369d05a3e13c0021f868a8372\",\\n        \"title\": \"Hunyuan-DiT: A Powerful Multi-Resolution Diffusion Transformer with Fine-Grained Chinese Understanding\",\\n        \"citationCount\": 201,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2405.08748, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"Huyuan-DiT, a text-to-image diffusion transformer with fine-grained understanding of both English and Chinese, sets a new state-of-the-art in Chinese-to-image generation compared with other open-source models.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-05-14\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2286959603\",\\n            \"name\": \"Zhimin Li\"\\n          },\\n          {\\n            \"authorId\": \"2301232669\",\\n            \"name\": \"Jianwei Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2301456725\",\\n            \"name\": \"Qin Lin\"\\n          },\\n          {\\n            \"authorId\": \"2291439620\",\\n            \"name\": \"Jiangfeng Xiong\"\\n          },\\n          {\\n            \"authorId\": \"2301272172\",\\n            \"name\": \"Yanxin Long\"\\n          },\\n          {\\n            \"authorId\": \"2291203760\",\\n            \"name\": \"Xinchi Deng\"\\n          },\\n          {\\n            \"authorId\": \"2301231346\",\\n            \"name\": \"Yingfang Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2278003955\",\\n            \"name\": \"Xingchao Liu\"\\n          },\\n          {\\n            \"authorId\": \"2162225343\",\\n            \"name\": \"Minbin Huang\"\\n          },\\n          {\\n            \"authorId\": \"2301268000\",\\n            \"name\": \"Zedong Xiao\"\\n          },\\n          {\\n            \"authorId\": \"2301264655\",\\n            \"name\": \"Dayou Chen\"\\n          },\\n          {\\n            \"authorId\": \"2268375443\",\\n            \"name\": \"Jiajun He\"\\n          },\\n          {\\n            \"authorId\": \"2276668269\",\\n            \"name\": \"Jiahao Li\"\\n          },\\n          {\\n            \"authorId\": \"2301265720\",\\n            \"name\": \"Wenyue Li\"\\n          },\\n          {\\n            \"authorId\": \"2279868541\",\\n            \"name\": \"Chen Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2194482010\",\\n            \"name\": \"Rongwei Quan\"\\n          },\\n          {\\n            \"authorId\": \"2301417687\",\\n            \"name\": \"Jianxiang Lu\"\\n          },\\n          {\\n            \"authorId\": \"2301265144\",\\n            \"name\": \"Jiabin Huang\"\\n          },\\n          {\\n            \"authorId\": \"2302916549\",\\n            \"name\": \"Xiaoyan Yuan\"\\n          },\\n          {\\n            \"authorId\": \"2282541108\",\\n            \"name\": \"Xiao-Ting Zheng\"\\n          },\\n          {\\n            \"authorId\": \"2238393614\",\\n            \"name\": \"Yixuan Li\"\\n          },\\n          {\\n            \"authorId\": \"2290433340\",\\n            \"name\": \"Jihong Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2256776298\",\\n            \"name\": \"Chao Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2279065846\",\\n            \"name\": \"Mengxi Chen\"\\n          },\\n          {\\n            \"authorId\": \"2290446584\",\\n            \"name\": \"Jie Liu\"\\n          },\\n          {\\n            \"authorId\": \"2180527302\",\\n            \"name\": \"Zheng Fang\"\\n          },\\n          {\\n            \"authorId\": \"2280852403\",\\n            \"name\": \"Weiyan Wang\"\\n          },\\n          {\\n            \"authorId\": \"2067731583\",\\n            \"name\": \"J. Xue\"\\n          },\\n          {\\n            \"authorId\": \"2267016579\",\\n            \"name\": \"Yang-Dan Tao\"\\n          },\\n          {\\n            \"authorId\": \"2141509708\",\\n            \"name\": \"Jianchen Zhu\"\\n          },\\n          {\\n            \"authorId\": \"2278809013\",\\n            \"name\": \"Kai Liu\"\\n          },\\n          {\\n            \"authorId\": \"2107932778\",\\n            \"name\": \"Si-Da Lin\"\\n          },\\n          {\\n            \"authorId\": \"2373974424\",\\n            \"name\": \"Yifu Sun\"\\n          },\\n          {\\n            \"authorId\": \"2261466479\",\\n            \"name\": \"Yun Li\"\\n          },\\n          {\\n            \"authorId\": \"2195078271\",\\n            \"name\": \"Dongdong Wang\"\\n          },\\n          {\\n            \"authorId\": \"2393630397\",\\n            \"name\": \"Mingtao Chen\"\\n          },\\n          {\\n            \"authorId\": \"2297268600\",\\n            \"name\": \"Zhichao Hu\"\\n          },\\n          {\\n            \"authorId\": \"2290971075\",\\n            \"name\": \"Xiao Xiao\"\\n          },\\n          {\\n            \"authorId\": \"2266017750\",\\n            \"name\": \"Yan Chen\"\\n          },\\n          {\\n            \"authorId\": \"2265797902\",\\n            \"name\": \"Yuhong Liu\"\\n          },\\n          {\\n            \"authorId\": \"2283172530\",\\n            \"name\": \"Wei Liu\"\\n          },\\n          {\\n            \"authorId\": \"2256286591\",\\n            \"name\": \"Dingyong Wang\"\\n          },\\n          {\\n            \"authorId\": \"2284490220\",\\n            \"name\": \"Yong Yang\"\\n          },\\n          {\\n            \"authorId\": \"2280399696\",\\n            \"name\": \"Jie Jiang\"\\n          },\\n          {\\n            \"authorId\": \"2268722917\",\\n            \"name\": \"Qinglin Lu\"\\n          }\\n        ],\\n        \"abstract\": \"We present Hunyuan-DiT, a text-to-image diffusion transformer with fine-grained understanding of both English and Chinese. To construct Hunyuan-DiT, we carefully design the transformer structure, text encoder, and positional encoding. We also build from scratch a whole data pipeline to update and evaluate data for iterative model optimization. For fine-grained language understanding, we train a Multimodal Large Language Model to refine the captions of the images. Finally, Hunyuan-DiT can perform multi-turn multimodal dialogue with users, generating and refining images according to the context. Through our holistic human evaluation protocol with more than 50 professional human evaluators, Hunyuan-DiT sets a new state-of-the-art in Chinese-to-image generation compared with other open-source models. Code and pretrained models are publicly available at github.com/Tencent/HunyuanDiT\"\\n      },\\n      {\\n        \"paperId\": \"7d2f92bfcded0fe63cb3926155d769263b9580c9\",\\n        \"url\": \"https://www.semanticscholar.org/paper/7d2f92bfcded0fe63cb3926155d769263b9580c9\",\\n        \"title\": \"RLCoder: Reinforcement Learning for Repository-Level Code Completion\",\\n        \"citationCount\": 39,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2407.19487, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work proposes RLCoder, a novel reinforcement learning framework, which can enable the retriever to learn to retrieve useful content for code completion without the need for labeled data and introduces a stop signal mechanism.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-07-28\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2279774134\",\\n            \"name\": \"Yanlin Wang\"\\n          },\\n          {\\n            \"authorId\": \"2279774134\",\\n            \"name\": \"Yanlin Wang\"\\n          },\\n          {\\n            \"authorId\": \"2314374754\",\\n            \"name\": \"Daya Guo\"\\n          },\\n          {\\n            \"authorId\": \"2254800142\",\\n            \"name\": \"Jiachi Chen\"\\n          },\\n          {\\n            \"authorId\": \"2305869100\",\\n            \"name\": \"Ruikai Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2305694096\",\\n            \"name\": \"Yuchi Ma\"\\n          },\\n          {\\n            \"authorId\": \"2267902535\",\\n            \"name\": \"Zibin Zheng\"\\n          }\\n        ],\\n        \"abstract\": \"Repository-level code completion aims to generate code for unfinished code snippets within the context of a specified repository. Existing approaches mainly rely on retrievalaugmented generation strategies due to limitations in input sequence length. However, traditional lexical-based retrieval methods like BM25 struggle to capture code semantics, while model-based retrieval methods face challenges due to the lack of labeled data for training. Therefore, we propose RLCoder, a novel reinforcement learning framework, which can enable the retriever to learn to retrieve useful content for code completion without the need for labeled data. Specifically, we iteratively evaluate the usefulness of retrieved content based on the perplexity of the target code when provided with the retrieved content as additional context, and provide feedback to update the retriever parameters. This iterative process enables the retriever to learn from its successes and failures, gradually improving its ability to retrieve relevant and high-quality content. Considering that not all situations require information beyond code files and not all retrieved context is helpful for generation, we also introduce a stop signal mechanism, allowing the retriever to decide when to retrieve and which candidates to retain autonomously. Extensive experimental results demonstrate that RLCoder consistently outperforms state-of-the-art methods on CrossCodeEval and RepoEval, achieving 12.2% EM improvement over previous methods. Moreover, experiments show that our framework can generalize across different programming languages and further improve previous methods like RepoCoder. We provide the code and data at https://github.com/DeepSoftwareAnalytics/RLCoder.\"\\n      },\\n      {\\n        \"paperId\": \"69ec41a25a4ead52bab62ea220103fdde06f7126\",\\n        \"url\": \"https://www.semanticscholar.org/paper/69ec41a25a4ead52bab62ea220103fdde06f7126\",\\n        \"title\": \"Logic-in-Frames: Dynamic Keyframe Search via Visual Semantic-Logical Verification for Long Video Understanding\",\\n        \"citationCount\": 15,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2503.13139, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work introduces a semantics-driven search framework that reformulates keyframe selection under the paradigm of Visual Semantic-Logical Search, validating its effectiveness in bridging the logical gap between textual queries and visual-temporal reasoning.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2025-03-17\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2257320765\",\\n            \"name\": \"Weiyu Guo\"\\n          },\\n          {\\n            \"authorId\": \"2347655949\",\\n            \"name\": \"Ziyang Chen\"\\n          },\\n          {\\n            \"authorId\": \"2347552474\",\\n            \"name\": \"Shaoguang Wang\"\\n          },\\n          {\\n            \"authorId\": \"2347022035\",\\n            \"name\": \"Jianxiang He\"\\n          },\\n          {\\n            \"authorId\": \"2294806563\",\\n            \"name\": \"Yijie Xu\"\\n          },\\n          {\\n            \"authorId\": \"2348899671\",\\n            \"name\": \"Jinhui Ye\"\\n          },\\n          {\\n            \"authorId\": \"2257321422\",\\n            \"name\": \"Ying Sun\"\\n          },\\n          {\\n            \"authorId\": \"2346984163\",\\n            \"name\": \"Hui Xiong\"\\n          }\\n        ],\\n        \"abstract\": \"Understanding long video content is a complex endeavor that often relies on densely sampled frame captions or end-to-end feature selectors, yet these techniques commonly overlook the logical relationships between textual queries and visual elements. In practice, computational constraints necessitate coarse frame subsampling, a challenge analogous to\\\\\"finding a needle in a haystack.\\\\\"To address this issue, we introduce a semantics-driven search framework that reformulates keyframe selection under the paradigm of Visual Semantic-Logical Search. Specifically, we systematically define four fundamental logical dependencies: 1) spatial co-occurrence, 2) temporal proximity, 3) attribute dependency, and 4) causal order. These relations dynamically update frame sampling distributions through an iterative refinement process, enabling context-aware identification of semantically critical frames tailored to specific query requirements. Our method establishes new SOTA performance on the manually annotated benchmark in key-frame selection metrics. Furthermore, when applied to downstream video question-answering tasks, the proposed approach demonstrates the best performance gains over existing methods on LongVideoBench and Video-MME, validating its effectiveness in bridging the logical gap between textual queries and visual-temporal reasoning. The code will be publicly available.\"\\n      },\\n      {\\n        \"paperId\": \"c5dae02bb601106a51bec4497f0581affd6af8a7\",\\n        \"url\": \"https://www.semanticscholar.org/paper/c5dae02bb601106a51bec4497f0581affd6af8a7\",\\n        \"title\": \"Zero-Shot Class Unlearning in CLIP with Synthetic Samples\",\\n        \"citationCount\": 10,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2407.07485, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work focuses on unlearning within CLIP, a dual vision-language encoder model trained on a massive dataset of image-text pairs using contrastive loss, and expands the application of Lipschitz regularization to the multimodal context of CLIP to achieve forgetting.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-07-10\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2310610834\",\\n            \"name\": \"Alexey Kravets\"\\n          },\\n          {\\n            \"authorId\": \"2310607749\",\\n            \"name\": \"Vinay Namboodiri\"\\n          }\\n        ],\\n        \"abstract\": \"Machine unlearning is a crucial area of research. It is driven by the need to remove sensitive information from models to safeguard individuals\\' right to be forgotten under rigorous regulations such as GDPR. In this work, we focus on unlearning within CLIP, a dual vision-language encoder model trained on a massive dataset of image-text pairs using contrastive loss. To achieve forgetting we expand the application of Lipschitz regularization to the multimodal context of CLIP. Specifically, we smooth both visual and textual embeddings associated with the class intended to be forgotten relative to the perturbation introduced to the samples from that class. Additionally, importantly, we remove the necessity for real forgetting data by generating synthetic samples via gradient ascent maximizing the target class. Our forgetting procedure is iterative, where we track accuracy on a synthetic forget set and stop when accuracy falls below a chosen threshold. We employ a selective layers update strategy based on their average absolute gradient value to mitigate over-forgetting. We validate our approach on several standard datasets and provide thorough ablation analysis and comparisons with previous work.\"\\n      },\\n      {\\n        \"paperId\": \"096df636db478a4eb7ab9c74f8a6cae97efed149\",\\n        \"url\": \"https://www.semanticscholar.org/paper/096df636db478a4eb7ab9c74f8a6cae97efed149\",\\n        \"title\": \"Real-time parameter updating for nonlinear digital twins using inverse mapping models and transient-based features\",\\n        \"citationCount\": 20,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://link.springer.com/content/pdf/10.1007/s11071-023-08354-5.pdf\",\\n          \"status\": \"HYBRID\",\\n          \"license\": \"CCBY\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1007/s11071-023-08354-5?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1007/s11071-023-08354-5, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"Various types of transient response features are introduced to update parameter values of nonlinear dynamical systems with increased computational efficiency and accuracy and it is shown that a smart selection of features, based on the frequency content of the transient response, can improve the accuracy of the estimated parameter values, leading to more accurate updated models.\"\\n        },\\n        \"publicationTypes\": null,\\n        \"publicationDate\": \"2023-03-26\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2133908735\",\\n            \"name\": \"B. M. Kessels\"\\n          },\\n          {\\n            \"authorId\": \"102993518\",\\n            \"name\": \"R. Fey\"\\n          },\\n          {\\n            \"authorId\": \"2246982557\",\\n            \"name\": \"N. van de Wouw\"\\n          }\\n        ],\\n        \"abstract\": \"In the context of digital twins, it is essential that a model gives an accurate description of the (controlled) dynamic behavior of a physical system during the system\\\\u2019s entire operational life. Therefore, model updating techniques are required that enable real-time updating of physically interpretable parameter values and are applicable to a wide range of (nonlinear) dynamical systems. As traditional, iterative, parameter updating methods may be computationally too expensive for real-time updating, the inverse mapping parameter updating (IMPU) method is proposed as an alternative. For this method, first, an artificial neural network (ANN) is trained offline using novel features of simulated transient response data. Then, in the online phase, this ANN maps, with little computational cost, a set of measured output response features to parameter estimates enabling real-time model updating. In this paper, various types of transient response features are introduced to update parameter values of nonlinear dynamical systems with increased computational efficiency and accuracy. To analyze the efficacy of these features, the IMPU method is applied to a (simulated) nonlinear multibody system. It is shown that a smart selection of features, based on, e.g., the frequency content of the transient response, can improve the accuracy of the estimated parameter values, leading to more accurate updated models. Furthermore, the generalization capabilities of the ANNs are analyzed for these feature types, by varying the number of training samples and assessing the effect of incomplete training data. It is shown that the IMPU method can predict parameter values that are not part of the training data with acceptable accuracy as well.\"\\n      },\\n      {\\n        \"paperId\": \"82b7f2ab4faec66f16a62b9b5c610c7a5aa1e410\",\\n        \"url\": \"https://www.semanticscholar.org/paper/82b7f2ab4faec66f16a62b9b5c610c7a5aa1e410\",\\n        \"title\": \"Meta-AF: Meta-Learning for Adaptive Filters\",\\n        \"citationCount\": 31,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://ieeexplore.ieee.org/ielx7/6570655/9970249/09961879.pdf\",\\n          \"status\": \"HYBRID\",\\n          \"license\": \"CCBYNCND\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2204.11942, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work seeks to improve upon hand-derived adaptive filter algorithms and present a comprehensive framework for learning online, adaptive signal processing algorithms or update rules directly from data and frames the development of adaptive filters as a meta-learning problem in the context of deep learning and uses a form of self-supervision to learn online iterative update rules for adaptive filters.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2022-04-25\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"50823831\",\\n            \"name\": \"Jonah Casebeer\"\\n          },\\n          {\\n            \"authorId\": \"32125125\",\\n            \"name\": \"Nicholas J. Bryan\"\\n          },\\n          {\\n            \"authorId\": \"1718742\",\\n            \"name\": \"Paris Smaragdis\"\\n          }\\n        ],\\n        \"abstract\": \"Adaptive filtering algorithms are pervasive throughout signal processing and have had a material impact on a wide variety of domains including audio processing, telecommunications, biomedical sensing, astrophysics and cosmology, seismology, and many more. Adaptive filters typically operate via specialized online, iterative optimization methods such as least-mean squares or recursive least squares and aim to process signals in unknown or nonstationary environments. Such algorithms, however, can be slow and laborious to develop, require domain expertise to create, and necessitate mathematical insight for improvement. In this work, we seek to improve upon hand-derived adaptive filter algorithms and present a comprehensive framework for learning online, adaptive signal processing algorithms or update rules directly from data. To do so, we frame the development of adaptive filters as a meta-learning problem in the context of deep learning and use a form of self-supervision to learn online iterative update rules for adaptive filters. To demonstrate our approach, we focus on audio applications and systematically develop meta-learned adaptive filters for five canonical audio problems including system identification, acoustic echo cancellation, blind equalization, multi-channel dereverberation, and beamforming.We compare our approach against common baselines and/or recent state-of-the-art methods. We show we can learn high-performing adaptive filters that operate in real-time and, in most cases, significantly outperform each method we compare against \\\\u2013 all using a single general-purpose configuration of our approach.\"\\n      },\\n      {\\n        \"paperId\": \"f55c7f2156ecc389132acf5c3fbaca4c3f832abf\",\\n        \"url\": \"https://www.semanticscholar.org/paper/f55c7f2156ecc389132acf5c3fbaca4c3f832abf\",\\n        \"title\": \"Flexible Task Scheduling in Data Relay Satellite Networks\",\\n        \"citationCount\": 22,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {\\'tldr\\'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/TAES.2021.3115587?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/TAES.2021.3115587, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": null,\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2022-04-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"46532001\",\\n            \"name\": \"Guohua Wu\"\\n          },\\n          {\\n            \"authorId\": \"30176488\",\\n            \"name\": \"Qizhang Luo\"\\n          },\\n          {\\n            \"authorId\": \"2153095510\",\\n            \"name\": \"Yanqi Zhu\"\\n          },\\n          {\\n            \"authorId\": \"2049670966\",\\n            \"name\": \"Xinjiang Chen\"\\n          },\\n          {\\n            \"authorId\": \"2115387723\",\\n            \"name\": \"Yang Feng\"\\n          },\\n          {\\n            \"authorId\": \"1731634\",\\n            \"name\": \"W. Pedrycz\"\\n          }\\n        ],\\n        \"abstract\": \"The task-schedulingalgorithm is a key module to satisfy various complex user requirements, and improve the usage flexibility and efficiency of data relay satellites networks (DRSN). In this context, we first propose a novel application mode for DRSN, in which users are allowed to submit multiple optional service time windows and specify a preferred antenna as well as an expected execution duration for each task. Meanwhile, the start time of a service time window can be adjusted within a specified range. A mathematical programming model that maximizes the completion ratio of tasks and the expectation satisfaction of users is established. Moreover, a conflict resolution-assisted iterative task-scheduling algorithm (CRITS) is designed, composing of five closely dependent operators: resource matching, service durations generation, conflict evaluation, conflict resolution, and solution update. To verify the effectiveness of the proposed CRITS, extensive experiments are carried out. The experimental results demonstrate the competitive performance of CRITS in addressing the DRSN scheduling problem. In comparison with two heuristic algorithms (heuristic algorithm based on time-freedom degree and a heuristic algorithm based on task priority) and a meta-heuristic algorithm (adaptive variable neighborhood descent combined with a tabu list), the proposed CRITS increases the overall completion ratio of tasks by 6.65, 10.26, and 10.96%, respectively.\"\\n      },\\n      {\\n        \"paperId\": \"b12f40443c189e665520811f99f0ec2b47d90194\",\\n        \"url\": \"https://www.semanticscholar.org/paper/b12f40443c189e665520811f99f0ec2b47d90194\",\\n        \"title\": \"RSS-Based Byzantine Fault-Tolerant Localization Algorithm Under NLOS Environment\",\\n        \"citationCount\": 28,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {\\'tldr\\'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/LCOMM.2020.3027904?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/LCOMM.2020.3027904, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": null,\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2021-02-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"26411201\",\\n            \"name\": \"Xiaojun Mei\"\\n          },\\n          {\\n            \"authorId\": \"35250883\",\\n            \"name\": \"Huafeng Wu\"\\n          },\\n          {\\n            \"authorId\": \"144922818\",\\n            \"name\": \"J. Xian\"\\n          },\\n          {\\n            \"authorId\": \"150041356\",\\n            \"name\": \"Bowen Chen\"\\n          }\\n        ],\\n        \"abstract\": \"Localization is one of the most critical tasks in wireless sensor networks, but achieving a relatively accurate location estimation is challenging when there have Byzantine fault and non-line-of-sight (NLOS) bias simultaneously. In this context, a localization method, based on received signal strength (RSS), is proposed in this letter to mitigate the impact of Byzantine fault and NLOS bias on the localization accuracy of wireless sensor networks. The proposed method relies on a Byzantine fault-tolerant localization algorithm (BFLA), which converts the localization problem into a generalized trust-region subproblem (GTRS) by applying certain approximations. In order to obtain a feasible solution to the GTRS, a block-coordinate update (BCU) function with a regularization term is used to divide the localization problem into two subproblems. An iterative method, whose start-point is obtained by an unconstrained squared-range (USR) algorithm, is then used to obtain a solution. Numerical simulations are carried out to show the effectiveness of the proposed method, compared with the state-of-the-art approaches in different scenarios.\"\\n      },\\n      {\\n        \"paperId\": \"c04e9454a696086ea69f37f59d45347f5c6d438c\",\\n        \"url\": \"https://www.semanticscholar.org/paper/c04e9454a696086ea69f37f59d45347f5c6d438c\",\\n        \"title\": \"Enhancing Transferability of Adversarial Examples with Spatial Momentum\",\\n        \"citationCount\": 10,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2203.13479, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A novel method named Spatial Momentum Iterative FGSM attack (SMI-FGSM), which introduces the mechanism of momentum accumulation from temporal domain to spatial domain by considering the context information from different regions within the image to enhance adversarial transferability.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2022-03-25\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2445322\",\\n            \"name\": \"Guoqiu Wang\"\\n          },\\n          {\\n            \"authorId\": \"27066021\",\\n            \"name\": \"Huanqian Yan\"\\n          },\\n          {\\n            \"authorId\": \"2769710\",\\n            \"name\": \"Xingxing Wei\"\\n          }\\n        ],\\n        \"abstract\": \"Many adversarial attack methods achieve satisfactory attack success rates under the white-box setting, but they usually show poor transferability when attacking other DNN models. Momentum-based attack is one effective method to improve transferability. It integrates the momentum term into the iterative process, which can stabilize the update directions by adding the gradients\\' temporal correlation for each pixel. We argue that only this temporal momentum is not enough, the gradients from the spatial domain within an image, i.e. gradients from the context pixels centered on the target pixel are also important to the stabilization. For that, we propose a novel method named Spatial Momentum Iterative FGSM attack (SMI-FGSM), which introduces the mechanism of momentum accumulation from temporal domain to spatial domain by considering the context information from different regions within the image. SMI-FGSM is then integrated with temporal momentum to simultaneously stabilize the gradients\\' update direction from both the temporal and spatial domains. Extensive experiments show that our method indeed further enhances adversarial transferability. It achieves the best transferability success rate for multiple mainstream undefended and defended models, which outperforms the state-of-the-art attack methods by a large margin of 10\\\\\\\\% on average.\"\\n      },\\n      {\\n        \"paperId\": \"8a1f15ef8875b2f2a80a5ddd52ea41967203cd26\",\\n        \"url\": \"https://www.semanticscholar.org/paper/8a1f15ef8875b2f2a80a5ddd52ea41967203cd26\",\\n        \"title\": \"xERTE: Explainable Reasoning on Temporal Knowledge Graphs for Forecasting Future Links\",\\n        \"citationCount\": 35,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2012.15537, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A future link forecasting framework that reasons over query-relevant subgraphs of temporal KGs and jointly models the graph structures and the temporal context information and conducts a survey to show that the reasoning arguments extracted by the model for link forecasting are aligned with human understanding.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Review\"\\n        ],\\n        \"publicationDate\": \"2020-12-31\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2765914\",\\n            \"name\": \"Zhen Han\"\\n          },\\n          {\\n            \"authorId\": \"2158172225\",\\n            \"name\": \"Peng Chen\"\\n          },\\n          {\\n            \"authorId\": \"10684484\",\\n            \"name\": \"Yunpu Ma\"\\n          },\\n          {\\n            \"authorId\": \"1742501819\",\\n            \"name\": \"Volker Tresp\"\\n          }\\n        ],\\n        \"abstract\": \"Interest has been rising lately towards modeling time-evolving knowledge graphs (KGs). Recently, graph representation learning approaches have become the dominant paradigm for link prediction on temporal KGs. However, the embedding-based approaches largely operate in a black-box fashion, lacking the ability to judge the results\\' reliability. This paper provides a future link forecasting framework that reasons over query-relevant subgraphs of temporal KGs and jointly models the graph structures and the temporal context information. Especially, we propose a temporal relational attention mechanism and a novel reverse representation update scheme to guide the extraction of an enclosing subgraph around the query. The subgraph is expanded by an iterative sampling of temporal neighbors and attention propagation. As a result, our approach provides human-understandable arguments for the prediction. We evaluate our model on four benchmark temporal knowledge graphs for the link forecasting task. While being more explainable, our model also obtains a relative improvement of up to 17.7 $\\\\\\\\%$ on MRR compared to the previous best KG forecasting methods. We also conduct a survey with 53 respondents, and the results show that the reasoning arguments extracted by the model for link forecasting are aligned with human understanding.\"\\n      },\\n      {\\n        \"paperId\": \"51458bf945a446271cf857016582d3fc881f8f9a\",\\n        \"url\": \"https://www.semanticscholar.org/paper/51458bf945a446271cf857016582d3fc881f8f9a\",\\n        \"title\": \"Joint User Grouping, Scheduling, and Precoding for Multicast Energy Efficiency in Multigroup Multicast Systems\",\\n        \"citationCount\": 17,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://doi.org/10.1109/twc.2020.3020081\",\\n          \"status\": \"HYBRID\",\\n          \"license\": \"CCBY\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2005.06843, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper studies the joint design of user grouping, scheduling and precoding to optimize energy efficiency (EE) for multigroup multicast scenarios in single-cell multiuser MISO downlink channels and proposes a convex-concave procedure framework based iterative algorithm for each optimization criteria.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2020-05-14\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"40370059\",\\n            \"name\": \"Ashok Bandi\"\\n          },\\n          {\\n            \"authorId\": \"153604607\",\\n            \"name\": \"Bhavani Shankar\"\\n          },\\n          {\\n            \"authorId\": \"1760292\",\\n            \"name\": \"S. Chatzinotas\"\\n          },\\n          {\\n            \"authorId\": \"102896981\",\\n            \"name\": \"B. Ottersten\"\\n          }\\n        ],\\n        \"abstract\": \"This paper studies the joint design of user grouping, scheduling (or admission control) and precoding to optimize energy efficiency (EE) for multigroup multicast scenarios in single-cell multiuser MISO downlink channels. Noticing that the existing definition of EE fails to account for group sizes, a new metric called multicast energy efficiency (MEE) is proposed. In this context, the joint design is considered for the maximization of MEE, EE, and scheduled users. Firstly, with the help of binary variables (associated with grouping and scheduling) the joint design problem is formulated as a mixed-Boolean fractional programming problem such that it facilitates the joint update of grouping, scheduling and precoding variables. Further, several novel optimization formulations are proposed to reveal the hidden difference of convex/ concave structure in the objective and associated constraints. Thereafter, we propose a convex-concave procedure framework based iterative algorithm for each optimization criteria where grouping, scheduling, and precoding variables are updated jointly in each iteration. Finally, we compare the performance of the three design criteria concerning three performance metrics namely MEE, EE, and scheduled users through Monte-Carlo simulations. These simulations establish the need for MEE and the improvement from the system optimization.\"\\n      },\\n      {\\n        \"paperId\": \"8c8595a050845a8926aa52b22998b2ca609a5d96\",\\n        \"url\": \"https://www.semanticscholar.org/paper/8c8595a050845a8926aa52b22998b2ca609a5d96\",\\n        \"title\": \"Automatic Distortion Rectification of Wide-Angle Images Using Outlier Refinement for Streamlining Vision Tasks\",\\n        \"citationCount\": 16,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://www.mdpi.com/1424-8220/20/3/894/pdf?version=1581513374\",\\n          \"status\": \"GOLD\",\\n          \"license\": \"CCBY\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC7039389, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"The proposed algorithm was able to rectify the distortions of wide-angle and fish-eye cameras even in extreme conditions such as heavy illumination changes and severe lens distortions.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2020-02-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"97740251\",\\n            \"name\": \"Vijay Kakani\"\\n          },\\n          {\\n            \"authorId\": \"2380060\",\\n            \"name\": \"Hakil Kim\"\\n          },\\n          {\\n            \"authorId\": \"2108780171\",\\n            \"name\": \"Jongso Lee\"\\n          },\\n          {\\n            \"authorId\": \"34630628\",\\n            \"name\": \"Choonwoo Ryu\"\\n          },\\n          {\\n            \"authorId\": \"14653761\",\\n            \"name\": \"Mahendar Kumbham\"\\n          }\\n        ],\\n        \"abstract\": \"The study proposes an outlier refinement methodology for automatic distortion rectification of wide-angle and fish-eye lens camera models in the context of streamlining vision-based tasks. The line-members sets are estimated in a scene through accumulation of line candidates emerging from the same edge source. An iterative optimization with an outlier refinement scheme was applied to the loss value, to simultaneously remove the extremely curved outliers from the line-members set and update the robust line members as well as estimating the best-fit distortion parameters with lowest possible loss. The proposed algorithm was able to rectify the distortions of wide-angle and fish-eye cameras even in extreme conditions such as heavy illumination changes and severe lens distortions. Experiments were conducted using various evaluation metrics both at the pixel-level (image quality, edge stretching effects, pixel-point error) as well as higher-level use-cases (object detection, height estimation) with respect to real and synthetic data from publicly available, privately acquired sources. The performance evaluations of the proposed algorithm have been investigated using an ablation study on various datasets in correspondence to the significance analysis of the refinement scheme and loss function. Several quantitative and qualitative comparisons were carried out on the proposed approach against various self-calibration approaches.\"\\n      },\\n      {\\n        \"paperId\": \"07503b6fa32459203e176ce880c2a5ba23f0f8e5\",\\n        \"url\": \"https://www.semanticscholar.org/paper/07503b6fa32459203e176ce880c2a5ba23f0f8e5\",\\n        \"title\": \"Learning to Decode: Reinforcement Learning for Decoding of Sparse Graph-Based Channel Codes\",\\n        \"citationCount\": 11,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2010.05637, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work model the CN update process as a multi-armed bandit process with dependent arms and employ a Q-learning scheme for optimizing the CN scheduling policy to improve decoding performance and reduce the decoding complexity dramatically once the model is learned.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2020-10-12\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"49916135\",\\n            \"name\": \"Salman Habib\"\\n          },\\n          {\\n            \"authorId\": \"93037496\",\\n            \"name\": \"Allison Beemer\"\\n          },\\n          {\\n            \"authorId\": \"2064748249\",\\n            \"name\": \"J. Kliewer\"\\n          }\\n        ],\\n        \"abstract\": \"We show in this work that reinforcement learning can be successfully applied to decoding short to moderate length sparse graph-based channel codes. Specifically, we focus on low-density parity check (LDPC) codes, which for example have been standardized in the context of 5G cellular communication systems due to their excellent error correcting performance. These codes are typically decoded via belief propagation iterative decoding on the corresponding bipartite (Tanner) graph of the code via flooding, i.e., all check and variable nodes in the Tanner graph are updated at once. In contrast, in this paper we utilize a sequential update policy which selects the optimum check node (CN) scheduling in order to improve decoding performance. In particular, we model the CN update process as a multi-armed bandit process with dependent arms and employ a Q-learning scheme for optimizing the CN scheduling policy. In order to reduce the learning complexity, we propose a novel graph-induced CN clustering approach to partition the state space in such a way that dependencies between clusters are minimized. Our results show that compared to other decoding approaches from the literature, the proposed reinforcement learning scheme not only significantly improves the decoding performance, but also reduces the decoding complexity dramatically once the model is learned.\"\\n      },\\n      {\\n        \"paperId\": \"9aa6a885754a27fe42a87e4dfaed87d618fd8518\",\\n        \"url\": \"https://www.semanticscholar.org/paper/9aa6a885754a27fe42a87e4dfaed87d618fd8518\",\\n        \"title\": \"Iterative Refinement of Project-Level Code Context for Precise Code Generation with Compiler Feedback\",\\n        \"citationCount\": 37,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2403.16792, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"CoGen is presented, a new code generation approach that uses compiler feedback to improve the LLM-generated code and consistently outperforms the existing retrieval-based code generation baselines.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-03-25\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2170260327\",\\n            \"name\": \"Zhangqian Bi\"\\n          },\\n          {\\n            \"authorId\": \"2273871682\",\\n            \"name\": \"Yao Wan\"\\n          },\\n          {\\n            \"authorId\": \"2293395709\",\\n            \"name\": \"Zheng Wang\"\\n          },\\n          {\\n            \"authorId\": \"2273385018\",\\n            \"name\": \"Hongyu Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2293312291\",\\n            \"name\": \"Batu Guan\"\\n          },\\n          {\\n            \"authorId\": \"2293769071\",\\n            \"name\": \"Fangxin Lu\"\\n          },\\n          {\\n            \"authorId\": \"2293399901\",\\n            \"name\": \"Zili Zhang\"\\n          },\\n          {\\n            \"authorId\": \"34296085\",\\n            \"name\": \"Yulei Sui\"\\n          },\\n          {\\n            \"authorId\": \"1678835\",\\n            \"name\": \"Xuanhua Shi\"\\n          },\\n          {\\n            \"authorId\": \"2277587874\",\\n            \"name\": \"Hai Jin\"\\n          }\\n        ],\\n        \"abstract\": \"Large Language Models (LLMs) have shown remarkable progress in automated code generation. Yet, LLM-generated code may contain errors in API usage, class, data structure, or missing project-specific information. As much of this project-specific context cannot fit into the prompts of LLMs, we must find ways to allow the model to explore the project-level code context. We present CoCoGen, a new code generation approach that uses compiler feedback to improve the LLM-generated code. CoCoGen first leverages static analysis to identify mismatches between the generated code and the project\\'s context. It then iteratively aligns and fixes the identified errors using information extracted from the code repository. We integrate CoCoGen with two representative LLMs, i.e., GPT-3.5-Turbo and Code Llama (13B), and apply it to Python code generation. Experimental results show that CoCoGen significantly improves the vanilla LLMs by over 80% in generating code dependent on the project context and consistently outperforms the existing retrieval-based code generation baselines.\"\\n      },\\n      {\\n        \"paperId\": \"3e43429b24beb5b93513775a259c23ce3a133f67\",\\n        \"url\": \"https://www.semanticscholar.org/paper/3e43429b24beb5b93513775a259c23ce3a133f67\",\\n        \"title\": \"Addressing Background Context Bias in Few-Shot Segmentation Through Iterative Modulation\",\\n        \"citationCount\": 23,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {\\'tldr\\', \\'abstract\\'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/CVPR52733.2024.00324?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/CVPR52733.2024.00324, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": null,\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-06-16\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"151101398\",\\n            \"name\": \"Lanyun Zhu\"\\n          },\\n          {\\n            \"authorId\": \"2261909262\",\\n            \"name\": \"Tianrun Chen\"\\n          },\\n          {\\n            \"authorId\": \"2510538\",\\n            \"name\": \"Jianxiong Yin\"\\n          },\\n          {\\n            \"authorId\": \"2250849553\",\\n            \"name\": \"Simon See\"\\n          },\\n          {\\n            \"authorId\": \"2321778822\",\\n            \"name\": \"Jun Liu\"\\n          }\\n        ],\\n        \"abstract\": null\\n      },\\n      {\\n        \"paperId\": \"8745157f991013b23fbb79d300ba560f9005c8d4\",\\n        \"url\": \"https://www.semanticscholar.org/paper/8745157f991013b23fbb79d300ba560f9005c8d4\",\\n        \"title\": \"Energy-Based Cross Attention for Bayesian Context Update in Text-to-Image Diffusion Models\",\\n        \"citationCount\": 29,\\n        \"openAccessPdf\": {\\n          \"url\": \"http://arxiv.org/pdf/2306.09869\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2306.09869, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A novel energy-based model (EBM) framework is presented that first formulate EBMs of latent image representations and text embeddings in each cross-attention layer of the denoising autoencoder, thereby implicitly minimizing a nested hierarchy of energy functions.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2023-06-16\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"153118937\",\\n            \"name\": \"Geon Yeong Park\"\\n          },\\n          {\\n            \"authorId\": \"2109216792\",\\n            \"name\": \"Jeongsol Kim\"\\n          },\\n          {\\n            \"authorId\": \"3332270\",\\n            \"name\": \"Beomsu Kim\"\\n          },\\n          {\\n            \"authorId\": \"2152577026\",\\n            \"name\": \"Sang Wan Lee\"\\n          },\\n          {\\n            \"authorId\": \"30547794\",\\n            \"name\": \"Jong-Chul Ye\"\\n          }\\n        ],\\n        \"abstract\": \"Despite the remarkable performance of text-to-image diffusion models in image generation tasks, recent studies have raised the issue that generated images sometimes cannot capture the intended semantic contents of the text prompts, which phenomenon is often called semantic misalignment. To address this, here we present a novel energy-based model (EBM) framework. Specifically, we first formulate EBMs of latent image representations and text embeddings in each cross-attention layer of the denoising autoencoder. Then, we obtain the gradient of the log posterior of context vectors, which can be updated and transferred to the subsequent cross-attention layer, thereby implicitly minimizing a nested hierarchy of energy functions. Our latent EBMs further allow zero-shot compositional generation as a linear combination of cross-attention outputs from different contexts. Using extensive experiments, we demonstrate that the proposed method is highly effective in handling various image generation tasks, including multi-concept generation, text-guided image inpainting, and real and synthetic image editing.\"\\n      },\\n      {\\n        \"paperId\": \"b217b6bc340af9a10bebbf8acc36ea30871769bd\",\\n        \"url\": \"https://www.semanticscholar.org/paper/b217b6bc340af9a10bebbf8acc36ea30871769bd\",\\n        \"title\": \"In-Context Learning with Iterative Demonstration Selection\",\\n        \"citationCount\": 69,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2310.09881, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"Iterative Demonstration Selection (IDS) iteratively selects examples that are diverse but still strongly correlated with the test sample as ICL demonstrations, and can consistently outperform existing ICL demonstration selection methods.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2023-10-15\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2084609980\",\\n            \"name\": \"Chengwei Qin\"\\n          },\\n          {\\n            \"authorId\": \"2258754564\",\\n            \"name\": \"Aston Zhang\"\\n          },\\n          {\\n            \"authorId\": \"1627060158\",\\n            \"name\": \"Anirudh Dagar\"\\n          },\\n          {\\n            \"authorId\": \"2258719488\",\\n            \"name\": \"Wenming Ye\"\\n          }\\n        ],\\n        \"abstract\": \"Spurred by advancements in scale, large language models (LLMs) have demonstrated strong few-shot learning ability via in-context learning (ICL). However, the performance of ICL has been shown to be highly sensitive to the selection of few-shot demonstrations. Selecting the most suitable examples as context remains an ongoing challenge and an open problem. Existing literature has highlighted the importance of selecting examples that are diverse or semantically similar to the test sample while ignoring the fact that the optimal selection dimension, i.e., diversity or similarity, is task-specific. Based on how the test sample is answered, we propose Iterative Demonstration Selection (IDS) to leverage the merits of both dimensions. Using zero-shot chain-of-thought reasoning (Zero-shot-CoT), IDS iteratively selects examples that are diverse but still strongly correlated with the test sample as ICL demonstrations. Specifically, IDS applies Zero-shot-CoT to the test sample before demonstration selection. The output reasoning path is then used to choose demonstrations that are prepended to the test sample for inference. The generated answer is followed by its corresponding reasoning path for extracting a new set of demonstrations in the next iteration. After several iterations, IDS adopts majority voting to obtain the final result. Through extensive experiments on tasks including reasoning, question answering, and topic classification, we demonstrate that IDS can consistently outperform existing ICL demonstration selection methods.\"\\n      },\\n      {\\n        \"paperId\": \"43ffae1c1c6a7371dfe89ac84ae45f3456fdc5a0\",\\n        \"url\": \"https://www.semanticscholar.org/paper/43ffae1c1c6a7371dfe89ac84ae45f3456fdc5a0\",\\n        \"title\": \"Iterative Forward Tuning Boosts In-context Learning in Language Models\",\\n        \"citationCount\": 19,\\n        \"openAccessPdf\": {\\n          \"url\": \"http://arxiv.org/pdf/2305.13016\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2305.13016, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A novel two-stage framework to boost ICL in LLMs is introduced, which delineates the ICL process into two distinct stages: Deep-Thinking and test stages, and incorporates a unique attention mechanism, i.e., iterative enhanced attention, which enables multiple rounds of information accumulation.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2023-05-22\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2135964855\",\\n            \"name\": \"Jiaxi Yang\"\\n          },\\n          {\\n            \"authorId\": \"151471590\",\\n            \"name\": \"Binyuan Hui\"\\n          },\\n          {\\n            \"authorId\": \"2144399900\",\\n            \"name\": \"Min Yang\"\\n          },\\n          {\\n            \"authorId\": \"66200440\",\\n            \"name\": \"Binhua Li\"\\n          },\\n          {\\n            \"authorId\": \"2087380523\",\\n            \"name\": \"Fei Huang\"\\n          },\\n          {\\n            \"authorId\": \"1527090216\",\\n            \"name\": \"Yongbin Li\"\\n          }\\n        ],\\n        \"abstract\": \"Despite the advancements in in-context learning (ICL) for large language models (LLMs), current research centers on specific prompt engineering, such as demonstration selection, with the expectation that a single iteration of demonstrations processing can generalize effectively to a given test sample. However, this perspective overlooks the potential benefits derived from multiple iterations involving demonstrations, a practice aligning more closely with the iterative decision-making process exhibited by humans, who often learn through analogy. In this study, we introduce a novel two-stage framework to boost ICL in LLMs. Specifically, our framework delineates the ICL process into two distinct stages: Deep-Thinking and test stages. The Deep-Thinking stage incorporates a unique attention mechanism, i.e., iterative enhanced attention, which enables multiple rounds of information accumulation. This mechanism operates by manipulating the Key-Value matrices without training, fostering enhanced understanding capabilities in LLMs by thinking demonstrations multiple times. We evaluated Deep-Thinking across a range of benchmarks and LLMs, showing its superior performance over vanilla ICL methods and its effectiveness in challenging tasks where demonstration selection is infeasible.\"\\n      },\\n      {\\n        \"paperId\": \"fa4ea8b773ffd6706ba5cf427f9671f81b04dcdd\",\\n        \"url\": \"https://www.semanticscholar.org/paper/fa4ea8b773ffd6706ba5cf427f9671f81b04dcdd\",\\n        \"title\": \"ALLURE: Auditing and Improving LLM-based Evaluation of Text using Iterative In-Context-Learning\",\\n        \"citationCount\": 17,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://arxiv.org/pdf/2309.13701\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2309.13701, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"Allure, a systematic approach to Auditing Large Language Models Understanding and Reasoning Errors, involves comparing LLM-generated evaluations with annotated data, and iteratively incorporating instances of significant deviation into the evaluator, which leverages in-context learning (ICL) to enhance and improve robust evaluation of text by LLMs.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2023-09-24\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2245381886\",\\n            \"name\": \"Hosein Hasanbeig\"\\n          },\\n          {\\n            \"authorId\": \"20013278\",\\n            \"name\": \"Hiteshi Sharma\"\\n          },\\n          {\\n            \"authorId\": \"15449757\",\\n            \"name\": \"Leo Betthauser\"\\n          },\\n          {\\n            \"authorId\": \"1844283112\",\\n            \"name\": \"F. Frujeri\"\\n          },\\n          {\\n            \"authorId\": \"2248289111\",\\n            \"name\": \"Ida Momennejad\"\\n          }\\n        ],\\n        \"abstract\": \"From grading papers to summarizing medical documents, large language models (LLMs) are evermore used for evaluation of text generated by humans and AI alike. However, despite their extensive utility, LLMs exhibit distinct failure modes, necessitating a thorough audit and improvement of their text evaluation capabilities. Here we introduce ALLURE, a systematic approach to Auditing Large Language Models Understanding and Reasoning Errors. ALLURE involves comparing LLM-generated evaluations with annotated data, and iteratively incorporating instances of significant deviation into the evaluator, which leverages in-context learning (ICL) to enhance and improve robust evaluation of text by LLMs. Through this iterative process, we refine the performance of the evaluator LLM, ultimately reducing reliance on human annotators in the evaluation process. We anticipate ALLURE to serve diverse applications of LLMs in various domains related to evaluation of textual data, such as medical summarization, education, and and productivity.\"\\n      },\\n      {\\n        \"paperId\": \"860bafe8a3aa0e3a981951f0757996272276b54e\",\\n        \"url\": \"https://www.semanticscholar.org/paper/860bafe8a3aa0e3a981951f0757996272276b54e\",\\n        \"title\": \"ALLURE: A Systematic Protocol for Auditing and Improving LLM-based Evaluation of Text using Iterative In-Context-Learning\",\\n        \"citationCount\": 12,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"AllURE, a systematic approach to Auditing Large Language Models Understanding and Reasoning Errors, involves comparing LLM-generated evaluations with annotated data, and iteratively incorporating instances of significant deviation into the evaluator, which leverages in-context learning (ICL) to enhance and improve robust evaluation of text by LLMs.\"\\n        },\\n        \"publicationTypes\": null,\\n        \"publicationDate\": null,\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2245381886\",\\n            \"name\": \"Hosein Hasanbeig\"\\n          },\\n          {\\n            \"authorId\": \"2258705787\",\\n            \"name\": \"Microsoft Usa HITESHI SHARMA\"\\n          },\\n          {\\n            \"authorId\": \"2258705783\",\\n            \"name\": \"Microsoft Usa LEO BETTHAUSER\"\\n          },\\n          {\\n            \"authorId\": \"2258705381\",\\n            \"name\": \"Microsoft Usa FELIPE VIEIRA FRUJERI\"\\n          },\\n          {\\n            \"authorId\": \"2248289111\",\\n            \"name\": \"Ida Momennejad\"\\n          }\\n        ],\\n        \"abstract\": null\\n      },\\n      {\\n        \"paperId\": \"3f27df9f8b30a8c35fa3e7df7fdf7ec1a5cc28cd\",\\n        \"url\": \"https://www.semanticscholar.org/paper/3f27df9f8b30a8c35fa3e7df7fdf7ec1a5cc28cd\",\\n        \"title\": \"Enhancing Iterative Learning Control With Fractional Power Update Law\",\\n        \"citationCount\": 14,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {\\'tldr\\'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/JAS.2023.123525?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/JAS.2023.123525, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": null,\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2023-05-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2118272992\",\\n            \"name\": \"Zihan Li\"\\n          },\\n          {\\n            \"authorId\": \"144986261\",\\n            \"name\": \"D. Shen\"\\n          },\\n          {\\n            \"authorId\": \"2156506994\",\\n            \"name\": \"Xinghuo Yu\"\\n          }\\n        ],\\n        \"abstract\": \"The P-type update law has been the mainstream technique used in iterative learning control (ILC) systems, which resembles linear feedback control with asymptotical convergence. In recent years, finite-time control strategies such as terminal sliding mode control have been shown to be effective in ramping up convergence speed by introducing fractional power with feedback. In this paper, we show that such mechanism can equally ramp up the learning speed in ILC systems. We first propose a fractional power update rule for ILC of single-input-single-output linear systems. A nonlinear error dynamics is constructed along the iteration axis to illustrate the evolutionary converging process. Using the nonlinear mapping approach, fast convergence towards the limit cycles of tracking errors inherently existing in ILC systems is proven. The limit cycles are shown to be tunable to determine the steady states. Numerical simulations are provided to verify the theoretical results.\"\\n      },\\n      {\\n        \"paperId\": \"38333f6e8f0388968edc4b2ea7a683ce69677e69\",\\n        \"url\": \"https://www.semanticscholar.org/paper/38333f6e8f0388968edc4b2ea7a683ce69677e69\",\\n        \"title\": \"Monte Carlo Tree Search Boosts Reasoning via Iterative Preference Learning\",\\n        \"citationCount\": 182,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2405.00451, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work leverages Monte Carlo Tree Search (MCTS) to iteratively collect preference data, utilizing its look-ahead ability to break down instance-level rewards into more granular step-level signals, to enhance consistency in intermediate steps.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-05-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2299483768\",\\n            \"name\": \"Yuxi Xie\"\\n          },\\n          {\\n            \"authorId\": \"1996705\",\\n            \"name\": \"Anirudh Goyal\"\\n          },\\n          {\\n            \"authorId\": \"2289857220\",\\n            \"name\": \"Wenyue Zheng\"\\n          },\\n          {\\n            \"authorId\": \"2257033898\",\\n            \"name\": \"Min-Yen Kan\"\\n          },\\n          {\\n            \"authorId\": \"2542999\",\\n            \"name\": \"T. Lillicrap\"\\n          },\\n          {\\n            \"authorId\": \"2266466003\",\\n            \"name\": \"Kenji Kawaguchi\"\\n          },\\n          {\\n            \"authorId\": \"2289844602\",\\n            \"name\": \"Michael Shieh\"\\n          }\\n        ],\\n        \"abstract\": \"We introduce an approach aimed at enhancing the reasoning capabilities of Large Language Models (LLMs) through an iterative preference learning process inspired by the successful strategy employed by AlphaZero. Our work leverages Monte Carlo Tree Search (MCTS) to iteratively collect preference data, utilizing its look-ahead ability to break down instance-level rewards into more granular step-level signals. To enhance consistency in intermediate steps, we combine outcome validation and stepwise self-evaluation, continually updating the quality assessment of newly generated data. The proposed algorithm employs Direct Preference Optimization (DPO) to update the LLM policy using this newly generated step-level preference data. Theoretical analysis reveals the importance of using on-policy sampled data for successful self-improving. Extensive evaluations on various arithmetic and commonsense reasoning tasks demonstrate remarkable performance improvements over existing models. For instance, our approach outperforms the Mistral-7B Supervised Fine-Tuning (SFT) baseline on GSM8K, MATH, and ARC-C, with substantial increases in accuracy to $81.8\\\\\\\\%$ (+$5.9\\\\\\\\%$), $34.7\\\\\\\\%$ (+$5.8\\\\\\\\%$), and $76.4\\\\\\\\%$ (+$15.8\\\\\\\\%$), respectively. Additionally, our research delves into the training and inference compute tradeoff, providing insights into how our method effectively maximizes performance gains. Our code is publicly available at https://github.com/YuxiXie/MCTS-DPO.\"\\n      },\\n      {\\n        \"paperId\": \"a1675f47125aa409525c5f759b5e6bcc1c8831aa\",\\n        \"url\": \"https://www.semanticscholar.org/paper/a1675f47125aa409525c5f759b5e6bcc1c8831aa\",\\n        \"title\": \"Enhancing Retrieval-Augmented Large Language Models with Iterative Retrieval-Generation Synergy\",\\n        \"citationCount\": 360,\\n        \"openAccessPdf\": {\\n          \"url\": \"http://arxiv.org/pdf/2305.15294\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2305.15294, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper shows that strong performance can be achieved by a method, called Iter-RetGen, which synergizes retrieval and generation in an iterative manner, and shows that it can flexibly leverage parametric knowledge and non-parametric knowledge, and is superior to or competitive with state-of-the-art retrieval-augmented baselines while causing fewer overheads of retrieval andgeneration.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2023-05-24\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"144485528\",\\n            \"name\": \"Zhihong Shao\"\\n          },\\n          {\\n            \"authorId\": \"2171182\",\\n            \"name\": \"Yeyun Gong\"\\n          },\\n          {\\n            \"authorId\": \"1752875\",\\n            \"name\": \"Yelong Shen\"\\n          },\\n          {\\n            \"authorId\": \"1730108\",\\n            \"name\": \"Minlie Huang\"\\n          },\\n          {\\n            \"authorId\": \"46429989\",\\n            \"name\": \"Nan Duan\"\\n          },\\n          {\\n            \"authorId\": \"2109136147\",\\n            \"name\": \"Weizhu Chen\"\\n          }\\n        ],\\n        \"abstract\": \"Large language models are powerful text processors and reasoners, but are still subject to limitations including outdated knowledge and hallucinations, which necessitates connecting them to the world. Retrieval-augmented large language models have raised extensive attention for grounding model generation on external knowledge. However, retrievers struggle to capture relevance, especially for queries with complex information needs. Recent work has proposed to improve relevance modeling by having large language models actively involved in retrieval, i.e., to improve retrieval with generation. In this paper, we show that strong performance can be achieved by a method we call Iter-RetGen, which synergizes retrieval and generation in an iterative manner. A model output shows what might be needed to finish a task, and thus provides an informative context for retrieving more relevant knowledge which in turn helps generate a better output in the next iteration. Compared with recent work which interleaves retrieval with generation when producing an output, Iter-RetGen processes all retrieved knowledge as a whole and largely preserves the flexibility in generation without structural constraints. We evaluate Iter-RetGen on multi-hop question answering, fact verification, and commonsense reasoning, and show that it can flexibly leverage parametric knowledge and non-parametric knowledge, and is superior to or competitive with state-of-the-art retrieval-augmented baselines while causing fewer overheads of retrieval and generation. We can further improve performance via generation-augmented retrieval adaptation.\"\\n      },\\n      {\\n        \"paperId\": \"af5c7848417882012203ac21399977ebda695a2b\",\\n        \"url\": \"https://www.semanticscholar.org/paper/af5c7848417882012203ac21399977ebda695a2b\",\\n        \"title\": \"RepoCoder: Repository-Level Code Completion Through Iterative Retrieval and Generation\",\\n        \"citationCount\": 325,\\n        \"openAccessPdf\": {\\n          \"url\": \"http://arxiv.org/pdf/2303.12570\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2303.12570, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"RepoCoder streamlines the repository-level code completion process by incorporating a similarity-based retriever and a pre-trained code language model in an iterative retrieval-generation pipeline and has the ability to generate code at various levels of granularity.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2023-03-22\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2158120018\",\\n            \"name\": \"Fengji Zhang\"\\n          },\\n          {\\n            \"authorId\": \"143876723\",\\n            \"name\": \"B. Chen\"\\n          },\\n          {\\n            \"authorId\": \"2211964951\",\\n            \"name\": \"Yue Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2155352529\",\\n            \"name\": \"Jin Liu\"\\n          },\\n          {\\n            \"authorId\": \"2134434187\",\\n            \"name\": \"Daoguang Zan\"\\n          },\\n          {\\n            \"authorId\": \"145469202\",\\n            \"name\": \"Yi Mao\"\\n          },\\n          {\\n            \"authorId\": \"153249455\",\\n            \"name\": \"Jian-Guang Lou\"\\n          },\\n          {\\n            \"authorId\": \"2109136147\",\\n            \"name\": \"Weizhu Chen\"\\n          }\\n        ],\\n        \"abstract\": \"The task of repository-level code completion is to continue writing the unfinished code based on a broader context of the repository. While for automated code completion tools, it is difficult to utilize the useful information scattered in different files. We propose RepoCoder, a simple, generic, and effective framework to address the challenge. It streamlines the repository-level code completion process by incorporating a similarity-based retriever and a pre-trained code language model in an iterative retrieval-generation pipeline. RepoCoder makes effective utilization of repository-level information for code completion and has the ability to generate code at various levels of granularity. Moreover, we propose a new benchmark RepoEval, which consists of the latest and high-quality real-world repositories covering line, API invocation, and function body completion scenarios. Experimental results indicate that RepoCoder significantly improves the In-File completion baseline by over 10% in all settings and consistently outperforms the vanilla retrieval-augmented code completion approach. Furthermore, we validate the effectiveness of RepoCoder through comprehensive analysis, providing valuable insights for future research. Our source code and benchmark are publicly available: https://github.com/microsoft/CodeT/tree/main/RepoCoder\"\\n      },\\n      {\\n        \"paperId\": \"cc26cab4d4e0c6e3dddbae96abd65ee2d1f87f69\",\\n        \"url\": \"https://www.semanticscholar.org/paper/cc26cab4d4e0c6e3dddbae96abd65ee2d1f87f69\",\\n        \"title\": \"An iterative method to compute conformal mappings and their inverses in the context of water waves over topographies\",\\n        \"citationCount\": 13,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://arxiv.org/pdf/2103.02022\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2103.02022, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"An iterative numerical method to compute the conformal mapping in the context of propagating water waves over uneven topographies using the MATLAB Schwarz-Christoffel toolbox mapping as a benchmark is investigated.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2021-03-02\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"104491233\",\\n            \"name\": \"M. Flamarion\"\\n          },\\n          {\\n            \"authorId\": \"1429030643\",\\n            \"name\": \"R. Ribeiro-Jr\"\\n          }\\n        ],\\n        \"abstract\": \"An iterative numerical method to compute the conformal mapping in the context of propagating water waves over uneven topographies is investigated. The map flattens the fluid domain onto a canonical strip in which computations are performed. The accuracy of the method is tested by using the MATLAB Schwarz\\\\u2013Christoffel toolbox mapping as a benchmark. Besides, we give a numerical alternative to compute the inverse of the conformal map.\"\\n      },\\n      {\\n        \"paperId\": \"edf23ce5689d0acf18cc7ff8556c696f9231644d\",\\n        \"url\": \"https://www.semanticscholar.org/paper/edf23ce5689d0acf18cc7ff8556c696f9231644d\",\\n        \"title\": \"Five-Precision GMRES-Based Iterative Refinement\",\\n        \"citationCount\": 36,\\n        \"openAccessPdf\": {\\n          \"url\": \"http://eprints.maths.manchester.ac.uk/2807/1/paper.pdf\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1137/23m1549079?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1137/23m1549079, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"GMRES-IR5 has the potential to solve relatively badly conditioned problems in less time and memory than GMRES-IR3, thanks to the use of lower precision arithmetic in the GMRES iterations.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-02-08\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"38318287\",\\n            \"name\": \"P. Amestoy\"\\n          },\\n          {\\n            \"authorId\": \"1760511\",\\n            \"name\": \"A. Buttari\"\\n          },\\n          {\\n            \"authorId\": \"1699285\",\\n            \"name\": \"N. Higham\"\\n          },\\n          {\\n            \"authorId\": \"1398545502\",\\n            \"name\": \"J. L\\\\u2019Excellent\"\\n          },\\n          {\\n            \"authorId\": \"144615299\",\\n            \"name\": \"Th\\\\u00e9o Mary\"\\n          },\\n          {\\n            \"authorId\": \"1720746305\",\\n            \"name\": \"Bastien Vieubl\\\\u00e9\"\\n          }\\n        ],\\n        \"abstract\": \". GMRES-based iterative re\\\\ufb01nement in three precisions (GMRES-IR3) uses a low pre- cision LU factorization to accelerate the solution of a linear system without compromising numerical stability or robustness. GMRES-IR3 solves the update equation using GMRES preconditioned by the LU factors, where all operations within GMRES are carried out in the working precision u , except for the matrix-vector products and the application of the preconditioner, which require the use of extra precision u 2 . The use of extra precision can be expensive, and is especially unattractive if it is not available in hardware; for this reason, existing implementations have not used extra precision, despite the absence of an error analysis for this approach. We relax the requirements on the precisions used within GMRES, allowing the use of arbitrary precisions u p (for applying the preconditioner) and u g (for the rest of the operations). We obtain the \\\\ufb01ve-precision GMRES-based iterative re\\\\ufb01nement (GMRES-IR5) algorithm. We carry out a rounding error analysis that generalizes that of GMRES-IR3, obtaining conditions under which the forward and backward errors converge to their limiting values. Our analysis makes use of a new result on the backward stability of MGS-GMRES in two precisions. On hardware where up to \\\\ufb01ve arithmetics are available, the number of possible combinations of precisions in GMRES-IR5 is extremely large, but our analysis identi\\\\ufb01es a small subset of relevant combinations. By choosing from within this subset one can achieve di\\\\ufb00erent levels of tradeo\\\\ufb00 between cost and robustness, which allows for a \\\\ufb01ner choice of precisions depending on the problem di\\\\ufb03culty and the available hardware. Our numerical experiments on both random dense matrices and real-life sparse matrices from a wide range of applications show that the practical behavior of GMRES-IR5 is in good agreement with our theoretical analysis. GMRES-IR5 therefore has the potential to solve relatively badly conditioned problems in less time and memory than GMRES-IR3, thanks to the use of lower precision arithmetic in the GMRES iterations.\"\\n      },\\n      {\\n        \"paperId\": \"dc256e179d4e8eff48879a40ddc414b15b0b2300\",\\n        \"url\": \"https://www.semanticscholar.org/paper/dc256e179d4e8eff48879a40ddc414b15b0b2300\",\\n        \"title\": \"RAT: Retrieval Augmented Thoughts Elicit Context-Aware Reasoning in Long-Horizon Generation\",\\n        \"citationCount\": 69,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2403.05313, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work explores how iterative revising a chain of thoughts with the help of information retrieval significantly improves large language models\\' reasoning and generation ability in long-horizon generation tasks, while hugely mitigating hallucination.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-03-08\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"47196237\",\\n            \"name\": \"Zihao Wang\"\\n          },\\n          {\\n            \"authorId\": \"70097297\",\\n            \"name\": \"Anji Liu\"\\n          },\\n          {\\n            \"authorId\": \"2257447835\",\\n            \"name\": \"Haowei Lin\"\\n          },\\n          {\\n            \"authorId\": \"2290567624\",\\n            \"name\": \"Jiaqi Li\"\\n          },\\n          {\\n            \"authorId\": \"2257636629\",\\n            \"name\": \"Xiaojian Ma\"\\n          },\\n          {\\n            \"authorId\": \"2257367774\",\\n            \"name\": \"Yitao Liang\"\\n          }\\n        ],\\n        \"abstract\": \"We explore how iterative revising a chain of thoughts with the help of information retrieval significantly improves large language models\\' reasoning and generation ability in long-horizon generation tasks, while hugely mitigating hallucination. In particular, the proposed method -- *retrieval-augmented thoughts* (RAT) -- revises each thought step one by one with retrieved information relevant to the task query, the current and the past thought steps, after the initial zero-shot CoT is generated. Applying RAT to GPT-3.5, GPT-4, and CodeLLaMA-7b substantially improves their performances on various long-horizon generation tasks; on average of relatively increasing rating scores by 13.63% on code generation, 16.96% on mathematical reasoning, 19.2% on creative writing, and 42.78% on embodied task planning. The demo page can be found at https://craftjarvis.github.io/RAT\"\\n      },\\n      {\\n        \"paperId\": \"ca5cfff2fcc87e1fb88c74b6eca3f7ee2c638bde\",\\n        \"url\": \"https://www.semanticscholar.org/paper/ca5cfff2fcc87e1fb88c74b6eca3f7ee2c638bde\",\\n        \"title\": \"Learned Two-Step Iterative Shrinkage Thresholding Algorithm for Deep Compressive Sensing\",\\n        \"citationCount\": 31,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {\\'tldr\\'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/TCSVT.2023.3325340?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/TCSVT.2023.3325340, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": null,\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-05-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"35361812\",\\n            \"name\": \"Hongping Gan\"\\n          },\\n          {\\n            \"authorId\": \"2260282205\",\\n            \"name\": \"Xiaoyang Wang\"\\n          },\\n          {\\n            \"authorId\": \"2260279661\",\\n            \"name\": \"Lijun He\"\\n          },\\n          {\\n            \"authorId\": \"2260264613\",\\n            \"name\": \"Jie Liu\"\\n          }\\n        ],\\n        \"abstract\": \"Deep unrolling architectures have revitalized compressive sensing (CS) by seamlessly blending deep neural networks with traditional optimization-based reconstruction algorithms. In pursuit of an efficient and deep interpretable approach, we propose LTwIST for CS problem, a novel deep unrolling framework that draws inspiration from the well-known two-step iterative shrinkage thresholding (TwIST) algorithm. LTwIST uses a trainable sensing matrix to adaptively learn structural information in images, and introduces a customized U-block architecture to solve the proximal mapping of nonlinear transformations connected with the sparsity-inducing regularizer. Specifically, each iteration recovery step of LTwIST corresponds to an iterative update step of the traditional TwIST algorithm. Moreover, the proposed method is designed to learn all the parameters end-to-end without manual tuning such as shrinkable thresholds, step sizes, etc. As a result, LTwIST obviates the need for manual parameter optimization, allows for high-quality image recovery and provides unambiguous interpretability. Moreover, our proposed LTwIST is also applicable to CS-based magnetic resonance imaging and exhibits a strong reconstruction performance. Extensive experiments on several public benchmark datasets demonstrate that the proposed LTwIST outperforms existing state-of-the-art deep CS methods by considerable margins in terms of quality evaluation metrics and visual performance. Our code is available on LTwIST.\"\\n      },\\n      {\\n        \"paperId\": \"53dcf71b6be11d543f1bc7753e8ff8e9945eff77\",\\n        \"url\": \"https://www.semanticscholar.org/paper/53dcf71b6be11d543f1bc7753e8ff8e9945eff77\",\\n        \"title\": \"Iterative and Scenario-based Requirements Specification in a System of Systems Context\",\\n        \"citationCount\": 10,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2102.05400, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper describes an extension for the scenario-modeling language for Kotlin (SMLK) to continuously and formally model requirements on SoS and CS level and combines SMLK with agile development techniques to support the iterative requirements specification and modeling.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2021-02-10\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"1423730807\",\\n            \"name\": \"Carsten Wiecher\"\\n          },\\n          {\\n            \"authorId\": \"2909345\",\\n            \"name\": \"Joel Greenyer\"\\n          },\\n          {\\n            \"authorId\": \"40500399\",\\n            \"name\": \"Carsten Wolff\"\\n          },\\n          {\\n            \"authorId\": \"152617022\",\\n            \"name\": \"H. Anacker\"\\n          },\\n          {\\n            \"authorId\": \"3261846\",\\n            \"name\": \"R. Dumitrescu\"\\n          }\\n        ],\\n        \"abstract\": \"[Context&Motivation]Due to the managerial ,operational and evolutionary independence of constituent systems (CSs) in a System of Systems (SoS) context, top-down and linear requirements engineering (RE) approaches are insufficient. RE techniques for SoS must support iterating, changing, synchronizing, and communicating requirements across different abstraction and hierarchy levels as well as scopes of responsibility. [Question/Problem] We address the challenge of SoS requirements specification, where requirements can describe the SoS behavior, but also the behavior of CSs that are developed independently. [Principal Ideas] To support the requirements specification in an SoS environment, we propose a scenario-based and iterative specification technique. This allows requirements engineers to continuously model and jointly execute and test the system behavior for the SoS and the CS in order to detect contradictions in the requirement specifications at an early stage. [Contribution] In this paper, we describe an extension for the scenario-modeling language for Kotlin (SMLK) to continuously and formally model requirements on SoS and CS level. To support the iterative requirements specification and modeling we combine SMLK with agile development techniques. We demonstrate the applicability of our approach with the help of an example from the field of e-mobility.\"\\n      },\\n      {\\n        \"paperId\": \"f6ca551535a92b669ebd97b246ac582e73af30ee\",\\n        \"url\": \"https://www.semanticscholar.org/paper/f6ca551535a92b669ebd97b246ac582e73af30ee\",\\n        \"title\": \"An attempt to analyse Iterative Data Snooping and L1-norm based on Monte Carlo simulation in the context of leveling networks\",\\n        \"citationCount\": 11,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://riunet.upv.es/bitstream/10251/193468/3/KleinSuracide%20-%20An%20attempt%20to%20analyse%20Iterative%20Data%20Snooping%20and%20L1-norm%20based%20on%20Monte%20Carlo%20si....pdf\",\\n          \"status\": \"GREEN\",\\n          \"license\": \"CCBYNC\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1080/00396265.2021.1878338?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1080/00396265.2021.1878338, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"Evaluating the outlier identification performance of iterative Data Snooping (IDS) and L1-norm in levelling networks by considering the redundancy of the network, number and size of the outliers shows that L 1-norm performs better than IDS for the case of networks with low redundancy.\"\\n        },\\n        \"publicationTypes\": null,\\n        \"publicationDate\": \"2021-02-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"31183250\",\\n            \"name\": \"I. Klein\"\\n          },\\n          {\\n            \"authorId\": \"1834987\",\\n            \"name\": \"S. Suraci\"\\n          },\\n          {\\n            \"authorId\": \"2125043152\",\\n            \"name\": \"Leonardo Castro de Oliveira\"\\n          },\\n          {\\n            \"authorId\": \"72414897\",\\n            \"name\": \"V. F. Rofatto\"\\n          },\\n          {\\n            \"authorId\": \"2059636056\",\\n            \"name\": \"M. T. Matsuoka\"\\n          },\\n          {\\n            \"authorId\": \"1691074\",\\n            \"name\": \"S. Baselga\"\\n          }\\n        ],\\n        \"abstract\": \"The goal of this paper is to evaluate the outlier identification performance of iterative Data Snooping (IDS) and L1-norm in levelling networks by considering the redundancy of the network, number and size of the outliers. For this purpose, several Monte-Carlo experiments were conducted into three different levelling networks configurations. In addition, a new way to compare the results of IDS based on Least Squares (LS) residuals and robust estimators such as the L1-norm has also been developed and presented. From the perspective of analysis only according to the success rate, it is shown that L1-norm performs better than IDS for the case of networks with low redundancy , especially for cases where more than one outlier is present in the dataset. In the relationship between false positive rate and outlier identification success rate, however, IDS performs better than L1-norm, independently of the levelling network configuration, number and size of outliers.\"\\n      },\\n      {\\n        \"paperId\": \"2a5e4419c4bf16178a441c975b970fa33a9f5361\",\\n        \"url\": \"https://www.semanticscholar.org/paper/2a5e4419c4bf16178a441c975b970fa33a9f5361\",\\n        \"title\": \"Nash Equilibria for Linear Quadratic Discrete-Time Dynamic Games via Iterative and Data-Driven Algorithms\",\\n        \"citationCount\": 15,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://ieeexplore.ieee.org/ielx7/9/4601496/10463100.pdf\",\\n          \"status\": \"HYBRID\",\\n          \"license\": \"CCBY\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/TAC.2024.3375249?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/TAC.2024.3375249, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This article proposes four different iterative algorithms to find Nash equilibrium strategies for discrete-time linear quadratic games based on the solution of either Lyapunov or Riccati equations for each player.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-10-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2046956775\",\\n            \"name\": \"Benita Nortmann\"\\n          },\\n          {\\n            \"authorId\": \"2267944139\",\\n            \"name\": \"Andrea Monti\"\\n          },\\n          {\\n            \"authorId\": \"1764871\",\\n            \"name\": \"M. Sassano\"\\n          },\\n          {\\n            \"authorId\": \"2695624\",\\n            \"name\": \"T. Mylvaganam\"\\n          }\\n        ],\\n        \"abstract\": \"Determining feedback Nash equilibrium solutions of nonzero-sum dynamic games is generally challenging. In this article, we propose four different iterative algorithms to find Nash equilibrium strategies for discrete-time linear quadratic games. The strategy update laws are based on the solution of either Lyapunov or Riccati equations for each player. Local convergence criteria are discussed. Motivated by the fact that in many practical scenarios each player in the game may have access to different (incomplete) information, we also introduce purely data-driven implementations of the algorithms. This allows the players to reach a Nash equilibrium solution of the game via scheduled experiments and without knowledge of each other\\'s performance criteria or of the system dynamics. The efficacy of the presented algorithms is illustrated via numerical examples and a practical example involving human\\\\u2013robot interaction.\"\\n      },\\n      {\\n        \"paperId\": \"d47ecf36e7e67d9349ecac29dfb981feaf229229\",\\n        \"url\": \"https://www.semanticscholar.org/paper/d47ecf36e7e67d9349ecac29dfb981feaf229229\",\\n        \"title\": \"iMSCGnet: Iterative Multi-Scale Context-Guided Segmentation of Skin Lesion in Dermoscopic Images\",\\n        \"citationCount\": 23,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://ieeexplore.ieee.org/ielx7/6287639/8948470/09007375.pdf\",\\n          \"status\": \"GOLD\",\\n          \"license\": \"CCBY\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ACCESS.2020.2974512?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ACCESS.2020.2974512, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A multi-scale context-guided network named as MSCGnet is proposed to segment the skin lesions accurately and generally outperforms the state-of-the-art methods.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": null,\\n        \"authors\": [\\n          {\\n            \"authorId\": \"5691344\",\\n            \"name\": \"Yujiao Tang\"\\n          },\\n          {\\n            \"authorId\": \"39271955\",\\n            \"name\": \"Zhiwen Fang\"\\n          },\\n          {\\n            \"authorId\": \"153431402\",\\n            \"name\": \"Shaofeng Yuan\"\\n          },\\n          {\\n            \"authorId\": \"30380386\",\\n            \"name\": \"C. Zhan\"\\n          },\\n          {\\n            \"authorId\": \"2072728505\",\\n            \"name\": \"Yanyan Xing\"\\n          },\\n          {\\n            \"authorId\": \"10638646\",\\n            \"name\": \"Joey Tianyi Zhou\"\\n          },\\n          {\\n            \"authorId\": \"2115412895\",\\n            \"name\": \"Feng Yang\"\\n          }\\n        ],\\n        \"abstract\": \"Despite much effort has been devoted to skin lesion segmentation, the performance of existing methods is still not satisfactory enough for practical applications. The challenges may include fuzzy lesion boundary, uneven and low contrast, and variation of colors across space, which often lead to fragmentary segmentation and inaccurate boundary. To alleviate this problem, we propose a multi-scale context-guided network named as MSCGnet to segment the skin lesions accurately. In MSCGnet, the context information is utilized to guide the feature encoding procedure. Moreover, because of the information loss in spatial down-sampling, a context-based attention structure (CAs) is designed to select effective context features in the decoding path. Furthermore, we boost the performance of MSCGnet with iterations and term this upgraded version as iterative MSCGnet, denoted as iMSCGnet. To supervise the training of iMSCGnet in an end-to-end fashion, a novel objective function of deep supervision, which consists of the terms of each encoding layers and the terms from each MSCGnet output of iMSCGnet, is employed. Our method is evaluated extensively on the four publicly available datasets, including ISBI2016 <xref ref-type=\\\\\"bibr\\\\\" rid=\\\\\"ref1\\\\\">[1]</xref>, ISBI2017 <xref ref-type=\\\\\"bibr\\\\\" rid=\\\\\"ref2\\\\\">[2]</xref>, ISIC2018 <xref ref-type=\\\\\"bibr\\\\\" rid=\\\\\"ref3\\\\\">[3]</xref> and PH2 <xref ref-type=\\\\\"bibr\\\\\" rid=\\\\\"ref4\\\\\">[4]</xref> datasets. The experimental results prove the effectiveness of proposed components and show that our method generally outperforms the state-of-the-art methods.\"\\n      },\\n      {\\n        \"paperId\": \"29528d8cb030a65f62a35b1237f1f5483077ad0a\",\\n        \"url\": \"https://www.semanticscholar.org/paper/29528d8cb030a65f62a35b1237f1f5483077ad0a\",\\n        \"title\": \"Inference Scaling for Long-Context Retrieval Augmented Generation\",\\n        \"citationCount\": 47,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2410.04343, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work investigates inference scaling for retrieval augmented generation (RAG), exploring the combination of multiple strategies beyond simply increasing the quantity of knowledge, including in-context learning and iterative prompting, and reveals that increasing inference computation leads to nearly linear gains in RAG performance when optimally allocated.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-10-06\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2028213158\",\\n            \"name\": \"Zhenrui Yue\"\\n          },\\n          {\\n            \"authorId\": \"39371343\",\\n            \"name\": \"Honglei Zhuang\"\\n          },\\n          {\\n            \"authorId\": \"2324782053\",\\n            \"name\": \"Aijun Bai\"\\n          },\\n          {\\n            \"authorId\": \"2261281337\",\\n            \"name\": \"Kai Hui\"\\n          },\\n          {\\n            \"authorId\": \"1886219\",\\n            \"name\": \"R. Jagerman\"\\n          },\\n          {\\n            \"authorId\": \"2324910979\",\\n            \"name\": \"Hansi Zeng\"\\n          },\\n          {\\n            \"authorId\": \"2099586642\",\\n            \"name\": \"Zhen Qin\"\\n          },\\n          {\\n            \"authorId\": \"2325158655\",\\n            \"name\": \"Dong Wang\"\\n          },\\n          {\\n            \"authorId\": \"2261356664\",\\n            \"name\": \"Xuanhui Wang\"\\n          },\\n          {\\n            \"authorId\": \"1815447\",\\n            \"name\": \"Michael Bendersky\"\\n          }\\n        ],\\n        \"abstract\": \"The scaling of inference computation has unlocked the potential of long-context large language models (LLMs) across diverse settings. For knowledge-intensive tasks, the increased compute is often allocated to incorporate more external knowledge. However, without effectively utilizing such knowledge, solely expanding context does not always enhance performance. In this work, we investigate inference scaling for retrieval augmented generation (RAG), exploring the combination of multiple strategies beyond simply increasing the quantity of knowledge, including in-context learning and iterative prompting. These strategies provide additional flexibility to scale test-time computation (e.g., by increasing retrieved documents or generation steps), thereby enhancing LLMs\\' ability to effectively acquire and utilize contextual information. We address two key questions: (1) How does RAG performance benefit from the scaling of inference computation when optimally configured? (2) Can we predict the optimal test-time compute allocation for a given budget by modeling the relationship between RAG performance and inference parameters? Our observations reveal that increasing inference computation leads to nearly linear gains in RAG performance when optimally allocated, a relationship we describe as the inference scaling laws for RAG. Building on this, we further develop the computation allocation model to estimate RAG performance across different inference configurations. The model predicts optimal inference parameters under various computation constraints, which align closely with the experimental results. By applying these optimal configurations, we demonstrate that scaling inference compute on long-context LLMs achieves up to 58.9% gains on benchmark datasets compared to standard RAG.\"\\n      },\\n      {\\n        \"paperId\": \"ca47c63a97848e60389037c93a4feb2daf849c3e\",\\n        \"url\": \"https://www.semanticscholar.org/paper/ca47c63a97848e60389037c93a4feb2daf849c3e\",\\n        \"title\": \"Iterative Data Smoothing: Mitigating Reward Overfitting and Overoptimization in RLHF\",\\n        \"citationCount\": 41,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2401.16335, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"The core idea is that during each training epoch, the model is updated with the data, but also update the date using the model, replacing hard labels with soft labels, and the empirical findings highlight the superior performance of this approach over the traditional methods.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-01-29\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"23111704\",\\n            \"name\": \"Banghua Zhu\"\\n          },\\n          {\\n            \"authorId\": \"2273930444\",\\n            \"name\": \"Michael I. Jordan\"\\n          },\\n          {\\n            \"authorId\": \"2258657022\",\\n            \"name\": \"Jiantao Jiao\"\\n          }\\n        ],\\n        \"abstract\": \"Reinforcement Learning from Human Feedback (RLHF) is a pivotal technique that aligns language models closely with human-centric values. The initial phase of RLHF involves learning human values using a reward model from ranking data. It is observed that the performance of the reward model degrades after one epoch of training, and optimizing too much against the learned reward model eventually hinders the true objective. This paper delves into these issues, leveraging the theoretical insights to design improved reward learning algorithm termed \\'Iterative Data Smoothing\\' (IDS). The core idea is that during each training epoch, we not only update the model with the data, but also update the date using the model, replacing hard labels with soft labels. Our empirical findings highlight the superior performance of this approach over the traditional methods.\"\\n      },\\n      {\\n        \"paperId\": \"8051818817a9a3815be6623a679d4a7f5a7b7964\",\\n        \"url\": \"https://www.semanticscholar.org/paper/8051818817a9a3815be6623a679d4a7f5a7b7964\",\\n        \"title\": \"Retrieve, Summarize, Plan: Advancing Multi-hop Question Answering with an Iterative Approach\",\\n        \"citationCount\": 36,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2407.13101, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A novel iterative RAG method called ReSP is proposed, equipped with a dual-function summarizer that compresses information from retrieved documents, targeting both the overarching question and the current sub-question concurrently.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Book\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-07-18\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2305310515\",\\n            \"name\": \"Zhouyu Jiang\"\\n          },\\n          {\\n            \"authorId\": \"2273904059\",\\n            \"name\": \"Mengshu Sun\"\\n          },\\n          {\\n            \"authorId\": \"2303947668\",\\n            \"name\": \"Lei Liang\"\\n          },\\n          {\\n            \"authorId\": \"2290024603\",\\n            \"name\": \"Zhiqiang Zhang\"\\n          }\\n        ],\\n        \"abstract\": \"Multi-hop question answering is a challenging task with distinct industrial relevance, and Retrieval-Augmented Generation (RAG) methods based on large language models (LLMs) have become a popular approach to tackle this task. Owing to the potential inability to retrieve all necessary information in a single iteration, a series of iterative RAG methods has been recently developed, showing significant performance improvements. However, existing methods still face two critical challenges: context overload resulting from multiple rounds of retrieval, and over-planning and repetitive planning due to the lack of a recorded retrieval trajectory. In this paper, we propose a novel iterative RAG method called ReSP, equipped with a dual-function summarizer. This summarizer compresses information from retrieved documents, targeting both the overarching question and the current sub-question concurrently. Experimental results on the multi-hop question-answering datasets HotpotQA and 2WikiMultihopQA demonstrate that our method significantly outperforms the state-of-the-art, and exhibits excellent robustness concerning context length.\"\\n      },\\n      {\\n        \"paperId\": \"9a1b5eb2a3e17621feab02c0f7f8c4df65ebf71c\",\\n        \"url\": \"https://www.semanticscholar.org/paper/9a1b5eb2a3e17621feab02c0f7f8c4df65ebf71c\",\\n        \"title\": \"CTIF-Net: A CNN-Transformer Iterative Fusion Network for Salient Object Detection\",\\n        \"citationCount\": 36,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {\\'tldr\\'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/TCSVT.2023.3321190?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/TCSVT.2023.3321190, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": null,\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-05-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2116395954\",\\n            \"name\": \"Junbin Yuan\"\\n          },\\n          {\\n            \"authorId\": \"2254058034\",\\n            \"name\": \"Aiqing Zhu\"\\n          },\\n          {\\n            \"authorId\": \"2575087\",\\n            \"name\": \"Qingzhen Xu\"\\n          },\\n          {\\n            \"authorId\": \"2757120\",\\n            \"name\": \"Kanoksak Wattanachote\"\\n          },\\n          {\\n            \"authorId\": \"2240697074\",\\n            \"name\": \"Yongyi Gong\"\\n          }\\n        ],\\n        \"abstract\": \"Capturing sufficient global context and rich spatial structure information is critical for dense prediction tasks. Convolutional Neural Network (CNN) is particularly adept at modeling fine-grained local features, while Transformer excels at modeling global context information. It is evident that CNN and Transformer exhibit complementary characteristics. Exploring the design of a network, that efficiently fuses these two models to leverage their strengths fully and achieve more accurate detection, represents a promising and worthwhile research topic. In this paper, we introduce a novel CNN-Transformer Iterative Fusion Network (CTIF-Net) for salient object detection. It efficiently combines CNN and Transformer to achieve superior performance by using a parallel dual encoder structure and a feature iterative fusion module. Firstly, CTIF-Net extracts features from the image using the CNN and the Transformer, respectively. Secondly, two feature convertors and a feature iterative fusion module are employed to combine and iteratively refine the two sets of features. The experimental results on multiple SOD datasets show that CTIF-Net outperforms 17 state-of-the-art methods, achieving higher performance in various mainstream evaluation metrics such as F-measure, S-measure, and MAE value. Code can be found at https://github.com/danielfaster/CTIF-Net/.\"\\n      },\\n      {\\n        \"paperId\": \"c811bedbe8f4c21d0cba9f9175f7c2eb203284a7\",\\n        \"url\": \"https://www.semanticscholar.org/paper/c811bedbe8f4c21d0cba9f9175f7c2eb203284a7\",\\n        \"title\": \"Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context\",\\n        \"citationCount\": 2813,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2403.05530, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"Gemini 1.5 models achieve near-perfect recall on long-context retrieval tasks across modalities, improve the state-of-the-art in long-document QA, long-video QA and long-context ASR, and match or surpass Gemini 1.0 Ultra\\'s state-of-the-art performance across a broad set of benchmarks.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-03-08\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"1557386977\",\\n            \"name\": \"Machel Reid\"\\n          },\\n          {\\n            \"authorId\": \"2417003\",\\n            \"name\": \"Nikolay Savinov\"\\n          },\\n          {\\n            \"authorId\": \"3035073\",\\n            \"name\": \"Denis Teplyashin\"\\n          },\\n          {\\n            \"authorId\": \"150077954\",\\n            \"name\": \"Dmitry Lepikhin\"\\n          },\\n          {\\n            \"authorId\": \"2542999\",\\n            \"name\": \"T. Lillicrap\"\\n          },\\n          {\\n            \"authorId\": \"2285263\",\\n            \"name\": \"Jean-Baptiste Alayrac\"\\n          },\\n          {\\n            \"authorId\": \"1737285\",\\n            \"name\": \"Radu Soricut\"\\n          },\\n          {\\n            \"authorId\": \"2672644\",\\n            \"name\": \"Angeliki Lazaridou\"\\n          },\\n          {\\n            \"authorId\": \"2273534960\",\\n            \"name\": \"Orhan Firat\"\\n          },\\n          {\\n            \"authorId\": \"4337102\",\\n            \"name\": \"Julian Schrittwieser\"\\n          },\\n          {\\n            \"authorId\": \"2460849\",\\n            \"name\": \"Ioannis Antonoglou\"\\n          },\\n          {\\n            \"authorId\": \"1508890387\",\\n            \"name\": \"Rohan Anil\"\\n          },\\n          {\\n            \"authorId\": \"148016269\",\\n            \"name\": \"Sebastian Borgeaud\"\\n          },\\n          {\\n            \"authorId\": \"2273563615\",\\n            \"name\": \"Andrew M. Dai\"\\n          },\\n          {\\n            \"authorId\": \"2143434227\",\\n            \"name\": \"Katie Millican\"\\n          },\\n          {\\n            \"authorId\": \"2275180676\",\\n            \"name\": \"Ethan Dyer\"\\n          },\\n          {\\n            \"authorId\": \"2143471164\",\\n            \"name\": \"Mia Glaese\"\\n          },\\n          {\\n            \"authorId\": \"2070364520\",\\n            \"name\": \"Thibault Sottiaux\"\\n          },\\n          {\\n            \"authorId\": \"2275292292\",\\n            \"name\": \"Ben-jamin Lee\"\\n          },\\n          {\\n            \"authorId\": \"2290484786\",\\n            \"name\": \"Fabio Viola\"\\n          },\\n          {\\n            \"authorId\": \"47447264\",\\n            \"name\": \"Malcolm Reynolds\"\\n          },\\n          {\\n            \"authorId\": \"2145139570\",\\n            \"name\": \"Yuanzhong Xu\"\\n          },\\n          {\\n            \"authorId\": \"2065370007\",\\n            \"name\": \"James Molloy\"\\n          },\\n          {\\n            \"authorId\": \"2249566095\",\\n            \"name\": \"Jilin Chen\"\\n          },\\n          {\\n            \"authorId\": \"2090818\",\\n            \"name\": \"M. Isard\"\\n          },\\n          {\\n            \"authorId\": \"152399055\",\\n            \"name\": \"P. Barham\"\\n          },\\n          {\\n            \"authorId\": \"2146532222\",\\n            \"name\": \"Tom Hennigan\"\\n          },\\n          {\\n            \"authorId\": \"2275176102\",\\n            \"name\": \"Ross Mcilroy\"\\n          },\\n          {\\n            \"authorId\": \"2275525680\",\\n            \"name\": \"Melvin Johnson\"\\n          },\\n          {\\n            \"authorId\": \"1698491\",\\n            \"name\": \"J. Schalkwyk\"\\n          },\\n          {\\n            \"authorId\": \"2275181648\",\\n            \"name\": \"Eli Collins\"\\n          },\\n          {\\n            \"authorId\": \"2143538252\",\\n            \"name\": \"Eliza Rutherford\"\\n          },\\n          {\\n            \"authorId\": \"2275185558\",\\n            \"name\": \"Erica Moreira\"\\n          },\\n          {\\n            \"authorId\": \"34122449\",\\n            \"name\": \"Kareem W. Ayoub\"\\n          },\\n          {\\n            \"authorId\": \"2275186741\",\\n            \"name\": \"Megha Goel\"\\n          },\\n          {\\n            \"authorId\": \"1406288863\",\\n            \"name\": \"Clemens Meyer\"\\n          },\\n          {\\n            \"authorId\": \"2005813\",\\n            \"name\": \"Gregory Thornton\"\\n          },\\n          {\\n            \"authorId\": \"2275729730\",\\n            \"name\": \"Zhen Yang\"\\n          },\\n          {\\n            \"authorId\": \"47407464\",\\n            \"name\": \"H. Michalewski\"\\n          },\\n          {\\n            \"authorId\": \"2275185143\",\\n            \"name\": \"Zaheer Abbas\"\\n          },\\n          {\\n            \"authorId\": \"74530494\",\\n            \"name\": \"Nathan Schucher\"\\n          },\\n          {\\n            \"authorId\": \"12679121\",\\n            \"name\": \"Ankesh Anand\"\\n          },\\n          {\\n            \"authorId\": \"2275185509\",\\n            \"name\": \"Richard Ives\"\\n          },\\n          {\\n            \"authorId\": \"2058168486\",\\n            \"name\": \"James Keeling\"\\n          },\\n          {\\n            \"authorId\": \"3257286\",\\n            \"name\": \"Karel Lenc\"\\n          },\\n          {\\n            \"authorId\": \"40269586\",\\n            \"name\": \"S. Haykal\"\\n          },\\n          {\\n            \"authorId\": \"2944868\",\\n            \"name\": \"Siamak Shakeri\"\\n          },\\n          {\\n            \"authorId\": \"67311962\",\\n            \"name\": \"Pranav Shyam\"\\n          },\\n          {\\n            \"authorId\": \"2841893\",\\n            \"name\": \"A. Chowdhery\"\\n          },\\n          {\\n            \"authorId\": \"81387328\",\\n            \"name\": \"Roman Ring\"\\n          },\\n          {\\n            \"authorId\": \"2135383313\",\\n            \"name\": \"Stephen Spencer\"\\n          },\\n          {\\n            \"authorId\": \"1413718981\",\\n            \"name\": \"Eren Sezener\"\\n          },\\n          {\\n            \"authorId\": \"2289035179\",\\n            \"name\": \"Luke Vilnis\"\\n          },\\n          {\\n            \"authorId\": \"2275186577\",\\n            \"name\": \"Os-car Chang\"\\n          },\\n          {\\n            \"authorId\": \"2288269273\",\\n            \"name\": \"Nobuyuki Morioka\"\\n          },\\n          {\\n            \"authorId\": \"2275183383\",\\n            \"name\": \"George Tucker\"\\n          },\\n          {\\n            \"authorId\": \"2276211400\",\\n            \"name\": \"Ce Zheng\"\\n          },\\n          {\\n            \"authorId\": \"2275186839\",\\n            \"name\": \"Oliver Woodman\"\\n          },\\n          {\\n            \"authorId\": \"80930649\",\\n            \"name\": \"Nithya Attaluri\"\\n          },\\n          {\\n            \"authorId\": \"2367821\",\\n            \"name\": \"Tom\\\\u00e1s Kocisk\\\\u00fd\"\\n          },\\n          {\\n            \"authorId\": \"2275187189\",\\n            \"name\": \"Evgenii Eltyshev\"\\n          },\\n          {\\n            \"authorId\": \"2275535939\",\\n            \"name\": \"Xi Chen\"\\n          },\\n          {\\n            \"authorId\": \"2275188933\",\\n            \"name\": \"Timothy Chung\"\\n          },\\n          {\\n            \"authorId\": \"1394635460\",\\n            \"name\": \"Vittorio Selo\"\\n          },\\n          {\\n            \"authorId\": \"1791585\",\\n            \"name\": \"Siddhartha Brahma\"\\n          },\\n          {\\n            \"authorId\": \"1737522\",\\n            \"name\": \"Petko Georgiev\"\\n          },\\n          {\\n            \"authorId\": \"133666998\",\\n            \"name\": \"Ambrose Slone\"\\n          },\\n          {\\n            \"authorId\": \"2275539055\",\\n            \"name\": \"Zhenkai Zhu\"\\n          },\\n          {\\n            \"authorId\": \"2266398107\",\\n            \"name\": \"James Lottes\"\\n          },\\n          {\\n            \"authorId\": \"2275178766\",\\n            \"name\": \"Siyuan Qiao\"\\n          },\\n          {\\n            \"authorId\": \"2290484287\",\\n            \"name\": \"Ben Caine\"\\n          },\\n          {\\n            \"authorId\": \"2287841795\",\\n            \"name\": \"Sebastian Riedel\"\\n          },\\n          {\\n            \"authorId\": \"2275176047\",\\n            \"name\": \"Alex Tomala\"\\n          },\\n          {\\n            \"authorId\": \"2159545857\",\\n            \"name\": \"Martin Chadwick\"\\n          },\\n          {\\n            \"authorId\": \"2253158807\",\\n            \"name\": \"J Christopher Love\"\\n          },\\n          {\\n            \"authorId\": \"2070068655\",\\n            \"name\": \"Peter Choy\"\\n          },\\n          {\\n            \"authorId\": \"2073395505\",\\n            \"name\": \"Sid Mittal\"\\n          },\\n          {\\n            \"authorId\": \"2815290\",\\n            \"name\": \"N. Houlsby\"\\n          },\\n          {\\n            \"authorId\": \"2269752766\",\\n            \"name\": \"Yunhao Tang\"\\n          },\\n          {\\n            \"authorId\": \"2289446231\",\\n            \"name\": \"Matthew Lamm\"\\n          },\\n          {\\n            \"authorId\": \"2275159462\",\\n            \"name\": \"Libin Bai\"\\n          },\\n          {\\n            \"authorId\": \"2197671266\",\\n            \"name\": \"Qiao Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2253917827\",\\n            \"name\": \"Luheng He\"\\n          },\\n          {\\n            \"authorId\": \"2275287219\",\\n            \"name\": \"Yong Cheng\"\\n          },\\n          {\\n            \"authorId\": \"2275186692\",\\n            \"name\": \"Peter Humphreys\"\\n          },\\n          {\\n            \"authorId\": \"2275290025\",\\n            \"name\": \"Yujia Li\"\\n          },\\n          {\\n            \"authorId\": \"1786259\",\\n            \"name\": \"Sergey Brin\"\\n          },\\n          {\\n            \"authorId\": \"51042571\",\\n            \"name\": \"Albin Cassirer\"\\n          },\\n          {\\n            \"authorId\": \"2283231534\",\\n            \"name\": \"Ying-Qi Miao\"\\n          },\\n          {\\n            \"authorId\": \"1780245\",\\n            \"name\": \"Luk\\\\u00e1s Zilka\"\\n          },\\n          {\\n            \"authorId\": \"2275189014\",\\n            \"name\": \"Taylor Tobin\"\\n          },\\n          {\\n            \"authorId\": \"2266735761\",\\n            \"name\": \"Kelvin Xu\"\\n          },\\n          {\\n            \"authorId\": \"2161966573\",\\n            \"name\": \"Lev Proleev\"\\n          },\\n          {\\n            \"authorId\": \"2275175792\",\\n            \"name\": \"Daniel Sohn\"\\n          },\\n          {\\n            \"authorId\": \"2275182383\",\\n            \"name\": \"Al-berto Magni\"\\n          },\\n          {\\n            \"authorId\": \"2258347245\",\\n            \"name\": \"L. Hendricks\"\\n          },\\n          {\\n            \"authorId\": \"2290513267\",\\n            \"name\": \"Isabel Gao\"\\n          },\\n          {\\n            \"authorId\": \"2217756237\",\\n            \"name\": \"Santiago Ontan\\'on\"\\n          },\\n          {\\n            \"authorId\": \"2275177720\",\\n            \"name\": \"Oskar Bunyan\"\\n          },\\n          {\\n            \"authorId\": \"2275185667\",\\n            \"name\": \"Nathan Byrd\"\\n          },\\n          {\\n            \"authorId\": \"2275537981\",\\n            \"name\": \"Abhanshu Sharma\"\\n          },\\n          {\\n            \"authorId\": \"48335426\",\\n            \"name\": \"Biao Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2290662953\",\\n            \"name\": \"Mario Pinto\"\\n          },\\n          {\\n            \"authorId\": \"2275170606\",\\n            \"name\": \"Rishika Sinha\"\\n          },\\n          {\\n            \"authorId\": \"18138802\",\\n            \"name\": \"Harsh Mehta\"\\n          },\\n          {\\n            \"authorId\": \"2275186531\",\\n            \"name\": \"Dawei Jia\"\\n          },\\n          {\\n            \"authorId\": \"1413064976\",\\n            \"name\": \"Sergi Caelles\"\\n          },\\n          {\\n            \"authorId\": \"1991019030\",\\n            \"name\": \"Albert Webson\"\\n          },\\n          {\\n            \"authorId\": \"2275162692\",\\n            \"name\": \"Alex Morris\"\\n          },\\n          {\\n            \"authorId\": \"2080504963\",\\n            \"name\": \"Becca Roelofs\"\\n          },\\n          {\\n            \"authorId\": \"2290634593\",\\n            \"name\": \"Yifan Ding\"\\n          },\\n          {\\n            \"authorId\": \"86898863\",\\n            \"name\": \"Robin Strudel\"\\n          },\\n          {\\n            \"authorId\": \"2275193471\",\\n            \"name\": \"Xuehan Xiong\"\\n          },\\n          {\\n            \"authorId\": \"39687627\",\\n            \"name\": \"Marvin Ritter\"\\n          },\\n          {\\n            \"authorId\": \"2256989598\",\\n            \"name\": \"Mostafa Dehghani\"\\n          },\\n          {\\n            \"authorId\": \"1706980\",\\n            \"name\": \"R. Chaabouni\"\\n          },\\n          {\\n            \"authorId\": \"2078909017\",\\n            \"name\": \"Abhijit Karmarkar\"\\n          },\\n          {\\n            \"authorId\": \"2290484705\",\\n            \"name\": \"Guangda Lai\"\\n          },\\n          {\\n            \"authorId\": \"3468078\",\\n            \"name\": \"Fabian Mentzer\"\\n          },\\n          {\\n            \"authorId\": \"2290664586\",\\n            \"name\": \"Bibo Xu\"\\n          },\\n          {\\n            \"authorId\": \"2261797906\",\\n            \"name\": \"YaGuang Li\"\\n          },\\n          {\\n            \"authorId\": \"2275534739\",\\n            \"name\": \"Yujing Zhang\"\\n          },\\n          {\\n            \"authorId\": \"40470211\",\\n            \"name\": \"T. Paine\"\\n          },\\n          {\\n            \"authorId\": \"40034895\",\\n            \"name\": \"Alex Goldin\"\\n          },\\n          {\\n            \"authorId\": \"3007442\",\\n            \"name\": \"Behnam Neyshabur\"\\n          },\\n          {\\n            \"authorId\": \"1734809439\",\\n            \"name\": \"Kate Baumli\"\\n          },\\n          {\\n            \"authorId\": \"6639036\",\\n            \"name\": \"Anselm Levskaya\"\\n          },\\n          {\\n            \"authorId\": \"2274104519\",\\n            \"name\": \"Michael Laskin\"\\n          },\\n          {\\n            \"authorId\": \"2275193644\",\\n            \"name\": \"Wenhao Jia\"\\n          },\\n          {\\n            \"authorId\": \"34269227\",\\n            \"name\": \"Jack W. Rae\"\\n          },\\n          {\\n            \"authorId\": \"2268673324\",\\n            \"name\": \"Kefan Xiao\"\\n          },\\n          {\\n            \"authorId\": \"2290485493\",\\n            \"name\": \"Antoine He\"\\n          },\\n          {\\n            \"authorId\": \"2290487747\",\\n            \"name\": \"Skye Giordano\"\\n          },\\n          {\\n            \"authorId\": \"2307454258\",\\n            \"name\": \"Lakshman Yagati\"\\n          },\\n          {\\n            \"authorId\": \"143783339\",\\n            \"name\": \"Jean-Baptiste Lespiau\"\\n          },\\n          {\\n            \"authorId\": \"122704930\",\\n            \"name\": \"Paul Natsev\"\\n          },\\n          {\\n            \"authorId\": \"2275185831\",\\n            \"name\": \"Sanjay Ganapathy\"\\n          },\\n          {\\n            \"authorId\": \"144097210\",\\n            \"name\": \"Fangyu Liu\"\\n          },\\n          {\\n            \"authorId\": \"2290487616\",\\n            \"name\": \"Danilo Martins\"\\n          },\\n          {\\n            \"authorId\": \"2249840944\",\\n            \"name\": \"Nanxin Chen\"\\n          },\\n          {\\n            \"authorId\": \"2275191526\",\\n            \"name\": \"Yunhan Xu\"\\n          },\\n          {\\n            \"authorId\": \"2275180117\",\\n            \"name\": \"Megan Barnes\"\\n          },\\n          {\\n            \"authorId\": \"2268760156\",\\n            \"name\": \"Rhys May\"\\n          },\\n          {\\n            \"authorId\": \"2275188533\",\\n            \"name\": \"Arpi Vezer\"\\n          },\\n          {\\n            \"authorId\": \"2275114643\",\\n            \"name\": \"Junhyuk Oh\"\\n          },\\n          {\\n            \"authorId\": \"2118834006\",\\n            \"name\": \"Ken Franko\"\\n          },\\n          {\\n            \"authorId\": \"2273670422\",\\n            \"name\": \"Sophie Bridgers\"\\n          },\\n          {\\n            \"authorId\": \"2275832693\",\\n            \"name\": \"Ruizhe Zhao\"\\n          },\\n          {\\n            \"authorId\": \"2275291909\",\\n            \"name\": \"Boxi Wu\"\\n          },\\n          {\\n            \"authorId\": \"40608942\",\\n            \"name\": \"Basil Mustafa\"\\n          },\\n          {\\n            \"authorId\": \"2290487447\",\\n            \"name\": \"Sean Sechrist\"\\n          },\\n          {\\n            \"authorId\": \"3166516\",\\n            \"name\": \"Emilio Parisotto\"\\n          },\\n          {\\n            \"authorId\": \"2598683\",\\n            \"name\": \"Thanumalayan Sankaranarayana Pillai\"\\n          },\\n          {\\n            \"authorId\": \"2290487293\",\\n            \"name\": \"Chris Larkin\"\\n          },\\n          {\\n            \"authorId\": \"2275149073\",\\n            \"name\": \"Chenjie Gu\"\\n          },\\n          {\\n            \"authorId\": \"2275186804\",\\n            \"name\": \"Christina Sorokin\"\\n          },\\n          {\\n            \"authorId\": \"2048712\",\\n            \"name\": \"M. Krikun\"\\n          },\\n          {\\n            \"authorId\": \"2275182203\",\\n            \"name\": \"Alexey Guseynov\"\\n          },\\n          {\\n            \"authorId\": \"2065404873\",\\n            \"name\": \"Jessica Landon\"\\n          },\\n          {\\n            \"authorId\": \"2275187147\",\\n            \"name\": \"Romina Datta\"\\n          },\\n          {\\n            \"authorId\": \"1863250\",\\n            \"name\": \"A. Pritzel\"\\n          },\\n          {\\n            \"authorId\": \"2151245633\",\\n            \"name\": \"Phoebe Thacker\"\\n          },\\n          {\\n            \"authorId\": \"2275801132\",\\n            \"name\": \"Fan Yang\"\\n          },\\n          {\\n            \"authorId\": \"2290487874\",\\n            \"name\": \"Kevin Hui\"\\n          },\\n          {\\n            \"authorId\": \"119556335\",\\n            \"name\": \"A.E. Hauth\"\\n          },\\n          {\\n            \"authorId\": \"2273556813\",\\n            \"name\": \"C. Yeh\"\\n          },\\n          {\\n            \"authorId\": \"2290481818\",\\n            \"name\": \"David Barker\"\\n          },\\n          {\\n            \"authorId\": \"1423275766\",\\n            \"name\": \"J. Mao-Jones\"\\n          },\\n          {\\n            \"authorId\": \"2166051497\",\\n            \"name\": \"Sophia Austin\"\\n          },\\n          {\\n            \"authorId\": \"2307453241\",\\n            \"name\": \"Hannah Sheahan\"\\n          },\\n          {\\n            \"authorId\": \"2620528\",\\n            \"name\": \"Parker Schuh\"\\n          },\\n          {\\n            \"authorId\": \"2275188153\",\\n            \"name\": \"James Svensson\"\\n          },\\n          {\\n            \"authorId\": \"2275193365\",\\n            \"name\": \"Rohan Jain\"\\n          },\\n          {\\n            \"authorId\": \"96641652\",\\n            \"name\": \"V. Ramasesh\"\\n          },\\n          {\\n            \"authorId\": \"2275185833\",\\n            \"name\": \"Anton Briukhov\"\\n          },\\n          {\\n            \"authorId\": \"2275180366\",\\n            \"name\": \"D. Chung\"\\n          },\\n          {\\n            \"authorId\": \"51029932\",\\n            \"name\": \"Tamara von Glehn\"\\n          },\\n          {\\n            \"authorId\": \"2275166845\",\\n            \"name\": \"Christina Butterfield\"\\n          },\\n          {\\n            \"authorId\": \"2275184551\",\\n            \"name\": \"Priya Jhakra\"\\n          },\\n          {\\n            \"authorId\": \"2275252154\",\\n            \"name\": \"Matt Wiethoff\"\\n          },\\n          {\\n            \"authorId\": \"2275193725\",\\n            \"name\": \"Justin Frye\"\\n          },\\n          {\\n            \"authorId\": \"2275175432\",\\n            \"name\": \"Jordan Grimstad\"\\n          },\\n          {\\n            \"authorId\": \"2158369306\",\\n            \"name\": \"Beer Changpinyo\"\\n          },\\n          {\\n            \"authorId\": \"153892869\",\\n            \"name\": \"Charline Le Lan\"\\n          },\\n          {\\n            \"authorId\": \"2275181572\",\\n            \"name\": \"Anna Bortsova\"\\n          },\\n          {\\n            \"authorId\": \"2275892922\",\\n            \"name\": \"Yonghui Wu\"\\n          },\\n          {\\n            \"authorId\": \"2767859\",\\n            \"name\": \"P. Voigtlaender\"\\n          },\\n          {\\n            \"authorId\": \"2279918122\",\\n            \"name\": \"Tara N. Sainath\"\\n          },\\n          {\\n            \"authorId\": \"2275575378\",\\n            \"name\": \"Charlotte Smith\"\\n          },\\n          {\\n            \"authorId\": \"2191689971\",\\n            \"name\": \"Will Hawkins\"\\n          },\\n          {\\n            \"authorId\": \"2275191626\",\\n            \"name\": \"Kris Cao\"\\n          },\\n          {\\n            \"authorId\": \"2275186515\",\\n            \"name\": \"James Besley\"\\n          },\\n          {\\n            \"authorId\": \"2059763226\",\\n            \"name\": \"S. Srinivasan\"\\n          },\\n          {\\n            \"authorId\": \"3175815\",\\n            \"name\": \"Mark Omernick\"\\n          },\\n          {\\n            \"authorId\": \"2160887964\",\\n            \"name\": \"Colin Gaffney\"\\n          },\\n          {\\n            \"authorId\": \"1956049835\",\\n            \"name\": \"G. Surita\"\\n          },\\n          {\\n            \"authorId\": \"2290484991\",\\n            \"name\": \"Ryan Burnell\"\\n          },\\n          {\\n            \"authorId\": \"2143374656\",\\n            \"name\": \"Bogdan Damoc\"\\n          },\\n          {\\n            \"authorId\": \"2275220028\",\\n            \"name\": \"Junwhan Ahn\"\\n          },\\n          {\\n            \"authorId\": \"2285740851\",\\n            \"name\": \"Andrew Brock\"\\n          },\\n          {\\n            \"authorId\": \"2146532125\",\\n            \"name\": \"Mantas Pajarskas\"\\n          },\\n          {\\n            \"authorId\": \"2275187155\",\\n            \"name\": \"Anastasia Petrushkina\"\\n          },\\n          {\\n            \"authorId\": \"30155667\",\\n            \"name\": \"Seb Noury\"\\n          },\\n          {\\n            \"authorId\": \"2275186192\",\\n            \"name\": \"Lorenzo Blanco\"\\n          },\\n          {\\n            \"authorId\": \"1754860\",\\n            \"name\": \"Kevin Swersky\"\\n          },\\n          {\\n            \"authorId\": \"2275185727\",\\n            \"name\": \"Arun Ahuja\"\\n          },\\n          {\\n            \"authorId\": \"2261737895\",\\n            \"name\": \"Thi Avrahami\"\\n          },\\n          {\\n            \"authorId\": \"40055795\",\\n            \"name\": \"Vedant Misra\"\\n          },\\n          {\\n            \"authorId\": \"2275184736\",\\n            \"name\": \"Raoul de Liedekerke\"\\n          },\\n          {\\n            \"authorId\": \"2275181534\",\\n            \"name\": \"Mariko Iinuma\"\\n          },\\n          {\\n            \"authorId\": \"144703404\",\\n            \"name\": \"A. Polozov\"\\n          },\\n          {\\n            \"authorId\": \"143981350\",\\n            \"name\": \"Sarah York\"\\n          },\\n          {\\n            \"authorId\": \"47568983\",\\n            \"name\": \"George van den Driessche\"\\n          },\\n          {\\n            \"authorId\": \"2275185732\",\\n            \"name\": \"Paul Michel\"\\n          },\\n          {\\n            \"authorId\": \"2273650801\",\\n            \"name\": \"Justin Chiu\"\\n          },\\n          {\\n            \"authorId\": \"46901218\",\\n            \"name\": \"Rory Blevins\"\\n          },\\n          {\\n            \"authorId\": \"2275185661\",\\n            \"name\": \"Zach Gleicher\"\\n          },\\n          {\\n            \"authorId\": \"39257069\",\\n            \"name\": \"Adri\\\\u00e0 Recasens\"\\n          },\\n          {\\n            \"authorId\": \"2275186093\",\\n            \"name\": \"Alban Rrustemi\"\\n          },\\n          {\\n            \"authorId\": \"1980809\",\\n            \"name\": \"E. Gribovskaya\"\\n          },\\n          {\\n            \"authorId\": \"2275277736\",\\n            \"name\": \"Au-rko Roy\"\\n          },\\n          {\\n            \"authorId\": \"2290487054\",\\n            \"name\": \"Wiktor Gworek\"\\n          },\\n          {\\n            \"authorId\": \"2275186656\",\\n            \"name\": \"S\\\\u00e9bastien M. R. Arnold\"\\n          },\\n          {\\n            \"authorId\": \"2275291886\",\\n            \"name\": \"Lisa Lee\"\\n          },\\n          {\\n            \"authorId\": \"2267341862\",\\n            \"name\": \"James Lee-Thorp\"\\n          },\\n          {\\n            \"authorId\": \"2090812426\",\\n            \"name\": \"M. Maggioni\"\\n          },\\n          {\\n            \"authorId\": \"2275183119\",\\n            \"name\": \"Enrique Piqueras\"\\n          },\\n          {\\n            \"authorId\": \"2051018967\",\\n            \"name\": \"Kartikeya Badola\"\\n          },\\n          {\\n            \"authorId\": \"2425230\",\\n            \"name\": \"S. Vikram\"\\n          },\\n          {\\n            \"authorId\": \"2275585027\",\\n            \"name\": \"Lucas Gonzalez\"\\n          },\\n          {\\n            \"authorId\": \"2275186584\",\\n            \"name\": \"Anirudh Baddepudi\"\\n          },\\n          {\\n            \"authorId\": \"2268665228\",\\n            \"name\": \"Evan Senter\"\\n          },\\n          {\\n            \"authorId\": \"2261961752\",\\n            \"name\": \"J. Devlin\"\\n          },\\n          {\\n            \"authorId\": \"47901308\",\\n            \"name\": \"James Qin\"\\n          },\\n          {\\n            \"authorId\": \"2275148073\",\\n            \"name\": \"Michael Azzam\"\\n          },\\n          {\\n            \"authorId\": \"1994939814\",\\n            \"name\": \"Maja Trebacz\"\\n          },\\n          {\\n            \"authorId\": \"35930544\",\\n            \"name\": \"M. Polacek\"\\n          },\\n          {\\n            \"authorId\": \"2290485355\",\\n            \"name\": \"Kashyap Krishnakumar\"\\n          },\\n          {\\n            \"authorId\": \"2275193337\",\\n            \"name\": \"Shuo-Yiin Chang\"\\n          },\\n          {\\n            \"authorId\": \"2275176212\",\\n            \"name\": \"Matthew Tung\"\\n          },\\n          {\\n            \"authorId\": \"2275187196\",\\n            \"name\": \"Ivo Penchev\"\\n          },\\n          {\\n            \"authorId\": \"2258551072\",\\n            \"name\": \"Rishabh Joshi\"\\n          },\\n          {\\n            \"authorId\": \"2275180557\",\\n            \"name\": \"Kate Olszewska\"\\n          },\\n          {\\n            \"authorId\": \"2275184985\",\\n            \"name\": \"Carrie Muir\"\\n          },\\n          {\\n            \"authorId\": \"2275185968\",\\n            \"name\": \"Mateo Wirth\"\\n          },\\n          {\\n            \"authorId\": \"2275184113\",\\n            \"name\": \"A. Hartman\"\\n          },\\n          {\\n            \"authorId\": \"2160888100\",\\n            \"name\": \"Joshua Newlan\"\\n          },\\n          {\\n            \"authorId\": \"2252586080\",\\n            \"name\": \"S. Kashem\"\\n          },\\n          {\\n            \"authorId\": \"2218882489\",\\n            \"name\": \"Vijay Bolina\"\\n          },\\n          {\\n            \"authorId\": \"2290484418\",\\n            \"name\": \"Elahe Dabir\"\\n          },\\n          {\\n            \"authorId\": \"3038326\",\\n            \"name\": \"Joost R. van Amersfoort\"\\n          },\\n          {\\n            \"authorId\": \"2275130349\",\\n            \"name\": \"Zafarali Ahmed\"\\n          },\\n          {\\n            \"authorId\": \"2275185511\",\\n            \"name\": \"James Cobon-Kerr\"\\n          },\\n          {\\n            \"authorId\": \"2269391198\",\\n            \"name\": \"Aishwarya B Kamath\"\\n          },\\n          {\\n            \"authorId\": \"2259962018\",\\n            \"name\": \"A. M. Hrafnkelsson\"\\n          },\\n          {\\n            \"authorId\": \"2274787555\",\\n            \"name\": \"Le Hou\"\\n          },\\n          {\\n            \"authorId\": \"2290485798\",\\n            \"name\": \"Ian Mackinnon\"\\n          },\\n          {\\n            \"authorId\": \"2156930381\",\\n            \"name\": \"Alexandre Frechette\"\\n          },\\n          {\\n            \"authorId\": \"51210148\",\\n            \"name\": \"Eric Noland\"\\n          },\\n          {\\n            \"authorId\": \"2275182246\",\\n            \"name\": \"Xi-ance Si\"\\n          },\\n          {\\n            \"authorId\": \"2779842\",\\n            \"name\": \"Emanuel Taropa\"\\n          },\\n          {\\n            \"authorId\": \"2350430090\",\\n            \"name\": \"Dong Li\"\\n          },\\n          {\\n            \"authorId\": \"2275183277\",\\n            \"name\": \"Phil Crone\"\\n          },\\n          {\\n            \"authorId\": \"4478284\",\\n            \"name\": \"Anmol Gulati\"\\n          },\\n          {\\n            \"authorId\": \"2275180682\",\\n            \"name\": \"S\\'ebastien Cevey\"\\n          },\\n          {\\n            \"authorId\": \"2275173231\",\\n            \"name\": \"Jonas Adler\"\\n          },\\n          {\\n            \"authorId\": \"2275786213\",\\n            \"name\": \"Ada Ma\"\\n          },\\n          {\\n            \"authorId\": \"2275185813\",\\n            \"name\": \"David Silver\"\\n          },\\n          {\\n            \"authorId\": \"148152480\",\\n            \"name\": \"Simon Tokumine\"\\n          },\\n          {\\n            \"authorId\": \"2067745837\",\\n            \"name\": \"Richard Powell\"\\n          },\\n          {\\n            \"authorId\": \"2275280377\",\\n            \"name\": \"Stephan Lee\"\\n          },\\n          {\\n            \"authorId\": \"2275571997\",\\n            \"name\": \"Michael B. Chang\"\\n          },\\n          {\\n            \"authorId\": \"2275396476\",\\n            \"name\": \"Samer Hassan\"\\n          },\\n          {\\n            \"authorId\": \"2007712128\",\\n            \"name\": \"Diana Mincu\"\\n          },\\n          {\\n            \"authorId\": \"2064599701\",\\n            \"name\": \"Antoine Yang\"\\n          },\\n          {\\n            \"authorId\": \"153898744\",\\n            \"name\": \"Nir Levine\"\\n          },\\n          {\\n            \"authorId\": \"2275186701\",\\n            \"name\": \"Jenny Brennan\"\\n          },\\n          {\\n            \"authorId\": \"2249764807\",\\n            \"name\": \"Mingqiu Wang\"\\n          },\\n          {\\n            \"authorId\": \"2265053608\",\\n            \"name\": \"Sarah Hodkinson\"\\n          },\\n          {\\n            \"authorId\": \"2144551262\",\\n            \"name\": \"Jeffrey Zhao\"\\n          },\\n          {\\n            \"authorId\": \"2290487597\",\\n            \"name\": \"Josh Lipschultz\"\\n          },\\n          {\\n            \"authorId\": \"20702300\",\\n            \"name\": \"Aedan Pope\"\\n          },\\n          {\\n            \"authorId\": \"2275571997\",\\n            \"name\": \"Michael B. Chang\"\\n          },\\n          {\\n            \"authorId\": \"2275767067\",\\n            \"name\": \"Cheng Li\"\\n          },\\n          {\\n            \"authorId\": \"2121764\",\\n            \"name\": \"Laurent El Shafey\"\\n          },\\n          {\\n            \"authorId\": \"2264591527\",\\n            \"name\": \"M. Paganini\"\\n          },\\n          {\\n            \"authorId\": \"2269733876\",\\n            \"name\": \"Sholto Douglas\"\\n          },\\n          {\\n            \"authorId\": \"2266464503\",\\n            \"name\": \"Bernd Bohnet\"\\n          },\\n          {\\n            \"authorId\": \"2274107421\",\\n            \"name\": \"Fabio Pardo\"\\n          },\\n          {\\n            \"authorId\": \"2275182230\",\\n            \"name\": \"Seth Odoom\"\\n          },\\n          {\\n            \"authorId\": \"2269541835\",\\n            \"name\": \"Mihaela Ro\\\\u0219ca\"\\n          },\\n          {\\n            \"authorId\": \"2267546965\",\\n            \"name\": \"Cicero Nogueira dos Santos\"\\n          },\\n          {\\n            \"authorId\": \"2275185640\",\\n            \"name\": \"Kedar Soparkar\"\\n          },\\n          {\\n            \"authorId\": \"35099444\",\\n            \"name\": \"A. Guez\"\\n          },\\n          {\\n            \"authorId\": \"2275187110\",\\n            \"name\": \"Tom Hudson\"\\n          },\\n          {\\n            \"authorId\": \"2275188563\",\\n            \"name\": \"Steven Hansen\"\\n          },\\n          {\\n            \"authorId\": \"50844587\",\\n            \"name\": \"Chulayuth Asawaroengchai\"\\n          },\\n          {\\n            \"authorId\": \"104000494\",\\n            \"name\": \"Ravichandra Addanki\"\\n          },\\n          {\\n            \"authorId\": \"2290486855\",\\n            \"name\": \"Tianhe Yu\"\\n          },\\n          {\\n            \"authorId\": \"3448463\",\\n            \"name\": \"Wojciech Stokowiec\"\\n          },\\n          {\\n            \"authorId\": \"2258793616\",\\n            \"name\": \"Mina Khan\"\\n          },\\n          {\\n            \"authorId\": \"2243002880\",\\n            \"name\": \"Justin Gilmer\"\\n          },\\n          {\\n            \"authorId\": \"2253808003\",\\n            \"name\": \"Jaehoon Lee\"\\n          },\\n          {\\n            \"authorId\": \"2290485108\",\\n            \"name\": \"Carrie Grimes Bostock\"\\n          },\\n          {\\n            \"authorId\": \"1996199677\",\\n            \"name\": \"Keran Rong\"\\n          },\\n          {\\n            \"authorId\": \"2263289033\",\\n            \"name\": \"Jonathan Caton\"\\n          },\\n          {\\n            \"authorId\": \"2275184275\",\\n            \"name\": \"Pedram Pejman\"\\n          },\\n          {\\n            \"authorId\": \"1696719\",\\n            \"name\": \"Filip Pavetic\"\\n          },\\n          {\\n            \"authorId\": \"2259937157\",\\n            \"name\": \"Geoff Brown\"\\n          },\\n          {\\n            \"authorId\": \"2290595918\",\\n            \"name\": \"Vivek Sharma\"\\n          },\\n          {\\n            \"authorId\": \"2170162986\",\\n            \"name\": \"Mario Luvci\\'c\"\\n          },\\n          {\\n            \"authorId\": \"2275176205\",\\n            \"name\": \"Rajku-mar Samuel\"\\n          },\\n          {\\n            \"authorId\": \"2941141\",\\n            \"name\": \"J. Djolonga\"\\n          },\\n          {\\n            \"authorId\": \"2063800905\",\\n            \"name\": \"Amol Mandhane\"\\n          },\\n          {\\n            \"authorId\": \"2275187845\",\\n            \"name\": \"Lars Lowe Sjosund\"\\n          },\\n          {\\n            \"authorId\": \"118801223\",\\n            \"name\": \"Elena Buchatskaya\"\\n          },\\n          {\\n            \"authorId\": \"2275158927\",\\n            \"name\": \"Elspeth White\"\\n          },\\n          {\\n            \"authorId\": \"2201776471\",\\n            \"name\": \"Natalie Clay\"\\n          },\\n          {\\n            \"authorId\": \"2260169185\",\\n            \"name\": \"Jiepu Jiang\"\\n          },\\n          {\\n            \"authorId\": \"2275798209\",\\n            \"name\": \"Hyeontaek Lim\"\\n          },\\n          {\\n            \"authorId\": \"38637384\",\\n            \"name\": \"Ross Hemsley\"\\n          },\\n          {\\n            \"authorId\": \"2275184618\",\\n            \"name\": \"Jane Labanowski\"\\n          },\\n          {\\n            \"authorId\": \"41019080\",\\n            \"name\": \"Nicola De Cao\"\\n          },\\n          {\\n            \"authorId\": \"2275188258\",\\n            \"name\": \"David Steiner\"\\n          },\\n          {\\n            \"authorId\": \"3362306\",\\n            \"name\": \"Sayed Hadi Hashemi\"\\n          },\\n          {\\n            \"authorId\": \"2288056644\",\\n            \"name\": \"Jacob Austin\"\\n          },\\n          {\\n            \"authorId\": \"2105841261\",\\n            \"name\": \"Anita Gergely\"\\n          },\\n          {\\n            \"authorId\": \"2221119859\",\\n            \"name\": \"Tim Blyth\"\\n          },\\n          {\\n            \"authorId\": \"2275190309\",\\n            \"name\": \"Joe Stanton\"\\n          },\\n          {\\n            \"authorId\": \"2272718153\",\\n            \"name\": \"K. Shivakumar\"\\n          },\\n          {\\n            \"authorId\": \"9356387\",\\n            \"name\": \"Aditya Siddhant\"\\n          },\\n          {\\n            \"authorId\": \"39552848\",\\n            \"name\": \"Anders Andreassen\"\\n          },\\n          {\\n            \"authorId\": \"2279996944\",\\n            \"name\": \"Carlos L. Araya\"\\n          },\\n          {\\n            \"authorId\": \"2275187415\",\\n            \"name\": \"Nikhil Sethi\"\\n          },\\n          {\\n            \"authorId\": \"2934334\",\\n            \"name\": \"Rakesh Shivanna\"\\n          },\\n          {\\n            \"authorId\": \"2275161833\",\\n            \"name\": \"Steven Hand\"\\n          },\\n          {\\n            \"authorId\": \"12295226\",\\n            \"name\": \"Ankur Bapna\"\\n          },\\n          {\\n            \"authorId\": \"2402489\",\\n            \"name\": \"A. Khodaei\"\\n          },\\n          {\\n            \"authorId\": \"19200186\",\\n            \"name\": \"Antoine Miech\"\\n          },\\n          {\\n            \"authorId\": \"2287809580\",\\n            \"name\": \"Garrett Tanzer\"\\n          },\\n          {\\n            \"authorId\": \"1394189636\",\\n            \"name\": \"Andy Swing\"\\n          },\\n          {\\n            \"authorId\": \"41037204\",\\n            \"name\": \"S. Thakoor\"\\n          },\\n          {\\n            \"authorId\": \"2291169360\",\\n            \"name\": \"Zhufeng Pan\"\\n          },\\n          {\\n            \"authorId\": \"81408931\",\\n            \"name\": \"Zachary Nado\"\\n          },\\n          {\\n            \"authorId\": \"2218062983\",\\n            \"name\": \"Stephanie Winkler\"\\n          },\\n          {\\n            \"authorId\": \"2256337021\",\\n            \"name\": \"Dian Yu\"\\n          },\\n          {\\n            \"authorId\": \"144413479\",\\n            \"name\": \"Mohammad Saleh\"\\n          },\\n          {\\n            \"authorId\": \"108173905\",\\n            \"name\": \"Lorenzo Maggiore\"\\n          },\\n          {\\n            \"authorId\": \"2159207795\",\\n            \"name\": \"Iain Barr\"\\n          },\\n          {\\n            \"authorId\": \"2275187490\",\\n            \"name\": \"Minh Giang\"\\n          },\\n          {\\n            \"authorId\": \"2275186582\",\\n            \"name\": \"Thais Kagohara\"\\n          },\\n          {\\n            \"authorId\": \"1841008\",\\n            \"name\": \"Ivo Danihelka\"\\n          },\\n          {\\n            \"authorId\": \"2275176043\",\\n            \"name\": \"Amit Marathe\"\\n          },\\n          {\\n            \"authorId\": \"2275181199\",\\n            \"name\": \"Vladimir Feinberg\"\\n          },\\n          {\\n            \"authorId\": \"2275176049\",\\n            \"name\": \"Mohamed Elhawaty\"\\n          },\\n          {\\n            \"authorId\": \"3404697\",\\n            \"name\": \"Nimesh Ghelani\"\\n          },\\n          {\\n            \"authorId\": \"48257711\",\\n            \"name\": \"Dan Horgan\"\\n          },\\n          {\\n            \"authorId\": \"2275121046\",\\n            \"name\": \"Helen Miller\"\\n          },\\n          {\\n            \"authorId\": \"2275184334\",\\n            \"name\": \"Lexi Walker\"\\n          },\\n          {\\n            \"authorId\": \"1825728\",\\n            \"name\": \"Richard Tanburn\"\\n          },\\n          {\\n            \"authorId\": \"2275180099\",\\n            \"name\": \"Mukarram Tariq\"\\n          },\\n          {\\n            \"authorId\": \"2275113487\",\\n            \"name\": \"Disha Shrivastava\"\\n          },\\n          {\\n            \"authorId\": \"2290487337\",\\n            \"name\": \"Fei Xia\"\\n          },\\n          {\\n            \"authorId\": \"2284761701\",\\n            \"name\": \"Chung-Cheng Chiu\"\\n          },\\n          {\\n            \"authorId\": \"2333511945\",\\n            \"name\": \"Zoe Ashwood\"\\n          },\\n          {\\n            \"authorId\": \"2290486431\",\\n            \"name\": \"Khuslen Baatarsukh\"\\n          },\\n          {\\n            \"authorId\": \"2412073\",\\n            \"name\": \"Sina Samangooei\"\\n          },\\n          {\\n            \"authorId\": \"2275177971\",\\n            \"name\": \"Fred Alcober\"\\n          },\\n          {\\n            \"authorId\": \"2163521750\",\\n            \"name\": \"Axel Stjerngren\"\\n          },\\n          {\\n            \"authorId\": \"2258235140\",\\n            \"name\": \"P. Komarek\"\\n          },\\n          {\\n            \"authorId\": \"2275185589\",\\n            \"name\": \"Katerina Tsihlas\"\\n          },\\n          {\\n            \"authorId\": \"11167300\",\\n            \"name\": \"Anudhyan Boral\"\\n          },\\n          {\\n            \"authorId\": \"89066101\",\\n            \"name\": \"R. Comanescu\"\\n          },\\n          {\\n            \"authorId\": \"2275275439\",\\n            \"name\": \"Jeremy Chen\"\\n          },\\n          {\\n            \"authorId\": \"7247867\",\\n            \"name\": \"Ruibo Liu\"\\n          },\\n          {\\n            \"authorId\": \"2275185808\",\\n            \"name\": \"Dawn Bloxwich\"\\n          },\\n          {\\n            \"authorId\": \"2182971260\",\\n            \"name\": \"Charlie Chen\"\\n          },\\n          {\\n            \"authorId\": \"2265240845\",\\n            \"name\": \"Yanhua Sun\"\\n          },\\n          {\\n            \"authorId\": \"2275173841\",\\n            \"name\": \"Fangxi-aoyu Feng\"\\n          },\\n          {\\n            \"authorId\": \"2251517316\",\\n            \"name\": \"M. Mauger\"\\n          },\\n          {\\n            \"authorId\": \"1404332584\",\\n            \"name\": \"Xerxes Dotiwalla\"\\n          },\\n          {\\n            \"authorId\": \"2297847306\",\\n            \"name\": \"V. Hellendoorn\"\\n          },\\n          {\\n            \"authorId\": \"2275184531\",\\n            \"name\": \"Michael Sharman\"\\n          },\\n          {\\n            \"authorId\": \"2275187038\",\\n            \"name\": \"Ivy Zheng\"\\n          },\\n          {\\n            \"authorId\": \"2256873459\",\\n            \"name\": \"Krishna Haridasan\"\\n          },\\n          {\\n            \"authorId\": \"1403998955\",\\n            \"name\": \"Gabriel Barth-Maron\"\\n          },\\n          {\\n            \"authorId\": \"2275181554\",\\n            \"name\": \"Craig Swanson\"\\n          },\\n          {\\n            \"authorId\": \"2275184739\",\\n            \"name\": \"Dominika Rogozi\\'nska\"\\n          },\\n          {\\n            \"authorId\": \"2290741315\",\\n            \"name\": \"Alek Andreev\"\\n          },\\n          {\\n            \"authorId\": \"2249760524\",\\n            \"name\": \"P. Rubenstein\"\\n          },\\n          {\\n            \"authorId\": \"2275189194\",\\n            \"name\": \"Ruoxin Sang\"\\n          },\\n          {\\n            \"authorId\": \"2265528853\",\\n            \"name\": \"Dan Hurt\"\\n          },\\n          {\\n            \"authorId\": \"2275189864\",\\n            \"name\": \"Gamaleldin Elsayed\"\\n          },\\n          {\\n            \"authorId\": \"2290529512\",\\n            \"name\": \"Ren-shen Wang\"\\n          },\\n          {\\n            \"authorId\": \"2290485332\",\\n            \"name\": \"Dave Lacey\"\\n          },\\n          {\\n            \"authorId\": \"2279830514\",\\n            \"name\": \"Anastasija Ili\\'c\"\\n          },\\n          {\\n            \"authorId\": \"2275112414\",\\n            \"name\": \"Yao Zhao\"\\n          },\\n          {\\n            \"authorId\": \"2215449616\",\\n            \"name\": \"Woohyun Han\"\\n          },\\n          {\\n            \"authorId\": \"2257256357\",\\n            \"name\": \"Lora Aroyo\"\\n          },\\n          {\\n            \"authorId\": \"2275177173\",\\n            \"name\": \"Chimezie Iwuanyanwu\"\\n          },\\n          {\\n            \"authorId\": \"48942032\",\\n            \"name\": \"Vitaly Nikolaev\"\\n          },\\n          {\\n            \"authorId\": \"40627523\",\\n            \"name\": \"Balaji Lakshminarayanan\"\\n          },\\n          {\\n            \"authorId\": \"2290484919\",\\n            \"name\": \"Sadegh Jazayeri\"\\n          },\\n          {\\n            \"authorId\": \"31713635\",\\n            \"name\": \"Raphael Lopez Kaufman\"\\n          },\\n          {\\n            \"authorId\": \"2150348369\",\\n            \"name\": \"Mani Varadarajan\"\\n          },\\n          {\\n            \"authorId\": \"118505443\",\\n            \"name\": \"Chetan Tekur\"\\n          },\\n          {\\n            \"authorId\": \"2275187305\",\\n            \"name\": \"Doug Fritz\"\\n          },\\n          {\\n            \"authorId\": \"2140488873\",\\n            \"name\": \"Misha Khalman\"\\n          },\\n          {\\n            \"authorId\": \"2257286979\",\\n            \"name\": \"David Reitter\"\\n          },\\n          {\\n            \"authorId\": \"2290487762\",\\n            \"name\": \"Kingshuk Dasgupta\"\\n          },\\n          {\\n            \"authorId\": \"1658856741\",\\n            \"name\": \"Shourya Sarcar\"\\n          },\\n          {\\n            \"authorId\": \"103861813\",\\n            \"name\": \"T. Ornduff\"\\n          },\\n          {\\n            \"authorId\": \"2265527968\",\\n            \"name\": \"Javier Snaider\"\\n          },\\n          {\\n            \"authorId\": \"2174667321\",\\n            \"name\": \"Fantine Huot\"\\n          },\\n          {\\n            \"authorId\": \"2275694953\",\\n            \"name\": \"Johnson Jia\"\\n          },\\n          {\\n            \"authorId\": \"2275186180\",\\n            \"name\": \"Rupert Kemp\"\\n          },\\n          {\\n            \"authorId\": \"1702423\",\\n            \"name\": \"Nejc Trdin\"\\n          },\\n          {\\n            \"authorId\": \"2275186554\",\\n            \"name\": \"Anitha Vijayakumar\"\\n          },\\n          {\\n            \"authorId\": \"2290490486\",\\n            \"name\": \"Lucy Kim\"\\n          },\\n          {\\n            \"authorId\": \"2269460640\",\\n            \"name\": \"Christof Angermueller\"\\n          },\\n          {\\n            \"authorId\": \"2290485653\",\\n            \"name\": \"Li Lao\"\\n          },\\n          {\\n            \"authorId\": \"2275249023\",\\n            \"name\": \"Tianqi Liu\"\\n          },\\n          {\\n            \"authorId\": \"2290556567\",\\n            \"name\": \"Haibin Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2290485958\",\\n            \"name\": \"David Engel\"\\n          },\\n          {\\n            \"authorId\": \"2275190069\",\\n            \"name\": \"Somer Greene\"\\n          },\\n          {\\n            \"authorId\": \"2275169305\",\\n            \"name\": \"Anais White\"\\n          },\\n          {\\n            \"authorId\": \"2290488307\",\\n            \"name\": \"Jessica Austin\"\\n          },\\n          {\\n            \"authorId\": \"2290666013\",\\n            \"name\": \"Lilly Taylor\"\\n          },\\n          {\\n            \"authorId\": \"2275181309\",\\n            \"name\": \"Shereen Ashraf\"\\n          },\\n          {\\n            \"authorId\": \"2290539499\",\\n            \"name\": \"Dangyi Liu\"\\n          },\\n          {\\n            \"authorId\": \"2280669236\",\\n            \"name\": \"Maria Georgaki\"\\n          },\\n          {\\n            \"authorId\": \"2290485789\",\\n            \"name\": \"Irene Cai\"\\n          },\\n          {\\n            \"authorId\": \"2275177999\",\\n            \"name\": \"Yana Kulizhskaya\"\\n          },\\n          {\\n            \"authorId\": \"2096063076\",\\n            \"name\": \"Sonam Goenka\"\\n          },\\n          {\\n            \"authorId\": \"4125424\",\\n            \"name\": \"Brennan Saeta\"\\n          },\\n          {\\n            \"authorId\": \"4529644\",\\n            \"name\": \"Kiran Vodrahalli\"\\n          },\\n          {\\n            \"authorId\": \"2290488254\",\\n            \"name\": \"Christian Frank\"\\n          },\\n          {\\n            \"authorId\": \"47182967\",\\n            \"name\": \"D. Cesare\"\\n          },\\n          {\\n            \"authorId\": \"2275186837\",\\n            \"name\": \"Brona Robenek\"\\n          },\\n          {\\n            \"authorId\": \"2290487825\",\\n            \"name\": \"Harry Richardson\"\\n          },\\n          {\\n            \"authorId\": \"2275186342\",\\n            \"name\": \"Mah-moud Alnahlawi\"\\n          },\\n          {\\n            \"authorId\": \"2275187959\",\\n            \"name\": \"Christo-pher Yew\"\\n          },\\n          {\\n            \"authorId\": \"2275181434\",\\n            \"name\": \"Priya Ponnapalli\"\\n          },\\n          {\\n            \"authorId\": \"1749128\",\\n            \"name\": \"M. Tagliasacchi\"\\n          },\\n          {\\n            \"authorId\": \"2275188906\",\\n            \"name\": \"Alex Korchemniy\"\\n          },\\n          {\\n            \"authorId\": \"2275999038\",\\n            \"name\": \"Yelin Kim\"\\n          },\\n          {\\n            \"authorId\": \"2275195333\",\\n            \"name\": \"Dinghua Li\"\\n          },\\n          {\\n            \"authorId\": \"2080520726\",\\n            \"name\": \"B. Rosgen\"\\n          },\\n          {\\n            \"authorId\": \"2275186260\",\\n            \"name\": \"Kyle Levin\"\\n          },\\n          {\\n            \"authorId\": \"2275187022\",\\n            \"name\": \"Jeremy Wiesner\"\\n          },\\n          {\\n            \"authorId\": \"2275187506\",\\n            \"name\": \"Praseem Banzal\"\\n          },\\n          {\\n            \"authorId\": \"2275182960\",\\n            \"name\": \"Praveen Srinivasan\"\\n          },\\n          {\\n            \"authorId\": \"2254035020\",\\n            \"name\": \"Hongkun Yu\"\\n          },\\n          {\\n            \"authorId\": \"2275186671\",\\n            \"name\": \"cCauglar Unlu\"\\n          },\\n          {\\n            \"authorId\": \"2275179889\",\\n            \"name\": \"David Reid\"\\n          },\\n          {\\n            \"authorId\": \"9941702\",\\n            \"name\": \"Zora Tung\"\\n          },\\n          {\\n            \"authorId\": \"2591720\",\\n            \"name\": \"D. Finchelstein\"\\n          },\\n          {\\n            \"authorId\": \"2290629265\",\\n            \"name\": \"Ravin Kumar\"\\n          },\\n          {\\n            \"authorId\": \"2288791213\",\\n            \"name\": \"A. Elisseeff\"\\n          },\\n          {\\n            \"authorId\": \"2290557316\",\\n            \"name\": \"Jin Huang\"\\n          },\\n          {\\n            \"authorId\": \"2290594698\",\\n            \"name\": \"Ming Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2070271342\",\\n            \"name\": \"Rui Zhu\"\\n          },\\n          {\\n            \"authorId\": \"2275185644\",\\n            \"name\": \"Ricardo Aguilar\"\\n          },\\n          {\\n            \"authorId\": \"2275181171\",\\n            \"name\": \"Mai Gim\\'enez\"\\n          },\\n          {\\n            \"authorId\": \"2275552322\",\\n            \"name\": \"Jiawei Xia\"\\n          },\\n          {\\n            \"authorId\": \"2770149\",\\n            \"name\": \"Olivier Dousse\"\\n          },\\n          {\\n            \"authorId\": \"145556052\",\\n            \"name\": \"W. Gierke\"\\n          },\\n          {\\n            \"authorId\": \"1735318\",\\n            \"name\": \"S. Yeganeh\"\\n          },\\n          {\\n            \"authorId\": \"2290486731\",\\n            \"name\": \"Damion Yates\"\\n          },\\n          {\\n            \"authorId\": \"153776147\",\\n            \"name\": \"Komal Jalan\"\\n          },\\n          {\\n            \"authorId\": \"2275716550\",\\n            \"name\": \"Lu Li\"\\n          },\\n          {\\n            \"authorId\": \"2275189350\",\\n            \"name\": \"Eri Latorre-Chimoto\"\\n          },\\n          {\\n            \"authorId\": \"2112293680\",\\n            \"name\": \"D. D. Nguyen\"\\n          },\\n          {\\n            \"authorId\": \"2275187515\",\\n            \"name\": \"Ken Durden\"\\n          },\\n          {\\n            \"authorId\": \"2561675\",\\n            \"name\": \"Praveen Kallakuri\"\\n          },\\n          {\\n            \"authorId\": \"2290524803\",\\n            \"name\": \"Yaxin Liu\"\\n          },\\n          {\\n            \"authorId\": \"2275221227\",\\n            \"name\": \"Matthew Johnson\"\\n          },\\n          {\\n            \"authorId\": \"2275175372\",\\n            \"name\": \"Tomy Tsai\"\\n          },\\n          {\\n            \"authorId\": \"2275188417\",\\n            \"name\": \"Alice Talbert\"\\n          },\\n          {\\n            \"authorId\": \"2275539011\",\\n            \"name\": \"Jasmine Liu\"\\n          },\\n          {\\n            \"authorId\": \"40390373\",\\n            \"name\": \"Alexander Neitz\"\\n          },\\n          {\\n            \"authorId\": \"2217508229\",\\n            \"name\": \"C. Elkind\"\\n          },\\n          {\\n            \"authorId\": \"2269473701\",\\n            \"name\": \"Marco Selvi\"\\n          },\\n          {\\n            \"authorId\": \"2275188903\",\\n            \"name\": \"Mimi Jasarevic\"\\n          },\\n          {\\n            \"authorId\": \"2258550407\",\\n            \"name\": \"Livio Baldini Soares\"\\n          },\\n          {\\n            \"authorId\": \"7353832\",\\n            \"name\": \"Livio Baldini Soares\"\\n          },\\n          {\\n            \"authorId\": \"2164862499\",\\n            \"name\": \"Pidong Wang\"\\n          },\\n          {\\n            \"authorId\": \"2290580195\",\\n            \"name\": \"A. Wang\"\\n          },\\n          {\\n            \"authorId\": \"2181807096\",\\n            \"name\": \"Xinyu Ye\"\\n          },\\n          {\\n            \"authorId\": \"2214770531\",\\n            \"name\": \"Krystal Kallarackal\"\\n          },\\n          {\\n            \"authorId\": \"2275188993\",\\n            \"name\": \"Lucia Loher\"\\n          },\\n          {\\n            \"authorId\": \"2290486901\",\\n            \"name\": \"Hoi Lam\"\\n          },\\n          {\\n            \"authorId\": \"2290485721\",\\n            \"name\": \"Josef Broder\"\\n          },\\n          {\\n            \"authorId\": \"1404655176\",\\n            \"name\": \"D. Holtmann-Rice\"\\n          },\\n          {\\n            \"authorId\": \"2275150753\",\\n            \"name\": \"Nina Martin\"\\n          },\\n          {\\n            \"authorId\": \"2257926827\",\\n            \"name\": \"Bramandia Ramadhana\"\\n          },\\n          {\\n            \"authorId\": \"1393948967\",\\n            \"name\": \"Daniel Toyama\"\\n          },\\n          {\\n            \"authorId\": \"2290488378\",\\n            \"name\": \"Mrinal Shukla\"\\n          },\\n          {\\n            \"authorId\": \"2266467648\",\\n            \"name\": \"Sujoy Basu\"\\n          },\\n          {\\n            \"authorId\": \"2290784246\",\\n            \"name\": \"Abhi Mohan\"\\n          }\\n        ],\\n        \"abstract\": \"In this report, we introduce the Gemini 1.5 family of models, representing the next generation of highly compute-efficient multimodal models capable of recalling and reasoning over fine-grained information from millions of tokens of context, including multiple long documents and hours of video and audio. The family includes two new models: (1) an updated Gemini 1.5 Pro, which exceeds the February version on the great majority of capabilities and benchmarks; (2) Gemini 1.5 Flash, a more lightweight variant designed for efficiency with minimal regression in quality. Gemini 1.5 models achieve near-perfect recall on long-context retrieval tasks across modalities, improve the state-of-the-art in long-document QA, long-video QA and long-context ASR, and match or surpass Gemini 1.0 Ultra\\'s state-of-the-art performance across a broad set of benchmarks. Studying the limits of Gemini 1.5\\'s long-context ability, we find continued improvement in next-token prediction and near-perfect retrieval (>99%) up to at least 10M tokens, a generational leap over existing models such as Claude 3.0 (200k) and GPT-4 Turbo (128k). Finally, we highlight real-world use cases, such as Gemini 1.5 collaborating with professionals on completing their tasks achieving 26 to 75% time savings across 10 different job categories, as well as surprising new capabilities of large language models at the frontier; when given a grammar manual for Kalamang, a language with fewer than 200 speakers worldwide, the model learns to translate English to Kalamang at a similar level to a person who learned from the same content.\"\\n      },\\n      {\\n        \"paperId\": \"ac4c1c56a196523f42643f0722fe2fde776fd1fa\",\\n        \"url\": \"https://www.semanticscholar.org/paper/ac4c1c56a196523f42643f0722fe2fde776fd1fa\",\\n        \"title\": \"Context Dependant Iterative Parameter Optimisation for Robust Robot Navigation\",\\n        \"citationCount\": 17,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {\\'tldr\\'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ICRA40945.2020.9196550?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ICRA40945.2020.9196550, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": null,\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2020-05-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"144621045\",\\n            \"name\": \"Adam Binch\"\\n          },\\n          {\\n            \"authorId\": \"145785386\",\\n            \"name\": \"Gautham P. Das\"\\n          },\\n          {\\n            \"authorId\": \"3291831\",\\n            \"name\": \"J. P. Fentanes\"\\n          },\\n          {\\n            \"authorId\": \"1728609\",\\n            \"name\": \"Marc Hanheide\"\\n          }\\n        ],\\n        \"abstract\": \"Progress in autonomous mobile robotics has seen significant advances in the development of many algorithms for motion control and path planning. However, robust performance from these algorithms can often only be expected if the parameters controlling them are tuned specifically for the respective robot model, and optimised for specific scenarios in the environment the robot is working in. Such parameter tuning can, depending on the underlying algorithm, amount to a substantial combinatorial challenge, often rendering extensive manual tuning of these parameters intractable. In this paper, we present a framework that permits the use of different navigation actions and/or parameters depending on the spatial context of the navigation task. We consider the respective navigation algorithms themselves mostly as a \\\\\"black box\\\\\", and find suitable parameters by means of an iterative optimisation, improving for performance metrics in simulated environments. We present a genetic algorithm incorporated into the framework, and empirically show that the resulting parameter sets lead to substantial performance improvements in both simulated and real-world environments in the domain of agricultural robots.\"\\n      },\\n      {\\n        \"paperId\": \"1d82f61ee52331a94141a50b59b186ccf105f6a9\",\\n        \"url\": \"https://www.semanticscholar.org/paper/1d82f61ee52331a94141a50b59b186ccf105f6a9\",\\n        \"title\": \"Building Math Agents with Multi-Turn Iterative Preference Learning\",\\n        \"citationCount\": 58,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2409.02392, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work introduces a multi-turn direct preference learning framework, tailored for this context, that leverages feedback from code interpreters and optimizes trajectory-level preferences and includes multi-turn DPO and multi-turn KTO as specific implementations.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-09-04\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2275119437\",\\n            \"name\": \"Wei Xiong\"\\n          },\\n          {\\n            \"authorId\": \"1382573171\",\\n            \"name\": \"Chengshuai Shi\"\\n          },\\n          {\\n            \"authorId\": \"2266463492\",\\n            \"name\": \"Jiaming Shen\"\\n          },\\n          {\\n            \"authorId\": \"2302798001\",\\n            \"name\": \"Aviv Rosenberg\"\\n          },\\n          {\\n            \"authorId\": \"2266819166\",\\n            \"name\": \"Zhen Qin\"\\n          },\\n          {\\n            \"authorId\": \"2439765\",\\n            \"name\": \"Daniele Calandriello\"\\n          },\\n          {\\n            \"authorId\": \"2140488873\",\\n            \"name\": \"Misha Khalman\"\\n          },\\n          {\\n            \"authorId\": \"2258551072\",\\n            \"name\": \"Rishabh Joshi\"\\n          },\\n          {\\n            \"authorId\": \"1808897\",\\n            \"name\": \"Bilal Piot\"\\n          },\\n          {\\n            \"authorId\": \"2316576401\",\\n            \"name\": \"Mohammad Saleh\"\\n          },\\n          {\\n            \"authorId\": \"2319960151\",\\n            \"name\": \"Chi Jin\"\\n          },\\n          {\\n            \"authorId\": \"2301173100\",\\n            \"name\": \"Tong Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2239381730\",\\n            \"name\": \"Tianqi Liu\"\\n          }\\n        ],\\n        \"abstract\": \"Recent studies have shown that large language models\\' (LLMs) mathematical problem-solving capabilities can be enhanced by integrating external tools, such as code interpreters, and employing multi-turn Chain-of-Thought (CoT) reasoning. While current methods focus on synthetic data generation and Supervised Fine-Tuning (SFT), this paper studies the complementary direct preference learning approach to further improve model performance. However, existing direct preference learning algorithms are originally designed for the single-turn chat task, and do not fully address the complexities of multi-turn reasoning and external tool integration required for tool-integrated mathematical reasoning tasks. To fill in this gap, we introduce a multi-turn direct preference learning framework, tailored for this context, that leverages feedback from code interpreters and optimizes trajectory-level preferences. This framework includes multi-turn DPO and multi-turn KTO as specific implementations. The effectiveness of our framework is validated through training of various language models using an augmented prompt set from the GSM8K and MATH datasets. Our results demonstrate substantial improvements: a supervised fine-tuned Gemma-1.1-it-7B model\\'s performance increased from 77.5% to 83.9% on GSM8K and from 46.1% to 51.2% on MATH. Similarly, a Gemma-2-it-9B model improved from 84.1% to 86.3% on GSM8K and from 51.0% to 54.5% on MATH.\"\\n      },\\n      {\\n        \"paperId\": \"a8176838a636651324b9bac1b3443c803b44e1b3\",\\n        \"url\": \"https://www.semanticscholar.org/paper/a8176838a636651324b9bac1b3443c803b44e1b3\",\\n        \"title\": \"Online Iterative Reinforcement Learning from Human Feedback with General Preference Model\",\\n        \"citationCount\": 29,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2402.07314, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work considers a standard mathematical formulation, the reverse-KL regularized minimax game between two LLMs for RLHF under general preference oracle, and proposes sample-efficient algorithms for both the offline learning from a pre-collected preference dataset and online learning where the preference oracle is queried along the way of training.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-02-11\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2105550407\",\\n            \"name\": \"Chen Ye\"\\n          },\\n          {\\n            \"authorId\": \"2275119437\",\\n            \"name\": \"Wei Xiong\"\\n          },\\n          {\\n            \"authorId\": \"2283877837\",\\n            \"name\": \"Yuheng Zhang\"\\n          },\\n          {\\n            \"authorId\": \"35279146\",\\n            \"name\": \"Hanze Dong\"\\n          },\\n          {\\n            \"authorId\": \"2275167899\",\\n            \"name\": \"Nan Jiang\"\\n          },\\n          {\\n            \"authorId\": \"2260473361\",\\n            \"name\": \"Tong Zhang\"\\n          }\\n        ],\\n        \"abstract\": \"We investigate Reinforcement Learning from Human Feedback (RLHF) in the context of a general preference oracle. In particular, we do not assume the existence of a reward function and an oracle preference signal drawn from the Bradley-Terry model as most of the prior works do. We consider a standard mathematical formulation, the reverse-KL regularized minimax game between two LLMs for RLHF under general preference oracle. The learning objective of this formulation is to find a policy so that it is consistently preferred by the KL-regularized preference oracle over any competing LLMs. We show that this framework is strictly more general than the reward-based one, and propose sample-efficient algorithms for both the offline learning from a pre-collected preference dataset and online learning where we can query the preference oracle along the way of training. Empirical studies verify the effectiveness of the proposed framework.\"\\n      },\\n      {\\n        \"paperId\": \"0145bde24bb4e8482c2e69dd28efe5ead1fe7d8c\",\\n        \"url\": \"https://www.semanticscholar.org/paper/0145bde24bb4e8482c2e69dd28efe5ead1fe7d8c\",\\n        \"title\": \"Generalized Iterative Bayesian Update and Applications to Mechanisms for Privacy Protection\",\\n        \"citationCount\": 13,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {\\'tldr\\'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/EuroSP48549.2020.00038?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/EuroSP48549.2020.00038, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": null,\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2020-09-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2292169\",\\n            \"name\": \"Ehab ElSalamouny\"\\n          },\\n          {\\n            \"authorId\": \"1722055\",\\n            \"name\": \"C. Palamidessi\"\\n          }\\n        ],\\n        \"abstract\": \"The iterative Bayesian update (IBU) and the matrix inversion (INV) are the main methods to retrieve the original distribution from noisy data resulting from the application of privacy protection mechanisms. We show that the foundations of IBU established in the literature are flawed, as they rely on an assumption that in general is not satisfied in typical datasets. We then propose an extension of the method, covering a more general privacy model, where different users are allowed to apply different privacy mechanisms. We call our algorithm GIBU, for Generalized IBU, and we prove its convergence to the maximum likelihood estimate, constructing a proof that does not rely on the problematic assumption, thus fixing also the theory of IBU. Finally we evaluate the precision of GIBU on data sanitized with k-RR, Rappor, geo-indistinguishability and exponential mechanisms. We show that, while GIBU and INV are comparable in the first two cases, the performance of GIBU is definitely superior in the latter cases.\"\\n      },\\n      {\\n        \"paperId\": \"7d49f330d114ec3d65ffb67eb9dc79aea7b8befc\",\\n        \"url\": \"https://www.semanticscholar.org/paper/7d49f330d114ec3d65ffb67eb9dc79aea7b8befc\",\\n        \"title\": \"Spontaneous Reward Hacking in Iterative Self-Refinement\",\\n        \"citationCount\": 15,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2407.04549, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"Using an essay editing task, it is shown that iterative self-refinement leads to deviation between the language model evaluator and human judgment, demonstrating that reward hacking can occur spontaneously in-context with the use of iterative self-refinement.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-07-05\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2311112294\",\\n            \"name\": \"Jane Pan\"\\n          },\\n          {\\n            \"authorId\": \"2321875898\",\\n            \"name\": \"He He\"\\n          },\\n          {\\n            \"authorId\": \"2297768298\",\\n            \"name\": \"Samuel R. Bowman\"\\n          },\\n          {\\n            \"authorId\": \"2297816489\",\\n            \"name\": \"Shi Feng\"\\n          }\\n        ],\\n        \"abstract\": \"Language models are capable of iteratively improving their outputs based on natural language feedback, thus enabling in-context optimization of user preference. In place of human users, a second language model can be used as an evaluator, providing feedback along with numerical ratings which the generator attempts to optimize. However, because the evaluator is an imperfect proxy of user preference, this optimization can lead to reward hacking, where the evaluator\\'s ratings improve while the generation quality remains stagnant or even decreases as judged by actual user preference. The concern of reward hacking is heightened in iterative self-refinement where the generator and the evaluator use the same underlying language model, in which case the optimization pressure can drive them to exploit shared vulnerabilities. Using an essay editing task, we show that iterative self-refinement leads to deviation between the language model evaluator and human judgment, demonstrating that reward hacking can occur spontaneously in-context with the use of iterative self-refinement. In addition, we study conditions under which reward hacking occurs and observe two factors that affect reward hacking severity: model size and context sharing between the generator and the evaluator.\"\\n      },\\n      {\\n        \"paperId\": \"21ed545a3b02bbcba6a62f5858bca948c2ed4641\",\\n        \"url\": \"https://www.semanticscholar.org/paper/21ed545a3b02bbcba6a62f5858bca948c2ed4641\",\\n        \"title\": \"Explainable Benchmarking for Iterative Optimization Heuristics\",\\n        \"citationCount\": 19,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://arxiv.org/pdf/2401.17842\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2401.17842, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"The IOHxplainer software library is introduced, for systematic analysing the performance of various optimization algorithms and the impact of their different components and hyperparameters, aiming to improve future benchmarking and algorithm design practices.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-01-31\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2218156728\",\\n            \"name\": \"N. V. Stein\"\\n          },\\n          {\\n            \"authorId\": \"108119673\",\\n            \"name\": \"Diederick Vermetten\"\\n          },\\n          {\\n            \"authorId\": \"3160375\",\\n            \"name\": \"Anna V. Kononova\"\\n          },\\n          {\\n            \"authorId\": \"2237990304\",\\n            \"name\": \"T. Back\"\\n          }\\n        ],\\n        \"abstract\": \"Benchmarking heuristic algorithms is vital to understand under which conditions and on what kind of problems certain algorithms perform well. In most current research into heuristic optimization algorithms, only a very limited number of scenarios, algorithm configurations and hyper-parameter settings are explored, leading to incomplete and often biased insights and results. This article presents a novel approach that we call explainable benchmarking. We introduce the IOHxplainer software library, for systematic analysing the performance of various optimization algorithms and the impact of their different components and hyperparameters. We showcase the methodology in the context of two modular optimization implementations. Through this library, we examine the impact of different algorithmic components and configurations, offering insights into their performance across diverse scenarios. We provide a systematic method for evaluating and interpreting the behaviour and efficiency of iterative optimization heuristics in a more transparent and comprehensible manner, aiming to improve future benchmarking and algorithm design practices.\"\\n      },\\n      {\\n        \"paperId\": \"0019cb24ec04498836b8215e9495b968f2c01666\",\\n        \"url\": \"https://www.semanticscholar.org/paper/0019cb24ec04498836b8215e9495b968f2c01666\",\\n        \"title\": \"IGEV++: Iterative Multi-Range Geometry Encoding Volumes for Stereo Matching\",\\n        \"citationCount\": 41,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2409.00638, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper introduces an adaptive patch matching module that efficiently and effectively computes matching costs for large disparity ranges and/or ill-posed regions, and proposes a selective geometry feature fusion module to adaptively fuse multi-range and multi-granularity geometry features in MGEV.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-09-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2158317969\",\\n            \"name\": \"Gangwei Xu\"\\n          },\\n          {\\n            \"authorId\": \"2289775621\",\\n            \"name\": \"Xianqi Wang\"\\n          },\\n          {\\n            \"authorId\": \"2319408912\",\\n            \"name\": \"Zhaoxing Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2049033628\",\\n            \"name\": \"Junda Cheng\"\\n          },\\n          {\\n            \"authorId\": \"2319622516\",\\n            \"name\": \"Chunyuan Liao\"\\n          },\\n          {\\n            \"authorId\": \"2265575132\",\\n            \"name\": \"Xin Yang\"\\n          }\\n        ],\\n        \"abstract\": \"Stereo matching is a core component in many computer vision and robotics systems. Despite significant advances over the last decade, handling matching ambiguities in ill-posed regions and large disparities remains an open challenge. In this paper, we propose a new deep network architecture, called IGEV++, for stereo matching. The proposed IGEV++ constructs Multi-range Geometry Encoding Volumes (MGEV), which encode coarse-grained geometry information for ill-posed regions and large disparities, while preserving fine-grained geometry information for details and small disparities. To construct MGEV, we introduce an adaptive patch matching module that efficiently and effectively computes matching costs for large disparity ranges and/or ill-posed regions. We further propose a selective geometry feature fusion module to adaptively fuse multi-range and multi-granularity geometry features in MGEV. Then, we input the fused geometry features into ConvGRUs to iteratively update the disparity map. MGEV allows to efficiently handle large disparities and ill-posed regions, such as occlusions and textureless regions, and enjoys rapid convergence during iterations. Our IGEV++ achieves the best performance on the Scene Flow test set across all disparity ranges, up to 768px. Our IGEV++ also achieves state-of-the-art accuracy on the Middlebury, ETH3D, KITTI 2012, and 2015 benchmarks. Specifically, IGEV++ achieves a 3.23% 2-pixel outlier rate (Bad 2.0) on the large disparity benchmark, Middlebury, representing error reductions of 31.9% and 54.8% compared to RAFT-Stereo and GMStereo, respectively. We also present a real-time version of IGEV++ that achieves the best performance among all published real-time methods on the KITTI benchmarks.\"\\n      },\\n      {\\n        \"paperId\": \"f305fddaf0aa92cd19b8714db6ef67ade6ece1ee\",\\n        \"url\": \"https://www.semanticscholar.org/paper/f305fddaf0aa92cd19b8714db6ef67ade6ece1ee\",\\n        \"title\": \"De-Hallucinator: Iterative Grounding for LLM-Based Code Completion\",\\n        \"citationCount\": 29,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {\\'abstract\\'}. Paper or abstract available at https://api.unpaywall.org/v2/10.48550/arXiv.2401.01701?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.48550/arXiv.2401.01701, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"De-Hallucinator is presented, an LLM-based code completion technique that grounds the predictions of a model through a novel combination of retrieving suitable API references and iteratively querying the model with increasingly suitable context information in the prompt.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": null,\\n        \"authors\": [\\n          {\\n            \"authorId\": \"46235560\",\\n            \"name\": \"A. Eghbali\"\\n          },\\n          {\\n            \"authorId\": \"2282536979\",\\n            \"name\": \"Michael Pradel\"\\n          }\\n        ],\\n        \"abstract\": null\\n      }\\n    ]\\n  },\\n  \"thematic coherence prompting\": {\\n    \"total\": 692,\\n    \"offset\": 0,\\n    \"next\": 50,\\n    \"data\": [\\n      {\\n        \"paperId\": \"f50be995ae7bc8b5889df68be77bf89d1d253a50\",\\n        \"url\": \"https://www.semanticscholar.org/paper/f50be995ae7bc8b5889df68be77bf89d1d253a50\",\\n        \"title\": \"Supporting best practice in reflexive thematic analysis reporting in Palliative Medicine: A review of published research and introduction to the Reflexive Thematic Analysis Reporting Guidelines (RTARG)\",\\n        \"citationCount\": 350,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://journals.sagepub.com/doi/pdf/10.1177/02692163241234800\",\\n          \"status\": \"HYBRID\",\\n          \"license\": \"CCBY\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC11157981, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"The Reflexive Thematic Analysis Reporting Guidelines (the RTARG) are developed, informed by this review, other reviews the authors have done and their values and experience as qualitative researchers, to support qualitative researchers in producing coherent and reflexively open reports of reflexive thematic analysis.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Review\"\\n        ],\\n        \"publicationDate\": \"2024-03-12\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2290884456\",\\n            \"name\": \"Virginia Braun\"\\n          },\\n          {\\n            \"authorId\": \"2287216787\",\\n            \"name\": \"Victoria Clarke\"\\n          }\\n        ],\\n        \"abstract\": \"Background: Reflexive thematic analysis is widely used in qualitative research published in Palliative Medicine, and in the broader field of health research. However, this approach is often not used well. Common problems in published reflexive thematic analysis in general include assuming thematic analysis is a singular approach, rather than a family of methods, confusing themes and topics, and treating and reporting reflexive thematic analysis as if it is atheoretical. Purpose: We reviewed 20 papers published in Palliative Medicine between 2014 and 2022 that cited Braun and Clarke, identified using the search term \\\\u2018thematic analysis\\\\u2019 and the default \\\\u2018relevance\\\\u2019 setting on the journal webpage. The aim of the review was to identify common problems and instances of good practice. Problems centred around a lack of methodological coherence, and a lack of reflexive openness, clarity and detail in reporting. We considered contributors to these common problems, including the use of reporting checklists that are not coherent with the values of reflexive thematic analysis. To support qualitative researchers in producing coherent and reflexively open reports of reflexive thematic analysis we have developed the Reflexive Thematic Analysis Reporting Guidelines (the RTARG; in Supplemental Materials) informed by this review, other reviews we have done and our values and experience as qualitative researchers. The RTARG is also intended for use by peer reviewers to encourage methodologically coherent reviewing. Key learning points: Methodological incoherence and a lack of transparency are common problems in reflexive thematic analysis research published in Palliative Medicine. Coherence can be facilitated by researchers and reviewers striving to be knowing \\\\u2013 thoughtful, deliberative, reflexive and theoretically aware \\\\u2013 practitioners and appraisers of reflexive thematic analysis and developing an understanding of the diversity within the thematic analysis family of methods.\"\\n      },\\n      {\\n        \"paperId\": \"8b12b59aef23566a7a3461f69230c83ab632a918\",\\n        \"url\": \"https://www.semanticscholar.org/paper/8b12b59aef23566a7a3461f69230c83ab632a918\",\\n        \"title\": \"Weak central coherence in adults with ASD: Evidence from eye-tracking and thematic content analysis of social scenes\",\\n        \"citationCount\": 11,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://figshare.com/articles/journal_contribution/Weak_central_coherence_in_adults_with_ASD_Evidence_from_eye-tracking_and_thematic_content_analysis_of_social_scenes/19633669/1/files/34870696.pdf\",\\n          \"status\": \"GREEN\",\\n          \"license\": \"CCBY\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1080/23279095.2022.2060105?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1080/23279095.2022.2060105, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": null,\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2021-08-24\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"1677019029\",\\n            \"name\": \"S. Tassini\"\\n          },\\n          {\\n            \"authorId\": \"39763547\",\\n            \"name\": \"M. C. Melo\"\\n          },\\n          {\\n            \"authorId\": \"2922008\",\\n            \"name\": \"O. Bueno\"\\n          },\\n          {\\n            \"authorId\": \"2222542539\",\\n            \"name\": \"Claudia Berlim de Mello\"\\n          }\\n        ],\\n        \"abstract\": \"Abstract Central Coherence Weakness has been defined as a tendency for local rather than global processing that may underlie core deficits in Autism Spectrum Disorder (ASD). In social contexts it may be expressed in difficulties to integrate social cues arising from the recognition of emotions in faces or from the environment in order to understand people\\'s interactions. A sample of 28 adults diagnosed with ASD Level 1 and 25 controls was submitted to a cartoon-like task with the instruction to describe social scenes and to Navon letter stimuli. Both quantitative measures and qualitative (thematic content analysis) procedures were used to assess performance. Heatmap and fixation preferences according to the stimuli quadrants were used to investigate eye-tracking patterns. A tendency to local processing, independently of the stimuli type, in the ASD participants was observed. Data from visual tracking by quadrants and from verbal reports suggest loss of social cues important for understanding context. Their reaction time and response duration were increased in relation to controls. The findings corroborate the idea that weak central coherence may be part of the cognitive phenotype in ASD. LAY ABSTRACT Autistic adults often report difficulties in interpreting social situations. These difficulties are commonly associated with a tendency to visually focus on specific parts of the situation (known as local processing) to the detriment of the whole situation. This way of looking at things has been given the name \\\\u201cweak central coherence,\\\\u201d and may result in difficulties in understanding a situation or other people\\\\u2019s behaviors. A group of ASD and controls were asked to describe two different types of image, one showing a common social situation, the other Navon figure. Eye-tracking technology was used to analyze how the participants looked at the images (which part of the image and for how long) and asked about what they had seen. The results showed that in the group of autistic participants there was a tendency to focus on the details in both types of images. The analysis of the verbal reports revealed that the interpretation of the social contexts by those with ASD was not what was expected, associated with a specific focus on details. These findings may be useful for a better understanding of some difficulties experienced by ASD in social contexts and contribute to therapeutic treatments.\"\\n      },\\n      {\\n        \"paperId\": \"fff1e9eb32f8591aea3f2db54035465093381b89\",\\n        \"url\": \"https://www.semanticscholar.org/paper/fff1e9eb32f8591aea3f2db54035465093381b89\",\\n        \"title\": \"Selective Prompting Tuning for Personalized Conversations with LLMs\",\\n        \"citationCount\": 13,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2406.18187, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work initializes a set of soft prompts and uses a trainable dense retriever to adaptively select suitable soft prompts for LLMs according to different input contexts, where the prompt retriever is dynamically updated through feedback from the LLMs.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-06-26\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2274015208\",\\n            \"name\": \"Qiushi Huang\"\\n          },\\n          {\\n            \"authorId\": \"2110814131\",\\n            \"name\": \"Xubo Liu\"\\n          },\\n          {\\n            \"authorId\": \"2273320275\",\\n            \"name\": \"Tom Ko\"\\n          },\\n          {\\n            \"authorId\": \"49814531\",\\n            \"name\": \"Boyong Wu\"\\n          },\\n          {\\n            \"authorId\": \"2239051433\",\\n            \"name\": \"Wenwu Wang\"\\n          },\\n          {\\n            \"authorId\": \"2273525536\",\\n            \"name\": \"Yu Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2189113746\",\\n            \"name\": \"Lilian Tang\"\\n          }\\n        ],\\n        \"abstract\": \"In conversational AI, personalizing dialogues with persona profiles and contextual understanding is essential. Despite large language models\\' (LLMs) improved response coherence, effective persona integration remains a challenge. In this work, we first study two common approaches for personalizing LLMs: textual prompting and direct fine-tuning. We observed that textual prompting often struggles to yield responses that are similar to the ground truths in datasets, while direct fine-tuning tends to produce repetitive or overly generic replies. To alleviate those issues, we propose \\\\\\\\textbf{S}elective \\\\\\\\textbf{P}rompt \\\\\\\\textbf{T}uning (SPT), which softly prompts LLMs for personalized conversations in a selective way. Concretely, SPT initializes a set of soft prompts and uses a trainable dense retriever to adaptively select suitable soft prompts for LLMs according to different input contexts, where the prompt retriever is dynamically updated through feedback from the LLMs. Additionally, we propose context-prompt contrastive learning and prompt fusion learning to encourage the SPT to enhance the diversity of personalized conversations. Experiments on the CONVAI2 dataset demonstrate that SPT significantly enhances response diversity by up to 90\\\\\\\\%, along with improvements in other critical performance indicators. Those results highlight the efficacy of SPT in fostering engaging and personalized dialogue generation. The SPT model code (https://github.com/hqsiswiliam/SPT) is publicly available for further exploration.\"\\n      },\\n      {\\n        \"paperId\": \"71e294c3b503f141b11cad8e3ebc033a667d2b3b\",\\n        \"url\": \"https://www.semanticscholar.org/paper/71e294c3b503f141b11cad8e3ebc033a667d2b3b\",\\n        \"title\": \"Exploring Learner Prompting Behavior and Its Effect on ChatGPT-Assisted English Writing Revision\",\\n        \"citationCount\": 12,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {\\'abstract\\'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1007/s40299-024-00930-6?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1007/s40299-024-00930-6, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"Findings underscore the necessity of incorporating prompt-writing instruction into English writing pedagogy to enhance learners\\\\u2019 ability to craft specific, goal-aligned prompts, leading to more productive feedback from AI tools like ChatGPT and facilitating meaningful improvements in higher-order writing skills.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-10-21\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"116938583\",\\n            \"name\": \"M. Hwang\"\\n          },\\n          {\\n            \"authorId\": \"121076013\",\\n            \"name\": \"Robert D. Jeens\"\\n          },\\n          {\\n            \"authorId\": \"2314224171\",\\n            \"name\": \"Hee-Kyung Lee\"\\n          }\\n        ],\\n        \"abstract\": null\\n      },\\n      {\\n        \"paperId\": \"b908824639d18f11883abcab21efeb22e315ab9c\",\\n        \"url\": \"https://www.semanticscholar.org/paper/b908824639d18f11883abcab21efeb22e315ab9c\",\\n        \"title\": \"Multimodal Procedural Planning via Dual Text-Image Prompting\",\\n        \"citationCount\": 51,\\n        \"openAccessPdf\": {\\n          \"url\": \"http://arxiv.org/pdf/2305.01795\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2305.01795, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"Text-Image Prompting (TIP) is proposed, a dual-modality prompting method that jointly leverages zero-shot reasoning ability in large language models (LLMs) and compelling text-to-image generation ability from diffusion-based models to tackle the key challenges of MPP.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2023-05-02\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"47006228\",\\n            \"name\": \"Yujie Lu\"\\n          },\\n          {\\n            \"authorId\": \"2887562\",\\n            \"name\": \"Pan Lu\"\\n          },\\n          {\\n            \"authorId\": \"2142370346\",\\n            \"name\": \"Zhiyu Chen\"\\n          },\\n          {\\n            \"authorId\": \"51439692\",\\n            \"name\": \"Wanrong Zhu\"\\n          },\\n          {\\n            \"authorId\": \"47120131\",\\n            \"name\": \"X. Wang\"\\n          },\\n          {\\n            \"authorId\": \"1682479\",\\n            \"name\": \"William Yang Wang\"\\n          }\\n        ],\\n        \"abstract\": \"Embodied agents have achieved prominent performance in following human instructions to complete tasks. However, the potential of providing instructions informed by texts and images to assist humans in completing tasks remains underexplored. To uncover this capability, we present the multimodal procedural planning (MPP) task, in which models are given a high-level goal and generate plans of paired text-image steps, providing more complementary and informative guidance than unimodal plans. The key challenges of MPP are to ensure the informativeness, temporal coherence,and accuracy of plans across modalities. To tackle this, we propose Text-Image Prompting (TIP), a dual-modality prompting method that jointly leverages zero-shot reasoning ability in large language models (LLMs) and compelling text-to-image generation ability from diffusion-based models. TIP improves the interaction in the dual modalities using Text-to-Image Bridge and Image-to-Text Bridge, allowing LLMs to guide the textual-grounded image plan generation and leveraging the descriptions of image plans to ground the textual plan reversely. To address the lack of relevant datasets, we collect WIKIPLAN and RECIPEPLAN as a testbed for MPP. Our results show compelling human preferences and automatic scores against unimodal and multimodal baselines on WIKIPLAN and RECIPEPLAN in terms of informativeness, temporal coherence, and plan accuracy. Our code and data: https://github.com/YujieLu10/MPP.\"\\n      },\\n      {\\n        \"paperId\": \"5e490270c0fd49418e6bc5732c375867f253a20f\",\\n        \"url\": \"https://www.semanticscholar.org/paper/5e490270c0fd49418e6bc5732c375867f253a20f\",\\n        \"title\": \"Fine-Grained Modeling of Narrative Context: A Coherence Perspective via Retrospective Questions\",\\n        \"citationCount\": 10,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://arxiv.org/pdf/2402.13551\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2402.13551, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work introduces an original and practical paradigm for narrative comprehension, stemming from the characteristics that individual passages within narratives tend to be more cohesively related than isolated, by formulating a graph dubbed NarCo, which explicitly depicts task-agnostic coherence dependencies that are ready to be consumed by various downstream tasks.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-02-21\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2276317005\",\\n            \"name\": \"Liyan Xu\"\\n          },\\n          {\\n            \"authorId\": \"153154545\",\\n            \"name\": \"JiangNan Li\"\\n          },\\n          {\\n            \"authorId\": \"2265525656\",\\n            \"name\": \"Mo Yu\"\\n          },\\n          {\\n            \"authorId\": \"2283871195\",\\n            \"name\": \"Jie Zhou\"\\n          }\\n        ],\\n        \"abstract\": \"This work introduces an original and practical paradigm for narrative comprehension, stemming from the characteristics that individual passages within narratives tend to be more cohesively related than isolated. Complementary to the common end-to-end paradigm, we propose a fine-grained modeling of narrative context, by formulating a graph dubbed NarCo, which explicitly depicts task-agnostic coherence dependencies that are ready to be consumed by various downstream tasks. In particular, edges in NarCo encompass free-form retrospective questions between context snippets, inspired by human cognitive perception that constantly reinstates relevant events from prior context. Importantly, our graph formalism is practically instantiated by LLMs without human annotations, through our designed two-stage prompting scheme. To examine the graph properties and its utility, we conduct three studies in narratives, each from a unique angle: edge relation efficacy, local context enrichment, and broader application in QA. All tasks could benefit from the explicit coherence captured by NarCo.\"\\n      },\\n      {\\n        \"paperId\": \"e47bce9eaa7d0cce48c64d614f11105d48fb3881\",\\n        \"url\": \"https://www.semanticscholar.org/paper/e47bce9eaa7d0cce48c64d614f11105d48fb3881\",\\n        \"title\": \"Conceptual and design thinking for thematic analysis\",\\n        \"citationCount\": 2681,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {\\'abstract\\'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1037/QUP0000196?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1037/QUP0000196, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": null,\\n        \"publicationTypes\": null,\\n        \"publicationDate\": \"2021-02-22\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"40465786\",\\n            \"name\": \"Virginia Braun\"\\n          },\\n          {\\n            \"authorId\": \"145881640\",\\n            \"name\": \"Victoria Clarke\"\\n          }\\n        ],\\n        \"abstract\": null\\n      },\\n      {\\n        \"paperId\": \"fb449d114d7d3550fb2fff424be6b5d8434d7924\",\\n        \"url\": \"https://www.semanticscholar.org/paper/fb449d114d7d3550fb2fff424be6b5d8434d7924\",\\n        \"title\": \"Prompting Large Language Models for Topic Modeling\",\\n        \"citationCount\": 45,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2312.09693, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper proposes PromptTopic, a novel topic modeling approach that harnesses the advanced language understanding of large language models (LLMs) to address challenges of short text datasets that lack co-occurring words.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2023-12-15\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2262111802\",\\n            \"name\": \"Han Wang\"\\n          },\\n          {\\n            \"authorId\": \"2218633063\",\\n            \"name\": \"Nirmalendu Prakash\"\\n          },\\n          {\\n            \"authorId\": \"2261746844\",\\n            \"name\": \"N. Hoang\"\\n          },\\n          {\\n            \"authorId\": \"72043108\",\\n            \"name\": \"Ming Shan Hee\"\\n          },\\n          {\\n            \"authorId\": \"2339777541\",\\n            \"name\": \"Usman Naseem\"\\n          },\\n          {\\n            \"authorId\": \"2266006388\",\\n            \"name\": \"Roy Ka-Wei Lee\"\\n          }\\n        ],\\n        \"abstract\": \"Topic modeling is a widely used technique for revealing underlying thematic structures within textual data. However, existing models have certain limitations, particularly when dealing with short text datasets that lack co-occurring words. Moreover, these models often neglect sentence-level semantics, focusing primarily on token-level semantics. In this paper, we propose PromptTopic, a novel topic modeling approach that harnesses the advanced language understanding of large language models (LLMs) to address these challenges. It involves extracting topics at the sentence level from individual documents, then aggregating and condensing these topics into a predefined quantity, ultimately providing coherent topics for texts of varying lengths. This approach eliminates the need for manual parameter tuning and improves the quality of extracted topics. We benchmark PromptTopic against the state-of-the-art baselines on three vastly diverse datasets, establishing its proficiency in discovering meaningful topics. Furthermore, qualitative analysis showcases PromptTopic\\\\u2019s ability to uncover relevant topics in multiple datasets.\"\\n      },\\n      {\\n        \"paperId\": \"3f5bc00fb6d1de56e3a4902df74ef305adfe4dcd\",\\n        \"url\": \"https://www.semanticscholar.org/paper/3f5bc00fb6d1de56e3a4902df74ef305adfe4dcd\",\\n        \"title\": \"Improved prompting and process for writing user personas with LLMs, using qualitative interviews: Capturing behaviour and personality traits of users\",\\n        \"citationCount\": 13,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://arxiv.org/pdf/2310.06391\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2310.06391, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This draft paper presents a workflow for creating User Personas with Large Language Models, using the results of a Thematic Analysis of qualitative interviews, using a recently released LLM which allows the processing of 16 thousand tokens and the possibility to offer a refined prompting for the creation of Personas.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2023-10-10\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"1774708\",\\n            \"name\": \"S. Paoli\"\\n          }\\n        ],\\n        \"abstract\": \"This draft paper presents a workflow for creating User Personas with Large Language Models, using the results of a Thematic Analysis of qualitative interviews. The proposed workflow uses improved prompting and a larger pool of Themes, compared to previous work conducted by the author for the same task. This is possible due to the capabilities of a recently released LLM which allows the processing of 16 thousand tokens (GPT3.5-Turbo-16k) and also due to the possibility to offer a refined prompting for the creation of Personas. The paper offers details of performing Phase 2 and 3 of Thematic Analysis, and then discusses the improved workflow for creating Personas. The paper also offers some reflections on the relationship between the proposed process and existing approaches to Personas such as the data-driven and qualitative Personas. Moreover, the paper offers reflections on the capacity of LLMs to capture user behaviours and personality traits, from the underlying dataset of qualitative interviews used for the analysis.\"\\n      },\\n      {\\n        \"paperId\": \"234e31a541d5d4d1f64497cdd98dfde5131a342a\",\\n        \"url\": \"https://www.semanticscholar.org/paper/234e31a541d5d4d1f64497cdd98dfde5131a342a\",\\n        \"title\": \"BERTopic Modeling of Natural Language Processing Abstracts: Thematic Structure and Trajectory\",\\n        \"citationCount\": 16,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://doi.org/10.30865/mib.v7i3.6426\",\\n          \"status\": \"GOLD\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.30865/mib.v7i3.6426?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.30865/mib.v7i3.6426, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This research aimed to apply unsupervised clustering techniques to 13,027 Scopus abstracts to uncover structure and themes in natural language processing (NLP) publications to uncover structure and themes in research trends.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2023-07-31\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2091271718\",\\n            \"name\": \"S. Samsir\"\\n          },\\n          {\\n            \"authorId\": \"2313547470\",\\n            \"name\": \"Reagan Surbakti Saragih\"\\n          },\\n          {\\n            \"authorId\": \"2313548926\",\\n            \"name\": \"S. Subagio\"\\n          },\\n          {\\n            \"authorId\": \"2200502353\",\\n            \"name\": \"Rahmad Aditiya\"\\n          },\\n          {\\n            \"authorId\": \"2091271983\",\\n            \"name\": \"Ronal Watrianthos\"\\n          }\\n        ],\\n        \"abstract\": \"The rapid growth in the academic literature presents challenges in identifying relevant studies. This research aimed to apply unsupervised clustering techniques to 13,027 Scopus abstracts to uncover structure and themes in natural language processing (NLP) publications. Abstracts were pre-processed with tokenization, lemmatization, and vectorization. The BERTopic algorithm was used for clustering, using the MiniLM-L6-v2 embedding model and a minimum topic size of 50. Quantitative analysis revealed eight main topics, with sizes ranging from 205 to 4089 abstracts per topic. The language models topic was most prominent with 4089 abstracts. The topics were evaluated using coherence scores between 0.42 and 0.58, indicating meaningful themes. Keywords and sample documents provided interpretable topic representations. The results showcase the ability to produce coherent topics and capture connections between NLP studies. Clustering supports focused browsing and identification of relevant literature. Unlike human-curated classifications, the unsupervised data-driven approach prevents bias. Given the need to understand research trends, clustering abstracts enables efficient knowledge discovery from scientific corpora. This methodology can be applied to various datasets and fields to uncover overlooked patterns. The ability to adjust parameters allows for customized analysis. In general, unsupervised clustering provides a versatile framework for navigating, summarizing, and analyzing academic literature as volumes expand exponentially.\"\\n      },\\n      {\\n        \"paperId\": \"baeb8b5869eb94a2e43ea8cd0df3b8077ec8e91c\",\\n        \"url\": \"https://www.semanticscholar.org/paper/baeb8b5869eb94a2e43ea8cd0df3b8077ec8e91c\",\\n        \"title\": \"SAM-OCTA: Prompting Segment-Anything for OCTA Image Segmentation\",\\n        \"citationCount\": 10,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://arxiv.org/pdf/2310.07183\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2310.07183, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"From comprehensive experimental results with the OCTA-500 dataset, the SAM-OCTA method has achieved state-of-the-art performance in common OCTA segmentation tasks related to RV and FAZ, and it also performs accurate segmentation of artery-vein and local vessels.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2023-10-11\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2243298334\",\\n            \"name\": \"Xinrun Chen\"\\n          },\\n          {\\n            \"authorId\": \"2243437545\",\\n            \"name\": \"Chengliang Wang\"\\n          },\\n          {\\n            \"authorId\": \"2216447644\",\\n            \"name\": \"Haojian Ning\"\\n          },\\n          {\\n            \"authorId\": \"2257137905\",\\n            \"name\": \"Shiying Li\"\\n          }\\n        ],\\n        \"abstract\": \"Segmenting specific targets or biomarkers is necessary to analyze optical coherence tomography angiography (OCTA) images. Previous methods typically segment all the targets in an OCTA sample, such as retinal vessels (RVs). Although these methods perform well in accuracy and precision, OCTA analyses often focusing local information within the images which has not been fulfilled. In this paper, we propose a method called SAM-OCTA for local segmentation in OCTA images. The method fine-tunes a pre-trained segment anything model (SAM) using low-rank adaptation (LoRA) and utilizes prompt points for local RVs, arteries, and veins segmentation in OCTA. To explore the effect and mechanism of prompt points, we set up global and local segmentation modes with two prompt point generation strategies, namely random selection and special annotation. Considering practical usage, we conducted extended experiments with different model scales and analyzed the model performance before and after fine-tuning besides the general segmentation task. From comprehensive experimental results with the OCTA-500 dataset, our SAM-OCTA method has achieved state-of-the-art performance in common OCTA segmentation tasks related to RV and FAZ, and it also performs accurate segmentation of artery-vein and local vessels. The code is available at https://github.com/ShellRedia/SAM-OCTA-extend.\"\\n      },\\n      {\\n        \"paperId\": \"78164b272db4afa960e3e0e62e3331dc36a48cd2\",\\n        \"url\": \"https://www.semanticscholar.org/paper/78164b272db4afa960e3e0e62e3331dc36a48cd2\",\\n        \"title\": \"Analyzable Chain-of-Musical-Thought Prompting for High-Fidelity Music Generation\",\\n        \"citationCount\": 12,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2503.19611, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"MusiCoT empowers the AR model to first outline an overall music structure before generating audio tokens, thereby enhancing the coherence and creativity of the resulting compositions, producing music quality that rivals state-of-the-art generation models.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2025-03-25\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2316946541\",\\n            \"name\": \"Max W. Y. Lam\"\\n          },\\n          {\\n            \"authorId\": \"2358193518\",\\n            \"name\": \"Yijin Xing\"\\n          },\\n          {\\n            \"authorId\": \"2352015696\",\\n            \"name\": \"Weiya You\"\\n          },\\n          {\\n            \"authorId\": \"2287919309\",\\n            \"name\": \"Jingcheng Wu\"\\n          },\\n          {\\n            \"authorId\": \"2118932090\",\\n            \"name\": \"Zongyu Yin\"\\n          },\\n          {\\n            \"authorId\": \"2352404752\",\\n            \"name\": \"Fuqiang Jiang\"\\n          },\\n          {\\n            \"authorId\": \"2319220700\",\\n            \"name\": \"Hangyu Liu\"\\n          },\\n          {\\n            \"authorId\": \"2319238631\",\\n            \"name\": \"Feng Liu\"\\n          },\\n          {\\n            \"authorId\": \"2292777550\",\\n            \"name\": \"Xingda Li\"\\n          },\\n          {\\n            \"authorId\": \"2352448900\",\\n            \"name\": \"Wei-Tsung Lu\"\\n          },\\n          {\\n            \"authorId\": \"2352227661\",\\n            \"name\": \"Hanyu Chen\"\\n          },\\n          {\\n            \"authorId\": \"2352042214\",\\n            \"name\": \"Tong Feng\"\\n          },\\n          {\\n            \"authorId\": \"2352908525\",\\n            \"name\": \"Tianwei Zhao\"\\n          },\\n          {\\n            \"authorId\": \"2352289761\",\\n            \"name\": \"Chien-Hung Liu\"\\n          },\\n          {\\n            \"authorId\": \"2300130251\",\\n            \"name\": \"Xuchen Song\"\\n          },\\n          {\\n            \"authorId\": \"2353891852\",\\n            \"name\": \"Yang Li\"\\n          },\\n          {\\n            \"authorId\": \"2353423689\",\\n            \"name\": \"Yahui Zhou\"\\n          }\\n        ],\\n        \"abstract\": \"Autoregressive (AR) models have demonstrated impressive capabilities in generating high-fidelity music. However, the conventional next-token prediction paradigm in AR models does not align with the human creative process in music composition, potentially compromising the musicality of generated samples. To overcome this limitation, we introduce MusiCoT, a novel chain-of-thought (CoT) prompting technique tailored for music generation. MusiCoT empowers the AR model to first outline an overall music structure before generating audio tokens, thereby enhancing the coherence and creativity of the resulting compositions. By leveraging the contrastive language-audio pretraining (CLAP) model, we establish a chain of\\\\\"musical thoughts\\\\\", making MusiCoT scalable and independent of human-labeled data, in contrast to conventional CoT methods. Moreover, MusiCoT allows for in-depth analysis of music structure, such as instrumental arrangements, and supports music referencing -- accepting variable-length audio inputs as optional style references. This innovative approach effectively addresses copying issues, positioning MusiCoT as a vital practical method for music prompting. Our experimental results indicate that MusiCoT consistently achieves superior performance across both objective and subjective metrics, producing music quality that rivals state-of-the-art generation models. Our samples are available at https://MusiCoT.github.io/.\"\\n      },\\n      {\\n        \"paperId\": \"8d7971734b904ece00f8c830b3ddd684deca50a3\",\\n        \"url\": \"https://www.semanticscholar.org/paper/8d7971734b904ece00f8c830b3ddd684deca50a3\",\\n        \"title\": \"ChatGPT in thematic analysis: Can AI become a research assistant in qualitative research?\",\\n        \"citationCount\": 18,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {\\'abstract\\'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1007/s11135-025-02165-z?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1007/s11135-025-02165-z, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This article evaluates how GenAI can support thematic analysis using a publicly available interview dataset from Lumivero and introduces Guided AI Thematic Analysis (GAITA), an adaptation of King et al.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2025-06-02\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"1450826804\",\\n            \"name\": \"Kien Nguyen-Trung\"\\n          }\\n        ],\\n        \"abstract\": null\\n      },\\n      {\\n        \"paperId\": \"94617fc3593e87c4ff72ea0a3f5456fde5896801\",\\n        \"url\": \"https://www.semanticscholar.org/paper/94617fc3593e87c4ff72ea0a3f5456fde5896801\",\\n        \"title\": \"Deductive Closure Training of Language Models for Coherence, Accuracy, and Updatability\",\\n        \"citationCount\": 21,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2401.08574, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"DCT shows that LMs\\' reasoning capabilities during inference can be leveraged during training to improve their reliability, and shows that LMs\\' reasoning capabilities during inference can be leveraged during training to improve their reliability.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-01-16\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"1754469865\",\\n            \"name\": \"Afra Feyza Aky\\\\u00fcrek\"\\n          },\\n          {\\n            \"authorId\": \"1992708068\",\\n            \"name\": \"Ekin Aky\\\\u00fcrek\"\\n          },\\n          {\\n            \"authorId\": \"41019330\",\\n            \"name\": \"Leshem Choshen\"\\n          },\\n          {\\n            \"authorId\": \"2129412\",\\n            \"name\": \"Derry Tanti Wijaya\"\\n          },\\n          {\\n            \"authorId\": \"2257268279\",\\n            \"name\": \"Jacob Andreas\"\\n          }\\n        ],\\n        \"abstract\": \"While language models (LMs) can sometimes generate factually correct text and estimate truth values of individual claims, these generally do not reflect a globally coherent, manipulable model of the world. As a consequence, current LMs also generate incorrect or nonsensical content, and are difficult to edit and bring up to date. We present a method called Deductive Closure Training (DCT) that uses LMs themselves to identify implications of (and contradictions within) the text that they generate, yielding an efficient self-supervised procedure for improving LM factuality. Given a collection of seed documents, DCT prompts LMs to generate additional text implied by these documents, reason globally about the correctness of this generated text, and finally fine-tune on text inferred to be correct. Given seed documents from a trusted source, DCT provides a tool for supervised model updating; if seed documents are sampled from the LM itself, DCT enables fully unsupervised fine-tuning for improved coherence and accuracy. Across the CREAK, MQUaKE, and Reversal Curse datasets, supervised DCT improves LM fact verification and text generation accuracy by 3-26%; on CREAK fully unsupervised DCT improves verification accuracy by 12%. These results show that LMs\\' reasoning capabilities during inference can be leveraged during training to improve their reliability.\"\\n      },\\n      {\\n        \"paperId\": \"e3f68363ec2c5b1472dad31d8551dd516861c134\",\\n        \"url\": \"https://www.semanticscholar.org/paper/e3f68363ec2c5b1472dad31d8551dd516861c134\",\\n        \"title\": \"Performance of Artificial Intelligence in Detecting Diabetic Macular Edema From Fundus Photography and Optical Coherence Tomography Images: A Systematic Review and Meta-analysis.\",\\n        \"citationCount\": 20,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://diabetesjournals.figshare.com/articles/figure/Performance_of_Artificial_Intelligence_in_Detecting_Diabetic_Macular_Edema_from_Fundus_Photographs_and_Optical_Coherence_Tomography_Images_A_Systematic_Review_and_Meta-analysis/24518287/1/files/43067464.pdf\",\\n          \"status\": \"GREEN\",\\n          \"license\": \"CCBYNCSA\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.2337/dc23-0993?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.2337/dc23-0993, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This meta-analysis demonstrates satisfactory performance of AI in detecting DME from FP or OCT images and indicates external validation is warranted for future studies to evaluate model generalizability.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Review\"\\n        ],\\n        \"publicationDate\": \"2024-01-19\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2279977400\",\\n            \"name\": \"Ching Lam\"\\n          },\\n          {\\n            \"authorId\": \"2279974752\",\\n            \"name\": \"Yiu Lun Wong\"\\n          },\\n          {\\n            \"authorId\": \"1758777204\",\\n            \"name\": \"Ziqi Tang\"\\n          },\\n          {\\n            \"authorId\": \"2191364808\",\\n            \"name\": \"Xiaoyan Hu\"\\n          },\\n          {\\n            \"authorId\": \"2191377385\",\\n            \"name\": \"Truong X. Nguyen\"\\n          },\\n          {\\n            \"authorId\": \"2277830890\",\\n            \"name\": \"Dawei Yang\"\\n          },\\n          {\\n            \"authorId\": \"2280039410\",\\n            \"name\": \"Shuyi Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2280058989\",\\n            \"name\": \"J. Ding\"\\n          },\\n          {\\n            \"authorId\": \"34785499\",\\n            \"name\": \"Simon K H Szeto\"\\n          },\\n          {\\n            \"authorId\": \"108011531\",\\n            \"name\": \"A. Ran\"\\n          },\\n          {\\n            \"authorId\": \"2249532719\",\\n            \"name\": \"C. Y. Cheung\"\\n          }\\n        ],\\n        \"abstract\": \"BACKGROUND\\\\nDiabetic macular edema (DME) is the leading cause of vision loss in people with diabetes. Application of artificial intelligence (AI) in interpreting fundus photography (FP) and optical coherence tomography (OCT) images allows prompt detection and intervention.\\\\n\\\\n\\\\nPURPOSE\\\\nTo evaluate the performance of AI in detecting DME from FP or OCT images and identify potential factors affecting model performances.\\\\n\\\\n\\\\nDATA SOURCES\\\\nWe searched seven electronic libraries up to 12 February 2023.\\\\n\\\\n\\\\nSTUDY SELECTION\\\\nWe included studies using AI to detect DME from FP or OCT images.\\\\n\\\\n\\\\nDATA EXTRACTION\\\\nWe extracted study characteristics and performance parameters.\\\\n\\\\n\\\\nDATA SYNTHESIS\\\\nFifty-three studies were included in the meta-analysis. FP-based algorithms of 25 studies yielded pooled area under the receiver operating characteristic curve (AUROC), sensitivity, and specificity of 0.964, 92.6%, and 91.1%, respectively. OCT-based algorithms of 28 studies yielded pooled AUROC, sensitivity, and specificity of 0.985, 95.9%, and 97.9%, respectively. Potential factors improving model performance included deep learning techniques, larger size, and more diversity in training data sets. Models demonstrated better performance when validated internally than externally, and those trained with multiple data sets showed better results upon external validation.\\\\n\\\\n\\\\nLIMITATIONS\\\\nAnalyses were limited by unstandardized algorithm outcomes and insufficient data in patient demographics, OCT volumetric scans, and external validation.\\\\n\\\\n\\\\nCONCLUSIONS\\\\nThis meta-analysis demonstrates satisfactory performance of AI in detecting DME from FP or OCT images. External validation is warranted for future studies to evaluate model generalizability. Further investigations may estimate optimal sample size, effect of class balance, patient demographics, and additional benefits of OCT volumetric scans.\"\\n      },\\n      {\\n        \"paperId\": \"9aa5a10147b893f13f55f7500c89ba0a5e5d065b\",\\n        \"url\": \"https://www.semanticscholar.org/paper/9aa5a10147b893f13f55f7500c89ba0a5e5d065b\",\\n        \"title\": \"Vision-Language Models for Feature Detection of Macular Diseases on Optical Coherence Tomography.\",\\n        \"citationCount\": 19,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1001/jamaophthalmol.2024.1165?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1001/jamaophthalmol.2024.1165, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"In this study, a generalist VLM demonstrated limited vision capabilities for feature detection and management of macular disease, however, it showed low self-contradiction, suggesting strong language capabilities.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-05-02\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"41132090\",\\n            \"name\": \"F. Antaki\"\\n          },\\n          {\\n            \"authorId\": \"49334294\",\\n            \"name\": \"Reena Chopra\"\\n          },\\n          {\\n            \"authorId\": \"5638585\",\\n            \"name\": \"P. Keane\"\\n          }\\n        ],\\n        \"abstract\": \"Importance\\\\nVision-language models (VLMs) are a novel artificial intelligence technology capable of processing image and text inputs. While demonstrating strong generalist capabilities, their performance in ophthalmology has not been extensively studied.\\\\n\\\\n\\\\nObjective\\\\nTo assess the performance of the Gemini Pro VLM in expert-level tasks for macular diseases from optical coherence tomography (OCT) scans.\\\\n\\\\n\\\\nDesign, Setting, and Participants\\\\nThis was a cross-sectional diagnostic accuracy study evaluating a generalist VLM on ophthalmology-specific tasks using the open-source Optical Coherence Tomography Image Database. The dataset included OCT B-scans from 50 unique patients: healthy individuals and those with macular hole, diabetic macular edema, central serous chorioretinopathy, and age-related macular degeneration. Each OCT scan was labeled for 10 key pathological features, referral recommendations, and treatments. The images were captured using a Cirrus high definition OCT machine (Carl Zeiss Meditec) at Sankara Nethralaya Eye Hospital, Chennai, India, and the dataset was published in December 2018. Image acquisition dates were not specified.\\\\n\\\\n\\\\nExposures\\\\nGemini Pro, using a standard prompt to extract structured responses on December 15, 2023.\\\\n\\\\n\\\\nMain Outcomes and Measures\\\\nThe primary outcome was model responses compared against expert labels, calculating F1 scores for each pathological feature. Secondary outcomes included accuracy in diagnosis, referral urgency, and treatment recommendation. The model\\'s internal concordance was evaluated by measuring the alignment between referral and treatment recommendations, independent of diagnostic accuracy.\\\\n\\\\n\\\\nResults\\\\nThe mean F1 score was 10.7% (95% CI, 2.4-19.2). Measurable F1 scores were obtained for macular hole (36.4%; 95% CI, 0-71.4), pigment epithelial detachment (26.1%; 95% CI, 0-46.2), subretinal hyperreflective material (24.0%; 95% CI, 0-45.2), and subretinal fluid (20.0%; 95% CI, 0-45.5). A correct diagnosis was achieved in 17 of 50 cases (34%; 95% CI, 22-48). Referral recommendations varied: 28 of 50 were correct (56%; 95% CI, 42-70), 10 of 50 were overcautious (20%; 95% CI, 10-32), and 12 of 50 were undercautious (24%; 95% CI, 12-36). Referral and treatment concordance were very high, with 48 of 50 (96%; 95 % CI, 90-100) and 48 of 49 (98%; 95% CI, 94-100) correct answers, respectively.\\\\n\\\\n\\\\nConclusions and Relevance\\\\nIn this study, a generalist VLM demonstrated limited vision capabilities for feature detection and management of macular disease. However, it showed low self-contradiction, suggesting strong language capabilities. As VLMs continue to improve, validating their performance on large benchmarking datasets will help ascertain their potential in ophthalmology.\"\\n      },\\n      {\\n        \"paperId\": \"4bd41e401be5ca91d3302c2a888f565e8fc13749\",\\n        \"url\": \"https://www.semanticscholar.org/paper/4bd41e401be5ca91d3302c2a888f565e8fc13749\",\\n        \"title\": \"Large Language Models Can Enable Inductive Thematic Analysis of a Social Media Corpus in a Single Prompt: Human Validation Study\",\\n        \"citationCount\": 21,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://s3.ca-central-1.amazonaws.com/assets.jmir.org/assets/preprints/preprint-59641-accepted.pdf\",\\n          \"status\": \"GOLD\",\\n          \"license\": \"CCBY\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC11393503, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"LLMs can effectively and efficiently process large social media\\\\u2013based health-related data sets and extract themes from such data that human subject matter experts deem reasonable, but it is unable to show that the LLMs tested can replicate the depth of analysis from human subject matter experts by consistently extracting the same themes from the same data.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-04-24\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"6436816\",\\n            \"name\": \"Michael S. Deiner\"\\n          },\\n          {\\n            \"authorId\": \"2211301918\",\\n            \"name\": \"Vlad Honcharov\"\\n          },\\n          {\\n            \"authorId\": \"1492112680\",\\n            \"name\": \"Jiawei Li\"\\n          },\\n          {\\n            \"authorId\": \"2237755335\",\\n            \"name\": \"T. Mackey\"\\n          },\\n          {\\n            \"authorId\": \"2274513993\",\\n            \"name\": \"Travis C Porco\"\\n          },\\n          {\\n            \"authorId\": \"2261922685\",\\n            \"name\": \"Urmimala Sarkar\"\\n          }\\n        ],\\n        \"abstract\": \"Background Manually analyzing public health\\\\u2013related content from social media provides valuable insights into the beliefs, attitudes, and behaviors of individuals, shedding light on trends and patterns that can inform public understanding, policy decisions, targeted interventions, and communication strategies. Unfortunately, the time and effort needed from well-trained human subject matter experts makes extensive manual social media listening unfeasible. Generative large language models (LLMs) can potentially summarize and interpret large amounts of text, but it is unclear to what extent LLMs can glean subtle health-related meanings in large sets of social media posts and reasonably report health-related themes. Objective We aimed to assess the feasibility of using LLMs for topic model selection or inductive thematic analysis of large contents of social media posts by attempting to answer the following question: Can LLMs conduct topic model selection and inductive thematic analysis as effectively as humans did in a prior manual study, or at least reasonably, as judged by subject matter experts? Methods We asked the same research question and used the same set of social media content for both the LLM selection of relevant topics and the LLM analysis of themes as was conducted manually in a published study about vaccine rhetoric. We used the results from that study as background for this LLM experiment by comparing the results from the prior manual human analyses with the analyses from 3 LLMs: GPT4-32K, Claude-instant-100K, and Claude-2-100K. We also assessed if multiple LLMs had equivalent ability and assessed the consistency of repeated analysis from each LLM. Results The LLMs generally gave high rankings to the topics chosen previously by humans as most relevant. We reject a null hypothesis (P<.001, overall comparison) and conclude that these LLMs are more likely to include the human-rated top 5 content areas in their top rankings than would occur by chance. Regarding theme identification, LLMs identified several themes similar to those identified by humans, with very low hallucination rates. Variability occurred between LLMs and between test runs of an individual LLM. Despite not consistently matching the human-generated themes, subject matter experts found themes generated by the LLMs were still reasonable and relevant. Conclusions LLMs can effectively and efficiently process large social media\\\\u2013based health-related data sets. LLMs can extract themes from such data that human subject matter experts deem reasonable. However, we were unable to show that the LLMs we tested can replicate the depth of analysis from human subject matter experts by consistently extracting the same themes from the same data. There is vast potential, once better validated, for automated LLM-based real-time social listening for common and rare health conditions, informing public health understanding of the public\\\\u2019s interests and concerns and determining the public\\\\u2019s ideas to address them.\"\\n      },\\n      {\\n        \"paperId\": \"4913529041ac69912680d1f8c1b1e4229a68b61b\",\\n        \"url\": \"https://www.semanticscholar.org/paper/4913529041ac69912680d1f8c1b1e4229a68b61b\",\\n        \"title\": \"The Intellectual Evolution of Educational Leadership Research: A Combined Bibliometric and Thematic Analysis Using SciMAT\",\\n        \"citationCount\": 15,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://www.mdpi.com/2227-7102/14/4/429/pdf?version=1713520131\",\\n          \"status\": \"GOLD\",\\n          \"license\": \"CCBY\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.3390/educsci14040429?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.3390/educsci14040429, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": null,\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-04-19\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"66769167\",\\n            \"name\": \"Turgut Karakose\"\\n          },\\n          {\\n            \"authorId\": \"114837130\",\\n            \"name\": \"K. Leithwood\"\\n          },\\n          {\\n            \"authorId\": \"3224712\",\\n            \"name\": \"Tijen T\\\\u00fcl\\\\u00fcba\\\\u015f\"\\n          }\\n        ],\\n        \"abstract\": \"This study aims to describe the century-long trajectory of educational leadership research (ELR), including changes over time in its main and subsidiary themes, as well as its most influential authors, papers, and journals. The study combines the bibliometric performance and science mapping analysis of 7282 articles retrieved from the Scopus and WoS databases. SciMAT software (version 1.1.04) was used to analyze changes over four sequential time periods and to exhibit the thematic evolution of the field\\\\u2014Period 1 (1907 to 2004), Period 2 (2005 to 2012), Period 3 (2013 to 2019), and Period 4 (2020\\\\u20132023). Research during Period 1 focused on principals and included efforts to distinguish between their administrative functions and forms of \\\\u2018strong\\\\u2019 leadership contributing to school improvement. Period 2 included research aimed at understanding what strong principal leadership entailed, including the development and testing of more coherent models of such leadership. While instructional and transformational leadership models were prominent during Periods 1 and 2, Period 3 research invested heavily in conceptions of leadership distribution. Early research about \\\\u2018social justice leadership\\\\u2019 appeared during this period and eventually flourished during Period 4. While principals were an active focus through all Periods, the leadership of others gradually dominated ELR and accounted for the broader leadership theme found in all four periods. The results point to the evolutionary nature of ELR development, which eventually produced a relatively robust knowledge base. Experiences with the COVID-19 pandemic suggest that crises such as this might prompt more revolutionary orientations in the ELR field.\"\\n      },\\n      {\\n        \"paperId\": \"587b50d31e99f37489e1a2b338d8ddb07026da0b\",\\n        \"url\": \"https://www.semanticscholar.org/paper/587b50d31e99f37489e1a2b338d8ddb07026da0b\",\\n        \"title\": \"Exploring Students\\\\u2019 Perceptions of ChatGPT: Thematic Analysis and Follow-Up Survey\",\\n        \"citationCount\": 271,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://doi.org/10.1109/access.2023.3268224\",\\n          \"status\": \"GOLD\",\\n          \"license\": \"CCBYNCND\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ACCESS.2023.3268224?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ACCESS.2023.3268224, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"It is concluded that ChatGPT can and should be used for learning, however, students should be aware of its limitations.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Review\"\\n        ],\\n        \"publicationDate\": null,\\n        \"authors\": [\\n          {\\n            \"authorId\": \"1754800\",\\n            \"name\": \"Abdulhadi Shoufan\"\\n          }\\n        ],\\n        \"abstract\": \"ChatGPT has sparked both excitement and skepticism in education. To analyze its impact on teaching and learning it is crucial to understand how students perceive ChatGPT and assess its potential and challenges. Toward this, we conducted a two-stage study with senior students in a computer engineering program ( $n=56$ ). In the first stage, we asked the students to evaluate ChatGPT using their own words after they used it to complete one learning activity. The returned responses (3136 words) were analyzed by coding and theme building (36 codes and 15 themes). In the second stage, we used the derived codes and themes to create a 27-item questionnaire. The students responded to this questionnaire three weeks later after completing other activities with the help of ChatGPT. The results show that the students admire the capabilities of ChatGPT and find it interesting, motivating, and helpful for study and work. They find it easy to use and appreciate its human-like interface that provides well-structured responses and good explanations. However, many students feel that ChatGPT\\\\u2019s answers are not always accurate and most of them believe that it requires good background knowledge to work with since it does not replace human intelligence. So, most students think that ChatGPT needs to be improved but are optimistic that this will happen soon. When it comes to the negative impact of ChatGPT on learning, academic integrity, jobs, and life, the students are divided. We conclude that ChatGPT can and should be used for learning. However, students should be aware of its limitations. Educators should try using ChatGPT and guide students on effective prompting techniques and how to assess generated responses. The developers should improve their models to enhance the accuracy of given answers. The study provides insights into the capabilities and limitations of ChatGPT in education and informs future research and development.\"\\n      },\\n      {\\n        \"paperId\": \"a01a9c4a114fbf201540268f928ccf77bc3f9357\",\\n        \"url\": \"https://www.semanticscholar.org/paper/a01a9c4a114fbf201540268f928ccf77bc3f9357\",\\n        \"title\": \"Fine-Grained Visual Prompting\",\\n        \"citationCount\": 90,\\n        \"openAccessPdf\": {\\n          \"url\": \"http://arxiv.org/pdf/2306.04356\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2306.04356, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper introduces a new zero-shot framework that leverages pixel-level annotations acquired from a generalist segmentation model for fine-grained visual prompting and reveals that a straightforward application of blur outside the target mask, referred to as the Blur Reverse Mask, exhibits exceptional effectiveness.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2023-06-07\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2146416206\",\\n            \"name\": \"Lingfeng Yang\"\\n          },\\n          {\\n            \"authorId\": \"2217456303\",\\n            \"name\": \"Yueze Wang\"\\n          },\\n          {\\n            \"authorId\": \"2144439048\",\\n            \"name\": \"Xiang Li\"\\n          },\\n          {\\n            \"authorId\": \"51316629\",\\n            \"name\": \"Xinlong Wang\"\\n          },\\n          {\\n            \"authorId\": \"2146236917\",\\n            \"name\": \"Jian Yang\"\\n          }\\n        ],\\n        \"abstract\": \"Vision-Language Models (VLMs), such as CLIP, have demonstrated impressive zero-shot transfer capabilities in image-level visual perception. However, these models have shown limited performance in instance-level tasks that demand precise localization and recognition. Previous works have suggested that incorporating visual prompts, such as colorful boxes or circles, can improve the ability of models to recognize objects of interest. Nonetheless, compared to language prompting, visual prompting designs are rarely explored. Existing approaches, which employ coarse visual cues such as colorful boxes or circles, often result in sub-optimal performance due to the inclusion of irrelevant and noisy pixels. In this paper, we carefully study the visual prompting designs by exploring more fine-grained markings, such as segmentation masks and their variations. In addition, we introduce a new zero-shot framework that leverages pixel-level annotations acquired from a generalist segmentation model for fine-grained visual prompting. Consequently, our investigation reveals that a straightforward application of blur outside the target mask, referred to as the Blur Reverse Mask, exhibits exceptional effectiveness. This proposed prompting strategy leverages the precise mask annotations to reduce focus on weakly related regions while retaining spatial coherence between the target and the surrounding background. Our Fine-Grained Visual Prompting (FGVP) demonstrates superior performance in zero-shot comprehension of referring expressions on the RefCOCO, RefCOCO+, and RefCOCOg benchmarks. It outperforms prior methods by an average margin of 3.0% to 4.6%, with a maximum improvement of 12.5% on the RefCOCO+ testA subset. Code is available at https://github.com/ylingfeng/FGVP.\"\\n      },\\n      {\\n        \"paperId\": \"e1770838ec0667cad48729a81764ed9964d6a8e6\",\\n        \"url\": \"https://www.semanticscholar.org/paper/e1770838ec0667cad48729a81764ed9964d6a8e6\",\\n        \"title\": \"LLM-in-the-loop: Leveraging Large Language Model for Thematic Analysis\",\\n        \"citationCount\": 108,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2310.15100, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work proposes a human-LLM collaboration framework (i.e., LLM-in-the-loop) to conduct TA with in-context learning (ICL), which yields similar coding quality to that of human coders but reduces TA\\'s labor and time demands.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\",\\n          \"Review\"\\n        ],\\n        \"publicationDate\": \"2023-10-23\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"3400291\",\\n            \"name\": \"Shih-Chieh Dai\"\\n          },\\n          {\\n            \"authorId\": \"2261362789\",\\n            \"name\": \"Aiping Xiong\"\\n          },\\n          {\\n            \"authorId\": \"1746959\",\\n            \"name\": \"Lun-Wei Ku\"\\n          }\\n        ],\\n        \"abstract\": \"Thematic analysis (TA) has been widely used for analyzing qualitative data in many disciplines and fields. To ensure reliable analysis, the same piece of data is typically assigned to at least two human coders. Moreover, to produce meaningful and useful analysis, human coders develop and deepen their data interpretation and coding over multiple iterations, making TA labor-intensive and time-consuming. Recently the emerging field of large language models (LLMs) research has shown that LLMs have the potential replicate human-like behavior in various tasks: in particular, LLMs outperform crowd workers on text-annotation tasks, suggesting an opportunity to leverage LLMs on TA. We propose a human-LLM collaboration framework (i.e., LLM-in-the-loop) to conduct TA with in-context learning (ICL). This framework provides the prompt to frame discussions with a LLM (e.g., GPT-3.5) to generate the final codebook for TA. We demonstrate the utility of this framework using survey datasets on the aspects of the music listening experience and the usage of a password manager. Results of the two case studies show that the proposed framework yields similar coding quality to that of human coders but reduces TA\\'s labor and time demands.\"\\n      },\\n      {\\n        \"paperId\": \"b54398b8ccba47ecab43a5311e5b2a6c61461936\",\\n        \"url\": \"https://www.semanticscholar.org/paper/b54398b8ccba47ecab43a5311e5b2a6c61461936\",\\n        \"title\": \"Who Evaluates the Evaluations? Objectively Scoring Text-to-Image Prompt Coherence Metrics with T2IScoreScore (TS2)\",\\n        \"citationCount\": 12,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2404.04251, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"T2IScoreScore is introduced, a curated set of semantic error graphs containing a prompt and a set of increasingly erroneous images to rigorously judge whether a given prompt faithfulness metric can correctly order images with respect to their objective error count and significantly discriminate between different error nodes, using meta-metric scores derived from established statistical tests.\"\\n        },\\n        \"publicationTypes\": null,\\n        \"publicationDate\": \"2024-04-05\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"48227633\",\\n            \"name\": \"Michael Stephen Saxon\"\\n          },\\n          {\\n            \"authorId\": \"2069493939\",\\n            \"name\": \"Fatima Jahara\"\\n          },\\n          {\\n            \"authorId\": \"2302799897\",\\n            \"name\": \"Mahsa Khoshnoodi\"\\n          },\\n          {\\n            \"authorId\": \"2257339858\",\\n            \"name\": \"Yujie Lu\"\\n          },\\n          {\\n            \"authorId\": \"2309678102\",\\n            \"name\": \"Aditya Sharma\"\\n          },\\n          {\\n            \"authorId\": \"2257130316\",\\n            \"name\": \"W. Wang\"\\n          }\\n        ],\\n        \"abstract\": \"With advances in the quality of text-to-image (T2I) models has come interest in benchmarking their prompt faithfulness -- the semantic coherence of generated images to the prompts they were conditioned on. A variety of T2I faithfulness metrics have been proposed, leveraging advances in cross-modal embeddings and vision-language models (VLMs). However, these metrics are not rigorously compared and benchmarked, instead presented with correlation to human Likert scores over a set of easy-to-discriminate images against seemingly weak baselines. We introduce T2IScoreScore, a curated set of semantic error graphs containing a prompt and a set of increasingly erroneous images. These allow us to rigorously judge whether a given prompt faithfulness metric can correctly order images with respect to their objective error count and significantly discriminate between different error nodes, using meta-metric scores derived from established statistical tests. Surprisingly, we find that the state-of-the-art VLM-based metrics (e.g., TIFA, DSG, LLMScore, VIEScore) we tested fail to significantly outperform simple (and supposedly worse) feature-based metrics like CLIPScore, particularly on a hard subset of naturally-occurring T2I model errors. TS2 will enable the development of better T2I prompt faithfulness metrics through more rigorous comparison of their conformity to expected orderings and separations under objective criteria.\"\\n      },\\n      {\\n        \"paperId\": \"5121e40b45f33c2fe08087173cfb8ddfc2bf4fa2\",\\n        \"url\": \"https://www.semanticscholar.org/paper/5121e40b45f33c2fe08087173cfb8ddfc2bf4fa2\",\\n        \"title\": \"Super-Resolution Cloth Animation with Spatial and Temporal Coherence\",\\n        \"citationCount\": 10,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1145/3658143?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/3658143, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper decomposes garment meshes into overlapping patches for adaptability to various styles and geometric continuity and achieves an 8\\\\u00d7 improvement in resolution for cloth animations, leveraging two core modules.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-07-19\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2312152389\",\\n            \"name\": \"Jiawang Yu\"\\n          },\\n          {\\n            \"authorId\": \"2312169459\",\\n            \"name\": \"Zhendong Wang\"\\n          }\\n        ],\\n        \"abstract\": \"Creating super-resolution cloth animations, which refine coarse cloth meshes with fine wrinkle details, faces challenges in preserving spatial consistency and temporal coherence across frames. In this paper, we introduce a general framework to address these issues, leveraging two core modules. The first module interleaves a simulator and a corrector. The simulator handles cloth dynamics, while the corrector rectifies differences in low-frequency features across various resolutions. This interleaving ensures prompt correction of spatial errors from the coarse simulation, effectively preventing their temporal propagation. The second module performs mesh-based super-resolution for detailed wrinkle enhancements. We decompose garment meshes into overlapping patches for adaptability to various styles and geometric continuity. Our method achieves an 8\\\\u00d7 improvement in resolution for cloth animations. We showcase the effectiveness of our method through diverse animation examples, including simple cloth pieces and intricate garments.\"\\n      },\\n      {\\n        \"paperId\": \"65319931eb2af60a8b922bb5f9ce1194f300e6df\",\\n        \"url\": \"https://www.semanticscholar.org/paper/65319931eb2af60a8b922bb5f9ce1194f300e6df\",\\n        \"title\": \"\\\\\"We Are Visual Thinkers, Not Verbal Thinkers!\\\\\": A Thematic Analysis of How Professional Designers Use Generative AI Image Generation Tools\",\\n        \"citationCount\": 12,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1145/3679318.3685370?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/3679318.3685370, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A qualitative study involving 16 professional designers from the automotive industry identified their challenges with existing GenAI image generation tools in daily design practices, and revealed the need for visual input-centric multi-modal interfaces that extend beyond textual prompts.\"\\n        },\\n        \"publicationTypes\": [\\n          \"Book\",\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-10-13\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2264963307\",\\n            \"name\": \"Hyerim Park\"\\n          },\\n          {\\n            \"authorId\": \"29415466\",\\n            \"name\": \"Joscha Eirich\"\\n          },\\n          {\\n            \"authorId\": \"2292312388\",\\n            \"name\": \"Andr\\\\u00e9 Luckow\"\\n          },\\n          {\\n            \"authorId\": \"2243254992\",\\n            \"name\": \"Michael Sedlmair\"\\n          }\\n        ],\\n        \"abstract\": \"Generative artificial intelligence (GenAI) has become increasingly popular, influencing various creative domains. However, while broader societal perspectives have been analyzed, specific examinations of how practitioners utilize GenAI tools to enhance their current workflows remain limited. To address this gap, we conducted a qualitative study involving 16 professional designers from the automotive industry. We aimed to identify their challenges with existing GenAI image generation tools in daily design practices. Thematic analysis revealed four key themes: (1) the need for visual input-centric multi-modal interfaces that extend beyond textual prompts, (2) the lack of support for the iterative nature of design processes in GenAI tools, (3) difficulties in controlling prompts to achieve desired outputs, and (4) the significance of incorporating human experiences and emotions into design. Based on our findings, we propose and discuss potential design considerations for enhancing future GenAI image generation tool interfaces.\"\\n      },\\n      {\\n        \"paperId\": \"22438bcb9ef4fb5be5ea9ea8307d4691a3209f4d\",\\n        \"url\": \"https://www.semanticscholar.org/paper/22438bcb9ef4fb5be5ea9ea8307d4691a3209f4d\",\\n        \"title\": \"Quantitative and qualitative assessment of anterior segment optical coherence tomography capture of disease state in childhood anterior uveitis\",\\n        \"citationCount\": 18,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://discovery.ucl.ac.uk/10144499/3/Solebo_AS_OCT_clean_230122.pdf\",\\n          \"status\": \"GREEN\",\\n          \"license\": \"other-oa\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1136/bjophthalmol-2021-320448?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1136/bjophthalmol-2021-320448, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"The findings suggest that OCT-based imaging holds the promise of deeper understanding of disease, improved patient experience and more granular monitoring of activity with resultant improved outcomes, but further work is needed to refine acquisition and analysis protocols.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2022-02-23\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"1967704903\",\\n            \"name\": \"Katie Etherton\"\\n          },\\n          {\\n            \"authorId\": \"145884297\",\\n            \"name\": \"J. Rahi\"\\n          },\\n          {\\n            \"authorId\": \"3971827\",\\n            \"name\": \"H. Petrushkin\"\\n          },\\n          {\\n            \"authorId\": \"2090508795\",\\n            \"name\": \"A. Dick\"\\n          },\\n          {\\n            \"authorId\": \"1573631346\",\\n            \"name\": \"S. Akbarali\"\\n          },\\n          {\\n            \"authorId\": \"15498366\",\\n            \"name\": \"R. Pattani\"\\n          },\\n          {\\n            \"authorId\": \"4326096\",\\n            \"name\": \"S. Hau\"\\n          },\\n          {\\n            \"authorId\": \"46521197\",\\n            \"name\": \"S. Lacassagne\"\\n          },\\n          {\\n            \"authorId\": \"46521856\",\\n            \"name\": \"Xiaoxuan Liu\"\\n          },\\n          {\\n            \"authorId\": \"145661041\",\\n            \"name\": \"A. Denniston\"\\n          },\\n          {\\n            \"authorId\": \"8489719\",\\n            \"name\": \"A. Solebo\"\\n          }\\n        ],\\n        \"abstract\": \"Background/aims Anterior segment optical coherence tomography (AS-OCT) assessment of anterior chamber inflammation is an emerging tool. We describe the performance of AS-OCT in a paediatric population. Methods A mixed-methods prospective study, using routine clinical assessment as reference standard, and AS-OCT, with Tomey CASIA2 or Heidelberg Spectralis HS1, as index test, with data collected on patient perceptions of imaging. Repeatability, diagnostic indices, responsiveness to clinical change and clinical correlations of imaging-based metrics (image cell count, size, density and brightness) were assessed, with construction of receiver operated characteristic curves. Exploratory thematic analysis of responses from families was undertaken. Results A total of 90 children (180 eyes) underwent imaging. Bland Altman limits of agreement for CASIA2 repeatability ranged from +17 cells (95%\\\\u2009CI 13.6 to 21.1) to \\\\u221219 cells (95%\\\\u2009CI \\\\u221215.6 to \\\\u221223.2) and HS1 from +1 (95% CI 0.9 to 1.2) to \\\\u22121.0 (\\\\u22121.2 to \\\\u22120.8) cells. CASIA2 imaging had higher sensitivity of 0.92 (95% CI 0.78 to 0.97) vs HS1 imaging 0.17 (95% CI 0.07 to 0.34), with positive correlation between clinical grade and CASIA2 cell count (coefficient 12.8, p=0.02, 95%\\\\u2009CI 2.2 to 23.4). Change in clinical grade at follow-up examinations correlated with change in image based \\\\u2018cell\\\\u2019 count (r2=0.79, p<0.001). Patients reported a potential positive impact of seeing their disease activity. Conclusion Our findings suggest that OCT-based imaging holds the promise of deeper understanding of disease, improved patient experience and more granular monitoring of activity with resultant improved outcomes, but further work is needed to refine acquisition and analysis protocols.\"\\n      },\\n      {\\n        \"paperId\": \"a0968924129aee94ca2070004e657df3d34a41dd\",\\n        \"url\": \"https://www.semanticscholar.org/paper/a0968924129aee94ca2070004e657df3d34a41dd\",\\n        \"title\": \"Academic publisher guidelines on AI usage: A ChatGPT supported thematic analysis\",\\n        \"citationCount\": 67,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://f1000research.com/articles/12-1398/pdf\",\\n          \"status\": \"GOLD\",\\n          \"license\": \"CCBY\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC10844801, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A novel analysis supported by GenAI tools is used to identify themes emerging in the policy landscape, underscoring the need for an informed, flexible approach to policy formulation that can adapt to the rapidly evolving landscape of AI technologies.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Review\"\\n        ],\\n        \"publicationDate\": \"2023-10-23\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"48765116\",\\n            \"name\": \"Mike Perkins\"\\n          },\\n          {\\n            \"authorId\": \"114397354\",\\n            \"name\": \"Jasper Roe\"\\n          }\\n        ],\\n        \"abstract\": \"Background As Artificial Intelligence (AI) technologies such as Generative AI (GenAI) have become more common in academic settings, it is necessary to examine how these tools interact with issues of authorship, academic integrity, and research methodologies. The current landscape lacks cohesive policies and guidelines for regulating AI\\\\u2019s role in academic research which has prompted discussions among publishers, authors, and institutions. Methods This study employs inductive thematic analysis to explore publisher policies regarding AI-assisted authorship and academic work. Our methods involved a two-fold analysis using both AI-assisted and traditional unassisted techniques to examine the available policies from leading academic publishers and other publishing or academic entities. The framework was designed to offer multiple perspectives, harnessing the strengths of AI for pattern recognition while leveraging human expertise for nuanced interpretation. The results of these two analyses are combined to form the final themes. Results Our findings indicate six overall themes, three of which were independently identified in both the AI-assisted and unassisted, manual analysis using common software tools. A broad consensus appears among publishers that human authorship remains paramount and that the use of GenAI tools is permissible but must be disclosed. However, GenAI tools are increasingly acknowledged for their supportive roles, including text generation and data analysis. The study also discusses the inherent limitations and biases of AI-assisted analysis, necessitating rigorous scrutiny by authors, reviewers, and editors. Conclusions There is a growing recognition of AI\\\\u2019s role as a valuable auxiliary tool in academic research, but one that comes with caveats pertaining to integrity, accountability, and interpretive limitations. This study used a novel analysis supported by GenAI tools to identify themes emerging in the policy landscape, underscoring the need for an informed, flexible approach to policy formulation that can adapt to the rapidly evolving landscape of AI technologies.\"\\n      },\\n      {\\n        \"paperId\": \"5c8ad3edc40efc0da8937fdafae7796d487efdf1\",\\n        \"url\": \"https://www.semanticscholar.org/paper/5c8ad3edc40efc0da8937fdafae7796d487efdf1\",\\n        \"title\": \"Pain experiences during intrauterine device procedures: a thematic analysis of tweets\",\\n        \"citationCount\": 14,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://srh.bmj.com/content/familyplanning/early/2024/06/11/bmjsrh-2023-202011.full.pdf\",\\n          \"status\": \"HYBRID\",\\n          \"license\": \"CCBYNC\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC11503099, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"The findings attest the need for strategies to improve the patient experience for those opting for IUD as a clinical priority and further research should explore IUD users\\' experiences, expectations and wishes around pain management.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-06-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"6247626\",\\n            \"name\": \"Neda Taghinejadi\"\\n          },\\n          {\\n            \"authorId\": \"6056729\",\\n            \"name\": \"H. van der Westhuizen\"\\n          },\\n          {\\n            \"authorId\": \"1723406956\",\\n            \"name\": \"F. I. Ayomoh\"\\n          },\\n          {\\n            \"authorId\": \"47340073\",\\n            \"name\": \"Wasim Ahmed\"\\n          },\\n          {\\n            \"authorId\": \"2300379872\",\\n            \"name\": \"Trisha Greenhalgh\"\\n          },\\n          {\\n            \"authorId\": \"144511766\",\\n            \"name\": \"A. Boylan\"\\n          }\\n        ],\\n        \"abstract\": \"Introduction In June 2021, high-profile testimonials in the media about pain during intrauterine device (IUD) procedures in the UK prompted significant discussion across platforms including Twitter (subsequently renamed X). We examined a sample of Twitter postings (tweets) to gain insight into public perspectives and experiences. Methods We harvested tweets posted or retweeted on 21\\\\u201322 June 2021 which contained the search terms coil, intrauterine system, IUD or intrauterine. We analysed the dataset thematically and selected illustrative tweets with the authors\\\\u2019 consent for publication. Results Following deduplication and screening, we included 1431 tweets in our analysis. We identified testimonials with descriptions of varied pain experiences. Twitter users reported that clinicians had not warned them that pain could be severe or explained the options for pain relief. Some raised concerns about pain being minimised or dismissed and linked this to the management of women\\\\u2019s pain in medicine more broadly. Twitter users described connecting to an online community with shared experiences as validating and used this as a springboard for collective action. Conclusions While we acknowledge the limitations of our sample, this study highlights important perspectives and accounts relating to pain during IUD procedures. Our findings attest to the need for strategies to improve the patient experience for those opting for IUD as a clinical priority. Further research should explore IUD users\\' experiences, expectations and wishes around pain management.\"\\n      },\\n      {\\n        \"paperId\": \"b09ab1ca37173fbea6c24cda2bbda0c93cea6c91\",\\n        \"url\": \"https://www.semanticscholar.org/paper/b09ab1ca37173fbea6c24cda2bbda0c93cea6c91\",\\n        \"title\": \"Singing off the same hymn sheet? Examining coherence in a talent development pathway (part 2)\",\\n        \"citationCount\": 12,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1080/02640414.2021.2021702?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1080/02640414.2021.2021702, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"Results suggest that the talent development environment provides a long-term development experience supplemented with individual athlete attention at international level, but there is a need for improved direction from the National Governing Body if systematic coherence towards talent development is to be achieved.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2022-01-05\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"122738315\",\\n            \"name\": \"O. Curran\"\\n          },\\n          {\\n            \"authorId\": \"91859067\",\\n            \"name\": \"D. Passmore\"\\n          },\\n          {\\n            \"authorId\": \"6973495\",\\n            \"name\": \"\\\\u00c1. Macnamara\"\\n          }\\n        ],\\n        \"abstract\": \"ABSTRACT Existing literature highlights the common characteristics of successful talent development environments, notably the need for long-term development, individual athlete attention, communication, alignment, and psycho-behavioural development. Little is known however about the complex talent development environment of an international sport organisation where multiple contexts and various stakeholders exist. Considering the lack of research relating to females in talent development, we examined a female national hockey talent development environment and more specifically the level of coherence that existed within the talent development environment from different stakeholder perspectives. Twenty-seven international female hockey players and fourteen pathway staff members from across the talent development pathway participated in semi-structured focus groups. An inductive\\\\u2013deductive thematic analysis was conducted. Results suggest that the talent development environment provides a long-term development experience supplemented with individual athlete attention at international level. However, a general lack of coherence and systematic development was evident across the talent development environment contexts with varying levels of coherence found within the higher-order themes of appropriate development, not early success, individualised and ongoing development, and wide-ranging coherent messages and support. This highlights a need for improved direction from the National Governing Body if systematic coherence towards talent development is to be achieved.\"\\n      },\\n      {\\n        \"paperId\": \"c8e142c6ee5b8a044e406670e01d98f97a5d26bf\",\\n        \"url\": \"https://www.semanticscholar.org/paper/c8e142c6ee5b8a044e406670e01d98f97a5d26bf\",\\n        \"title\": \"Comparing the Efficacy and Efficiency of Human and Generative AI: Qualitative Thematic Analyses\",\\n        \"citationCount\": 32,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://doi.org/10.2196/54482\",\\n          \"status\": \"GOLD\",\\n          \"license\": \"CCBY\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC11329846, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"The promising consistency in the themes generated by human coders and GenAI suggests that these technologies hold promise in reducing the resource intensiveness of qualitative thematic analysis; however, the relatively lower reliability in coding between them suggests that hybrid approaches are necessary.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2023-11-11\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"34245122\",\\n            \"name\": \"Maximo R. Prescott\"\\n          },\\n          {\\n            \"authorId\": \"2305191451\",\\n            \"name\": \"S. Yeager\"\\n          },\\n          {\\n            \"authorId\": \"2275218415\",\\n            \"name\": \"Lillian Ham\"\\n          },\\n          {\\n            \"authorId\": \"2051442834\",\\n            \"name\": \"C. R. Rivera Saldana\"\\n          },\\n          {\\n            \"authorId\": \"2305195892\",\\n            \"name\": \"Vanessa Serrano\"\\n          },\\n          {\\n            \"authorId\": \"2305192478\",\\n            \"name\": \"J. Narez\"\\n          },\\n          {\\n            \"authorId\": \"84640204\",\\n            \"name\": \"D. Paltin\"\\n          },\\n          {\\n            \"authorId\": \"2262498185\",\\n            \"name\": \"Jorge Delgado\"\\n          },\\n          {\\n            \"authorId\": \"2300816756\",\\n            \"name\": \"David J Moore\"\\n          },\\n          {\\n            \"authorId\": \"2301912568\",\\n            \"name\": \"Jessica Montoya\"\\n          }\\n        ],\\n        \"abstract\": \"Background Qualitative methods are incredibly beneficial to the dissemination and implementation of new digital health interventions; however, these methods can be time intensive and slow down dissemination when timely knowledge from the data sources is needed in ever-changing health systems. Recent advancements in generative artificial intelligence (GenAI) and their underlying large language models (LLMs) may provide a promising opportunity to expedite the qualitative analysis of textual data, but their efficacy and reliability remain unknown. Objective The primary objectives of our study were to evaluate the consistency in themes, reliability of coding, and time needed for inductive and deductive thematic analyses between GenAI (ie, ChatGPT and Bard) and human coders. Methods The qualitative data for this study consisted of 40 brief SMS text message reminder prompts used in a digital health intervention for promoting antiretroviral medication adherence among people with HIV who use methamphetamine. Inductive and deductive thematic analyses of these SMS text messages were conducted by 2 independent teams of human coders. An independent human analyst conducted analyses following both approaches using ChatGPT and Bard. The consistency in themes (or the extent to which the themes were the same) and reliability (or agreement in coding of themes) between methods were compared. Results The themes generated by GenAI (both ChatGPT and Bard) were consistent with 71% (5/7) of the themes identified by human analysts following inductive thematic analysis. The consistency in themes was lower between humans and GenAI following a deductive thematic analysis procedure (ChatGPT: 6/12, 50%; Bard: 7/12, 58%). The percentage agreement (or intercoder reliability) for these congruent themes between human coders and GenAI ranged from fair to moderate (ChatGPT, inductive: 31/66, 47%; ChatGPT, deductive: 22/59, 37%; Bard, inductive: 20/54, 37%; Bard, deductive: 21/58, 36%). In general, ChatGPT and Bard performed similarly to each other across both types of qualitative analyses in terms of consistency of themes (inductive: 6/6, 100%; deductive: 5/6, 83%) and reliability of coding (inductive: 23/62, 37%; deductive: 22/47, 47%). On average, GenAI required significantly less overall time than human coders when conducting qualitative analysis (20, SD 3.5 min vs 567, SD 106.5 min). Conclusions The promising consistency in the themes generated by human coders and GenAI suggests that these technologies hold promise in reducing the resource intensiveness of qualitative thematic analysis; however, the relatively lower reliability in coding between them suggests that hybrid approaches are necessary. Human coders appeared to be better than GenAI at identifying nuanced and interpretative themes. Future studies should consider how these powerful technologies can be best used in collaboration with human coders to improve the efficiency of qualitative research in hybrid approaches while also mitigating potential ethical risks that they may pose.\"\\n      },\\n      {\\n        \"paperId\": \"0edcd6dd2e952523c35e77d0f9cce5927b97d63e\",\\n        \"url\": \"https://www.semanticscholar.org/paper/0edcd6dd2e952523c35e77d0f9cce5927b97d63e\",\\n        \"title\": \"CoAScore: Chain-of-Aspects Prompting for NLG Evaluation\",\\n        \"citationCount\": 15,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2312.10355, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"The experimental findings highlight that, in comparison to individual aspect evaluation, CoAScore exhibits a higher correlation with human judgments, which significantly outperforms existing unsupervised evaluation metrics, whether for assessing overall quality or other aspects.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2023-12-16\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2275055956\",\\n            \"name\": \"Peiyuan Gong\"\\n          },\\n          {\\n            \"authorId\": \"2275056887\",\\n            \"name\": \"Jiaxin Mao\"\\n          }\\n        ],\\n        \"abstract\": \"Recently, natural language generation (NLG) evaluation has shifted from a single-aspect to a multi-aspect paradigm, allowing for a more accurate assessment. Large language models (LLMs) achieve superior performance on various NLG evaluation tasks. However, current work often employs the LLM to independently evaluate different aspects, which largely ignores the rich correlation between various aspects. To fill this research gap, in this work, we propose an NLG evaluation metric called CoAScore. Powered by LLMs, the CoAScore utilizes multi-aspect knowledge through a CoA (\\\\\\\\textbf{C}hain-\\\\\\\\textbf{o}f-\\\\\\\\textbf{A}spects) prompting framework when assessing the quality of a certain aspect. Specifically, for a given aspect to evaluate, we first prompt the LLM to generate a chain of aspects that are relevant to the target aspect and could be useful for the evaluation. We then collect evaluation scores for each generated aspect, and finally, leverage the knowledge of these aspects to improve the evaluation of the target aspect. We evaluate CoAScore across five NLG evaluation tasks (e.g., summarization, dialog response generation, etc) and nine aspects (e.g., overall quality, relevance, coherence, etc). Our experimental findings highlight that, in comparison to individual aspect evaluation, CoAScore exhibits a higher correlation with human judgments. This improvement significantly outperforms existing unsupervised evaluation metrics, whether for assessing overall quality or other aspects. We also conducted extensive ablation studies to validate the effectiveness of the three stages within the CoAScore framework and conducted case studies to show how the LLM performs in these stages. Our code and scripts are available.\"\\n      },\\n      {\\n        \"paperId\": \"0cc88cef19caa4a22639e2393407258489aa61fb\",\\n        \"url\": \"https://www.semanticscholar.org/paper/0cc88cef19caa4a22639e2393407258489aa61fb\",\\n        \"title\": \"Thematic Analysis and Artificial Intelligence: A Step-by-Step Process for Using ChatGPT in Thematic Analysis\",\\n        \"citationCount\": 20,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1177/16094069251333886?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1177/16094069251333886, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": null\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2025-04-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2356693836\",\\n            \"name\": \"Muhammad Naeem\"\\n          },\\n          {\\n            \"authorId\": \"2357287042\",\\n            \"name\": \"Tracy Smith\"\\n          },\\n          {\\n            \"authorId\": \"2357851988\",\\n            \"name\": \"Lorna Thomas\"\\n          }\\n        ],\\n        \"abstract\": \"This study sets out how to use generative artificial intelligence (AI) in the six steps of systematic thematic analysis. It leverages AI to address the limitations of traditional thematic analysis. This paper developed prompts (inputs) for ChatGPT (a generative AI chatbot based on a large language model) that are based on many researchers\\\\u2019 discussions and criticisms of qualitative data analysis. The contributions of this paper are twofold. First, it addresses a critical research gap by showcasing ChatGPT prompts for each step of the six steps of systematic thematic analysis, which also addresses researcher training in thematic analysis. Second, it contributes to the development of input to train AI in thematic analysis, including a description of how to familiarize an AI system with the context of a research study and the researcher\\\\u2019s methodological and theoretical considerations; this approach helps to reduce human bias and improves accountability and transparency in thematic analysis.\"\\n      },\\n      {\\n        \"paperId\": \"87fd1c20910a10f4c98e5da9c5144d7231de7901\",\\n        \"url\": \"https://www.semanticscholar.org/paper/87fd1c20910a10f4c98e5da9c5144d7231de7901\",\\n        \"title\": \"Thematic analysis of interview data with ChatGPT: designing and testing a reliable research protocol for qualitative research\",\\n        \"citationCount\": 12,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {\\'abstract\\'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1007/s11135-025-02199-3?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1007/s11135-025-02199-3, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"The ChatGPT model in its current state is unable to substitute the contextual insights and subtle metaphorical nuances associated with human qualitative analysis, interpretation and reflexivity, and the protocol design is able to reliably identify different thematic patterns emerging from the text.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Review\"\\n        ],\\n        \"publicationDate\": \"2025-06-20\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2365893719\",\\n            \"name\": \"Manuel Goyanes\"\\n          },\\n          {\\n            \"authorId\": \"2279054737\",\\n            \"name\": \"Carlos Lopezosa\"\\n          },\\n          {\\n            \"authorId\": \"2142957088\",\\n            \"name\": \"Beatriz Jord\\\\u00e1\"\\n          }\\n        ],\\n        \"abstract\": null\\n      },\\n      {\\n        \"paperId\": \"973b86e015b48e1f4a77f8dacfe4364e3959acd5\",\\n        \"url\": \"https://www.semanticscholar.org/paper/973b86e015b48e1f4a77f8dacfe4364e3959acd5\",\\n        \"title\": \"From Overdiagnosis to Overtreatment of Low-Risk Thyroid Cancer: A Thematic Analysis of Attitudes and Beliefs of Endocrinologists, Surgeons, and Patients\",\\n        \"citationCount\": 69,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7232663\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1089/thy.2019.0587?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1089/thy.2019.0587, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"Surgeons\\' and endocrinologists\\' attitudes and beliefs about overtreatment focus on the automaticity of overdiagnosis, and both patients and providers are cognizant of the cascade of clinical events that propel patients from incidental discovery of a thyroid nodule to surgery.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2020-01-07\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"1481094858\",\\n            \"name\": \"Catherine B. Jensen\"\\n          },\\n          {\\n            \"authorId\": \"17311230\",\\n            \"name\": \"Megan C. Saucke\"\\n          },\\n          {\\n            \"authorId\": \"4717672\",\\n            \"name\": \"D. Francis\"\\n          },\\n          {\\n            \"authorId\": \"113479047\",\\n            \"name\": \"C. Voils\"\\n          },\\n          {\\n            \"authorId\": \"3861366\",\\n            \"name\": \"Susan C. Pitt\"\\n          }\\n        ],\\n        \"abstract\": \"Introduction: The optimal management for patients with small, low-risk thyroid cancer is often debated. We aimed to characterize the attitudes and beliefs of providers and patients about management of small, low-risk thyroid cancer and how they relate to overtreatment. Methods: We conducted 34 semi-structured interviews with surgeons (n\\\\u2009=\\\\u200912), endocrinologists (n\\\\u2009=\\\\u200912), and patients with <1.5\\\\u2009cm papillary thyroid cancer (n\\\\u2009=\\\\u200910). Interviews probed about diagnosis and treatment decision-making, including nonoperative options. We used thematic analysis to identify themes related to overtreatment and created concept diagrams to map observed relationships between themes. Results: When providers discussed management of small, low-risk thyroid cancer, most felt that overtreatment was a problem, and some brought it up without prompting. Providers often believed that overtreatment results from overdiagnosis and relayed how the process commonly starts with incidental discovery of a thyroid nodule on imaging. Providers viewed biopsy of the nodule as a reflexive or habitual action. They ascribed inappropriate biopsy to lack of adherence to or knowledge of guidelines, radiologist recommendations, and the desire of patients and physicians to minimize diagnostic uncertainty. Providers described subsequent cancer diagnosis as an event that \\\\u201copens Pandora\\'s box\\\\u201d and often provokes a strong instinctive, culturally rooted need to proceed with surgery\\\\u2014specifically total thyroidectomy. Consequently, most providers felt that it is easier to prevent overdiagnosis than overtreatment and recommended strategies such as improving guideline adherence, resetting patients\\' expectations, and engaging the media. In contrast, patients did not bring up or openly discuss overtreatment or overdiagnosis. Some patients described the seemingly automatic process from an incidental finding to surgery. Their statements confirmed that the \\\\u201cneed to know\\\\u201d was a major motivation for biopsying their nodule. Patients felt that once they had a cancer diagnosis, surgery was a foregone conclusion. Patients admitted their knowledge about thyroid nodules and cancer was low, leaving room for education about the need for biopsy and less extensive treatment options. Conclusions: Surgeons\\' and endocrinologists\\' attitudes and beliefs about overtreatment focus on the automaticity of overdiagnosis. Both patients and providers are cognizant of the cascade of clinical events that propel patients from incidental discovery of a thyroid nodule to surgery.\"\\n      },\\n      {\\n        \"paperId\": \"2f7899c5c8908c4696fa94d9601f44f019f1a68c\",\\n        \"url\": \"https://www.semanticscholar.org/paper/2f7899c5c8908c4696fa94d9601f44f019f1a68c\",\\n        \"title\": \"Optical coherence tomography confirms non\\\\u2010malignant pigmented lesions in phacomatosis pigmentokeratotica using a support vector machine learning algorithm\",\\n        \"citationCount\": 12,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://onlinelibrary.wiley.com/doi/pdfdirect/10.1111/srt.13377\",\\n          \"status\": \"HYBRID\",\\n          \"license\": \"CCBY\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC10228288, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": null\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2023-05-30\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2213561309\",\\n            \"name\": \"Jenna Lee\"\\n          },\\n          {\\n            \"authorId\": \"2086942038\",\\n            \"name\": \"M. Beirami\"\\n          },\\n          {\\n            \"authorId\": \"1707975\",\\n            \"name\": \"R. Ebrahimpour\"\\n          },\\n          {\\n            \"authorId\": \"51195617\",\\n            \"name\": \"Carolina Puyana\"\\n          },\\n          {\\n            \"authorId\": \"5516425\",\\n            \"name\": \"Maria Tsoukas\"\\n          },\\n          {\\n            \"authorId\": \"150169927\",\\n            \"name\": \"K. Avanaki\"\\n          }\\n        ],\\n        \"abstract\": \"Phacomatosis pigmentokeratotica (PPK), an epidermal nevus syndrome, is characterized by the coexistence of nevus spilus and nevus sebaceus. Within the nevus spilus, an extensive range of atypical nevi of different morphologies may manifest. Pigmented lesions may fulfill the ABCDE criteria for melanoma, which may prompt a physician to perform a full\\\\u2010thickness biopsy.\"\\n      },\\n      {\\n        \"paperId\": \"847ee2ae8b588e94fcc3ff33be0601cd354df75b\",\\n        \"url\": \"https://www.semanticscholar.org/paper/847ee2ae8b588e94fcc3ff33be0601cd354df75b\",\\n        \"title\": \"Professional noticing coherence: exploring relationships between component processes\",\\n        \"citationCount\": 15,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1080/10986065.2021.1977086?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1080/10986065.2021.1977086, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": null,\\n        \"publicationTypes\": null,\\n        \"publicationDate\": \"2021-09-22\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2118528180\",\\n            \"name\": \"Jonathan Thomas\"\\n          },\\n          {\\n            \"authorId\": \"66391853\",\\n            \"name\": \"David M. Dueber\"\\n          },\\n          {\\n            \"authorId\": \"50589798\",\\n            \"name\": \"Molly H. Fisher\"\\n          },\\n          {\\n            \"authorId\": \"71615154\",\\n            \"name\": \"C. Jong\"\\n          },\\n          {\\n            \"authorId\": \"73308684\",\\n            \"name\": \"E. Schack\"\\n          }\\n        ],\\n        \"abstract\": \"ABSTRACT Teacher noticing and related variants have ascended in prominence among the mathematics education research community. While the component processes of such noticing (e.g., attending, interpreting and deciding) have been cast as interrelated, capturing the relationships amongst the components has been more elusive. We focused on the component processes of teacher noticing with particular attention given to interrelatedness. Specifically, we were interested in how and the extent to which the component processes of professional noticing (attending, interpreting, deciding) are thematically connected when preservice elementary teachers are engaged in an assessment approximating professional noticing. We refer to this thematic linkage in this paper as coherence. Our findings suggest a complex interplay between the creation and continuation of themes when enacting professional noticing, and the quality of such noticing.\"\\n      },\\n      {\\n        \"paperId\": \"6312d430e9fd98c23af1198ea0f57e3f6e59d542\",\\n        \"url\": \"https://www.semanticscholar.org/paper/6312d430e9fd98c23af1198ea0f57e3f6e59d542\",\\n        \"title\": \"An Investigation of the Coherence of Oral Narratives: Associations With Mental Health, Social Support and the Coherence of Written Narratives\",\\n        \"citationCount\": 11,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://www.frontiersin.org/articles/10.3389/fpsyg.2020.602725/pdf\",\\n          \"status\": \"GOLD\",\\n          \"license\": \"CCBY\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC7838430, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"The ability to elaborate emotionally and make meaning out of important life events in oral narratives is, to a certain extent, related to better mental health and more social support and thematic coherence may be a relatively stable feature of individuals\\\\u2019 narrative styles that is reflected in narratives of different modalities.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2021-01-13\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"84155118\",\\n            \"name\": \"L. Vanaken\"\\n          },\\n          {\\n            \"authorId\": \"4062885\",\\n            \"name\": \"P. Bijttebier\"\\n          },\\n          {\\n            \"authorId\": \"145065355\",\\n            \"name\": \"D. Hermans\"\\n          }\\n        ],\\n        \"abstract\": \"Research Questions In a first research question, we examined whether the relations that are generally observed between the coherence of written autobiographical narratives and outcomes of mental health and social support, can be replicated for the coherence of oral narratives. Second, we studied whether the coherence of oral narratives is related to the coherence of written narratives. Methods Pearson correlations and t-tests were calculated on data of two separate studies to examine the research questions. Results First, only thematic coherence of oral narratives was significantly, although moderately, negatively associated to symptoms of depression, anxiety and negative social interactions. Second, the coherence of oral narratives was higher than the coherence of written narratives. Only the thematic coherence of oral narratives was positively associated with thematic and total coherence of written narratives. Furthermore, correlations between written and oral narratives were stronger for negative narratives as compared to positive narratives. Discussion The ability to elaborate emotionally and make meaning out of important life events in oral narratives is, to a certain extent, related to better mental health and more social support. Furthermore, thematic coherence may be a relatively stable feature of individuals\\\\u2019 narrative styles that is reflected in narratives of different modalities. Nonetheless, these topics need to be further researched to overcome present limitations.\"\\n      },\\n      {\\n        \"paperId\": \"5442ac92fbdbe515cbb71cd03ac5063ebb1d5e08\",\\n        \"url\": \"https://www.semanticscholar.org/paper/5442ac92fbdbe515cbb71cd03ac5063ebb1d5e08\",\\n        \"title\": \"Deep Learning Model Based on 3D Optical Coherence Tomography Images for the Automated Detection of Pathologic Myopia\",\\n        \"citationCount\": 30,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://www.mdpi.com/2075-4418/12/3/742/pdf?version=1647601223\",\\n          \"status\": \"GOLD\",\\n          \"license\": \"CCBY\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC8947335, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This study aimed to develop an algorithm that uses 3D optical coherence tomography volumetric images (C-scan) to automatically diagnose patients with pathologic myopia and found the model based on EfficientNetB4 showed the best performance.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2022-03-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2115235853\",\\n            \"name\": \"S. Park\"\\n          },\\n          {\\n            \"authorId\": \"2380782\",\\n            \"name\": \"T. Ko\"\\n          },\\n          {\\n            \"authorId\": \"8593047\",\\n            \"name\": \"Chan-Kee Park\"\\n          },\\n          {\\n            \"authorId\": \"2124919953\",\\n            \"name\": \"Yong-Chan Kim\"\\n          },\\n          {\\n            \"authorId\": \"2159622586\",\\n            \"name\": \"In-Young Choi\"\\n          }\\n        ],\\n        \"abstract\": \"Pathologic myopia causes vision impairment and blindness, and therefore, necessitates a prompt diagnosis. However, there is no standardized definition of pathologic myopia, and its interpretation by 3D optical coherence tomography images is subjective, requiring considerable time and money. Therefore, there is a need for a diagnostic tool that can automatically and quickly diagnose pathologic myopia in patients. This study aimed to develop an algorithm that uses 3D optical coherence tomography volumetric images (C-scan) to automatically diagnose patients with pathologic myopia. The study was conducted using 367 eyes of patients who underwent optical coherence tomography tests at the Ophthalmology Department of Incheon St. Mary\\\\u2019s Hospital and Seoul St. Mary\\\\u2019s Hospital from January 2012 to May 2020. To automatically diagnose pathologic myopia, a deep learning model was developed using 3D optical coherence tomography images. The model was developed using transfer learning based on four pre-trained convolutional neural networks (ResNet18, ResNext50, EfficientNetB0, EfficientNetB4). Grad-CAM was used to visualize features affecting the detection of pathologic myopia. The performance of each model was evaluated and compared based on accuracy, sensitivity, specificity, and area under the receiver operating characteristic curve (AUROC). The model based on EfficientNetB4 showed the best performance (95% accuracy, 93% sensitivity, 96% specificity, and 98% AUROC) in identifying pathologic myopia.\"\\n      },\\n      {\\n        \"paperId\": \"1157efb8ddfbfdbe3e23078ae823507e3b8971ea\",\\n        \"url\": \"https://www.semanticscholar.org/paper/1157efb8ddfbfdbe3e23078ae823507e3b8971ea\",\\n        \"title\": \"Discursive Fields and the Diversity-Coherence Paradox: An Ecological Perspective on the Blockchain Community Discourse\",\\n        \"citationCount\": 23,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.25300/misq/2022/15736?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.25300/misq/2022/15736, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": null,\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2022-09-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2088902\",\\n            \"name\": \"S. Miranda\"\\n          },\\n          {\\n            \"authorId\": \"2111270075\",\\n            \"name\": \"D. Wang\"\\n          },\\n          {\\n            \"authorId\": \"2159799633\",\\n            \"name\": \"Chuan (Annie) Tian\"\\n          }\\n        ],\\n        \"abstract\": \"Innovation breakthroughs prompt sensemaking discourses that promote community learning and socially construct the innovation. Through this discourse, interested actors advance diverse frames, appealing to consumers with disparate preferences but raising concerns for the coherence of that discourse. We unpack this diversity-coherence paradox by recasting coherence as the relatedness of innovation frames and spotlighting the role of discursive fields that circumscribe meaning. Our empirical context is the first six years of blockchain discourse across seven discursive fields. Our research offers three insights in furtherance of an ecological perspective on innovation discourse. First, framing diversity emanates from discursive fields rather than from actors. Second, fields play differentiated roles in the framing process. Enactment fields comprised of actors with direct experience with the technology limit diversity. They do so by erecting walls that circumscribe discourse through imprinting on their original frame and retracting from or abandoning frames learned from other fields. In contrast, mediated fields, in which actors lack direct experience with the technology, enhance diversity. They do so by imitating or learning from other fields and foreshadowing or anticipating the frames used by other fields, thereby building bridges. Third, rather than opposing each other, diversity and coherence coevolve as the diversity induced by mediated fields increases framing redundancies, synthesizing frames into a coherent community understanding of the innovation. Our research signals to the actors who serve as innovation ambassadors and gatekeepers that diverse views of an innovation are not only inevitable, given the many discourse fields in which those views are formulated, but can also be coherent and desirable.\"\\n      },\\n      {\\n        \"paperId\": \"5fe7d2d670adf17a2ae2a34d76129e43fb7c2d9f\",\\n        \"url\": \"https://www.semanticscholar.org/paper/5fe7d2d670adf17a2ae2a34d76129e43fb7c2d9f\",\\n        \"title\": \"The Irish Football Player Pathway: Examining Stakeholder Coherence Throughout and Across the Player Development System\",\\n        \"citationCount\": 20,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://www.frontiersin.org/articles/10.3389/fspor.2022.834633/pdf\",\\n          \"status\": \"GOLD\",\\n          \"license\": \"CCBY\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC8884116, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A lack of stakeholder coherence is highlighted across the Irish player pathway to maximize long-term player development in the future, with findings highlighting the need for organizational intervention and structural change across the pathway.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2022-02-14\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"119033786\",\\n            \"name\": \"Liam Sweeney\"\\n          },\\n          {\\n            \"authorId\": \"6973495\",\\n            \"name\": \"\\\\u00c1. Macnamara\"\\n          },\\n          {\\n            \"authorId\": \"103263632\",\\n            \"name\": \"Daniel M. Horan\"\\n          }\\n        ],\\n        \"abstract\": \"Maximizing the efficiency of the player development system is a strategic priority for any professional football club or association. However, the successful development of a young footballer is largely dependent upon the roles and relationships of the different stakeholders invested in the developmental process. This study examined the level of horizontal (i.e., extent to which stakeholders across a pathway stage work with players in an agreed fashion to optimize their experience) and vertical (i.e., extent to which multiple stages of the pathway are coordinated and build chronologically from previous involvement toward long-term needs) stakeholder coherence throughout the Irish football player pathway following a restructuring of development policies and the implementation of a nationwide academy system between 2016 and 2020 under the Football Association of Ireland\\'s (FAI) Player Development Plan. As a second aim, we explored each of the key stakeholders\\' alignment to academic talent development principles in order to provide practical recommendations for future player and coach development policies. Accordingly, a series of interviews were conducted with 31 key stakeholders currently engaged in the player pathway. These key stakeholders consisted of parents, coaches and members of the FAI as the National Governing Body for football in Ireland. Data were analyzed using Reflexive Thematic Analysis, with findings highlighting a lack of stakeholder coherence across the pathway, both vertically and horizontally. Stakeholders displayed inconsistency in their understanding of the purpose of the player pathway and its long-term strategic aims, as well as demonstrating poor and incohesive relationships with each of the different stakeholders. Moreover, talent development principles between the different stakeholders appeared well-understood overall, although the practical implementation of several of these principles in applied practice did not appear to exist. Results highlight the need for organizational intervention and structural change across the Irish player pathway to maximize long-term player development in the future. Practical implications for the FAI are discussed and recommendations are made to support optimal player development policies moving forward.\"\\n      },\\n      {\\n        \"paperId\": \"e55ed5e7e6f998874e4edeed7f257ec8d148fc11\",\\n        \"url\": \"https://www.semanticscholar.org/paper/e55ed5e7e6f998874e4edeed7f257ec8d148fc11\",\\n        \"title\": \"An Analysis of Cohesion and Coherence of Descriptive Texts Written by Junior High School Students\",\\n        \"citationCount\": 10,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://www.atlantis-press.com/article/125956016.pdf\",\\n          \"status\": \"GOLD\",\\n          \"license\": \"CCBYNC\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.2991/ASSEHR.K.210427.030?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.2991/ASSEHR.K.210427.030, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": null,\\n        \"publicationTypes\": null,\\n        \"publicationDate\": \"2021-04-28\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2121899244\",\\n            \"name\": \"Galis Muthia Zahra\"\\n          },\\n          {\\n            \"authorId\": \"2345026648\",\\n            \"name\": \"Emi Emilia\"\\n          },\\n          {\\n            \"authorId\": \"77566729\",\\n            \"name\": \"Iyen Nurlaelawati\"\\n          }\\n        ],\\n        \"abstract\": \"The study aims to investigate the cohesion and coherence of descriptive texts written by seventh grade junior high school students. This study was conducted in the even semester of the 2019/2020 academic year during the Covid-19 pandemic. This study used a qualitative case study design, and the data were obtained from six texts representing high, middle, and low achiever students. To identify the texts\\\\u2019 cohesion and coherence, the grammar of textual metafunction from systemic functional linguistics (SFL), that is the theme system was used. The findings show that all students had the ability to make descriptive texts in terms of cohesion and coherence. All texts successfully used different types of themes, including topical and textual themes, and thematic progression, including the zigzag and reiteration patterns to create coherence especially at the clause level. Various cohesive devices such as reference, conjunction, lexical, and ellipsis were also used to create a cohesive text. It was also found that the texts written by high achiever students were more coherent than the texts written by middle and low achiever students due to several aspects such as the more diverse pattern and the more frequent use of pattern. In addition, the high and middle achiever texts seemed more cohesive than the low achiever texts due to the high number of cohesive devices used in the middle achiever texts, and high number of conjunctions used in the middle and high achiever texts. Based on the findings, more support is needed from teachers when teaching descriptive text to middle and low achiever students, especially in a pandemic era.\"\\n      },\\n      {\\n        \"paperId\": \"46812cd7024c70dadaaecba7d854252f27b582ec\",\\n        \"url\": \"https://www.semanticscholar.org/paper/46812cd7024c70dadaaecba7d854252f27b582ec\",\\n        \"title\": \"Molecular Contrast Optical Coherence Tomography and Its Applications in Medicine\",\\n        \"citationCount\": 13,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://www.mdpi.com/1422-0067/23/6/3038/pdf?version=1647248458\",\\n          \"status\": \"GOLD\",\\n          \"license\": \"CCBY\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC8949853, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"The recent advances of molecular contrast OCT imaging techniques, the corresponding contrast agents, especially the nanoparticle-based ones, and their applications are reviewed and the properties, design criteria, merit, and demerit of those contrast agents are summarized.\"\\n        },\\n        \"publicationTypes\": [\\n          \"Review\",\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2022-03-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"35035162\",\\n            \"name\": \"Ancong Wang\"\\n          },\\n          {\\n            \"authorId\": \"2105875970\",\\n            \"name\": \"Wenliu Qi\"\\n          },\\n          {\\n            \"authorId\": \"4707531\",\\n            \"name\": \"Tianxin Gao\"\\n          },\\n          {\\n            \"authorId\": \"2118488394\",\\n            \"name\": \"Xiaoying Tang\"\\n          }\\n        ],\\n        \"abstract\": \"The growing need to understand the molecular mechanisms of diseases has prompted the revolution in molecular imaging techniques along with nanomedicine development. Conventional optical coherence tomography (OCT) is a low-cost in vivo imaging modality that provides unique high spatial and temporal resolution anatomic images but little molecular information. However, given the widespread adoption of OCT in research and clinical practice, its robust molecular imaging extensions are strongly desired to combine with anatomical images. A range of relevant approaches has been reported already. In this article, we review the recent advances of molecular contrast OCT imaging techniques, the corresponding contrast agents, especially the nanoparticle-based ones, and their applications. We also summarize the properties, design criteria, merit, and demerit of those contrast agents. In the end, the prospects and challenges for further research and development in this field are outlined.\"\\n      },\\n      {\\n        \"paperId\": \"5b906cd1a282df558db06fb94741c66294c77a43\",\\n        \"url\": \"https://www.semanticscholar.org/paper/5b906cd1a282df558db06fb94741c66294c77a43\",\\n        \"title\": \"The Textual-Visual Thematic Analysis: A Framework to Analyze the Conjunction and Interaction of Visual and Textual Data\",\\n        \"citationCount\": 25,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://nsuworks.nova.edu/cgi/viewcontent.cgi?article=5456&context=tqr\",\\n          \"status\": \"GOLD\",\\n          \"license\": \"CCBYNCSA\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.46743/2160-3715/2022.5456?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.46743/2160-3715/2022.5456, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": null,\\n        \"publicationTypes\": null,\\n        \"publicationDate\": null,\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2158241407\",\\n            \"name\": \"Gabriela Trombeta\"\\n          },\\n          {\\n            \"authorId\": \"120232113\",\\n            \"name\": \"S. Cox\"\\n          }\\n        ],\\n        \"abstract\": \"Visual methods offer an innovative approach to qualitative research through their potential to prompt dialogue, enrich verbal and textual data, and enable participants to communicate about difficult topics. However, the use of visual methods requires that researchers rethink methodological aspects of data generation and analysis, especially when working with participant-generated images. Although there are now many analytical frameworks and guidebooks providing instructions on the analysis of textual and visual materials, detailed descriptions of how these elements are brought together are often missing from research reports, precluding novice and other researchers from understanding how findings were attained. Our aim in this article is to describe and illustrate the Textual-Visual Thematic Analysis (TVTA), a framework we developed to collaboratively analyze the conjunction and interaction of textual and visual data in a photo-elicitation study. Given that the ethical and methodological aspects are deeply entwined, we begin the article by contextualizing the data obtained from the photo-elicitation study and then consider confidentiality and approaches to valuing participants\\' voices. Next, we share the TVTA framework, its procedural implementation, and insights derived from evolving our data analysis approach. We conclude by offering reflections on the limitations and possibilities for future research.\"\\n      },\\n      {\\n        \"paperId\": \"b2f8cbaa22e4a8aac716a6f035f324fd667d7e8f\",\\n        \"url\": \"https://www.semanticscholar.org/paper/b2f8cbaa22e4a8aac716a6f035f324fd667d7e8f\",\\n        \"title\": \"Assessment of COVID-19 pandemic responses in African countries: thematic synthesis of WHO intra-action review reports\",\\n        \"citationCount\": 27,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://bmjopen.bmj.com/content/bmjopen/12/5/e056896.full.pdf\",\\n          \"status\": \"GOLD\",\\n          \"license\": \"CCBY\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC9062458, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"It is suggested that African countries\\\\u2019 response to the COVID-19 pandemic was prompt and may have contributed to the lower cases and deaths in the region compared with countries in other regions.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Review\"\\n        ],\\n        \"publicationDate\": \"2022-05-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"6312621\",\\n            \"name\": \"A. Talisuna\"\\n          },\\n          {\\n            \"authorId\": \"8406874\",\\n            \"name\": \"C. Iwu\"\\n          },\\n          {\\n            \"authorId\": \"6612736\",\\n            \"name\": \"J. Okeibunor\"\\n          },\\n          {\\n            \"authorId\": \"31474564\",\\n            \"name\": \"Mary Stephen\"\\n          },\\n          {\\n            \"authorId\": \"144645159\",\\n            \"name\": \"E. Musa\"\\n          },\\n          {\\n            \"authorId\": \"32861033\",\\n            \"name\": \"B. Herring\"\\n          },\\n          {\\n            \"authorId\": \"6327315\",\\n            \"name\": \"Otim Patrick Cossy Ramadan\"\\n          },\\n          {\\n            \"authorId\": \"15071000\",\\n            \"name\": \"Daniel Yota\"\\n          },\\n          {\\n            \"authorId\": \"6230122\",\\n            \"name\": \"M. Nanyunja\"\\n          },\\n          {\\n            \"authorId\": \"6894832\",\\n            \"name\": \"A. Mpairwe\"\\n          },\\n          {\\n            \"authorId\": \"1442059469\",\\n            \"name\": \"F. Banza\"\\n          },\\n          {\\n            \"authorId\": \"153908087\",\\n            \"name\": \"A. Diallo\"\\n          },\\n          {\\n            \"authorId\": \"1442059350\",\\n            \"name\": \"Roland Kimbi Wango\"\\n          },\\n          {\\n            \"authorId\": \"2129120286\",\\n            \"name\": \"Christian Massidi\"\\n          },\\n          {\\n            \"authorId\": \"134071938\",\\n            \"name\": \"Hilary K. Njenge\"\\n          },\\n          {\\n            \"authorId\": \"2128400616\",\\n            \"name\": \"M. Traore\"\\n          },\\n          {\\n            \"authorId\": \"1442061510\",\\n            \"name\": \"Antonio Oke\"\\n          },\\n          {\\n            \"authorId\": \"1483613748\",\\n            \"name\": \"Boukare Bonkoungou\"\\n          },\\n          {\\n            \"authorId\": \"1381443808\",\\n            \"name\": \"Landry Ndriko Mayigane\"\\n          },\\n          {\\n            \"authorId\": \"6166357\",\\n            \"name\": \"I. Conteh\"\\n          },\\n          {\\n            \"authorId\": \"2129119453\",\\n            \"name\": \"Fekadu Senait\"\\n          },\\n          {\\n            \"authorId\": \"6150958\",\\n            \"name\": \"S. Chungong\"\\n          },\\n          {\\n            \"authorId\": \"10314719\",\\n            \"name\": \"B. Impouma\"\\n          },\\n          {\\n            \"authorId\": \"2121935091\",\\n            \"name\": \"Nsenga Ngoy\"\\n          },\\n          {\\n            \"authorId\": \"6979323\",\\n            \"name\": \"C. Wiysonge\"\\n          },\\n          {\\n            \"authorId\": \"4284508\",\\n            \"name\": \"Z. Yoti\"\\n          },\\n          {\\n            \"authorId\": \"2969537\",\\n            \"name\": \"A. Gueye\"\\n          }\\n        ],\\n        \"abstract\": \"Objectives We conducted a review of intra-action review (IAR) reports of the national response to the COVID-19 pandemic in Africa. We highlight best practices and challenges and offer perspectives for the future. Design A thematic analysis across 10 preparedness and response domains, namely, governance, leadership, and coordination; planning and monitoring; risk communication and community engagement; surveillance, rapid response, and case investigation; infection prevention and control; case management; screening and monitoring at points of entry; national laboratory system; logistics and supply chain management; and maintaining essential health services during the COVID-19 pandemic. Setting All countries in the WHO African Region were eligible for inclusion in the study. National IAR reports submitted by March 2021 were analysed. Results We retrieved IAR reports from 18 African countries. The COVID-19 pandemic response in African countries has relied on many existing response systems such as laboratory systems, surveillance systems for previous outbreaks of highly infectious diseases and a logistics management information system. These best practices were backed by strong political will. The key challenges included low public confidence in governments, inadequate adherence to infection prevention and control measures, shortages of personal protective equipment, inadequate laboratory capacity, inadequate contact tracing, poor supply chain and logistics management systems, and lack of training of key personnel at national and subnational levels. Conclusion These findings suggest that African countries\\\\u2019 response to the COVID-19 pandemic was prompt and may have contributed to the lower cases and deaths in the region compared with countries in other regions. The IARs demonstrate that many technical areas still require immediate improvement to guide decisions in subsequent waves or future outbreaks.\"\\n      },\\n      {\\n        \"paperId\": \"7b61104ddb7302f51dc30a77cfc766b483db2fdc\",\\n        \"url\": \"https://www.semanticscholar.org/paper/7b61104ddb7302f51dc30a77cfc766b483db2fdc\",\\n        \"title\": \"Multi-Temporal Sentinel-1 Backscatter and Coherence for Rainforest Mapping\",\\n        \"citationCount\": 33,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://www.mdpi.com/2072-4292/12/5/847/pdf?version=1583481683\",\\n          \"status\": \"GOLD\",\\n          \"license\": \"CCBY\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.3390/rs12050847?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.3390/rs12050847, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper presents how the interferometric capabilities of the Sentinel-1 satellites constellation can be exploited for the monthly mapping of the Amazon rainforest and an analysis on the benefits of the use of textural information, derived from Sentinel-2 backscatter, in order to enhance the classification accuracy.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2020-03-06\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2049260\",\\n            \"name\": \"A. Pulella\"\\n          },\\n          {\\n            \"authorId\": \"1617617187\",\\n            \"name\": \"Rodrigo Arag\\\\u00e3o Santos\"\\n          },\\n          {\\n            \"authorId\": \"51243828\",\\n            \"name\": \"F. Sica\"\\n          },\\n          {\\n            \"authorId\": \"70278054\",\\n            \"name\": \"Philipp Posovszky\"\\n          },\\n          {\\n            \"authorId\": \"2261116\",\\n            \"name\": \"P. Rizzoli\"\\n          }\\n        ],\\n        \"abstract\": \"This paper reports recent advancements in the field of Synthetic Aperture Radar (SAR) for forest mapping by using interferometric short-time-series. In particular, we first present how the interferometric capabilities of the Sentinel-1 satellites constellation can be exploited for the monthly mapping of the Amazon rainforest. Indeed, the evolution in time of the interferometric coherence can be properly modeled as an exponential decay and the retrieved interferometric parameters can be used, together with the backscatter, as input features to the machine learning Random Forests classifier. Furthermore, we present an analysis on the benefits of the use of textural information, derived from Sentinel-1 backscatter, in order to enhance the classification accuracy. These textures are computed through the Sum And Difference Histograms methodology and the final classification accuracy, resulting by adding them to the aforementioned features, is a thematic map that exceeds an overall agreement of 85%, when validated using the optical external reference Finer Resolution Observation and Monitoring of Global Land Cover (FROM-GLC) map. The experiments presented in the final part of the paper are enriched with a further analysis and discussion on the selected scenes using updated multispectral Sentinel-2 acquisitions.\"\\n      },\\n      {\\n        \"paperId\": \"a37bd8ad2c2356a39fa0271479c6b46e8b108d2b\",\\n        \"url\": \"https://www.semanticscholar.org/paper/a37bd8ad2c2356a39fa0271479c6b46e8b108d2b\",\\n        \"title\": \"\\\\u2018This bloody rona!\\\\u2019: using the digital story completion method and thematic analysis to explore the mental health impacts of COVID-19 in Australia\",\\n        \"citationCount\": 15,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://bmjopen.bmj.com/content/bmjopen/12/1/e057393.full.pdf\",\\n          \"status\": \"GOLD\",\\n          \"license\": \"CCBYNC\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC8764712, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"It is suggested that points of convergence across stories indicate a level of shared experience among participants relating to COVID-19 in Australia, due to intensive media coverage of the pandemic, persistent public health messaging, engagement with social media and instant messaging technologies, and extended lockdowns that impacted the mental health of vast numbers of Australians.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2022-01-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"153603815\",\\n            \"name\": \"Priya Vaughan\"\\n          },\\n          {\\n            \"authorId\": \"40481393\",\\n            \"name\": \"Caroline Lenette\"\\n          },\\n          {\\n            \"authorId\": \"5498463\",\\n            \"name\": \"K. Boydell\"\\n          }\\n        ],\\n        \"abstract\": \"Objective To use the digital story completion method to prompt participants to describe thoughts, fears and mental health experiences in response to a story stem about COVID-19, to capture a specific sociohistoric moment. Design We used digital story completion, a qualitative research method, to gather narratives from Australians coping with physical distancing and social restriction measures. Our reflexive thematic analysis of the data was underpinned by a constructionist approach to reflect the importance of social context in understanding health experiences. Setting Australia. Participants 52 people living in Australia (aged 18 years and over). Results Four meta-themes were prevalent across 52 stories submitted: (1) expressions of mental distress linked to COVID-19; (2) various coping strategies offered by characters in stories; (3) narratives outlining social support offered to alleviate distress; and (4) specialised COVID-19 vocabulary. Conclusion We cautiously propose that points of convergence across stories indicate a level of shared experience among participants relating to COVID-19 in Australia. We suggest this is due to intensive media coverage of the pandemic, persistent public health messaging, engagement with social media and instant messaging technologies, and extended lockdowns that impacted the mental health of vast numbers of Australians.\"\\n      },\\n      {\\n        \"paperId\": \"b3535815a1cd7ed648ea64c591802301822e576a\",\\n        \"url\": \"https://www.semanticscholar.org/paper/b3535815a1cd7ed648ea64c591802301822e576a\",\\n        \"title\": \"Achieving healthy ageing through the perspective of sense of coherence among senior-only households: a qualitative study\",\\n        \"citationCount\": 20,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1080/13607863.2020.1725805?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1080/13607863.2020.1725805, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"It is suggested that sense of coherence towards the pursuit of healthy ageing can be addressed by reducing the unpredictability of ageing-related processes and vulnerabilities (comprehensibility), supporting active adoption of actions which promotes physical, mental and social health (manageability) and individual reflection in making sense of old age to seek motivation in living each day purposefully (meaningfulness).\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2020-02-18\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"14137710\",\\n            \"name\": \"Betsy Seah\"\\n          },\\n          {\\n            \"authorId\": \"6102067\",\\n            \"name\": \"G. Espnes\"\\n          },\\n          {\\n            \"authorId\": \"1870959\",\\n            \"name\": \"E. Ang\"\\n          },\\n          {\\n            \"authorId\": \"71772268\",\\n            \"name\": \"Jian Yang Lim\"\\n          },\\n          {\\n            \"authorId\": \"6631742\",\\n            \"name\": \"Y. Kowitlawakul\"\\n          },\\n          {\\n            \"authorId\": \"2395324152\",\\n            \"name\": \"Wenru Wang\"\\n          }\\n        ],\\n        \"abstract\": \"Abstract Objectives: Explore perceptions towards healthy ageing through the perspective of sense of coherence among older adults residing in senior-only households. Methods: A qualitative study using focus group interviews was conducted and appreciative inquiry was adopted as a strengths-based interviewing approach. 27 older adults who either live alone or with their spouses only were involved in six focus group discussions at a community centre in Singapore. Data saturation was achieved and thematic analysis was performed to analyse the data. Results: The four emerging themes were (1) contending evolving vulnerabilities, (2) intrinsic value of health, (3) taking care of oneself is a personal responsibility, and (4) taking one day at a time: outlook towards later part of life. Older adults\\\\u2019 underlying pathogenic orientation towards health contributed to their perceived unpredictable confrontations with vicissitudes including illness and death. This played a part to their short outlook towards old age. Consequently, this could limit their will and abilities to seek meaningful pursuits or valued aspirations and movement towards the salutogenic health pole. Conclusion: By reframing the definition of health to pursuing and fulfilling valued accomplishments, optimal health can be achieved regardless of physical health state. This study suggested that sense of coherence towards the pursuit of healthy ageing can be addressed by reducing the unpredictability of ageing-related processes and vulnerabilities (comprehensibility), supporting active adoption of actions which promotes physical, mental and social health (manageability) and individual reflection in making sense of old age to seek motivation in living each day purposefully (meaningfulness).\"\\n      },\\n      {\\n        \"paperId\": \"dd80b198ab64a4c04cde06ed5cf31662270b86af\",\\n        \"url\": \"https://www.semanticscholar.org/paper/dd80b198ab64a4c04cde06ed5cf31662270b86af\",\\n        \"title\": \"The experience of using prompting technology from the perspective of people with Dementia and their primary carers\",\\n        \"citationCount\": 26,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1080/13607863.2020.1745145?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1080/13607863.2020.1745145, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"Carers and participants were influenced by their approaches to technology and determined the usefulness of the prompter according to whether it worked for them and fitted into their routines, consistent with the Technology Acceptance Model.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2020-03-30\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"145617572\",\\n            \"name\": \"N. Evans\"\\n          },\\n          {\\n            \"authorId\": \"48063215\",\\n            \"name\": \"H. Boyd\"\\n          },\\n          {\\n            \"authorId\": \"48688534\",\\n            \"name\": \"N. Harris\"\\n          },\\n          {\\n            \"authorId\": \"38349961\",\\n            \"name\": \"K. Noonan\"\\n          },\\n          {\\n            \"authorId\": \"40227345\",\\n            \"name\": \"T. Ingram\"\\n          },\\n          {\\n            \"authorId\": \"48756330\",\\n            \"name\": \"A. Jarvis\"\\n          },\\n          {\\n            \"authorId\": \"1613011708\",\\n            \"name\": \"J. Ridgers\"\\n          },\\n          {\\n            \"authorId\": \"144757940\",\\n            \"name\": \"R. Cheston\"\\n          }\\n        ],\\n        \"abstract\": \"Abstract Objectives People who are living with dementia typically experience difficulties in completing multi-step, everyday tasks. However, digital technology such as touchscreen tablets provide a means of delivering concise personalised prompts that combine audio, text and pictures. This study was one component of a broader, mixed methods study that tested how an application (app) \\\\u2013based prompter running on a touchscreen tablet computer could support everyday activities in individuals with mild to moderate dementia. In this study we set out to understand the experiences of people living with dementia and their primary carer in using the prompter over a four-week period. Method We collected qualitative data using semi-structured interviews from 26 dyads, composed of a person living with dementia and their carer. Dyads were interviewed at the start and end of this period. Transcripts were then analysed using thematic analysis. Results The study identified three overarching themes related to: participants\\\\u2019 attitudes towards the technology; their judgements about how useful the prompter would be; and the emotional impact of using it. Conclusion Consistent with the Technology Acceptance Model, carers and participants were influenced by their approaches to technology and determined the usefulness of the prompter according to whether it worked for them and fitted into their routines. In addition, participants\\\\u2019 decisions about using the prompter were also determined by the extent to which doing so would impact on their self-identity.\"\\n      },\\n      {\\n        \"paperId\": \"fb1d85fe28b5e92e22d084eca674d4a2b48cdc5a\",\\n        \"url\": \"https://www.semanticscholar.org/paper/fb1d85fe28b5e92e22d084eca674d4a2b48cdc5a\",\\n        \"title\": \"Prompting to Distill: Boosting Data-Free Knowledge Distillation via Reinforced Prompt\",\\n        \"citationCount\": 12,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://arxiv.org/pdf/2205.07523\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2205.07523, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"PromptDFD leverages a pre-trained generative model to provide language priors and introduces a reinforced topic prompter to control data synthesis, making the generated samples thematically relevant and semantically plausible, and thus friendly to downstream tasks.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2022-05-16\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"15532066\",\\n            \"name\": \"Xinyin Ma\"\\n          },\\n          {\\n            \"authorId\": \"48631088\",\\n            \"name\": \"Xinchao Wang\"\\n          },\\n          {\\n            \"authorId\": \"150110431\",\\n            \"name\": \"Gongfan Fang\"\\n          },\\n          {\\n            \"authorId\": \"1471660296\",\\n            \"name\": \"Yongliang Shen\"\\n          },\\n          {\\n            \"authorId\": \"1776903\",\\n            \"name\": \"Weiming Lu\"\\n          }\\n        ],\\n        \"abstract\": \"Data-free knowledge distillation (DFKD) conducts knowledge distillation via eliminating the dependence of original training data, and has recently achieved impressive results in accelerating pre-trained language models. At the heart of DFKD is to reconstruct a synthetic dataset by inverting the parameters of the uncompressed model. Prior DFKD approaches, however, have largely relied on hand-crafted priors of the target data distribution for the reconstruction, which can be inevitably biased and often incompetent to capture the intrinsic distributions. To address this problem, we propose a prompt-based method, termed as PromptDFD, that allows us to take advantage of learned language priors, which effectively harmonizes the synthetic sentences to be semantically and grammatically correct. Specifically, PromptDFD leverages a pre-trained generative model to provide language priors and introduces a reinforced topic prompter to control data synthesis, making the generated samples thematically relevant and semantically plausible, and thus friendly to downstream tasks. As shown in our experiments, the proposed method substantially improves the synthesis quality and achieves considerable improvements on distillation performance. In some cases, PromptDFD even gives rise to results on par with those from the data-driven knowledge distillation with access to the original training data.\"\\n      },\\n      {\\n        \"paperId\": \"89bbc885e69ff2098b478893cf2e17d8a13464ff\",\\n        \"url\": \"https://www.semanticscholar.org/paper/89bbc885e69ff2098b478893cf2e17d8a13464ff\",\\n        \"title\": \"Rethinking sense of coherence: Perceptions of comprehensibility, manageability, and meaningfulness in a group of Palestinian health care providers operating in the West Bank and Israel\",\\n        \"citationCount\": 12,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1177/1363461520941386?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1177/1363461520941386, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"It is found that context-specific features of SOC can mobilize generalized resistance resources for coping with traumatic and stressful experiences, even in an environment characterized by political instability, military violence, and social trauma.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2020-08-26\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"35149021\",\\n            \"name\": \"G. Veronese\"\\n          },\\n          {\\n            \"authorId\": \"1912267580\",\\n            \"name\": \"Yamina Dhaouadi\"\\n          },\\n          {\\n            \"authorId\": \"51039166\",\\n            \"name\": \"Abdelhamid Afana\"\\n          }\\n        ],\\n        \"abstract\": \"Drawing on a salutogenic perspective, we explored sense of coherence (SOC) in a group of Palestinian mental health care providers living and working in Israel and the occupied Palestinian territories (West Bank). Specifically, we conducted a qualitative exploration of the cultural characteristics of SOC and its components (comprehensibility, manageability, and meaningfulness) in two groups of Palestinian Muslim helpers. We found that context-specific features of SOC can mobilize generalized resistance resources for coping with traumatic and stressful experiences, even in an environment characterized by political instability, military violence, and social trauma. Ten main themes emerged from the thematic content analysis: acceptance, reacting to adversity, acknowledging human insecurity (comprehensibility), self-control, talking to family, education as a resource for survival, connecting to the severity of the event, responsibility as a source of control (manageability), religiosity, and sense of belonging (meaningfulness). The Islamic faith, as expressed through the concepts of Sumud and Taslim, seemed to permeate individuals\\\\u2019 ability to attribute meaning to historical and transgenerational trauma, as well as to their ongoing traumatic conditions, thus acting as their ultimate source of health and wellbeing. A holistic, spiritual, and collectivist outlook helped respondents to approach their lives with optimism. We discuss the implications for mental health care providers and future research directions.\"\\n      },\\n      {\\n        \"paperId\": \"9adfdd1b97b98638b23dc1e08842143136dea497\",\\n        \"url\": \"https://www.semanticscholar.org/paper/9adfdd1b97b98638b23dc1e08842143136dea497\",\\n        \"title\": \"Joint Segmentation and Quantification of Chorioretinal Biomarkers in Optical Coherence Tomography Scans: A Deep Learning Approach\",\\n        \"citationCount\": 44,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {\\'tldr\\'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/TIM.2021.3077988?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/TIM.2021.3077988, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": null,\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": null,\\n        \"authors\": [\\n          {\\n            \"authorId\": \"15356244\",\\n            \"name\": \"Bilal Hassan\"\\n          },\\n          {\\n            \"authorId\": \"121165014\",\\n            \"name\": \"S. Qin\"\\n          },\\n          {\\n            \"authorId\": \"38509414\",\\n            \"name\": \"Taimur Hassan\"\\n          },\\n          {\\n            \"authorId\": \"40988624\",\\n            \"name\": \"Ramsha Ahmed\"\\n          },\\n          {\\n            \"authorId\": \"1802072\",\\n            \"name\": \"N. Werghi\"\\n          }\\n        ],\\n        \"abstract\": \"In ophthalmology, chorioretinal biomarkers (CRBMs) play a significant role in detecting, quantifying, and ameliorating the treatment of chronic eye conditions. Optical coherence tomography (OCT) imaging is primarily used for investigating various CRBMs and prompt intervention of retinal conditions. However, with extensive clinical applications and increasing prevalence of ocular diseases, the number of OCT scans obtained globally exceeds ophthalmologists\\\\u2019 capacity to examine these in a meaningful manner. Instead, the emergence of deep learning provides a cost-effective and reliable alternative for automated analysis of scans, assisting ophthalmologists in clinical routines and research. This article presents a residual learning-based framework (RASP-Net) that integrates atrous spatial pyramid pooling, coherent preprocessing, and postprocessing mechanisms to achieve joint segmentation and quantification of 11 CRBMs. We used a total of 7000 annotated scans for training, validation, and testing purposes of RASP-Net. Moreover, a novel algorithm for 3-D macular profiles reconstruction is presented to give a more intuitive way for characterizing the CRBMs based on coarse contouring and quantification. The proposed framework is evaluated through several experiments using different performance metrics. The results presented in this study validate the optimal performance of RASP-Net in precise detection and segmentation of CRBMs, with mean balanced accuracy, intersection over union, and dice score values of 0.916, 0.634, and 0.776 respectively. The proposed RASP-Net model characterizes a wide range of CRBMs with fine-grained pixelwise segmentation, extraction, and quantification in the context of retinal pathologies. This proposed system can allow retina experts to monitor the improvement and deterioration of the underlying ocular conditions.\"\\n      }\\n    ]\\n  }\\n}', additional_kwargs={}, response_metadata={}, id='72bc78d3-e637-430e-b36d-02b9a73baa04')]}}\n",
      "\u001b[1m[values]\u001b[0m {'messages': [HumanMessage(content=\"Dynamic Prompt Adaptation:\\n                Problem: Large Language Models (LLMs) often struggle with maintaining coherence over extended interactions or creative tasks, leading to thematic inconsistencies and reduced reader engagement.\\n                Existing Methods: Current methods often use fixed prompts or few-shot examples, which may not adapt to the evolving context of a conversation or creative narrative. Techniques such as Chain-of-Thought prompting are utilized, but they do not inherently address continuity and adaptability across interactions.\\n                Motivation: Human creative writing often involves iterative dialogue and adaptation to the flow of discussion. The style and context of interactions can shift dynamically based on prior exchanges. Dynamic adaptation mirrors how authors and conversationalists adjust based on audience feedback and shifting themes.\\n                Proposed Method: We propose Dynamic Prompt Adaptation, involving three phases: (1) Contextual Analysis - Analyze previous outputs and user prompts to extract key themes and tonal shifts, applying a prompting structure like 'Reflect on the previous topic of [theme] and build on it.' (2) Adaptive Prompt Generation - Using insights from the analysis, generate updated prompts that introduce new elements or clarify past responses, e.g., 'Continuing from your last thought on [theme], can you expand on how this might be represented in [new context]?' (3) Iterative Context Update - As the dialogue progresses, generate a synthesis of all prior interactions to maintain thematic coherence, prompting with 'Summarize the key points discussed so far to keep track of our narrative.'\\n                Experiment Plan: Test against static prompting strategies by evaluating engagement scores, coherence assessments, and user satisfaction in storytelling scenarios using standard text generation metrics such as BLEU and ROUGE. Incorporate user feedback on naturalness and adaptability during the interaction. Datasets could include the 'Story Cloze Test' dataset and user-generated dialogue interactions scraped from platforms like Reddit to assess conversational engagement.\", additional_kwargs={}, response_metadata={}, id='63ebf44d-2223-4537-afd7-c4eb3eec973e'), AIMessage(content='{\\n  \"research_question\": \"How can dynamic prompt adaptation maintain thematic coherence and reader engagement across extended LLM conversations and creative narratives?\",\\n  \"problem_domain\": \"natural language processing and dialogue systems\",\\n  \"methodology_keywords\": [\\n    \"Dynamic Prompt Adaptation\",\\n    \"Contextual Analysis\",\\n    \"Adaptive Prompt Generation\",\\n    \"Iterative Context Update\",\\n    \"thematic coherence\",\\n    \"narrative continuity\",\\n    \"long-context dialogue\"\\n  ],\\n  \"key_concepts\": [\\n    \"Dynamic Prompt Adaptation\",\\n    \"Contextual Analysis of previous outputs and prompts\",\\n    \"Adaptive Prompt Generation\",\\n    \"Iterative Context Update\",\\n    \"thematic coherence\",\\n    \"audience feedback and shifting themes\",\\n    \"prompt templating for theme reflection\",\\n    \"synthesis of prior interactions\"\\n  ],\\n  \"existing_methods\": [\\n    \"fixed prompts\",\\n    \"few-shot examples\",\\n    \"Chain-of-Thought prompting\"\\n  ],\\n  \"claimed_novelty\": [\\n    \"Dynamic Prompt Adaptation to maintain coherence across extended interactions and creative tasks\",\\n    \"Three-phase framework: Contextual Analysis; Adaptive Prompt Generation; Iterative Context Update\",\\n    \"Iterative synthesis of all prior interactions to maintain thematic coherence\",\\n    \"Evaluation against static prompting strategies using BLEU and ROUGE on Story Cloze Test and Reddit dialogues\"\\n  ]\\n}', additional_kwargs={}, response_metadata={}, id='35ea71c3-1efb-4474-a8bb-b51930753e82'), AIMessage(content='{\\n  \"queries\": [\\n    {\\n      \"query_string\": \"dynamic prompt adaptation coherence\",\\n      \"rationale\": \"Targets work on dynamic prompt adaptation to sustain thematic coherence across extended LLM conversations and narratives\",\\n      \"priority_concept\": \"Dynamic Prompt Adaptation\"\\n    },\\n    {\\n      \"query_string\": \"long context dialogue\",\\n      \"rationale\": \"Searches for literature on maintaining coherence in long-context dialogues and continuous narratives\",\\n      \"priority_concept\": \"Long-context dialogue\"\\n    },\\n    {\\n      \"query_string\": \"adaptive prompt generation narrative\",\\n      \"rationale\": \"Looks for studies on generating prompts adaptively to guide narrative tasks and creative outputs\",\\n      \"priority_concept\": \"Adaptive Prompt Generation\"\\n    },\\n    {\\n      \"query_string\": \"iterative context update\",\\n      \"rationale\": \"Finds work on iterative updates to context across turns for consistency and coherence\",\\n      \"priority_concept\": \"Iterative Context Update\"\\n    },\\n    {\\n      \"query_string\": \"thematic coherence prompting\",\\n      \"rationale\": \"Identifies prompting strategies that preserve or reflect themes within extended interactions\",\\n      \"priority_concept\": \"Thematic coherence\"\\n    }\\n  ]\\n}', additional_kwargs={}, response_metadata={}, id='44970bb0-ab27-4ee3-8006-a6ceeaa39143'), AIMessage(content='{\\n  \"dynamic prompt adaptation coherence\": {\\n    \"total\": 99,\\n    \"offset\": 0,\\n    \"next\": 50,\\n    \"data\": [\\n      {\\n        \"paperId\": \"246482d9758e93d0b349e2926996d887417174d8\",\\n        \"url\": \"https://www.semanticscholar.org/paper/246482d9758e93d0b349e2926996d887417174d8\",\\n        \"title\": \"DPCore: Dynamic Prompt Coreset for Continual Test-Time Adaptation\",\\n        \"citationCount\": 13,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2406.10737, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"DPCore is proposed, a method designed for robust performance across diverse domain change patterns while ensuring computational efficiency, and it is demonstrated that DPCore consistently outperforms various CTTA methods.\"\\n        },\\n        \"publicationTypes\": null,\\n        \"publicationDate\": \"2024-06-15\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2221081346\",\\n            \"name\": \"Yunbei Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2090707280\",\\n            \"name\": \"Akshay Mehra\"\\n          },\\n          {\\n            \"authorId\": \"2261749247\",\\n            \"name\": \"Shuaicheng Niu\"\\n          },\\n          {\\n            \"authorId\": \"4832622\",\\n            \"name\": \"Jihun Hamm\"\\n          }\\n        ],\\n        \"abstract\": \"Continual Test-Time Adaptation (CTTA) seeks to adapt source pre-trained models to continually changing, unseen target domains. While existing CTTA methods assume structured domain changes with uniform durations, real-world environments often exhibit dynamic patterns where domains recur with varying frequencies and durations. Current approaches, which adapt the same parameters across different domains, struggle in such dynamic conditions-they face convergence issues with brief domain exposures, risk forgetting previously learned knowledge, or misapplying it to irrelevant domains. To remedy this, we propose DPCore, a method designed for robust performance across diverse domain change patterns while ensuring computational efficiency. DPCore integrates three key components: Visual Prompt Adaptation for efficient domain alignment, a Prompt Coreset for knowledge preservation, and a Dynamic Update mechanism that intelligently adjusts existing prompts for similar domains while creating new ones for substantially different domains. Extensive experiments on four benchmarks demonstrate that DPCore consistently outperforms various CTTA methods, achieving state-of-the-art performance in both structured and dynamic settings while reducing trainable parameters by 99% and computation time by 64% compared to previous approaches.\"\\n      },\\n      {\\n        \"paperId\": \"6b533de65b3d2cee190415f5a2a2e6afe2ac7c78\",\\n        \"url\": \"https://www.semanticscholar.org/paper/6b533de65b3d2cee190415f5a2a2e6afe2ac7c78\",\\n        \"title\": \"Dynamic Adapter Meets Prompt Tuning: Parameter-Efficient Transfer Learning for Point Cloud Analysis\",\\n        \"citationCount\": 41,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://arxiv.org/pdf/2403.01439\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2403.01439, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper freezes the parameters of the default pretrained models and proposes the Dynamic Adapter, which generates a dynamic scale for each token, considering the token significance to the downstream task, and seamlessly integrates Dynamic Adapter with Prompt Tuning (DAPT) by constructing Internal Prompts.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-03-03\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2238201928\",\\n            \"name\": \"Xin Zhou\"\\n          },\\n          {\\n            \"authorId\": \"94882716\",\\n            \"name\": \"Dingkang Liang\"\\n          },\\n          {\\n            \"authorId\": \"2284804294\",\\n            \"name\": \"Wei Xu\"\\n          },\\n          {\\n            \"authorId\": \"2284636478\",\\n            \"name\": \"Xingkui Zhu\"\\n          },\\n          {\\n            \"authorId\": \"2290029091\",\\n            \"name\": \"Yihan Xu\"\\n          },\\n          {\\n            \"authorId\": \"2287848763\",\\n            \"name\": \"Zhikang Zou\"\\n          },\\n          {\\n            \"authorId\": \"2238115894\",\\n            \"name\": \"Xiang Bai\"\\n          }\\n        ],\\n        \"abstract\": \"Point cloud analysis has achieved outstanding performance by transferring point cloud pretrained models. However, existing methods for model adaptation usually update all model parameters, i.e., full fine-tuning paradigm, which is inefficient as it relies on high computational costs (e.g., training GPU memory) and massive storage space. In this paper, we aim to study parameter-efficient transfer learning for point cloud analysis with an ideal tradeoff between task performance and parameter efficiency. To achieve this goal, we freeze the parameters of the default pretrained models and then propose the Dynamic Adapter, which generates a dynamic scale for each token, considering the token significance to the downstream task. We further seamlessly integrate Dynamic Adapter with Prompt Tuning (DAPT) by constructing Internal Prompts, capturing the instance-specific features for interaction. Extensive experiments conducted on five challenging datasets demonstrate that the proposed DAPT achieves superior performance compared to the full fine-tuning counterparts while significantly reducing the trainable parameters and training GPU memory by 95% and 35%, respectively. Code is available at https://github.com/LMD0311/DAPT.\"\\n      },\\n      {\\n        \"paperId\": \"eacb61136023a2f30c5a0313f222d50e5f63ac9b\",\\n        \"url\": \"https://www.semanticscholar.org/paper/eacb61136023a2f30c5a0313f222d50e5f63ac9b\",\\n        \"title\": \"Domain Prompt Tuning via Meta Relabeling for Unsupervised Adversarial Adaptation\",\\n        \"citationCount\": 14,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {\\'tldr\\'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/TMM.2023.3272742?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/TMM.2023.3272742, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": null,\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": null,\\n        \"authors\": [\\n          {\\n            \"authorId\": \"1485402830\",\\n            \"name\": \"Xin Jin\"\\n          },\\n          {\\n            \"authorId\": \"40093162\",\\n            \"name\": \"Cuiling Lan\"\\n          },\\n          {\\n            \"authorId\": \"1634494276\",\\n            \"name\": \"Wenjun Zeng\"\\n          },\\n          {\\n            \"authorId\": \"31482866\",\\n            \"name\": \"Zhibo Chen\"\\n          }\\n        ],\\n        \"abstract\": \"Unsupervised adversarial domain adaptation (ADA) aims to learn domain-invariant features by confusing a domain discriminator. As training goes on, the feature distributions of source and target samples are increasingly aligned/indistinguishable. The discrimination capability of the domain discriminator w.r.t. those aligned samples deteriorates due to the domain label of each sample is still fixed all through the learning process, which thus cannot effectively further drive the feature learning. A recently proposed method named Re-enforceable Adversarial Domain Adaptation (RADA) [1] tend to re-energize the domain discriminator during the training by using dynamic domain labels. Specifically, RADA sets up a heuristic criterion and uses it to relabel the well aligned target domain samples as source domain samples on the fly. In our study, we identify a critical problem of RADA: it is a kind of heuristic domain data re-partition solution without explicitly serving the adaptation task itself, suggesting that the criteria of RADA on which sample should be relabeled is hard to decide. To address the problem, we revisit domain relabeling process from a perspective of prompt tuning, and introduce a meta-optimized learnable prompts into RADA to replace some hand-craft designs in dynamic relabeling process, which scheme is named as RADA-prompt. Particularly, we employ a module of meta-prompter, which learns to adaptively relabel the samples based on the objective of serving UDA task. To train the meta-prompter, we leverage a domain alignment measurement and a classification measurement as the meta optimization objective. Extensive experiments on multiple unsupervised domain adaptation benchmarks demonstrate the effectiveness and superiority of RADA-prompt, this scheme also achieves state-of-the-art performance.\"\\n      },\\n      {\\n        \"paperId\": \"165503c48e553a5559190ce74cda823f4e166b54\",\\n        \"url\": \"https://www.semanticscholar.org/paper/165503c48e553a5559190ce74cda823f4e166b54\",\\n        \"title\": \"Decorate the Newcomers: Visual Domain Prompt for Continual Test Time Adaptation\",\\n        \"citationCount\": 128,\\n        \"openAccessPdf\": {\\n          \"url\": \"http://arxiv.org/pdf/2212.04145\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2212.04145, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper proposes to learn an image-layer visual domain prompt for target domains while having the source model parameters frozen, and designs a homeostasis-based adaptation strategy to suppress domain-sensitive parameters in domain-invariant prompts to learn domain-shared knowledge more effectively.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2022-12-08\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2142286278\",\\n            \"name\": \"Yulu Gan\"\\n          },\\n          {\\n            \"authorId\": \"1387903470\",\\n            \"name\": \"Xianzheng Ma\"\\n          },\\n          {\\n            \"authorId\": \"9920529\",\\n            \"name\": \"Yihang Lou\"\\n          },\\n          {\\n            \"authorId\": \"145079192\",\\n            \"name\": \"Yan Bai\"\\n          },\\n          {\\n            \"authorId\": \"2115713503\",\\n            \"name\": \"Renrui Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2194622137\",\\n            \"name\": \"Nian Shi\"\\n          },\\n          {\\n            \"authorId\": \"2194260623\",\\n            \"name\": \"Lin Luo\"\\n          }\\n        ],\\n        \"abstract\": \"Continual Test-Time Adaptation (CTTA) aims to adapt the source model to continually changing unlabeled target domains without access to the source data. Existing methods mainly focus on model-based adaptation in a self-training manner, such as predicting pseudo labels for new domain datasets. Since pseudo labels are noisy and unreliable, these methods suffer from catastrophic forgetting and error accumulation when dealing with dynamic data distributions. Motivated by the prompt learning in NLP, in this paper, we propose to learn an image-layer visual domain prompt for target domains while having the source model parameters frozen. During testing, the changing target datasets can be adapted to the source model by reformulating the input data with the learned visual prompts. Specifically, we devise two types of prompts, i.e., domains-specific prompts and domains-agnostic prompts, to extract current domain knowledge and maintain the domain-shared knowledge in the continual adaptation. Furthermore, we design a homeostasis-based adaptation strategy to suppress domain-sensitive parameters in domain-invariant prompts to learn domain-shared knowledge more effectively. This transition from the model-dependent paradigm to the model-free one enables us to bypass the catastrophic forgetting and error accumulation problems. Experiments show that our proposed method achieves significant performance gains over state-of-the-art methods on four widely-used benchmarks, including CIFAR-10C, CIFAR-100C, ImageNet-C, and VLCS datasets.\"\\n      },\\n      {\\n        \"paperId\": \"5db3cfc974c42bfa2d9518a8910762790b516037\",\\n        \"url\": \"https://www.semanticscholar.org/paper/5db3cfc974c42bfa2d9518a8910762790b516037\",\\n        \"title\": \"Robust Test-Time Adaptation for Zero-Shot Prompt Tuning\",\\n        \"citationCount\": 12,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://ojs.aaai.org/index.php/AAAI/article/download/29611/31034\",\\n          \"status\": \"GOLD\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1609/aaai.v38i15.29611?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1609/aaai.v38i15.29611, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This study proposes robust test-time Adaptation for zeroshot Prompt tuning (ADAPROMPT), which alleviates model bias, adapts to data bias and mostly outperforms the state-of-the-art methods at a small time cost and demonstrates that ADAPROMPT hardly encounters any performance degradation on these datasets.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-03-24\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2229108213\",\\n            \"name\": \"Ding-Chu Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2149135777\",\\n            \"name\": \"Zhi Zhou\"\\n          },\\n          {\\n            \"authorId\": \"2276808245\",\\n            \"name\": \"Yufeng Li\"\\n          }\\n        ],\\n        \"abstract\": \"CLIP has demonstrated remarkable generalization across diverse downstream tasks. By aligning images and texts in a shared feature space, they enable zero-shot classification via hand-crafted prompts. However, recent studies have shown that hand-crafted prompts may be unsuitable in practical applications. Specifically, choosing an appropriate prompt for a given task requires accurate data and knowledge, which may not be obtainable in practical situations. An inappropriate prompt can result in poor performance. Moreover, if there is no training data, tuning prompts arbitrarily through unlabeled test data may lead to serious performance degradation when giving hand-crafted prompts. Our study reveals that the aforementioned problems are mainly due to the biases in testing data (Data Bias) and pre-trained CLIP model (Model Bias). The Data Bias makes it challenging to choose an appropriate prompt, while Model Bias renders some predictions inaccurate and biased, which leads to error accumulation. To address these biases, we propose robust test-time Adaptation for zeroshot Prompt tuning (ADAPROMPT). Specifically, we ensemble multiple prompts to avoid the worst-case results and dynamically tune prompts to adapt to Data Bias during testing. Furthermore, we adopt a confidence-aware buffer to store balanced and confident unlabeled test data to tune prompts in order to overcome Model Bias. Our extensive experiments on several benchmarks demonstrate that ADAPROMPT alleviates model bias, adapts to data bias and mostly outperforms the state-of-the-art methods at a small time cost. Moreover, our experimental results reveal that ADAPROMPT hardly encounters any performance degradation on these datasets.\"\\n      },\\n      {\\n        \"paperId\": \"4362edfd3907204cf1b7ec8e3c16c56db5cd14cf\",\\n        \"url\": \"https://www.semanticscholar.org/paper/4362edfd3907204cf1b7ec8e3c16c56db5cd14cf\",\\n        \"title\": \"Large Language Models-guided Dynamic Adaptation for Temporal Knowledge Graph Reasoning\",\\n        \"citationCount\": 30,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2405.14170, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A Large Language Models-guided Dynamic Adaptation (LLM-DA) method for reasoning on TKGs that harnesses the capabilities of LLMs to analyze historical data and extract temporal logical rules that unveil temporal patterns and facilitate interpretable reasoning.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-05-23\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2163833353\",\\n            \"name\": \"Jiapu Wang\"\\n          },\\n          {\\n            \"authorId\": \"2220259324\",\\n            \"name\": \"Kai Sun\"\\n          },\\n          {\\n            \"authorId\": \"2238130759\",\\n            \"name\": \"Linhao Luo\"\\n          },\\n          {\\n            \"authorId\": \"2302892265\",\\n            \"name\": \"Wei Wei\"\\n          },\\n          {\\n            \"authorId\": \"2140879677\",\\n            \"name\": \"Yongli Hu\"\\n          },\\n          {\\n            \"authorId\": \"2243282363\",\\n            \"name\": \"Alan Wee-Chung Liew\"\\n          },\\n          {\\n            \"authorId\": \"2294378361\",\\n            \"name\": \"Shirui Pan\"\\n          },\\n          {\\n            \"authorId\": \"2239088112\",\\n            \"name\": \"Baocai Yin\"\\n          }\\n        ],\\n        \"abstract\": \"Temporal Knowledge Graph Reasoning (TKGR) is the process of utilizing temporal information to capture complex relations within a Temporal Knowledge Graph (TKG) to infer new knowledge. Conventional methods in TKGR typically depend on deep learning algorithms or temporal logical rules. However, deep learning-based TKGRs often lack interpretability, whereas rule-based TKGRs struggle to effectively learn temporal rules that capture temporal patterns. Recently, Large Language Models (LLMs) have demonstrated extensive knowledge and remarkable proficiency in temporal reasoning. Consequently, the employment of LLMs for Temporal Knowledge Graph Reasoning (TKGR) has sparked increasing interest among researchers. Nonetheless, LLMs are known to function as black boxes, making it challenging to comprehend their reasoning process. Additionally, due to the resource-intensive nature of fine-tuning, promptly updating LLMs to integrate evolving knowledge within TKGs for reasoning is impractical. To address these challenges, in this paper, we propose a Large Language Models-guided Dynamic Adaptation (LLM-DA) method for reasoning on TKGs. Specifically, LLM-DA harnesses the capabilities of LLMs to analyze historical data and extract temporal logical rules. These rules unveil temporal patterns and facilitate interpretable reasoning. To account for the evolving nature of TKGs, a dynamic adaptation strategy is proposed to update the LLM-generated rules with the latest events. This ensures that the extracted rules always incorporate the most recent knowledge and better generalize to the predictions on future events. Experimental results show that without the need of fine-tuning, LLM-DA significantly improves the accuracy of reasoning over several common datasets, providing a robust framework for TKGR tasks.\"\\n      },\\n      {\\n        \"paperId\": \"759b5f58e58a76f79a7d845acd3169dc899d0ac2\",\\n        \"url\": \"https://www.semanticscholar.org/paper/759b5f58e58a76f79a7d845acd3169dc899d0ac2\",\\n        \"title\": \"Domain Adaptation via Prompt Learning\",\\n        \"citationCount\": 210,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://arxiv.org/pdf/2202.06687\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2202.06687, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This article introduces a novel prompt learning paradigm for UDA, named domain adaptation via prompt learning (DAPrompt), which outperforms previous methods on several cross-domain benchmarks but also is very efficient to train and easy to implement.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2022-02-14\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2130368185\",\\n            \"name\": \"Chunjiang Ge\"\\n          },\\n          {\\n            \"authorId\": null,\\n            \"name\": \"Rui Huang\"\\n          },\\n          {\\n            \"authorId\": \"2112817811\",\\n            \"name\": \"Mixue Xie\"\\n          },\\n          {\\n            \"authorId\": \"51451501\",\\n            \"name\": \"Zihang Lai\"\\n          },\\n          {\\n            \"authorId\": \"1760750\",\\n            \"name\": \"Shiji Song\"\\n          },\\n          {\\n            \"authorId\": null,\\n            \"name\": \"Shuang Li\"\\n          },\\n          {\\n            \"authorId\": \"2115218570\",\\n            \"name\": \"Gao Huang\"\\n          }\\n        ],\\n        \"abstract\": \"Unsupervised domain adaptation (UDA) aims to adapt models learned from a well-annotated source domain to a target domain, where only unlabeled samples are given. Current UDA approaches learn domain-invariant features by aligning source and target feature spaces through statistical discrepancy minimization or adversarial training. However, these constraints could lead to the distortion of semantic feature structures and loss of class discriminability. In this article, we introduce a novel prompt learning paradigm for UDA, named domain adaptation via prompt learning (DAPrompt). In contrast to prior works, our approach learns the underlying label distribution for target domain rather than aligning domains. The main idea is to embed domain information into prompts, a form of representation generated from natural language, which is then used to perform classification. This domain information is shared only by images from the same domain, thereby dynamically adapting the classifier according to each domain. By adopting this paradigm, we show that our model not only outperforms previous methods on several cross-domain benchmarks but also is very efficient to train and easy to implement.\"\\n      },\\n      {\\n        \"paperId\": \"b32861f83f5426ddb0b797cfe9a5ea80b0adc3cf\",\\n        \"url\": \"https://www.semanticscholar.org/paper/b32861f83f5426ddb0b797cfe9a5ea80b0adc3cf\",\\n        \"title\": \"Dynamic 3D imaging of cerebral blood flow in awake mice using self-supervised-learning-enhanced optical coherence Doppler tomography\",\\n        \"citationCount\": 13,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://www.nature.com/articles/s42003-023-04656-x.pdf\",\\n          \"status\": \"GOLD\",\\n          \"license\": \"CCBY\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC10030663, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"The 3D imaging platform presented provides a powerful tool to study dynamic changes in vessel diameters and morphology alongside CBFv networks in the brain of awake animals that can advance the understanding of the effects of drugs and disease conditions.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2023-03-21\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"1826992\",\\n            \"name\": \"Yingtian Pan\"\\n          },\\n          {\\n            \"authorId\": \"2449537\",\\n            \"name\": \"Kicheon Park\"\\n          },\\n          {\\n            \"authorId\": \"48115953\",\\n            \"name\": \"Jiaxiang Ren\"\\n          },\\n          {\\n            \"authorId\": \"2184066\",\\n            \"name\": \"N. Volkow\"\\n          },\\n          {\\n            \"authorId\": \"81281627\",\\n            \"name\": \"H. Ling\"\\n          },\\n          {\\n            \"authorId\": \"1898658\",\\n            \"name\": \"A. Koretsky\"\\n          },\\n          {\\n            \"authorId\": \"2024857\",\\n            \"name\": \"C. Du\"\\n          }\\n        ],\\n        \"abstract\": \"Cerebral blood flow (CBF) is widely used to assess brain function. However, most preclinical CBF studies have been performed under anesthesia, which confounds findings. High spatiotemporal-resolution CBF imaging of awake animals is challenging due to motion artifacts and background noise, particularly for Doppler-based flow imaging. Here, we report ultrahigh-resolution optical coherence Doppler tomography (\\\\u00b5ODT) for 3D imaging of CBF velocity (CBFv) dynamics in awake mice by developing self-supervised deep-learning for effective image denoising and motion-artifact removal. We compare cortical CBFv in awake vs. anesthetized mice and their dynamic responses in arteriolar, venular and capillary networks to acute cocaine (1\\\\u2009mg/kg, i.v .), a highly addictive drug associated with neurovascular toxicity. Compared with awake, isoflurane (2-2.5%) induces vasodilation and increases CBFv within 2-4\\\\u2009min, whereas dexmedetomidine (0.025\\\\u2009mg/kg, i.p .) does not change vessel diameters nor flow. Acute cocaine decreases CBFv to the same extent in dexmedetomidine and awake states, whereas decreases are larger under isoflurane, suggesting that isoflurane-induced vasodilation might have facilitated detection of cocaine-induced vasoconstriction. Awake mice after chronic cocaine show severe vasoconstriction, CBFv decreases and vascular adaptations with extended diving arteriolar/venular vessels that prioritize blood supply to deeper cortical capillaries. The 3D imaging platform we present provides a powerful tool to study dynamic changes in vessel diameters and morphology alongside CBFv networks in the brain of awake animals that can advance our understanding of the effects of drugs and disease conditions (ischemia, tumors, wound healing). An imaging platform with self-supervised deep learning allows for the imaging of cerebral blood flows under the effect of cocaine in awake mice using 3D ultrahigh-resolution optical coherence Doppler tomography.\"\\n      },\\n      {\\n        \"paperId\": \"83ac79bb8e8695fb3c3c024be74790d862adea74\",\\n        \"url\": \"https://www.semanticscholar.org/paper/83ac79bb8e8695fb3c3c024be74790d862adea74\",\\n        \"title\": \"TEMPO: Prompt-based Generative Pre-trained Transformer for Time Series Forecasting\",\\n        \"citationCount\": 199,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://arxiv.org/pdf/2310.04948\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2310.04948, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"TEMPO expands the capability for dynamically modeling real-world temporal phenomena from data within diverse domains by utilizing two essential inductive biases of the time series task for pre-trained models, and introducing the design of prompts to facilitate distribution adaptation in different types of time series.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2023-10-08\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"120783624\",\\n            \"name\": \"Defu Cao\"\\n          },\\n          {\\n            \"authorId\": \"2256985863\",\\n            \"name\": \"Furong Jia\"\\n          },\\n          {\\n            \"authorId\": \"2676352\",\\n            \"name\": \"Sercan \\\\u00d6. Arik\"\\n          },\\n          {\\n            \"authorId\": \"1945962\",\\n            \"name\": \"Tomas Pfister\"\\n          },\\n          {\\n            \"authorId\": \"2257061490\",\\n            \"name\": \"Yixiang Zheng\"\\n          },\\n          {\\n            \"authorId\": \"2256992266\",\\n            \"name\": \"Wen Ye\"\\n          },\\n          {\\n            \"authorId\": \"2257088730\",\\n            \"name\": \"Yan Liu\"\\n          }\\n        ],\\n        \"abstract\": \"The past decade has witnessed significant advances in time series modeling with deep learning. While achieving state-of-the-art results, the best-performing architectures vary highly across applications and domains. Meanwhile, for natural language processing, the Generative Pre-trained Transformer (GPT) has demonstrated impressive performance via training one general-purpose model across various textual datasets. It is intriguing to explore whether GPT-type architectures can be effective for time series, capturing the intrinsic dynamic attributes and leading to significant accuracy improvements. In this paper, we propose a novel framework, TEMPO, that can effectively learn time series representations. We focus on utilizing two essential inductive biases of the time series task for pre-trained models: (i) decomposition of the complex interaction between trend, seasonal and residual components; and (ii) introducing the design of prompts to facilitate distribution adaptation in different types of time series. TEMPO expands the capability for dynamically modeling real-world temporal phenomena from data within diverse domains. Our experiments demonstrate the superior performance of TEMPO over state-of-the-art methods on zero shot setting for a number of time series benchmark datasets. This performance gain is observed not only in scenarios involving previously unseen datasets but also in scenarios with multi-modal inputs. This compelling finding highlights TEMPO\\'s potential to constitute a foundational model-building framework.\"\\n      },\\n      {\\n        \"paperId\": \"e3bb7012edfaac357311eca07515d193b4cf26bb\",\\n        \"url\": \"https://www.semanticscholar.org/paper/e3bb7012edfaac357311eca07515d193b4cf26bb\",\\n        \"title\": \"Towards Dynamic Spatial-Temporal Graph Learning: A Decoupled Perspective\",\\n        \"citationCount\": 29,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://ojs.aaai.org/index.php/AAAI/article/download/28759/29459\",\\n          \"status\": \"GOLD\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1609/aaai.v38i8.28759?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1609/aaai.v38i8.28759, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A decoupled learning framework (DLF) is proposed in this paper, which consists of a spatial-temporal graph learning network (DSTG) with a specialized decoupling training strategy to enable prompt adaptation to the evolving distribution of the dynamic graph.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-03-24\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"9071547\",\\n            \"name\": \"Binwu Wang\"\\n          },\\n          {\\n            \"authorId\": \"2108814780\",\\n            \"name\": \"Pengkun Wang\"\\n          },\\n          {\\n            \"authorId\": \"2292591250\",\\n            \"name\": \"Yudong Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2108599981\",\\n            \"name\": \"Xu Wang\"\\n          },\\n          {\\n            \"authorId\": \"6231985\",\\n            \"name\": \"Zhengyang Zhou\"\\n          },\\n          {\\n            \"authorId\": \"2187108259\",\\n            \"name\": \"Lei Bai\"\\n          },\\n          {\\n            \"authorId\": \"46396284\",\\n            \"name\": \"Yang Wang\"\\n          }\\n        ],\\n        \"abstract\": \"With the progress of urban transportation systems, a significant amount of high-quality traffic data is continuously collected through streaming manners, which has propelled the prosperity of the field of spatial-temporal graph prediction. In this paper, rather than solely focusing on designing powerful models for static graphs, we shift our focus to spatial-temporal graph prediction in the dynamic scenario, which involves a continuously expanding and evolving underlying graph. To address inherent challenges, a decoupled learning framework (DLF) is proposed in this paper, which consists of a spatial-temporal graph learning network (DSTG) with a specialized decoupling training strategy. Incorporating inductive biases of time-series structures, DSTG can interpret time dependencies into latent trend and seasonal terms. To enable prompt adaptation to the evolving distribution of the dynamic graph, our decoupling training strategy is devised to iteratively update these two types of patterns. Specifically, for learning seasonal patterns, we conduct thorough training for the model using a long time series (e.g., three months of data). To enhance the learning ability of the model, we also introduce the masked auto-encoding mechanism. During this period, we frequently update trend patterns to expand new information from dynamic graphs. Considering both effectiveness and efficiency, we develop a subnet sampling strategy to select a few representative nodes for fine-tuning the weights of the model. These sampled nodes cover unseen patterns and previously learned patterns. Experiments on dynamic spatial-temporal graph datasets further demonstrate the competitive performance, superior efficiency, and strong scalability of the proposed framework.\"\\n      },\\n      {\\n        \"paperId\": \"650a24da1702beca7eb70011a26f1f3238efad4b\",\\n        \"url\": \"https://www.semanticscholar.org/paper/650a24da1702beca7eb70011a26f1f3238efad4b\",\\n        \"title\": \"Time-LlaMA: Adapting Large Language Models for Time Series Modeling via Dynamic Low-rank Adaptation\",\\n        \"citationCount\": 12,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.18653/v1/2025.acl-srw.90?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.18653/v1/2025.acl-srw.90, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"The proposed Time-LlaMA framework achieves the state-of-the-art (SOTA) performance and have potentials for wide industrial usages.\"\\n        },\\n        \"publicationTypes\": null,\\n        \"publicationDate\": null,\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2346152667\",\\n            \"name\": \"Juyuan Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2334602368\",\\n            \"name\": \"Jiechao Gao\"\\n          },\\n          {\\n            \"authorId\": \"2374971695\",\\n            \"name\": \"Wenwen Ouyang\"\\n          },\\n          {\\n            \"authorId\": \"2334718086\",\\n            \"name\": \"Wei Zhu\"\\n          },\\n          {\\n            \"authorId\": \"2321408906\",\\n            \"name\": \"Hui Yi Leong\"\\n          }\\n        ],\\n        \"abstract\": \"Time series modeling holds significant importance in many industrial applications and has been extensively studied. A series of recent studies have demonstrated that large language models (LLMs) possess robust pattern recognition and semantic understanding capabilities over time series data. However, the current literature have yet striked a high-quality balance between (a) effectively aligning the time series and natural language modalities and (b) keeping the inference efficiency for industrial deployment. To address the above issues, we now propose the Time-LlaMA framework. Time-LlaMA first converts the time series input into token embeddings through a linear tokenization mechanism. Second, the time series token embeddings are aligned with the text prompts. Third, to further adapt the large languag model (LLM) backbone for time series modeling, we have developed a dynamic low-rank adaptation technique (DynaLoRA). DynaLoRA dynamically chooses the most suitable LoRA modules at each layer of the Transformer backbone for each time series input, enhancing the model\\\\u2019s predictive capabilities. Our experimental re-sults on an extensive collection of challenging open and proprietary time series tasks confirm that our proposed method achieves the state-of-the-art (SOTA) performance and have potentials for wide industrial usages. 1\"\\n      },\\n      {\\n        \"paperId\": \"0609513205d94646c740fc219e79b01043e80ba6\",\\n        \"url\": \"https://www.semanticscholar.org/paper/0609513205d94646c740fc219e79b01043e80ba6\",\\n        \"title\": \"Domain-Agnostic Mutual Prompting for Unsupervised Domain Adaptation\",\\n        \"citationCount\": 42,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://arxiv.org/pdf/2403.02899\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2403.02899, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"Domain-Agnostic Mutual Prompting (DAMP) is proposed to exploit domain-invariant semantics by mutually aligning visual and textual embeddings to exploit domain-invariant semantics.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-03-05\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"82980465\",\\n            \"name\": \"Zhekai Du\"\\n          },\\n          {\\n            \"authorId\": \"2108482672\",\\n            \"name\": \"Xinyao Li\"\\n          },\\n          {\\n            \"authorId\": \"2211944242\",\\n            \"name\": \"Fengling Li\"\\n          },\\n          {\\n            \"authorId\": \"1655484744\",\\n            \"name\": \"Ke Lu\"\\n          },\\n          {\\n            \"authorId\": \"2268796475\",\\n            \"name\": \"Lei Zhu\"\\n          },\\n          {\\n            \"authorId\": \"2109058078\",\\n            \"name\": \"Jingjing Li\"\\n          }\\n        ],\\n        \"abstract\": \"Conventional Unsupervised Domain Adaptation (UDA) strives to minimize distribution discrepancy between do-mains, which neglects to harness rich semantics from data and struggles to handle complex domain shifts. A promising technique is to leverage the knowledge of large-scale pretrained vision-language models for more guided adaptation. Despite some endeavors, current methods often learn textual prompts to embed domain semantics for source and target domains separately and perform classification within each domain, limiting cross-domain knowledge transfer. Moreover, prompting only the language branch lacks flex-ibility to adapt both modalities dynamically. To bridge this gap, we propose Domain-Agnostic Mutual Prompting (DAMP) to exploit domain-invariant semantics by mutually aligning visual and textual embeddings. Specifically, the image contextual information is utilized to prompt the language branch in a domain-agnostic and instance-conditioned way. Meanwhile, visual prompts are im-posed based on the domain-agnostic textual prompt to elicit domain-invariant visual embeddings. These two branches of prompts are learned mutually with a cross-attention module and regularized with a semantic-consistency loss and an instance-discrimination contrastive loss. Experiments on three UDA benchmarks demonstrate the superiority of DAMP over state-of-the-art approaches 1.\"\\n      },\\n      {\\n        \"paperId\": \"3f46e1b49c7289eb944365e1bef7bab5b2c891e4\",\\n        \"url\": \"https://www.semanticscholar.org/paper/3f46e1b49c7289eb944365e1bef7bab5b2c891e4\",\\n        \"title\": \"DLTTA: Dynamic Learning Rate for Test-Time Adaptation on Cross-Domain Medical Images\",\\n        \"citationCount\": 35,\\n        \"openAccessPdf\": {\\n          \"url\": \"http://arxiv.org/pdf/2205.13723\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2205.13723, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A novel dynamic learning rate adjustment method for test-time adaptation, called DLTTA, which dynamically modulates the amount of weights update for each test image to account for the differences in their distribution shift.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2022-05-27\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2162926363\",\\n            \"name\": \"Hongzheng Yang\"\\n          },\\n          {\\n            \"authorId\": \"1390805683\",\\n            \"name\": \"Cheng Chen\"\\n          },\\n          {\\n            \"authorId\": \"2050138741\",\\n            \"name\": \"Meirui Jiang\"\\n          },\\n          {\\n            \"authorId\": \"51306676\",\\n            \"name\": \"Quande Liu\"\\n          },\\n          {\\n            \"authorId\": \"48981374\",\\n            \"name\": \"Jianfeng Cao\"\\n          },\\n          {\\n            \"authorId\": \"1714602\",\\n            \"name\": \"P. Heng\"\\n          },\\n          {\\n            \"authorId\": \"35647880\",\\n            \"name\": \"Q. Dou\"\\n          }\\n        ],\\n        \"abstract\": \"Test-time adaptation (TTA) has increasingly been an important topic to efficiently tackle the cross-domain distribution shift at test time for medical images from different institutions. Previous TTA methods have a common limitation of using a fixed learning rate for all the test samples. Such a practice would be sub-optimal for TTA, because test data may arrive sequentially therefore the scale of distribution shift would change frequently. To address this problem, we propose a novel dynamic learning rate adjustment method for test-time adaptation, called DLTTA, which dynamically modulates the amount of weights update for each test image to account for the differences in their distribution shift. Specifically, our DLTTA is equipped with a memory bank based estimation scheme to effectively measure the discrepancy of a given test sample. Based on this estimated discrepancy, a dynamic learning rate adjustment strategy is then developed to achieve a suitable degree of adaptation for each test sample. The effectiveness and general applicability of our DLTTA is extensively demonstrated on three tasks including retinal optical coherence tomography (OCT) segmentation, histopathological image classification, and prostate 3D MRI segmentation. Our method achieves effective and fast test-time adaptation with consistent performance improvement over current state-of-the-art test-time adaptation methods. Code is available at https://github.com/med-air/DLTTA.\"\\n      },\\n      {\\n        \"paperId\": \"54150c69b5e273cdec053c6c95f5ccc707d0c9af\",\\n        \"url\": \"https://www.semanticscholar.org/paper/54150c69b5e273cdec053c6c95f5ccc707d0c9af\",\\n        \"title\": \"Prompt Learning on Temporal Interaction Graphs\",\\n        \"citationCount\": 13,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2402.06326, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work introduces Temporal Interaction Graph Prompting (TIGPrompt), a versatile framework that seamlessly integrates with TIG models, bridging both the temporal and semantic gaps, and proposes a temporal prompt generator to offer temporally-aware prompts for different tasks.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-02-09\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2283939419\",\\n            \"name\": \"Xi Chen\"\\n          },\\n          {\\n            \"authorId\": \"2237941783\",\\n            \"name\": \"Siwei Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2212411539\",\\n            \"name\": \"Yun Xiong\"\\n          },\\n          {\\n            \"authorId\": \"2181344415\",\\n            \"name\": \"Xixi Wu\"\\n          },\\n          {\\n            \"authorId\": \"2283820229\",\\n            \"name\": \"Jiawei Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2265792034\",\\n            \"name\": \"Xiangguo Sun\"\\n          },\\n          {\\n            \"authorId\": \"2283767676\",\\n            \"name\": \"Yao Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2334050630\",\\n            \"name\": \"Feng Zhao\"\\n          },\\n          {\\n            \"authorId\": \"2283820192\",\\n            \"name\": \"Yulin Kang\"\\n          }\\n        ],\\n        \"abstract\": \"Temporal Interaction Graphs (TIGs) are widely utilized to represent real-world systems. To facilitate representation learning on TIGs, researchers have proposed a series of TIG models. However, these models are still facing two tough gaps between the pre-training and downstream predictions in their ``pre-train, predict\\'\\' training paradigm. First, the temporal discrepancy between the pre-training and inference data severely undermines the models\\' applicability in distant future predictions on the dynamically evolving data. Second, the semantic divergence between pretext and downstream tasks hinders their practical applications, as they struggle to align with their learning and prediction capabilities across application scenarios. Recently, the ``pre-train, prompt\\'\\' paradigm has emerged as a lightweight mechanism for model generalization. Applying this paradigm is a potential solution to solve the aforementioned challenges. However, the adaptation of this paradigm to TIGs is not straightforward. The application of prompting in static graph contexts falls short in temporal settings due to a lack of consideration for time-sensitive dynamics and a deficiency in expressive power. To address this issue, we introduce Temporal Interaction Graph Prompting (TIGPrompt), a versatile framework that seamlessly integrates with TIG models, bridging both the temporal and semantic gaps. In detail, we propose a temporal prompt generator to offer temporally-aware prompts for different tasks. These prompts stand out for their minimalistic design, relying solely on the tuning of the prompt generator with very little supervision data. To cater to varying computational resource demands, we propose an extended ``pre-train, prompt-based fine-tune\\'\\' paradigm, offering greater flexibility. Through extensive experiments, the TIGPrompt demonstrates the SOTA performance and remarkable efficiency advantages.\"\\n      },\\n      {\\n        \"paperId\": \"a042571caf46d9053d81441da72ede243f8b421f\",\\n        \"url\": \"https://www.semanticscholar.org/paper/a042571caf46d9053d81441da72ede243f8b421f\",\\n        \"title\": \"PURE: Prompt Evolution with Graph ODE for Out-of-distribution Fluid Dynamics Modeling\",\\n        \"citationCount\": 17,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.52202/079017-3333?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.52202/079017-3333, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"The core of the PURE is to learn time-evolving prompts using a graph ODE to adapt spatio-temporal forecasting models to different scenarios and minimize the mutual information between prompt embeddings and observation embeddings to enhance the robustness of the model to different distributions.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": null,\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2288389213\",\\n            \"name\": \"Hao Wu\"\\n          },\\n          {\\n            \"authorId\": \"2146410855\",\\n            \"name\": \"C. Wang\"\\n          },\\n          {\\n            \"authorId\": \"2217950059\",\\n            \"name\": \"Fan Xu\"\\n          },\\n          {\\n            \"authorId\": \"2345442589\",\\n            \"name\": \"Jinbao Xue\"\\n          },\\n          {\\n            \"authorId\": \"2257378991\",\\n            \"name\": \"Chong Chen\"\\n          },\\n          {\\n            \"authorId\": \"2238119871\",\\n            \"name\": \"Xian-Sheng Hua\"\\n          },\\n          {\\n            \"authorId\": \"2345411659\",\\n            \"name\": \"Xiao Luo\"\\n          }\\n        ],\\n        \"abstract\": \"This work studies the problem of out-of-distribution fluid dynamics modeling. Previous works usually design effective neural operators to learn from mesh-based data structures. However, in real-world applications, they would suffer from distribution shifts from the variance of system parameters and temporal evolution of the dynamical system. In this paper, we propose a novel approach named Prompt Evolution with Graph ODE (PURE) for out-of-distribution fluid dynamics modeling. The core of our PURE is to learn time-evolving prompts using a graph ODE to adapt spatio-temporal forecasting models to different scenarios. In particular, our PURE first learns from historical observations and system parameters in the frequency domain to explore multi-view context information, which could effectively initialize prompt embeddings. More importantly, we incorporate the interpolation of observation sequences into a graph ODE, which can capture the temporal evolution of prompt embeddings for model adaptation. These time-evolving prompt embeddings are then incorporated into basic forecasting models to overcome temporal distribution shifts. We also minimize the mutual information between prompt embeddings and observation embeddings to enhance the robustness of our model to different distributions. Extensive experiments on various benchmark datasets validate the superiority of the proposed PURE in comparison to various baselines. Our codes are available at https://github.com/easylearningscores/\"\\n      },\\n      {\\n        \"paperId\": \"0d0dbfb1b315a43216020abaf74d289456198219\",\\n        \"url\": \"https://www.semanticscholar.org/paper/0d0dbfb1b315a43216020abaf74d289456198219\",\\n        \"title\": \"MaPLe: Multi-modal Prompt Learning\",\\n        \"citationCount\": 816,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://arxiv.org/pdf/2210.03117\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2210.03117, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"The design promotes strong coupling between the vision-language prompts to ensure mutual synergy and discourages learning independent uni-modal solutions, and the effectiveness of the approach is evaluated on three representative tasks of generalization to novel classes, new target datasets and unseen domain shifts.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2022-10-06\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2175250687\",\\n            \"name\": \"Muhammad Uzair Khattak\"\\n          },\\n          {\\n            \"authorId\": \"2097712964\",\\n            \"name\": \"H. Rasheed\"\\n          },\\n          {\\n            \"authorId\": \"32437679\",\\n            \"name\": \"Muhammad Maaz\"\\n          },\\n          {\\n            \"authorId\": \"2111181927\",\\n            \"name\": \"Salman H. Khan\"\\n          },\\n          {\\n            \"authorId\": \"2358803\",\\n            \"name\": \"F. Khan\"\\n          }\\n        ],\\n        \"abstract\": \"Pre-trained vision-language (V-L) models such as CLIP have shown excellent generalization ability to downstream tasks. However, they are sensitive to the choice of input text prompts and require careful selection of prompt templates to perform well. Inspired by the Natural Language Processing (NLP) literature, recent CLIP adaptation approaches learn prompts as the textual inputs to fine-tune CLIP for downstream tasks. We note that using prompting to adapt representations in a single branch of CLIP (language or vision) is sub-optimal since it does not allow the flexibility to dynamically adjust both representation spaces on a downstream task. In this work, we propose Multi-modal Prompt Learning (MaPLe) for both vision and language branches to improve alignment between the vision and language representations. Our design promotes strong coupling between the vision-language prompts to ensure mutual synergy and discourages learning independent uni-modal solutions. Further, we learn separate prompts across different early stages to progressively model the stage-wise feature relationships to allow rich context learning. We evaluate the effectiveness of our approach on three representative tasks of generalization to novel classes, new target datasets and unseen domain shifts. Compared with the state-of-the-art method Co-CoOp, MaPLe exhibits favorable performance and achieves an absolute gain of 3.45% on novel classes and 2.72% on overall harmonic-mean, averaged over 11 diverse image recognition datasets. Our code and pre-trained models are available at https://github.com/muzairkhattak/multimodal-prompt-learning.\"\\n      },\\n      {\\n        \"paperId\": \"ceac663aa9213bfdb457deacba9e5ed9ddd7ae92\",\\n        \"url\": \"https://www.semanticscholar.org/paper/ceac663aa9213bfdb457deacba9e5ed9ddd7ae92\",\\n        \"title\": \"CLIPArTT: Adaptation of CLIP to New Domains at Test Time\",\\n        \"citationCount\": 10,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2405.00754, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This study introduces CLIP Adaptation duRing Test-Time (CLIPArTT), a fully test-time adaptation (TTA) approach for CLIP, which involves automatic text prompts construction during inference for their use as text supervision and pioneer the standardization of TTA benchmarks in the realm of VLMs.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-05-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2037886454\",\\n            \"name\": \"G. Hakim\"\\n          },\\n          {\\n            \"authorId\": \"2188345071\",\\n            \"name\": \"David Osowiechi\"\\n          },\\n          {\\n            \"authorId\": \"1380287805\",\\n            \"name\": \"Mehrdad Noori\"\\n          },\\n          {\\n            \"authorId\": \"2188346816\",\\n            \"name\": \"Milad Cheraghalikhani\"\\n          },\\n          {\\n            \"authorId\": \"108062243\",\\n            \"name\": \"Ali Bahri\"\\n          },\\n          {\\n            \"authorId\": \"2168705225\",\\n            \"name\": \"Moslem Yazdanpanah\"\\n          },\\n          {\\n            \"authorId\": \"144019647\",\\n            \"name\": \"Ismail Ben Ayed\"\\n          },\\n          {\\n            \"authorId\": \"2260340228\",\\n            \"name\": \"Christian Desrosiers\"\\n          }\\n        ],\\n        \"abstract\": \"Pre-trained vision-language models (VLMs), exemplified by CLIP, demonstrate remarkable adaptability across zero-shot classification tasks without additional training. However, their performance diminishes in the presence of domain shifts. In this study, we introduce CLIP Adaptation duRing Test-Time (CLIPArTT), a fully test-time adaptation (TTA) approach for CLIP, which involves automatic text prompts construction during inference for their use as text supervision. Our method employs a unique, minimally invasive text prompt tuning process, wherein multiple predicted classes are aggregated into a single new text prompt, used as pseudo label to re-classify inputs in a transductive manner. Additionally, we pioneer the standardization of TTA benchmarks (e.g., TENT) in the realm of VLMs. Our findings demonstrate that, without requiring additional transformations nor new trainable modules, CLIPArTT enhances performance dynamically across non-corrupted datasets such as CIFAR-100, corrupted datasets like CIFAR-100-C and ImageNet-C, alongside synthetic datasets such as VisDA-C. This research underscores the potential for improving VLMs\\' adaptability through novel test-time strategies, offering insights for robust performance across varied datasets and environments. The code can be found at: https://github.com/dosowiechi/CLIPArTT.git\"\\n      },\\n      {\\n        \"paperId\": \"b7e42f7bc9c3ef6bf9315736fa149dff82fccf8f\",\\n        \"url\": \"https://www.semanticscholar.org/paper/b7e42f7bc9c3ef6bf9315736fa149dff82fccf8f\",\\n        \"title\": \"Learning Domain-Aware Detection Head with Prompt Tuning\",\\n        \"citationCount\": 21,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://arxiv.org/pdf/2306.05718\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2306.05718, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A novel DAOD framework named Domain-Aware detection head with Prompt tuning (DA-Pro), which applies the learnable domain-adaptive prompt to generate the dynamic detection head for each domain.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2023-06-09\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2156607591\",\\n            \"name\": \"Haochen Li\"\\n          },\\n          {\\n            \"authorId\": \"2118404461\",\\n            \"name\": \"Rui Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2222738\",\\n            \"name\": \"Hantao Yao\"\\n          },\\n          {\\n            \"authorId\": \"2109332957\",\\n            \"name\": \"Xinkai Song\"\\n          },\\n          {\\n            \"authorId\": \"2232700\",\\n            \"name\": \"Yifan Hao\"\\n          },\\n          {\\n            \"authorId\": \"46317288\",\\n            \"name\": \"Yongwei Zhao\"\\n          },\\n          {\\n            \"authorId\": \"3353457\",\\n            \"name\": \"Ling Li\"\\n          },\\n          {\\n            \"authorId\": \"7377735\",\\n            \"name\": \"Yunji Chen\"\\n          }\\n        ],\\n        \"abstract\": \"Domain adaptive object detection (DAOD) aims to generalize detectors trained on an annotated source domain to an unlabelled target domain. However, existing methods focus on reducing the domain bias of the detection backbone by inferring a discriminative visual encoder, while ignoring the domain bias in the detection head. Inspired by the high generalization of vision-language models (VLMs), applying a VLM as the robust detection backbone following a domain-aware detection head is a reasonable way to learn the discriminative detector for each domain, rather than reducing the domain bias in traditional methods. To achieve the above issue, we thus propose a novel DAOD framework named Domain-Aware detection head with Prompt tuning (DA-Pro), which applies the learnable domain-adaptive prompt to generate the dynamic detection head for each domain. Formally, the domain-adaptive prompt consists of the domain-invariant tokens, domain-specific tokens, and the domain-related textual description along with the class label. Furthermore, two constraints between the source and target domains are applied to ensure that the domain-adaptive prompt can capture the domains-shared and domain-specific knowledge. A prompt ensemble strategy is also proposed to reduce the effect of prompt disturbance. Comprehensive experiments over multiple cross-domain adaptation tasks demonstrate that using the domain-adaptive prompt can produce an effectively domain-related detection head for boosting domain-adaptive object detection. Our code is available at https://github.com/Therock90421/DA-Pro.\"\\n      },\\n      {\\n        \"paperId\": \"27cffd2e86a8af951679b00f091b89400376d711\",\\n        \"url\": \"https://www.semanticscholar.org/paper/27cffd2e86a8af951679b00f091b89400376d711\",\\n        \"title\": \"Hybrid Prompt-Driven Large Language Model for Robust State-of-Charge Estimation of Multitype Li-ion Batteries\",\\n        \"citationCount\": 12,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {\\'tldr\\'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/TTE.2024.3391938?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/TTE.2024.3391938, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": null,\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2025-02-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2034349220\",\\n            \"name\": \"Chong Bian\"\\n          },\\n          {\\n            \"authorId\": \"2186400636\",\\n            \"name\": \"Xue Han\"\\n          },\\n          {\\n            \"authorId\": \"2284894497\",\\n            \"name\": \"Zhiyu Duan\"\\n          },\\n          {\\n            \"authorId\": \"2237426629\",\\n            \"name\": \"Chao Deng\"\\n          },\\n          {\\n            \"authorId\": \"2269234236\",\\n            \"name\": \"Shunkun Yang\"\\n          },\\n          {\\n            \"authorId\": \"2144086553\",\\n            \"name\": \"Junlan Feng\"\\n          }\\n        ],\\n        \"abstract\": \"State-of-charge (SOC) estimation is critical for reliable operation of Li-ion batteries (LIBs). However, the distinct electrochemical characteristics coupled with harsh low-temperature environments make a single estimator struggle to robustly estimate the volatile SOC of multitype LIBs. To address these issues, this article proposes a hard-soft hybrid prompt learning method to unleash the potential of a pretrained large language model (LLM) for SOC estimation. A textual encoder is introduced to convert LIB measurements into hard text prompts for language modeling, naturally eliciting the pretrained LLM to capture the intrarelations of measured values over time and their interrelations with contextual semantics for accurate estimates. A side adapter network is constructed to reparameterize model adaptation towards different LIB tasks into optimizations within a low-dimensional subspace, strengthening the estimation generalization of the pretrained LLM in a parameter-efficient manner. A knowledge infusion mechanism is designed to encapsulate task-specific information as soft prompt vectors for model integration along forward propagation, dynamically conditioning the hidden states inside the pretrained LLM to enhance the estimation robustness against SOC volatilities. Extensive experiments verify that the hybrid prompt-driven LLM can simultaneously perform estimations for multitype LIBs under diverse operations and sub-zero temperatures with superior accuracy, generalization, and robustness.\"\\n      },\\n      {\\n        \"paperId\": \"564812800caef63f1c83dda51f71c23f4e14cd00\",\\n        \"url\": \"https://www.semanticscholar.org/paper/564812800caef63f1c83dda51f71c23f4e14cd00\",\\n        \"title\": \"Choriocapillaris Impairment Is Associated With Delayed Rod-Mediated Dark Adaptation in Age-Related Macular Degeneration\",\\n        \"citationCount\": 26,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://doi.org/10.1167/iovs.64.12.41\",\\n          \"status\": \"GOLD\",\\n          \"license\": \"CCBYNCND\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC10540875, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"Choriocapillaris FD% were associated with AMD severity and with impaired vision, especially RMDA, which is a causal factor for high-risk soft drusen formation.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2023-09-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"52465212\",\\n            \"name\": \"Deepayan Kar\"\\n          },\\n          {\\n            \"authorId\": \"6550345\",\\n            \"name\": \"G. Corradetti\"\\n          },\\n          {\\n            \"authorId\": \"40654031\",\\n            \"name\": \"Thomas A. Swain\"\\n          },\\n          {\\n            \"authorId\": \"6894914\",\\n            \"name\": \"Mark E. Clark\"\\n          },\\n          {\\n            \"authorId\": \"144347384\",\\n            \"name\": \"G. McGwin\"\\n          },\\n          {\\n            \"authorId\": \"144561682\",\\n            \"name\": \"C. Owsley\"\\n          },\\n          {\\n            \"authorId\": \"1420016526\",\\n            \"name\": \"S. Sadda\"\\n          },\\n          {\\n            \"authorId\": \"4477577\",\\n            \"name\": \"C. Curcio\"\\n          }\\n        ],\\n        \"abstract\": \"Purpose Progress toward treatment and prevention of age-related macular degeneration (AMD) requires imaging end points that relate to vision. We investigated choriocapillaris flow signal deficits (FD%) and visual function in eyes of individuals aged \\\\u226560 years, with and without AMD. Methods One eye of each participant in the baseline visit of the Alabama Study on Early Age-Related Macular Degeneration 2 (ALSTAR2; NCT04112667) was studied. AMD presence and severity was determined using the Age-Related Eye Disease Study (AREDS) grading system. FD% was quantified using macular spectral domain optical coherence tomography angiography (OCTA) scans. Vision tests included rod-mediated dark adaptation (RMDA), best-corrected visual acuity, and contrast sensitivity (photopic and mesopic), and microperimetric light sensitivity (scotopic, mesopic, and photopic). Presence of subretinal drusenoid deposits (SDD) was determined using multimodal imaging. Results In 410 study eyes of 410 participants (mean [SD] age = 71.7 years [5.9]), FD% was higher in early AMD (mean [SD] = 54.0% [5.5], N = 122) and intermediate AMD (59.8% [7.4], N = 92), compared to normal (52.1% [5.3], N = 196) eyes. Among visual functions evaluated, RMDA showed the strongest association with FD% (r = 0.35, P < 0.0001), followed by contrast sensitivity (r = \\\\u22120.22, P < 0.0001). Eyes with SDD had worse FD% (58.3% [7.4], N = 87), compared to eyes without SDD (53.4% [6.0], N = 323, P = < 0.0001). Conclusions Choriocapillaris FD% were associated with AMD severity and with impaired vision, especially RMDA. Reduced metabolic transport and exchange across the choriocapillaris-Bruch\\'s membrane retinal pigment epithelium (RPE) complex, a causal factor for high-risk soft drusen formation, also may impair photoreceptor sustenance from the circulation. This includes retinoid resupply, essential to dynamic rod function.\"\\n      },\\n      {\\n        \"paperId\": \"5f3adb1a1099169733a268c7597af130f200a5d7\",\\n        \"url\": \"https://www.semanticscholar.org/paper/5f3adb1a1099169733a268c7597af130f200a5d7\",\\n        \"title\": \"DA-LSTM: A Dynamic Drift-Adaptive Learning Framework for Interval Load Forecasting with LSTM Networks\",\\n        \"citationCount\": 53,\\n        \"openAccessPdf\": {\\n          \"url\": \"http://arxiv.org/pdf/2305.08767\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2305.08767, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A dynamic drift-adaptive Long Short-Term Memory (DA-LSTM) framework that can improve the performance of load forecasting models without requiring a drift threshold setting is proposed.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2023-05-15\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2047342626\",\\n            \"name\": \"Firas Bayram\"\\n          },\\n          {\\n            \"authorId\": \"2128949725\",\\n            \"name\": \"Phil Aupke\"\\n          },\\n          {\\n            \"authorId\": \"1991147\",\\n            \"name\": \"Bestoun S. Ahmed\"\\n          },\\n          {\\n            \"authorId\": \"10281628\",\\n            \"name\": \"A. Kassler\"\\n          },\\n          {\\n            \"authorId\": \"152623862\",\\n            \"name\": \"A. Theocharis\"\\n          },\\n          {\\n            \"authorId\": \"2086342190\",\\n            \"name\": \"Jonas Forsman\"\\n          }\\n        ],\\n        \"abstract\": \"Load forecasting is a crucial topic in energy management systems (EMS) due to its vital role in optimizing energy scheduling and enabling more flexible and intelligent power grid systems. As a result, these systems allow power utility companies to respond promptly to demands in the electricity market. Deep learning (DL) models have been commonly employed in load forecasting problems supported by adaptation mechanisms to cope with the changing pattern of consumption by customers, known as concept drift. A drift magnitude threshold should be defined to design change detection methods to identify drifts. While the drift magnitude in load forecasting problems can vary significantly over time, existing literature often assumes a fixed drift magnitude threshold, which should be dynamically adjusted rather than fixed during system evolution. To address this gap, in this paper, we propose a dynamic drift-adaptive Long Short-Term Memory (DA-LSTM) framework that can improve the performance of load forecasting models without requiring a drift threshold setting. We integrate several strategies into the framework based on active and passive adaptation approaches. To evaluate DA-LSTM in real-life settings, we thoroughly analyze the proposed framework and deploy it in a real-world problem through a cloud-based environment. Efficiency is evaluated in terms of the prediction performance of each approach and computational cost. The experiments show performance improvements on multiple evaluation metrics achieved by our framework compared to baseline methods from the literature. Finally, we present a trade-off analysis between prediction performance and computational costs.\"\\n      },\\n      {\\n        \"paperId\": \"8feb33300c04fffa050e0dca59c3fdcafc920a3b\",\\n        \"url\": \"https://www.semanticscholar.org/paper/8feb33300c04fffa050e0dca59c3fdcafc920a3b\",\\n        \"title\": \"FlowZero: Zero-Shot Text-to-Video Synthesis with LLM-Driven Dynamic Scene Syntax\",\\n        \"citationCount\": 20,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2311.15813, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"By using spatio-temporal syntaxes to guide the diffusion process, FlowZero achieves improvement in zero-shot video synthesis, generating coherent videos with vivid motion.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2023-11-27\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2247164321\",\\n            \"name\": \"Yu Lu\"\\n          },\\n          {\\n            \"authorId\": \"2948393\",\\n            \"name\": \"Linchao Zhu\"\\n          },\\n          {\\n            \"authorId\": \"3446334\",\\n            \"name\": \"Hehe Fan\"\\n          },\\n          {\\n            \"authorId\": \"2257587812\",\\n            \"name\": \"Yi Yang\"\\n          }\\n        ],\\n        \"abstract\": \"Text-to-video (T2V) generation is a rapidly growing research area that aims to translate the scenes, objects, and actions within complex video text into a sequence of coherent visual frames. We present FlowZero, a novel framework that combines Large Language Models (LLMs) with image diffusion models to generate temporally-coherent videos. FlowZero uses LLMs to understand complex spatio-temporal dynamics from text, where LLMs can generate a comprehensive dynamic scene syntax (DSS) containing scene descriptions, object layouts, and background motion patterns. These elements in DSS are then used to guide the image diffusion model for video generation with smooth object motions and frame-to-frame coherence. Moreover, FlowZero incorporates an iterative self-refinement process, enhancing the alignment between the spatio-temporal layouts and the textual prompts for the videos. To enhance global coherence, we propose enriching the initial noise of each frame with motion dynamics to control the background movement and camera motion adaptively. By using spatio-temporal syntaxes to guide the diffusion process, FlowZero achieves improvement in zero-shot video synthesis, generating coherent videos with vivid motion.\"\\n      },\\n      {\\n        \"paperId\": \"d1e202d981f03d17ebaa61f941d4366e4db39578\",\\n        \"url\": \"https://www.semanticscholar.org/paper/d1e202d981f03d17ebaa61f941d4366e4db39578\",\\n        \"title\": \"Accelerating Chain-of-Thought Reasoning: When Goal-Gradient Importance Meets Dynamic Skipping\",\\n        \"citationCount\": 10,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2505.08392, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work proposes Adaptive GoGI-Skip, a novel framework learning dynamic CoT compression via supervised fine-tuning that achieves substantial efficiency gains and significantly outperforms existing baselines by preserving accuracy even at high effective compression rates, advancing the state of the art in the CoT reasoning efficiency-accuracy trade-off.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2025-05-13\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2293320862\",\\n            \"name\": \"Zhuang Ren\"\\n          },\\n          {\\n            \"authorId\": \"2279253652\",\\n            \"name\": \"Ben Wang\"\\n          },\\n          {\\n            \"authorId\": \"2282384934\",\\n            \"name\": \"Shuifa Sun\"\\n          }\\n        ],\\n        \"abstract\": \"Large Language Models leverage Chain-of-Thought (CoT) prompting for complex tasks, but their reasoning traces are often excessively verbose and inefficient, leading to significant computational costs and latency. Current CoT compression techniques typically rely on generic importance metrics and static compression rates, which may inadvertently remove functionally critical tokens or fail to adapt to varying reasoning complexity. To overcome these limitations, we propose Adaptive GoGI-Skip, a novel framework learning dynamic CoT compression via supervised fine-tuning. This approach introduces two synergistic innovations: (1) Goal-Gradient Importance (GoGI), a novel metric accurately identifying functionally relevant tokens by measuring the gradient influence of their intermediate representations on the final answer loss, and (2) Adaptive Dynamic Skipping (ADS), a mechanism dynamically regulating the compression rate based on runtime model uncertainty while ensuring local coherence through an adaptive N-token constraint. To our knowledge, this is the first work unifying a goal-oriented, gradient-based importance metric with dynamic, uncertainty-aware skipping for CoT compression. Trained on compressed MATH data, Adaptive GoGI-Skip demonstrates strong cross-domain generalization across diverse reasoning benchmarks including AIME, GPQA, and GSM8K. It achieves substantial efficiency gains - reducing CoT token counts by over 45% on average and delivering 1.6-2.0 times inference speedups - while maintaining high reasoning accuracy. Notably, it significantly outperforms existing baselines by preserving accuracy even at high effective compression rates, advancing the state of the art in the CoT reasoning efficiency-accuracy trade-off.\"\\n      },\\n      {\\n        \"paperId\": \"1cf87d8c98636f88a076f7d245e69b819c6e556b\",\\n        \"url\": \"https://www.semanticscholar.org/paper/1cf87d8c98636f88a076f7d245e69b819c6e556b\",\\n        \"title\": \"FineCLIPER: Multi-modal Fine-grained CLIP for Dynamic Facial Expression Recognition with AdaptERs\",\\n        \"citationCount\": 34,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://arxiv.org/pdf/2407.02157\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2407.02157, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A novel framework, named Multi-modal Fine-grained CLIP for DFER with AdaptERs (FineCLIPER), incorporating the following novel designs, which achieves SOTA performance on the DFEW, FERV39k, and MAFW datasets in both supervised and zero-shot settings with few tunable parameters.\"\\n        },\\n        \"publicationTypes\": [\\n          \"Book\",\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-07-02\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2271664660\",\\n            \"name\": \"Haodong Chen\"\\n          },\\n          {\\n            \"authorId\": \"2296857672\",\\n            \"name\": \"Haojian Huang\"\\n          },\\n          {\\n            \"authorId\": \"2309753429\",\\n            \"name\": \"Junhao Dong\"\\n          },\\n          {\\n            \"authorId\": \"2333836576\",\\n            \"name\": \"Mingzhe Zheng\"\\n          },\\n          {\\n            \"authorId\": \"2301156673\",\\n            \"name\": \"Dian Shao\"\\n          }\\n        ],\\n        \"abstract\": \"Dynamic Facial Expression Recognition (DFER) is crucial for understanding human behavior. However, current methods exhibit limited performance mainly due to the insufficient utilization of facial dynamics, and the ambiguity of expression semantics, etc. To this end, we propose a novel framework, named Multi-modal Fine-grained CLIP for DFER with AdaptERs (FineCLIPER), incorporating the following novel designs: 1) To better distinguish between similar facial expressions, we extend the class labels to textual descriptions from both positive and negative aspects, and obtain supervision by calculating the cross-modal similarity based on the CLIP model; 2) Our FineCLIPER adopts a hierarchical manner to effectively mine useful cues from DFE videos. Specifically, besides directly embedding video frames as input (low semantic level), we propose to extract the face segmentation masks and landmarks based on each frame (middle semantic level) and utilize the Multi-modal Large Language Model (MLLM) to further generate detailed descriptions of facial changes across frames with designed prompts (high semantic level). Additionally, we also adopt Parameter-Efficient Fine-Tuning (PEFT) to enable efficient adaptation of large pre-trained models (i.e., CLIP) for this task. Our FineCLIPER achieves SOTA performance on the DFEW, FERV39k, and MAFW datasets in both supervised and zero-shot settings with few tunable parameters. Project page: https://haroldchen19.github.io/FineCLIPER-Page/\"\\n      },\\n      {\\n        \"paperId\": \"7af0f6f5113b69644c3a232a5bee61a1bd15df25\",\\n        \"url\": \"https://www.semanticscholar.org/paper/7af0f6f5113b69644c3a232a5bee61a1bd15df25\",\\n        \"title\": \"Machine learning based estimation of dynamic balance and gait adaptability in persons with neurological diseases using inertial sensors\",\\n        \"citationCount\": 17,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://www.nature.com/articles/s41598-023-35744-x.pdf\",\\n          \"status\": \"GOLD\",\\n          \"license\": \"CCBY\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC10224964, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"Results confirm that steady-state walking measurements provide information about dynamic balance and gait adaptability and can help clinicians identify important features to improve upon during rehabilitation.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2023-05-27\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2123025486\",\\n            \"name\": \"Piergiuseppe Liuzzi\"\\n          },\\n          {\\n            \"authorId\": \"4873550\",\\n            \"name\": \"I. Carpinella\"\\n          },\\n          {\\n            \"authorId\": \"51909015\",\\n            \"name\": \"D. Anastasi\"\\n          },\\n          {\\n            \"authorId\": \"5116603\",\\n            \"name\": \"E. Gervasoni\"\\n          },\\n          {\\n            \"authorId\": \"4840695\",\\n            \"name\": \"T. Lencioni\"\\n          },\\n          {\\n            \"authorId\": \"34764126\",\\n            \"name\": \"R. Bertoni\"\\n          },\\n          {\\n            \"authorId\": \"1798096\",\\n            \"name\": \"M. Carrozza\"\\n          },\\n          {\\n            \"authorId\": \"2687643\",\\n            \"name\": \"D. Cattaneo\"\\n          },\\n          {\\n            \"authorId\": \"1713849\",\\n            \"name\": \"M. Ferrarin\"\\n          },\\n          {\\n            \"authorId\": \"1758858\",\\n            \"name\": \"A. Mannini\"\\n          }\\n        ],\\n        \"abstract\": \"Poor dynamic balance and impaired gait adaptation to different contexts are hallmarks of people with neurological disorders (PwND), leading to difficulties in daily life and increased fall risk. Frequent assessment of dynamic balance and gait adaptability is therefore essential for monitoring the evolution of these impairments and/or the long-term effects of rehabilitation. The modified dynamic gait index (mDGI) is a validated clinical test specifically devoted to evaluating gait facets in clinical settings under a physiotherapist\\\\u2019s supervision. The need of a clinical environment, consequently, limits the number of assessments. Wearable sensors are increasingly used to measure balance and locomotion in real-world contexts and may permit an increase in monitoring frequency. This study aims to provide a preliminary test of this opportunity by using nested cross-validated machine learning regressors to predict the mDGI scores of 95 PwND via inertial signals collected from short steady-state walking bouts derived from the 6-minute walk test. Four different models were compared, one for each pathology (multiple sclerosis, Parkinson\\\\u2019s disease, and stroke) and one for the pooled multipathological cohort. Model explanations were computed on the best-performing solution; the model trained on the multipathological cohort yielded a median (interquartile range) absolute test error of 3.58 (5.38) points. In total, 76% of the predictions were within the mDGI\\\\u2019s minimal detectable change of 5 points. These results confirm that steady-state walking measurements provide information about dynamic balance and gait adaptability and can help clinicians identify important features to improve upon during rehabilitation. Future developments will include training of the method using short steady-state walking bouts in real-world settings, analysing the feasibility of this solution to intensify performance monitoring, providing prompt detection of worsening/improvements, and complementing clinical assessments.\"\\n      },\\n      {\\n        \"paperId\": \"088623b50102baf2cf84ea126d558b574e267967\",\\n        \"url\": \"https://www.semanticscholar.org/paper/088623b50102baf2cf84ea126d558b574e267967\",\\n        \"title\": \"Towards Robust and Efficient Cloud-Edge Elastic Model Adaptation via Selective Entropy Distillation\",\\n        \"citationCount\": 12,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2402.17316, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A Cloud-Edge Elastic Model Adaptation (CEMA) paradigm in which the edge models only need to perform forward propagation and the edge models can be adapted online and to reduce the communication burden is established.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-02-27\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"1816749481\",\\n            \"name\": \"Yaofo Chen\"\\n          },\\n          {\\n            \"authorId\": \"1411039233\",\\n            \"name\": \"Shuaicheng Niu\"\\n          },\\n          {\\n            \"authorId\": \"2157422974\",\\n            \"name\": \"Shoukai Xu\"\\n          },\\n          {\\n            \"authorId\": \"2287873264\",\\n            \"name\": \"Hengjie Song\"\\n          },\\n          {\\n            \"authorId\": \"2288039852\",\\n            \"name\": \"Yaowei Wang\"\\n          },\\n          {\\n            \"authorId\": \"2287854562\",\\n            \"name\": \"Mingkui Tan\"\\n          }\\n        ],\\n        \"abstract\": \"The conventional deep learning paradigm often involves training a deep model on a server and then deploying the model or its distilled ones to resource-limited edge devices. Usually, the models shall remain fixed once deployed (at least for some period) due to the potential high cost of model adaptation for both the server and edge sides. However, in many real-world scenarios, the test environments may change dynamically (known as distribution shifts), which often results in degraded performance. Thus, one has to adapt the edge models promptly to attain promising performance. Moreover, with the increasing data collected at the edge, this paradigm also fails to further adapt the cloud model for better performance. To address these, we encounter two primary challenges: 1) the edge model has limited computation power and may only support forward propagation; 2) the data transmission budget between cloud and edge devices is limited in latency-sensitive scenarios. In this paper, we establish a Cloud-Edge Elastic Model Adaptation (CEMA) paradigm in which the edge models only need to perform forward propagation and the edge models can be adapted online. In our CEMA, to reduce the communication burden, we devise two criteria to exclude unnecessary samples from uploading to the cloud, i.e., dynamic unreliable and low-informative sample exclusion. Based on the uploaded samples, we update and distribute the affine parameters of normalization layers by distilling from the stronger foundation model to the edge model with a sample replay strategy. Extensive experimental results on ImageNet-C and ImageNet-R verify the effectiveness of our CEMA.\"\\n      },\\n      {\\n        \"paperId\": \"5006e5be4c3cb1c4af84e4a2717ef886b3a22464\",\\n        \"url\": \"https://www.semanticscholar.org/paper/5006e5be4c3cb1c4af84e4a2717ef886b3a22464\",\\n        \"title\": \"DART: Dual-Modal Adaptive Online Prompting and Knowledge Retention for Test-Time Adaptation\",\\n        \"citationCount\": 12,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://ojs.aaai.org/index.php/AAAI/article/download/29320/30490\",\\n          \"status\": \"GOLD\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1609/aaai.v38i13.29320?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1609/aaai.v38i13.29320, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"To increase the learning capacity, DART captures knowledge from each test sample by learning class-specific text prompts and instance-level image prompts, and utilizes dual-modal knowledge retention prompts to adaptively retain the acquired knowledge, thereby enhancing the predictions on subsequent test samples.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-03-24\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2293575785\",\\n            \"name\": \"Zichen Liu\"\\n          },\\n          {\\n            \"authorId\": \"2261902040\",\\n            \"name\": \"Hongbo Sun\"\\n          },\\n          {\\n            \"authorId\": \"2267159976\",\\n            \"name\": \"Yuxin Peng\"\\n          },\\n          {\\n            \"authorId\": \"2261798088\",\\n            \"name\": \"Jiahuan Zhou\"\\n          }\\n        ],\\n        \"abstract\": \"As an up-and-coming area, CLIP-based pre-trained vision-language models can readily facilitate downstream tasks through the zero-shot or few-shot fine-tuning manners. However, they still face critical challenges in test-time generalization due to the shifts between the training and test data distributions, hindering the further improvement of the performance. To address this crucial problem, the latest works have introduced Test-Time Adaptation (TTA) techniques to CLIP which dynamically learn text prompts using only test samples. However, their limited learning capacity due to the overlook of visual modality information, and the underutilization of knowledge in previously seen test samples result in reduced performance. In this paper, we propose a novel Dual-modal Adaptive online prompting and knowledge ReTention method called DART to overcome these challenges. To increase the learning capacity, DART captures knowledge from each test sample by learning class-specific text prompts and instance-level image prompts. Additionally, to fully leverage the knowledge from previously seen test samples, DART utilizes dual-modal knowledge retention prompts to adaptively retain the acquired knowledge, thereby enhancing the predictions on subsequent test samples. Extensive experiments on various large-scale benchmarks demonstrate the effectiveness of our proposed DART against state-of-the-art methods.\"\\n      },\\n      {\\n        \"paperId\": \"5076bbbf831a92174c9cc1b347bd0584560435fc\",\\n        \"url\": \"https://www.semanticscholar.org/paper/5076bbbf831a92174c9cc1b347bd0584560435fc\",\\n        \"title\": \"Adaptive-Solver Framework for Dynamic Strategy Selection in Large Language Model Reasoning\",\\n        \"citationCount\": 10,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://arxiv.org/pdf/2310.01446\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2310.01446, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"An Adaptive-Solver (AS) framework that dynamically adapts solving strategies to suit various problems, enabling the flexible allocation of test-time computational resources and can help reduce computational consumption and improve performance.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2023-10-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2256299370\",\\n            \"name\": \"Jianpeng Zhou\"\\n          },\\n          {\\n            \"authorId\": \"81970097\",\\n            \"name\": \"Wanjun Zhong\"\\n          },\\n          {\\n            \"authorId\": \"2214155529\",\\n            \"name\": \"Yanlin Wang\"\\n          },\\n          {\\n            \"authorId\": \"2110182921\",\\n            \"name\": \"Jiahai Wang\"\\n          }\\n        ],\\n        \"abstract\": \"Large Language Models (LLMs) demonstrate impressive ability in handling reasoning tasks. However, unlike humans who can instinctively adapt their problem-solving strategies to the complexity of task, most LLM-based methods adopt a one-size-fits-all approach. These methods employ consistent models, sample sizes, prompting methods and levels of problem decomposition, regardless of the problem complexity. The inflexibility of these methods can bring unnecessary computational overhead or sub-optimal performance. To address this limitation, we introduce an Adaptive-Solver (AS) framework tha dynamically adapts solving strategies to suit various problems, enabling the flexible allocation of test-time computational resources. The framework functions with two primary modules. The initial evaluation module assesses the reliability of the current solution using answer consistency. If the solution is deemed unreliable, the subsequent adaptation module comes into play. Within this module, various types of adaptation strategies are employed collaboratively. Through such dynamic and multi-faceted adaptations, our framework can help reduce computational consumption and improve performance. Experimental results from complex reasoning benchmarks reveal that our method can significantly reduce API costs (up to 85%) while maintaining original performance. Alternatively, it achieves up to 4.5% higher accuracy compared to the baselines at the same cost. The code and dataset are available at https://github.com/john1226966735/Adaptive-Solver.\"\\n      },\\n      {\\n        \"paperId\": \"f4460f3e44e4d035830ab2c676c06e193e51d203\",\\n        \"url\": \"https://www.semanticscholar.org/paper/f4460f3e44e4d035830ab2c676c06e193e51d203\",\\n        \"title\": \"Intrinsic signal optoretinography of dark adaptation kinetics\",\\n        \"citationCount\": 13,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://www.nature.com/articles/s41598-022-06562-4.pdf\",\\n          \"status\": \"GOLD\",\\n          \"license\": \"CCBY\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2112.07838, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"Functional OCT of dark adaptation kinetics promises an objective method for rapid ORG assessment of physiological integrity of retinal photoreceptors.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2021-12-15\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"82743798\",\\n            \"name\": \"Tae-Hoon Kim\"\\n          },\\n          {\\n            \"authorId\": \"2111184482\",\\n            \"name\": \"Jie Ding\"\\n          },\\n          {\\n            \"authorId\": \"8073475\",\\n            \"name\": \"Xincheng Yao\"\\n          }\\n        ],\\n        \"abstract\": \"Delayed dark adaptation due to impaired rod photoreceptor homeostasis has been reported as the earliest symptom of eye diseases such as age-related macular degeneration, diabetic retinopathy, and retinitis pigmentosa. Objective measurement of dark adaptation can facilitate early diagnosis to enable prompt intervention to prevent vision loss. However, there is a lack of noninvasive methods capable of spatiotemporal monitoring of photoreceptor changes during dark adaptation. Here we demonstrate functional optical coherence tomography (OCT) for in vivo intrinsic signal optoretinography (ORG) of dark adaptation kinetics in the C57BL/6J mouse retina. Functional OCT revealed a shortening of the outer retina, a rearrangement of the cone and rod photoreceptor interdigitation zone, and a reduction in intrinsic signal amplitude at the photoreceptor inner segment ellipsoid (ISe). A strong positive correlation between the outer retinal shortening and ISe intensity reduction was also confirmed. Functional OCT of dark adaptation kinetics promises an objective method for rapid ORG assessment of physiological integrity of retinal photoreceptors.\"\\n      },\\n      {\\n        \"paperId\": \"9fe222cb8464e8157b3654ab96ef719331dd2357\",\\n        \"url\": \"https://www.semanticscholar.org/paper/9fe222cb8464e8157b3654ab96ef719331dd2357\",\\n        \"title\": \"Meta-Learning the Difference: Preparing Large Language Models for Efficient Adaptation\",\\n        \"citationCount\": 20,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl_a_00517/2059907/tacl_a_00517.pdf\",\\n          \"status\": \"GOLD\",\\n          \"license\": \"CCBY\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2207.03509, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"Experiments on few-shot dialogue completion, low-resource abstractive summarization, and multi-domain language modeling show improvements in adaptation time and performance over direct finetuning or preparation via domain-adaptive pretraining.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2022-07-07\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"26563401\",\\n            \"name\": \"Zejiang Hou\"\\n          },\\n          {\\n            \"authorId\": \"143733211\",\\n            \"name\": \"Julian Salazar\"\\n          },\\n          {\\n            \"authorId\": \"1402376936\",\\n            \"name\": \"George Polovets\"\\n          }\\n        ],\\n        \"abstract\": \"Abstract Large pretrained language models (PLMs) are often domain- or task-adapted via finetuning or prompting. Finetuning requires modifying all of the parameters and having enough data to avoid overfitting while prompting requires no training and few examples but limits performance. Instead, we prepare PLMs for data- and parameter-efficient adaptation by learning to learn the difference between general and adapted PLMs. This difference is expressed in terms of model weights and sublayer structure through our proposed dynamic low-rank reparameterization and learned architecture controller. Experiments on few-shot dialogue completion, low-resource abstractive summarization, and multi-domain language modeling show improvements in adaptation time and performance over direct finetuning or preparation via domain-adaptive pretraining. Ablations show our task-adaptive reparameterization (TARP) and model search (TAMS) components individually improve on other parameter-efficient transfer like adapters and structure-learning methods like learned sparsification.\"\\n      },\\n      {\\n        \"paperId\": \"792bdea700be87488c5fa135ecef27b94f0d5c31\",\\n        \"url\": \"https://www.semanticscholar.org/paper/792bdea700be87488c5fa135ecef27b94f0d5c31\",\\n        \"title\": \"Sparse-Based Domain Adaptation Network for OCTA Image Super-Resolution Reconstruction\",\\n        \"citationCount\": 11,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://arxiv.org/pdf/2207.11882\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2207.11882, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A multi-level super-resolution model is proposed for the fully-supervised reconstruction of the synthetic data, guiding the reconstructing of the realistic LR images through a generative-adversarial strategy that allows the synthetic and realism LR images to be unified in the feature domain.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2022-07-25\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"47412750\",\\n            \"name\": \"Huaying Hao\"\\n          },\\n          {\\n            \"authorId\": \"2216350318\",\\n            \"name\": \"C. Xu\"\\n          },\\n          {\\n            \"authorId\": \"2109982918\",\\n            \"name\": \"Dan Zhang\"\\n          },\\n          {\\n            \"authorId\": \"151482895\",\\n            \"name\": \"Qifeng Yan\"\\n          },\\n          {\\n            \"authorId\": \"2116222083\",\\n            \"name\": \"Jiong Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2119033864\",\\n            \"name\": \"Yue Liu\"\\n          },\\n          {\\n            \"authorId\": \"1956017\",\\n            \"name\": \"Yitian Zhao\"\\n          }\\n        ],\\n        \"abstract\": \"Retinal Optical Coherence Tomography Angiography (OCTA) with high-resolution is important for the quantification and analysis of retinal vasculature. However, the resolution of OCTA images is inversely proportional to the field of view at the same sampling frequency, which is not conducive to clinicians for analyzing larger vascular areas. In this paper, we propose a novel <bold>S</bold>parse-based domain <bold>A</bold>daptation <bold>S</bold>uper-<bold>R</bold>esolution network (SASR) for the reconstruction of realistic <inline-formula><tex-math notation=\\\\\"LaTeX\\\\\">$6\\\\\\\\times \\\\\\\\text{6}{\\\\\\\\rm{mm}}^{2}$</tex-math></inline-formula>/low-resolution (LR) OCTA images to high-resolution (HR) representations. To be more specific, we first perform a simple degradation of the <inline-formula><tex-math notation=\\\\\"LaTeX\\\\\">$3\\\\\\\\times \\\\\\\\text{3}\\\\\\\\,{\\\\\\\\rm{mm}}^{2}$</tex-math></inline-formula>/high-resolution (HR) image to obtain the synthetic LR image. An efficient registration method is then employed to register the synthetic LR with its corresponding <inline-formula><tex-math notation=\\\\\"LaTeX\\\\\">$3\\\\\\\\times \\\\\\\\text{3}\\\\\\\\,{\\\\\\\\rm{mm}}^{2}$</tex-math></inline-formula> image region within the <inline-formula><tex-math notation=\\\\\"LaTeX\\\\\">$6\\\\\\\\times \\\\\\\\text{6}\\\\\\\\,{\\\\\\\\rm{mm}}^{2}$</tex-math></inline-formula> image to obtain the cropped realistic LR image. We then propose a multi-level super-resolution model for the fully-supervised reconstruction of the synthetic data, guiding the reconstruction of the realistic LR images through a generative-adversarial strategy that allows the synthetic and realistic LR images to be unified in the feature domain. Finally, a novel sparse edge-aware loss is designed to dynamically optimize the vessel edge structure. Extensive experiments on two OCTA sets have shown that our method performs better than state-of-the-art super-resolution reconstruction methods. In addition, we have investigated the performance of the reconstruction results on retina structure segmentations, which further validate the effectiveness of our approach.\"\\n      },\\n      {\\n        \"paperId\": \"3836a5ca7b7884b2333299b65f3436d210587f07\",\\n        \"url\": \"https://www.semanticscholar.org/paper/3836a5ca7b7884b2333299b65f3436d210587f07\",\\n        \"title\": \"Dynamics of Smallholder Farmers\\\\u2019 Livelihood Adaptation Decision-Making in Central Ethiopia\",\\n        \"citationCount\": 22,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://www.mdpi.com/2071-1050/12/11/4526/pdf?version=1591863876\",\\n          \"status\": \"GOLD\",\\n          \"license\": \"CCBY\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.3390/su12114526?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.3390/su12114526, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": null,\\n        \"publicationTypes\": null,\\n        \"publicationDate\": \"2020-06-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"13422136\",\\n            \"name\": \"D. Etana\"\\n          },\\n          {\\n            \"authorId\": \"4266116\",\\n            \"name\": \"D. Snelder\"\\n          },\\n          {\\n            \"authorId\": \"2051704940\",\\n            \"name\": \"C. V. van Wesenbeeck\"\\n          },\\n          {\\n            \"authorId\": \"3606491\",\\n            \"name\": \"T. de Cock Buning\"\\n          }\\n        ],\\n        \"abstract\": \"In previous studies mainly focusing on determinants of adaptation, evidence of the dynamic process of adaptation decision-making is negligible. The objective of this study was to investigate the effects of socio-cultural factors, changes in household characteristics, and climate variables on the transition from non-use to use of adaptation strategies. The study integrated primary data collected from households with secondary rainfall and temperature data. The quantitative and qualitative data were analysed using a dynamic random-effects probit model and a thematic approach, respectively. The result shows strong evidence of path dependence in which use of a strategy during the previous year significantly increases its current use. Climate-related risk perception and factual knowledge may not necessarily prompt adaptation action, whereas access to financial resources and farming-related trainings were consistent positive predictors of farmers\\\\u2019 adaptation decisions. The findings entail that economic capacity and the associated intrinsic motivation help few farmers to utilise robust and contesting adaptation strategies. For most households, economic problems and the consequent fatalistic attitude and risk-avoidance behaviour induce either non-use or use of responsive and accommodating strategies aimed at ensuring survival. Path dependence in non-use of adaptation strategies and sub-optimal adaptation actions demand effective institutional supports to address the behavioural and economic barriers of these households in order to build overall community resilience.\"\\n      },\\n      {\\n        \"paperId\": \"fdb97e3f5fa700fa19c3274f4a35834adb214c23\",\\n        \"url\": \"https://www.semanticscholar.org/paper/fdb97e3f5fa700fa19c3274f4a35834adb214c23\",\\n        \"title\": \"\\\\u201cThe emotions were like a roller-coaster\\\\u201d: a qualitative analysis of e-diary data on healthcare worker resilience and adaptation during the COVID-19 outbreak in Singapore\",\\n        \"citationCount\": 12,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://human-resources-health.biomedcentral.com/counter/pdf/10.1186/s12960-022-00756-7\",\\n          \"status\": \"GOLD\",\\n          \"license\": \"CCBY\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC9285872, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"What characterises risk, vulnerability and resilient responses of HCWs during the early stages of the outbreak in Singapore is explored and the Loads\\\\u2013Levers\\\\u2013Lifts model of resilience is used to guide analysis to inform intervention designs.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Review\"\\n        ],\\n        \"publicationDate\": \"2022-07-15\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2014928933\",\\n            \"name\": \"Alyssa Yenyi Chan\"\\n          },\\n          {\\n            \"authorId\": \"1491848786\",\\n            \"name\": \"Celene Ting\"\\n          },\\n          {\\n            \"authorId\": \"6374024\",\\n            \"name\": \"L. G. Chan\"\\n          },\\n          {\\n            \"authorId\": \"6130448\",\\n            \"name\": \"Z. Hildon\"\\n          }\\n        ],\\n        \"abstract\": \"Background Uncertainties related to COVID-19 have strained the mental health of healthcare workers (HCWs) worldwide. Gaining the ability to adapt and thrive under pressure will be key to addressing this. We explore what characterises risk, vulnerability and resilient responses of HCWs during the early stages of the outbreak in Singapore. Methods We undertook qualitative theory-guided thematic analysis of e-diary entries from HCWs who navigated the outbreak from June\\\\u2013August 2020. Data were extracted from a subset of an online survey of n \\\\u2009=\\\\u20093616 participants collected across 9 institutions, including restructured hospitals, hospices and affiliated primary care partners. Results N \\\\u2009=\\\\u2009663 or 18% submitted qualitative journal entries included for analyses. All professional cadres, local as well as foreign HCWs participated. Themes are reported according to the Loads\\\\u2013Levers\\\\u2013Lifts model of resilience and highlighted in italics. The model assumes that resilience is a dynamic process. Key factors threatening mental health (loading) risk included a notable rise in anxiety, the effects of being separated from loved ones, and experiencing heightened emotions and emotional overload . Bad situations were made worse, prompting vulnerable outcomes when HCWs experienced stigma in the community and effects of \\\\u201cpublic paranoia\\\\u201d; or under conditions where HCWs ended up feeling like a prisoner with little control or choice when either confined to staff accommodation or placed on quarantine/Stay Home Notices. Those with strife in their place of residence also described already difficult situations at work being aggravated by home life. Protection (lifts) came from being able to muster a sense of optimism about the future or feeling grateful for the pace of life slowing down and having the space to reprioritise. In contrast, when risk factors were present , balancing these in the direction of resilient outcomes was achieved by choosing to re-direct stress into positive narratives, drawing on inner agency, uptake of therapeutic activities , social support as well as faith and prayer and drawing comfort from religious community among other factors. Conclusion The Loads\\\\u2013Levers\\\\u2013Lifts model is used to guide analysis to inform intervention designs. Levers promoting resilience through targeting therapies, workplace policies and awareness campaigns accounting for identified loads are proposed.\"\\n      },\\n      {\\n        \"paperId\": \"1a031fead8b57e4c24b04ba79b15b12d4eca583b\",\\n        \"url\": \"https://www.semanticscholar.org/paper/1a031fead8b57e4c24b04ba79b15b12d4eca583b\",\\n        \"title\": \"The Geometry of Prompting: Unveiling Distinct Mechanisms of Task Adaptation in Language Models\",\\n        \"citationCount\": 10,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2502.08009, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work investigates how different prompting methods affect the geometry of representations in decoder-only language models and reveals that various prompting techniques, while achieving similar performance, operate through distinct representational mechanisms for task adaptation.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2025-02-11\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2345003776\",\\n            \"name\": \"Artem Kirsanov\"\\n          },\\n          {\\n            \"authorId\": \"2276206718\",\\n            \"name\": \"Chi-Ning Chou\"\\n          },\\n          {\\n            \"authorId\": \"2345694335\",\\n            \"name\": \"Kyunghyun Cho\"\\n          },\\n          {\\n            \"authorId\": \"2267869180\",\\n            \"name\": \"SueYeon Chung\"\\n          }\\n        ],\\n        \"abstract\": \"Decoder-only language models have the ability to dynamically switch between various computational tasks based on input prompts. Despite many successful applications of prompting, there is very limited understanding of the internal mechanism behind such flexibility. In this work, we investigate how different prompting methods affect the geometry of representations in these models. Employing a framework grounded in statistical physics, we reveal that various prompting techniques, while achieving similar performance, operate through distinct representational mechanisms for task adaptation. Our analysis highlights the critical role of input distribution samples and label semantics in few-shot in-context learning. We also demonstrate evidence of synergistic and interfering interactions between different tasks on the representational level. Our work contributes to the theoretical understanding of large language models and lays the groundwork for developing more effective, representation-aware prompting strategies.\"\\n      },\\n      {\\n        \"paperId\": \"9e0d9c5d858aac35a51b88e0807ace1c3369f863\",\\n        \"url\": \"https://www.semanticscholar.org/paper/9e0d9c5d858aac35a51b88e0807ace1c3369f863\",\\n        \"title\": \"Quantification of intrinsic optical signals in the outer human retina using optical coherence tomography\",\\n        \"citationCount\": 10,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://onlinelibrary.wiley.com/doi/pdfdirect/10.1111/nyas.14721\",\\n          \"status\": \"HYBRID\",\\n          \"license\": \"CCBYNC\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC9299665, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"The presented approach allowed for assess to dynamic changes in the outer retina in response to light and the change in the subretinal space occurring in the context of light adaptation could be measured using a standard OCT platform and a dedicated signal model.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2021-12-10\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"47182275\",\\n            \"name\": \"A. Messner\"\\n          },\\n          {\\n            \"authorId\": \"9535158\",\\n            \"name\": \"V. Aranha dos Santos\"\\n          },\\n          {\\n            \"authorId\": \"51902564\",\\n            \"name\": \"Hannes Stegmann\"\\n          },\\n          {\\n            \"authorId\": \"2494261\",\\n            \"name\": \"S. Puchner\"\\n          },\\n          {\\n            \"authorId\": \"7533266\",\\n            \"name\": \"D. Schmidl\"\\n          },\\n          {\\n            \"authorId\": \"2084180\",\\n            \"name\": \"R. Leitgeb\"\\n          },\\n          {\\n            \"authorId\": \"144820338\",\\n            \"name\": \"L. Schmetterer\"\\n          },\\n          {\\n            \"authorId\": \"4052780\",\\n            \"name\": \"R. Werkmeister\"\\n          }\\n        ],\\n        \"abstract\": \"Intrinsic optical signals constitute a noninvasive biomarker promising the objective assessment of retinal photoreceptor function. We employed a commercial optical coherence tomography (OCT) system and an OCT signal model for evaluation of optical path length (OPL) changes in the temporal outer retina of five healthy subjects during light adaptation. Data were acquired at 30 time points, in ambient light and during long duration stimulation with white light, and analyzed, employing a signal model based on the sum of seven Gaussian curves corresponding to all relevant anatomical structures of the outer retina. During light stimulation, mean OPL between rod outer segment tips (ROST) and the retinal pigment epithelium (RPE) decreased by 21.4 \\\\u00b1 3.5%. Further, OPL between the external\\\\u2010limiting membrane (ELM) and the RPE decreased by 5.2 \\\\u00b1 0.9% versus baseline, while OPL between ELM and ROST showed an initial decrease by 2.1 \\\\u00b1 1.6% versus baseline and, thereafter, increased by 2.8 \\\\u00b1 2.1% versus baseline. Thus, the presented approach allowed for assess to dynamic changes in the outer retina in response to light. The change in the subretinal space occurring in the context of light adaptation could be measured using a standard OCT platform and a dedicated signal model.\"\\n      },\\n      {\\n        \"paperId\": \"0681da799c86c6a92bb0e31f4471656958e60d4f\",\\n        \"url\": \"https://www.semanticscholar.org/paper/0681da799c86c6a92bb0e31f4471656958e60d4f\",\\n        \"title\": \"Teacher adaptation to flexible learning environments\",\\n        \"citationCount\": 43,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {\\'abstract\\'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1007/s10984-019-09302-0?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1007/s10984-019-09302-0, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": null,\\n        \"publicationTypes\": null,\\n        \"publicationDate\": \"2020-07-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2647966\",\\n            \"name\": \"Craig Deed\"\\n          },\\n          {\\n            \"authorId\": \"1713923\",\\n            \"name\": \"D. Blake\"\\n          },\\n          {\\n            \"authorId\": \"50070183\",\\n            \"name\": \"Joanne Henriksen\"\\n          },\\n          {\\n            \"authorId\": \"6355499\",\\n            \"name\": \"A. Mooney\"\\n          },\\n          {\\n            \"authorId\": \"8245143\",\\n            \"name\": \"V. Prain\"\\n          },\\n          {\\n            \"authorId\": \"8348550\",\\n            \"name\": \"R. Tytler\"\\n          },\\n          {\\n            \"authorId\": \"115009297\",\\n            \"name\": \"Tina Zitzlaff\"\\n          },\\n          {\\n            \"authorId\": \"48003546\",\\n            \"name\": \"M. Edwards\"\\n          },\\n          {\\n            \"authorId\": \"117230810\",\\n            \"name\": \"S. Emery\"\\n          },\\n          {\\n            \"authorId\": \"47751037\",\\n            \"name\": \"T. Muir\"\\n          },\\n          {\\n            \"authorId\": \"71944724\",\\n            \"name\": \"K. Swabey\"\\n          },\\n          {\\n            \"authorId\": \"1586673162\",\\n            \"name\": \"Damon P. Thomas\"\\n          },\\n          {\\n            \"authorId\": \"51467279\",\\n            \"name\": \"Cathleen Farrelly\"\\n          },\\n          {\\n            \"authorId\": \"83362649\",\\n            \"name\": \"Valerie Lovejoy\"\\n          },\\n          {\\n            \"authorId\": \"87961076\",\\n            \"name\": \"N. Meyers\"\\n          },\\n          {\\n            \"authorId\": \"113948066\",\\n            \"name\": \"Doug Fingland\"\\n          }\\n        ],\\n        \"abstract\": null\\n      },\\n      {\\n        \"paperId\": \"f7175048d39ed1bceea572b872b72633167fc79d\",\\n        \"url\": \"https://www.semanticscholar.org/paper/f7175048d39ed1bceea572b872b72633167fc79d\",\\n        \"title\": \"Diff-Tracker: Text-to-Image Diffusion Models are Unsupervised Trackers\",\\n        \"citationCount\": 17,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2407.08394, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"Diff-Tracker, a novel approach for the challenging unsupervised visual tracking task leveraging the pre-trained text-to-image diffusion model, designs an initial prompt learner to enable the diffusion model to recognize the tracking target by learning a prompt representing the target.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-07-11\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2309491187\",\\n            \"name\": \"Zhengbo Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2288264614\",\\n            \"name\": \"Li Xu\"\\n          },\\n          {\\n            \"authorId\": \"2067913944\",\\n            \"name\": \"Duo Peng\"\\n          },\\n          {\\n            \"authorId\": \"2265553215\",\\n            \"name\": \"Hossein Rahmani\"\\n          },\\n          {\\n            \"authorId\": \"2309177751\",\\n            \"name\": \"Jun Liu\"\\n          }\\n        ],\\n        \"abstract\": \"We introduce Diff-Tracker, a novel approach for the challenging unsupervised visual tracking task leveraging the pre-trained text-to-image diffusion model. Our main idea is to leverage the rich knowledge encapsulated within the pre-trained diffusion model, such as the understanding of image semantics and structural information, to address unsupervised visual tracking. To this end, we design an initial prompt learner to enable the diffusion model to recognize the tracking target by learning a prompt representing the target. Furthermore, to facilitate dynamic adaptation of the prompt to the target\\'s movements, we propose an online prompt updater. Extensive experiments on five benchmark datasets demonstrate the effectiveness of our proposed method, which also achieves state-of-the-art performance.\"\\n      },\\n      {\\n        \"paperId\": \"cd652dfc58befc0cdf5d36b024aa89ec58e25e53\",\\n        \"url\": \"https://www.semanticscholar.org/paper/cd652dfc58befc0cdf5d36b024aa89ec58e25e53\",\\n        \"title\": \"Exploring a Structural Basis for Delayed Rod-Mediated Dark Adaptation in Age-Related Macular Degeneration Via Deep Learning\",\\n        \"citationCount\": 25,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://doi.org/10.1167/tvst.9.2.62\",\\n          \"status\": \"GOLD\",\\n          \"license\": \"CCBYNCND\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC7745629, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A novel framework for imaging biomarker discovery using deep learning is reported and its ability to identify and localize a previously undescribed biomarker in retinal imaging is demonstrated, strengthening the rationale for RMDA as an outcome measure in early AMD clinical trials.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2020-01-28\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"33556235\",\\n            \"name\": \"Aaron Y. Lee\"\\n          },\\n          {\\n            \"authorId\": \"1390581576\",\\n            \"name\": \"Cecilia S. Lee\"\\n          },\\n          {\\n            \"authorId\": \"1768174127\",\\n            \"name\": \"Marian Blazes\"\\n          },\\n          {\\n            \"authorId\": \"34629227\",\\n            \"name\": \"J. Owen\"\\n          },\\n          {\\n            \"authorId\": \"10732089\",\\n            \"name\": \"Y. Bagdasarova\"\\n          },\\n          {\\n            \"authorId\": \"2109036097\",\\n            \"name\": \"Yue Wu\"\\n          },\\n          {\\n            \"authorId\": \"1471465327\",\\n            \"name\": \"Ted Spaide\"\\n          },\\n          {\\n            \"authorId\": \"11022483\",\\n            \"name\": \"R. Yanagihara\"\\n          },\\n          {\\n            \"authorId\": \"47702381\",\\n            \"name\": \"Y. Kihara\"\\n          },\\n          {\\n            \"authorId\": \"6894914\",\\n            \"name\": \"Mark E. Clark\"\\n          },\\n          {\\n            \"authorId\": \"48882557\",\\n            \"name\": \"M. Kwon\"\\n          },\\n          {\\n            \"authorId\": \"144561682\",\\n            \"name\": \"C. Owsley\"\\n          },\\n          {\\n            \"authorId\": \"4477577\",\\n            \"name\": \"C. Curcio\"\\n          }\\n        ],\\n        \"abstract\": \"Purpose Delayed rod-mediated dark adaptation (RMDA) is a functional biomarker for incipient age-related macular degeneration (AMD). We used anatomically restricted spectral domain optical coherence tomography (SD-OCT) imaging data to localize de novo imaging features associated with and to test hypotheses about delayed RMDA. Methods Rod intercept time (RIT) was measured in participants with and without AMD at 5 degrees from the fovea, and macular SD-OCT images were obtained. A deep learning model was trained with anatomically restricted information using a single representative B-scan through the fovea of each eye. Mean-occlusion masking was utilized to isolate the relevant imaging features. Results The model identified hyporeflective outer retinal bands on macular SD-OCT associated with delayed RMDA. The validation mean standard error (MSE) registered to the foveal B-scan localized the lowest error to 0.5 mm temporal to the fovea center, within an overall low-error region across the rod-free zone and adjoining parafovea. Mean absolute error (MAE) on the test set was 4.71 minutes (8.8% of the dynamic range). Conclusions We report a novel framework for imaging biomarker discovery using deep learning and demonstrate its ability to identify and localize a previously undescribed biomarker in retinal imaging. The hyporeflective outer retinal bands in central macula on SD-OCT demonstrate a structural basis for dysfunctional rod vision that correlates to published histopathologic findings. Translational Relevance This agnostic approach to anatomic biomarker discovery strengthens the rationale for RMDA as an outcome measure in early AMD clinical trials, and also expands the utility of deep learning beyond automated diagnosis to fundamental discovery.\"\\n      },\\n      {\\n        \"paperId\": \"9752a2e33d76d3b2fd91c0285b21d8a6db5bcef4\",\\n        \"url\": \"https://www.semanticscholar.org/paper/9752a2e33d76d3b2fd91c0285b21d8a6db5bcef4\",\\n        \"title\": \"Effective Adaptation in Multi-Task Co-Training for Unified Autonomous Driving\",\\n        \"citationCount\": 46,\\n        \"openAccessPdf\": {\\n          \"url\": \"http://arxiv.org/pdf/2209.08953\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2209.08953, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A simple yet effective pretrain-adapt-finetune paradigm for general multi-task training, where the off-the-shelf pretrained models can be effectively adapted without increasing the training overhead is proposed.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2022-09-19\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"51291599\",\\n            \"name\": \"Xiwen Liang\"\\n          },\\n          {\\n            \"authorId\": \"51255576\",\\n            \"name\": \"Yangxin Wu\"\\n          },\\n          {\\n            \"authorId\": \"47180442\",\\n            \"name\": \"Jianhua Han\"\\n          },\\n          {\\n            \"authorId\": \"2143534132\",\\n            \"name\": \"Hang Xu\"\\n          },\\n          {\\n            \"authorId\": \"1691522\",\\n            \"name\": \"Chunjing Xu\"\\n          },\\n          {\\n            \"authorId\": \"2153397698\",\\n            \"name\": \"Xiaodan Liang\"\\n          }\\n        ],\\n        \"abstract\": \"Aiming towards a holistic understanding of multiple downstream tasks simultaneously, there is a need for extracting features with better transferability. Though many latest self-supervised pre-training methods have achieved impressive performance on various vision tasks under the prevailing pretrain-finetune paradigm, their generalization capacity to multi-task learning scenarios is yet to be explored. In this paper, we extensively investigate the transfer performance of various types of self-supervised methods, e.g., MoCo and SimCLR, on three downstream tasks, including semantic segmentation, drivable area segmentation, and traffic object detection, on the large-scale driving dataset BDD100K. We surprisingly find that their performances are sub-optimal or even lag far behind the single-task baseline, which may be due to the distinctions of training objectives and architectural design lied in the pretrain-finetune paradigm. To overcome this dilemma as well as avoid redesigning the resource-intensive pre-training stage, we propose a simple yet effective pretrain-adapt-finetune paradigm for general multi-task training, where the off-the-shelf pretrained models can be effectively adapted without increasing the training overhead. During the adapt stage, we utilize learnable multi-scale adapters to dynamically adjust the pretrained model weights supervised by multi-task objectives while leaving the pretrained knowledge untouched. Furthermore, we regard the vision-language pre-training model CLIP as a strong complement to the pretrain-adapt-finetune paradigm and propose a novel adapter named LV-Adapter, which incorporates language priors in the multi-task model via task-specific prompting and alignment between visual and textual features.\"\\n      },\\n      {\\n        \"paperId\": \"4001af48eb49bf367a2c0428fd66feec6db54180\",\\n        \"url\": \"https://www.semanticscholar.org/paper/4001af48eb49bf367a2c0428fd66feec6db54180\",\\n        \"title\": \"MEVG: Multi-event Video Generation with Text-to-Video Models\",\\n        \"citationCount\": 33,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2312.04086, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A last frame-aware diffusion process to preserve visual coherence between consecutive videos where each video consists of different events by initializing the latent and simultaneously adjusting noise in the latent to enhance the motion dynamic in a generated video.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2023-12-07\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2162960534\",\\n            \"name\": \"Gyeongrok Oh\"\\n          },\\n          {\\n            \"authorId\": \"2272756256\",\\n            \"name\": \"Jaehwan Jeong\"\\n          },\\n          {\\n            \"authorId\": \"2311572749\",\\n            \"name\": \"Sieun Kim\"\\n          },\\n          {\\n            \"authorId\": \"145965455\",\\n            \"name\": \"Wonmin Byeon\"\\n          },\\n          {\\n            \"authorId\": \"2239053726\",\\n            \"name\": \"Jinkyu Kim\"\\n          },\\n          {\\n            \"authorId\": \"2311578473\",\\n            \"name\": \"Sungwoong Kim\"\\n          },\\n          {\\n            \"authorId\": \"2142668751\",\\n            \"name\": \"Sangpil Kim\"\\n          }\\n        ],\\n        \"abstract\": \"We introduce a novel diffusion-based video generation method, generating a video showing multiple events given multiple individual sentences from the user. Our method does not require a large-scale video dataset since our method uses a pre-trained diffusion-based text-to-video generative model without a fine-tuning process. Specifically, we propose a last frame-aware diffusion process to preserve visual coherence between consecutive videos where each video consists of different events by initializing the latent and simultaneously adjusting noise in the latent to enhance the motion dynamic in a generated video. Furthermore, we find that the iterative update of latent vectors by referring to all the preceding frames maintains the global appearance across the frames in a video clip. To handle dynamic text input for video generation, we utilize a novel prompt generator that transfers course text messages from the user into the multiple optimal prompts for the text-to-video diffusion model. Extensive experiments and user studies show that our proposed method is superior to other video-generative models in terms of temporal coherency of content and semantics. Video examples are available on our project page: https://kuai-lab.github.io/eccv2024mevg.\"\\n      },\\n      {\\n        \"paperId\": \"a6f2f252c88983965f60dfa8325afcd2eed03eb1\",\\n        \"url\": \"https://www.semanticscholar.org/paper/a6f2f252c88983965f60dfa8325afcd2eed03eb1\",\\n        \"title\": \"SAM-OCTA: A Fine-Tuning Strategy for Applying Foundation Model OCTA Image Segmentation Tasks\",\\n        \"citationCount\": 22,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://arxiv.org/pdf/2309.11758\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2309.11758, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"The low-rank adaptation technique is adopted for foundation model fine-tuning and corresponding prompt point generation strategies to process various segmentation tasks on OCTA datasets to achieve state-of-the-art performance metrics.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2023-09-21\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2243437545\",\\n            \"name\": \"Chengliang Wang\"\\n          },\\n          {\\n            \"authorId\": \"2243298334\",\\n            \"name\": \"Xinrun Chen\"\\n          },\\n          {\\n            \"authorId\": \"2216447644\",\\n            \"name\": \"Haojian Ning\"\\n          },\\n          {\\n            \"authorId\": \"48831702\",\\n            \"name\": \"Shiying Li\"\\n          }\\n        ],\\n        \"abstract\": \"In the analysis of optical coherence tomography angiography (OCTA) images, the operation of segmenting specific targets is necessary. Existing methods typically train on supervised datasets with limited samples (approximately a few hundred), which can lead to overfitting. To address this, the low-rank adaptation technique is adopted for foundation model fine-tuning and proposed corresponding prompt point generation strategies to process various segmentation tasks on OCTA datasets. This method is named SAM-OCTA and has been experimented on the publicly available OCTA-500 dataset. While achieving state-of-the-art performance metrics, this method accomplishes local vessel segmentation as well as effective artery-vein segmentation, which was not well-solved in previous works. The code is available at: https://github.com/ShellRedia/SAM-OCTA.\"\\n      },\\n      {\\n        \"paperId\": \"7cf6085c39c60cbc45cd06aaa70242069828fda9\",\\n        \"url\": \"https://www.semanticscholar.org/paper/7cf6085c39c60cbc45cd06aaa70242069828fda9\",\\n        \"title\": \"Prompt-Based Multi-Modal Image Segmentation\",\\n        \"citationCount\": 17,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work proposes a system that can generate image segmentations based on arbitrary prompts at test time with a transformer-based decoder that enables dense prediction and allows for dynamic adaptation to generalized queries involving affordances or properties.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": null,\\n        \"authors\": [\\n          {\\n            \"authorId\": \"73235537\",\\n            \"name\": \"Timo L\\\\u00fcddecke\"\\n          },\\n          {\\n            \"authorId\": \"1746183\",\\n            \"name\": \"Alexander S. Ecker\"\\n          }\\n        ],\\n        \"abstract\": null\\n      },\\n      {\\n        \"paperId\": \"2d0b030d314a5aa8feaa03695e8471270130bdf9\",\\n        \"url\": \"https://www.semanticscholar.org/paper/2d0b030d314a5aa8feaa03695e8471270130bdf9\",\\n        \"title\": \"Imagine while Reasoning in Space: Multimodal Visualization-of-Thought\",\\n        \"citationCount\": 90,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2501.07542, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work proposes a new reasoning paradigm, Multimodal Visualization-of-Thought (MVoT), which enables visual thinking in MLLMs by generating image visualizations of their reasoning traces and introduces token discrepancy loss into autoregressive MLLMs to ensure high-quality visualization.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2025-01-13\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2155795167\",\\n            \"name\": \"Chengzu Li\"\\n          },\\n          {\\n            \"authorId\": \"51198241\",\\n            \"name\": \"Wenshan Wu\"\\n          },\\n          {\\n            \"authorId\": \"2339967968\",\\n            \"name\": \"Huanyu Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2258547658\",\\n            \"name\": \"Yan Xia\"\\n          },\\n          {\\n            \"authorId\": \"2273419590\",\\n            \"name\": \"Shaoguang Mao\"\\n          },\\n          {\\n            \"authorId\": \"2294850817\",\\n            \"name\": \"Li Dong\"\\n          },\\n          {\\n            \"authorId\": \"2339667880\",\\n            \"name\": \"Ivan Vuli\\'c\"\\n          },\\n          {\\n            \"authorId\": \"2249539478\",\\n            \"name\": \"Furu Wei\"\\n          }\\n        ],\\n        \"abstract\": \"Chain-of-Thought (CoT) prompting has proven highly effective for enhancing complex reasoning in Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs). Yet, it struggles in complex spatial reasoning tasks. Nonetheless, human cognition extends beyond language alone, enabling the remarkable capability to think in both words and images. Inspired by this mechanism, we propose a new reasoning paradigm, Multimodal Visualization-of-Thought (MVoT). It enables visual thinking in MLLMs by generating image visualizations of their reasoning traces. To ensure high-quality visualization, we introduce token discrepancy loss into autoregressive MLLMs. This innovation significantly improves both visual coherence and fidelity. We validate this approach through several dynamic spatial reasoning tasks. Experimental results reveal that MVoT demonstrates competitive performance across tasks. Moreover, it exhibits robust and reliable improvements in the most challenging scenarios where CoT fails. Ultimately, MVoT establishes new possibilities for complex reasoning tasks where visual thinking can effectively complement verbal reasoning.\"\\n      },\\n      {\\n        \"paperId\": \"06cb9838ae2e386360aaacbf50775e164a741bd9\",\\n        \"url\": \"https://www.semanticscholar.org/paper/06cb9838ae2e386360aaacbf50775e164a741bd9\",\\n        \"title\": \"AdaCLIP: Adapting CLIP with Hybrid Learnable Prompts for Zero-Shot Anomaly Detection\",\\n        \"citationCount\": 108,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2407.15795, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This study introduces AdaCLIP for the ZSAD task, leveraging a pre-trained vision-language model (VLM), CLIP, and incorporates learnable prompts into CLIP and optimizes them through training on auxiliary annotated anomaly detection data.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-07-22\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2165889312\",\\n            \"name\": \"Yunkang Cao\"\\n          },\\n          {\\n            \"authorId\": \"2281792059\",\\n            \"name\": \"Jiangning Zhang\"\\n          },\\n          {\\n            \"authorId\": \"1788584596\",\\n            \"name\": \"Luca Frittoli\"\\n          },\\n          {\\n            \"authorId\": \"2257750846\",\\n            \"name\": \"Yuqi Cheng\"\\n          },\\n          {\\n            \"authorId\": \"2257204312\",\\n            \"name\": \"Weiming Shen\"\\n          },\\n          {\\n            \"authorId\": \"2319130369\",\\n            \"name\": \"Giacomo Boracchi\"\\n          }\\n        ],\\n        \"abstract\": \"Zero-shot anomaly detection (ZSAD) targets the identification of anomalies within images from arbitrary novel categories. This study introduces AdaCLIP for the ZSAD task, leveraging a pre-trained vision-language model (VLM), CLIP. AdaCLIP incorporates learnable prompts into CLIP and optimizes them through training on auxiliary annotated anomaly detection data. Two types of learnable prompts are proposed: static and dynamic. Static prompts are shared across all images, serving to preliminarily adapt CLIP for ZSAD. In contrast, dynamic prompts are generated for each test image, providing CLIP with dynamic adaptation capabilities. The combination of static and dynamic prompts is referred to as hybrid prompts, and yields enhanced ZSAD performance. Extensive experiments conducted across 14 real-world anomaly detection datasets from industrial and medical domains indicate that AdaCLIP outperforms other ZSAD methods and can generalize better to different categories and even domains. Finally, our analysis highlights the importance of diverse auxiliary data and optimized prompts for enhanced generalization capacity. Code is available at https://github.com/caoyunkang/AdaCLIP.\"\\n      },\\n      {\\n        \"paperId\": \"8da668b01beb344624e8c47ee5b2110da0f339dd\",\\n        \"url\": \"https://www.semanticscholar.org/paper/8da668b01beb344624e8c47ee5b2110da0f339dd\",\\n        \"title\": \"VideoJAM: Joint Appearance-Motion Representations for Enhanced Motion Generation in Video Models\",\\n        \"citationCount\": 42,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2502.02492, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"VideoJAM is a novel framework that instills an effective motion prior to video generators, by encouraging the model to learn a joint appearance-motion representation, and introduces Inner-Guidance, a mechanism that steers the generation toward coherent motion by leveraging the model\\'s own evolving motion prediction as a dynamic guidance signal.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2025-02-04\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2038268012\",\\n            \"name\": \"Hila Chefer\"\\n          },\\n          {\\n            \"authorId\": \"88622696\",\\n            \"name\": \"Uriel Singer\"\\n          },\\n          {\\n            \"authorId\": \"2266842423\",\\n            \"name\": \"Amit Zohar\"\\n          },\\n          {\\n            \"authorId\": \"2044194129\",\\n            \"name\": \"Yuval Kirstain\"\\n          },\\n          {\\n            \"authorId\": \"33964593\",\\n            \"name\": \"Adam Polyak\"\\n          },\\n          {\\n            \"authorId\": \"2188620\",\\n            \"name\": \"Yaniv Taigman\"\\n          },\\n          {\\n            \"authorId\": \"2336960746\",\\n            \"name\": \"Lior Wolf\"\\n          },\\n          {\\n            \"authorId\": \"2086827528\",\\n            \"name\": \"Shelly Sheynin\"\\n          }\\n        ],\\n        \"abstract\": \"Despite tremendous recent progress, generative video models still struggle to capture real-world motion, dynamics, and physics. We show that this limitation arises from the conventional pixel reconstruction objective, which biases models toward appearance fidelity at the expense of motion coherence. To address this, we introduce VideoJAM, a novel framework that instills an effective motion prior to video generators, by encouraging the model to learn a joint appearance-motion representation. VideoJAM is composed of two complementary units. During training, we extend the objective to predict both the generated pixels and their corresponding motion from a single learned representation. During inference, we introduce Inner-Guidance, a mechanism that steers the generation toward coherent motion by leveraging the model\\'s own evolving motion prediction as a dynamic guidance signal. Notably, our framework can be applied to any video model with minimal adaptations, requiring no modifications to the training data or scaling of the model. VideoJAM achieves state-of-the-art performance in motion coherence, surpassing highly competitive proprietary models while also enhancing the perceived visual quality of the generations. These findings emphasize that appearance and motion can be complementary and, when effectively integrated, enhance both the visual quality and the coherence of video generation. Project website: https://hila-chefer.github.io/videojam-paper.github.io/\"\\n      },\\n      {\\n        \"paperId\": \"6851fb7196c27fdec6b787d4196f0a1c1c279e96\",\\n        \"url\": \"https://www.semanticscholar.org/paper/6851fb7196c27fdec6b787d4196f0a1c1c279e96\",\\n        \"title\": \"DialogGen: Multi-modal Interactive Dialogue System for Multi-turn Text-to-Image Generation\",\\n        \"citationCount\": 18,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2403.08857, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"The Multi-modal Dialogue Benchmark (DialogBen) is introduced, a comprehensive bilingual benchmark designed to assess the ability of MLLMs to generate accurate and coherent multi-modal content that supports image editing and contains two evaluation metrics to measure the model\\'s ability to switch modalities and the coherence of the output images.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-03-13\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2162225343\",\\n            \"name\": \"Minbin Huang\"\\n          },\\n          {\\n            \"authorId\": \"2072953799\",\\n            \"name\": \"Yanxin Long\"\\n          },\\n          {\\n            \"authorId\": \"2291203760\",\\n            \"name\": \"Xinchi Deng\"\\n          },\\n          {\\n            \"authorId\": \"2291135101\",\\n            \"name\": \"Ruihang Chu\"\\n          },\\n          {\\n            \"authorId\": \"2291439620\",\\n            \"name\": \"Jiangfeng Xiong\"\\n          },\\n          {\\n            \"authorId\": \"2291389227\",\\n            \"name\": \"Xiaodan Liang\"\\n          },\\n          {\\n            \"authorId\": \"2290858486\",\\n            \"name\": \"Hong Cheng\"\\n          },\\n          {\\n            \"authorId\": \"2268722917\",\\n            \"name\": \"Qinglin Lu\"\\n          },\\n          {\\n            \"authorId\": \"2269146248\",\\n            \"name\": \"Wei Liu\"\\n          }\\n        ],\\n        \"abstract\": \"Text-to-image (T2I) generation models have significantly advanced in recent years. However, effective interaction with these models is challenging for average users due to the need for specialized prompt engineering knowledge and the inability to perform multi-turn image generation, hindering a dynamic and iterative creation process. Recent attempts have tried to equip Multi-modal Large Language Models (MLLMs) with T2I models to bring the user\\'s natural language instructions into reality. Hence, the output modality of MLLMs is extended, and the multi-turn generation quality of T2I models is enhanced thanks to the strong multi-modal comprehension ability of MLLMs. However, many of these works face challenges in identifying correct output modalities and generating coherent images accordingly as the number of output modalities increases and the conversations go deeper. Therefore, we propose DialogGen, an effective pipeline to align off-the-shelf MLLMs and T2I models to build a Multi-modal Interactive Dialogue System (MIDS) for multi-turn Text-to-Image generation. It is composed of drawing prompt alignment, careful training data curation, and error correction. Moreover, as the field of MIDS flourishes, comprehensive benchmarks are urgently needed to evaluate MIDS fairly in terms of output modality correctness and multi-modal output coherence. To address this issue, we introduce the Multi-modal Dialogue Benchmark (DialogBen), a comprehensive bilingual benchmark designed to assess the ability of MLLMs to generate accurate and coherent multi-modal content that supports image editing. It contains two evaluation metrics to measure the model\\'s ability to switch modalities and the coherence of the output images. Our extensive experiments on DialogBen and user study demonstrate the effectiveness of DialogGen compared with other State-of-the-Art models.\"\\n      },\\n      {\\n        \"paperId\": \"fff1e9eb32f8591aea3f2db54035465093381b89\",\\n        \"url\": \"https://www.semanticscholar.org/paper/fff1e9eb32f8591aea3f2db54035465093381b89\",\\n        \"title\": \"Selective Prompting Tuning for Personalized Conversations with LLMs\",\\n        \"citationCount\": 13,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2406.18187, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work initializes a set of soft prompts and uses a trainable dense retriever to adaptively select suitable soft prompts for LLMs according to different input contexts, where the prompt retriever is dynamically updated through feedback from the LLMs.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-06-26\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2274015208\",\\n            \"name\": \"Qiushi Huang\"\\n          },\\n          {\\n            \"authorId\": \"2110814131\",\\n            \"name\": \"Xubo Liu\"\\n          },\\n          {\\n            \"authorId\": \"2273320275\",\\n            \"name\": \"Tom Ko\"\\n          },\\n          {\\n            \"authorId\": \"49814531\",\\n            \"name\": \"Boyong Wu\"\\n          },\\n          {\\n            \"authorId\": \"2239051433\",\\n            \"name\": \"Wenwu Wang\"\\n          },\\n          {\\n            \"authorId\": \"2273525536\",\\n            \"name\": \"Yu Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2189113746\",\\n            \"name\": \"Lilian Tang\"\\n          }\\n        ],\\n        \"abstract\": \"In conversational AI, personalizing dialogues with persona profiles and contextual understanding is essential. Despite large language models\\' (LLMs) improved response coherence, effective persona integration remains a challenge. In this work, we first study two common approaches for personalizing LLMs: textual prompting and direct fine-tuning. We observed that textual prompting often struggles to yield responses that are similar to the ground truths in datasets, while direct fine-tuning tends to produce repetitive or overly generic replies. To alleviate those issues, we propose \\\\\\\\textbf{S}elective \\\\\\\\textbf{P}rompt \\\\\\\\textbf{T}uning (SPT), which softly prompts LLMs for personalized conversations in a selective way. Concretely, SPT initializes a set of soft prompts and uses a trainable dense retriever to adaptively select suitable soft prompts for LLMs according to different input contexts, where the prompt retriever is dynamically updated through feedback from the LLMs. Additionally, we propose context-prompt contrastive learning and prompt fusion learning to encourage the SPT to enhance the diversity of personalized conversations. Experiments on the CONVAI2 dataset demonstrate that SPT significantly enhances response diversity by up to 90\\\\\\\\%, along with improvements in other critical performance indicators. Those results highlight the efficacy of SPT in fostering engaging and personalized dialogue generation. The SPT model code (https://github.com/hqsiswiliam/SPT) is publicly available for further exploration.\"\\n      },\\n      {\\n        \"paperId\": \"5139b7a2744749517163dcfdccf13588aab287bf\",\\n        \"url\": \"https://www.semanticscholar.org/paper/5139b7a2744749517163dcfdccf13588aab287bf\",\\n        \"title\": \"Transformer-Squared: Self-adaptive LLMs\",\\n        \"citationCount\": 10,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2501.06252, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"Transformer-Squared is introduced, a novel self-adaptation framework that adapts LLMs for unseen tasks in real-time by selectively adjusting only the singular components of their weight matrices, and demonstrates versatility across different LLM architectures and modalities, including vision-language tasks.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2025-01-09\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2326487319\",\\n            \"name\": \"Qi Sun\"\\n          },\\n          {\\n            \"authorId\": \"2326294214\",\\n            \"name\": \"Edoardo Cetin\"\\n          },\\n          {\\n            \"authorId\": \"2326422916\",\\n            \"name\": \"Yujin Tang\"\\n          }\\n        ],\\n        \"abstract\": \"Self-adaptive large language models (LLMs) aim to solve the challenges posed by traditional fine-tuning methods, which are often computationally intensive and static in their ability to handle diverse tasks. We introduce Transformer-Squared, a novel self-adaptation framework that adapts LLMs for unseen tasks in real-time by selectively adjusting only the singular components of their weight matrices. During inference, Transformer-Squared employs a two-pass mechanism: first, a dispatch system identifies the task properties, and then task-specific \\'expert\\' vectors, trained using reinforcement learning, are dynamically mixed to obtain targeted behavior for the incoming prompt. Our method consistently outperforms ubiquitous approaches such as LoRA, with fewer parameters and greater efficiency. Furthermore, Transformer-Squared demonstrates versatility across different LLM architectures and modalities, including vision-language tasks. Transformer-Squared represents a significant leap forward, offering a scalable, efficient solution for enhancing the adaptability and task-specific performance of LLMs, paving the way for truly dynamic, self-organizing AI systems.\"\\n      },\\n      {\\n        \"paperId\": \"77711615485303032fc878d495943139ab2558e1\",\\n        \"url\": \"https://www.semanticscholar.org/paper/77711615485303032fc878d495943139ab2558e1\",\\n        \"title\": \"Vivid-ZOO: Multi-View Video Generation with Diffusion Model\",\\n        \"citationCount\": 30,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2406.08659, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A novel diffusion-based pipeline is proposed that generates high-quality multi-view videos centered around a dynamic 3D object from text, exhibiting vivid motions, temporal coherence, and multi-view consistency, given a variety of text prompts.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-06-12\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2287925633\",\\n            \"name\": \"Bing Li\"\\n          },\\n          {\\n            \"authorId\": \"2306182343\",\\n            \"name\": \"Cheng Zheng\"\\n          },\\n          {\\n            \"authorId\": \"2306084700\",\\n            \"name\": \"Wenxuan Zhu\"\\n          },\\n          {\\n            \"authorId\": \"2005711751\",\\n            \"name\": \"Jinjie Mai\"\\n          },\\n          {\\n            \"authorId\": \"2129438190\",\\n            \"name\": \"Biao Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2262444458\",\\n            \"name\": \"Peter Wonka\"\\n          },\\n          {\\n            \"authorId\": \"2288557868\",\\n            \"name\": \"Bernard Ghanem\"\\n          }\\n        ],\\n        \"abstract\": \"While diffusion models have shown impressive performance in 2D image/video generation, diffusion-based Text-to-Multi-view-Video (T2MVid) generation remains underexplored. The new challenges posed by T2MVid generation lie in the lack of massive captioned multi-view videos and the complexity of modeling such multi-dimensional distribution. To this end, we propose a novel diffusion-based pipeline that generates high-quality multi-view videos centered around a dynamic 3D object from text. Specifically, we factor the T2MVid problem into viewpoint-space and time components. Such factorization allows us to combine and reuse layers of advanced pre-trained multi-view image and 2D video diffusion models to ensure multi-view consistency as well as temporal coherence for the generated multi-view videos, largely reducing the training cost. We further introduce alignment modules to align the latent spaces of layers from the pre-trained multi-view and the 2D video diffusion models, addressing the reused layers\\' incompatibility that arises from the domain gap between 2D and multi-view data. In support of this and future research, we further contribute a captioned multi-view video dataset. Experimental results demonstrate that our method generates high-quality multi-view videos, exhibiting vivid motions, temporal coherence, and multi-view consistency, given a variety of text prompts.\"\\n      },\\n      {\\n        \"paperId\": \"4be040c953d8580f0127105924a8bbf139c4804d\",\\n        \"url\": \"https://www.semanticscholar.org/paper/4be040c953d8580f0127105924a8bbf139c4804d\",\\n        \"title\": \"Unbounded: A Generative Infinite Game of Character Life Simulation\",\\n        \"citationCount\": 11,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2410.18975, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A specialized, distilled large language model (LLM) that dynamically generates game mechanics, narratives, and character interactions in real-time, and a new dynamic regional image prompt Adapter (IP-Adapter) for vision models that ensures consistent yet flexible visual generation of a character across multiple environments.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-10-24\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2108961694\",\\n            \"name\": \"Jialu Li\"\\n          },\\n          {\\n            \"authorId\": \"2167749913\",\\n            \"name\": \"Yuanzhen Li\"\\n          },\\n          {\\n            \"authorId\": \"2248172496\",\\n            \"name\": \"Neal Wadhwa\"\\n          },\\n          {\\n            \"authorId\": \"1782328\",\\n            \"name\": \"Y. Pritch\"\\n          },\\n          {\\n            \"authorId\": \"2248055731\",\\n            \"name\": \"David E. Jacobs\"\\n          },\\n          {\\n            \"authorId\": \"2248163830\",\\n            \"name\": \"Michael Rubinstein\"\\n          },\\n          {\\n            \"authorId\": \"2276608813\",\\n            \"name\": \"Mohit Bansal\"\\n          },\\n          {\\n            \"authorId\": \"2248173214\",\\n            \"name\": \"Nataniel Ruiz\"\\n          }\\n        ],\\n        \"abstract\": \"We introduce the concept of a generative infinite game, a video game that transcends the traditional boundaries of finite, hard-coded systems by using generative models. Inspired by James P. Carse\\'s distinction between finite and infinite games, we leverage recent advances in generative AI to create Unbounded: a game of character life simulation that is fully encapsulated in generative models. Specifically, Unbounded draws inspiration from sandbox life simulations and allows you to interact with your autonomous virtual character in a virtual world by feeding, playing with and guiding it - with open-ended mechanics generated by an LLM, some of which can be emergent. In order to develop Unbounded, we propose technical innovations in both the LLM and visual generation domains. Specifically, we present: (1) a specialized, distilled large language model (LLM) that dynamically generates game mechanics, narratives, and character interactions in real-time, and (2) a new dynamic regional image prompt Adapter (IP-Adapter) for vision models that ensures consistent yet flexible visual generation of a character across multiple environments. We evaluate our system through both qualitative and quantitative analysis, showing significant improvements in character life simulation, user instruction following, narrative coherence, and visual consistency for both characters and the environments compared to traditional related approaches.\"\\n      }\\n    ]\\n  },\\n  \"long context dialogue\": {\\n    \"message\": \"Too Many Requests. Please wait and try again or apply for a key for higher rate limits. https://www.semanticscholar.org/product/api#api-key-form\",\\n    \"code\": \"429\"\\n  },\\n  \"adaptive prompt generation narrative\": {\\n    \"total\": 410,\\n    \"offset\": 0,\\n    \"next\": 50,\\n    \"data\": [\\n      {\\n        \"paperId\": \"ef399ed62fcc00e73b02f286012080351652693c\",\\n        \"url\": \"https://www.semanticscholar.org/paper/ef399ed62fcc00e73b02f286012080351652693c\",\\n        \"title\": \"Prompt-prompted Adaptive Structured Pruning for Efficient LLM Generation\",\\n        \"citationCount\": 15,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2404.01365, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A novel training-free and calibration-free method that selects unique FF experts at the sequence level for efficient generation across a plethora of LLMs with different non-ReLU activation functions, which is possible due to a critical observation that many trained LLMs naturally produce highly structured FF activation patterns within a sequence, which the authors call flocking.\"\\n        },\\n        \"publicationTypes\": null,\\n        \"publicationDate\": \"2024-04-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2171964328\",\\n            \"name\": \"Harry Dong\"\\n          },\\n          {\\n            \"authorId\": \"2282555057\",\\n            \"name\": \"Beidi Chen\"\\n          },\\n          {\\n            \"authorId\": \"2284063779\",\\n            \"name\": \"Yuejie Chi\"\\n          }\\n        ],\\n        \"abstract\": \"With the development of transformer-based large language models (LLMs), they have been applied to many fields due to their remarkable utility, but this comes at a considerable computational cost at deployment. Fortunately, some methods such as pruning or constructing a mixture of experts (MoE) aim at exploiting sparsity in transformer feedforward (FF) blocks to gain boosts in speed and reduction in memory requirements. However, these techniques can be very costly and inflexible in practice, as they often require training or are restricted to specific types of architectures. To address this, we introduce GRIFFIN, a novel training-free and calibration-free method that selects unique FF experts at the sequence level for efficient generation across a plethora of LLMs with different non-ReLU activation functions. This is possible due to a critical observation that many trained LLMs naturally produce highly structured FF activation patterns within a sequence, which we call flocking. Despite our method\\'s simplicity, we show with 50% of the FF parameters, GRIFFIN maintains the original model\\'s performance with little to no degradation on a variety of classification and generation tasks, all while improving latency (e.g. 1.29$\\\\\\\\times$ and 1.25$\\\\\\\\times$ speed-ups in Gemma 7B and Llama 2 13B, respectively, on an NVIDIA L40). Code is available at https://github.com/hdong920/GRIFFIN.\"\\n      },\\n      {\\n        \"paperId\": \"bde9fc752842a2a91c70933169fb8cee2b81f8b2\",\\n        \"url\": \"https://www.semanticscholar.org/paper/bde9fc752842a2a91c70933169fb8cee2b81f8b2\",\\n        \"title\": \"Prompt Expansion for Adaptive Text-to-Image Generation\",\\n        \"citationCount\": 23,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2312.16720, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A human evaluation study shows that images generated through Prompt Expansion are more aesthetically pleasing and diverse than those generated by baseline methods.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2023-12-27\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2070966889\",\\n            \"name\": \"Siddhartha Datta\"\\n          },\\n          {\\n            \"authorId\": \"2276608298\",\\n            \"name\": \"Alexander Ku\"\\n          },\\n          {\\n            \"authorId\": \"2275054270\",\\n            \"name\": \"Deepak Ramachandran\"\\n          },\\n          {\\n            \"authorId\": \"2276610768\",\\n            \"name\": \"Peter Anderson\"\\n          }\\n        ],\\n        \"abstract\": \"Text-to-image generation models are powerful but difficult to use. Users craft specific prompts to get better images, though the images can be repetitive. This paper proposes a Prompt Expansion framework that helps users generate high-quality, diverse images with less effort. The Prompt Expansion model takes a text query as input and outputs a set of expanded text prompts that are optimized such that when passed to a text-to-image model, generates a wider variety of appealing images. We conduct a human evaluation study that shows that images generated through Prompt Expansion are more aesthetically pleasing and diverse than those generated by baseline methods. Overall, this paper presents a novel and effective approach to improving the text-to-image generation experience.\"\\n      },\\n      {\\n        \"paperId\": \"c36cb4a41369369d837ea170397f7818d02150dd\",\\n        \"url\": \"https://www.semanticscholar.org/paper/c36cb4a41369369d837ea170397f7818d02150dd\",\\n        \"title\": \"Soft Prompt Generation for Domain Generalization\",\\n        \"citationCount\": 23,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2404.19286, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A simple yet efficient method for the Domain Generalization (DG) task, namely Soft Prompt Generation (SPG), which consists of a two-stage training phase and an inference phase, aiming to incorporate the generative model domain knowledge.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-04-30\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2274938328\",\\n            \"name\": \"Shuanghao Bai\"\\n          },\\n          {\\n            \"authorId\": \"2290728057\",\\n            \"name\": \"Yuedi Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2275025483\",\\n            \"name\": \"Wanqi Zhou\"\\n          },\\n          {\\n            \"authorId\": \"7778036\",\\n            \"name\": \"Zhirong Luan\"\\n          },\\n          {\\n            \"authorId\": \"2275030348\",\\n            \"name\": \"Badong Chen\"\\n          }\\n        ],\\n        \"abstract\": \"Large pre-trained vision language models (VLMs) have shown impressive zero-shot ability on downstream tasks with manually designed prompt. To further adapt VLMs to downstream tasks, soft prompt is proposed to replace manually designed prompt, which undergoes fine-tuning based on specific domain data. Prior prompt learning methods primarily learn a fixed prompt or residuled prompt from training samples. However, the learned prompts lack diversity and ignore information about unseen domains. In this paper, we reframe the prompt learning framework from a generative perspective and propose a simple yet efficient method for the Domain Generalization (DG) task, namely Soft Prompt Generation (SPG). Specifically, SPG consists of a two-stage training phase and an inference phase. During the training phase, we introduce soft prompt label for each domain, aiming to incorporate the generative model domain knowledge. During the inference phase, the generator of the generative model is employed to obtain instance-specific soft prompts for the unseen target domain. Extensive experiments on five domain generalization benchmarks of three DG tasks demonstrate that SPG achieves state-of-the-art performance. The code is available at https://github.com/renytek13/Soft-Prompt-Generation-with-CGAN.\"\\n      },\\n      {\\n        \"paperId\": \"1d954600f17a7c3f16aa726a5eaa902d6851e808\",\\n        \"url\": \"https://www.semanticscholar.org/paper/1d954600f17a7c3f16aa726a5eaa902d6851e808\",\\n        \"title\": \"SAFREE: Training-Free and Adaptive Guard for Safe Text-to-Image And Video Generation\",\\n        \"citationCount\": 50,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2410.12761, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work proposes SAFREE, a novel, training-free approach for safe T2I and T2V, that does not alter the model\\'s weights and incorporates a novel self-validating filtering mechanism that dynamically adjusts the denoising steps when applying the filtered embeddings.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-10-16\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"13563486\",\\n            \"name\": \"Jaehong Yoon\"\\n          },\\n          {\\n            \"authorId\": \"2164249715\",\\n            \"name\": \"Shoubin Yu\"\\n          },\\n          {\\n            \"authorId\": \"2061083016\",\\n            \"name\": \"Vaidehi Patil\"\\n          },\\n          {\\n            \"authorId\": \"2267311471\",\\n            \"name\": \"Huaxiu Yao\"\\n          },\\n          {\\n            \"authorId\": \"2276608813\",\\n            \"name\": \"Mohit Bansal\"\\n          }\\n        ],\\n        \"abstract\": \"Recent advances in diffusion models have significantly enhanced their ability to generate high-quality images and videos, but they have also increased the risk of producing unsafe content. Existing unlearning/editing-based methods for safe generation remove harmful concepts from models but face several challenges: (1) They cannot instantly remove harmful concepts without training. (2) Their safe generation capabilities depend on collected training data. (3) They alter model weights, risking degradation in quality for content unrelated to toxic concepts. To address these, we propose SAFREE, a novel, training-free approach for safe T2I and T2V, that does not alter the model\\'s weights. Specifically, we detect a subspace corresponding to a set of toxic concepts in the text embedding space and steer prompt embeddings away from this subspace, thereby filtering out harmful content while preserving intended semantics. To balance the trade-off between filtering toxicity and preserving safe concepts, SAFREE incorporates a novel self-validating filtering mechanism that dynamically adjusts the denoising steps when applying the filtered embeddings. Additionally, we incorporate adaptive re-attention mechanisms within the diffusion latent space to selectively diminish the influence of features related to toxic concepts at the pixel level. In the end, SAFREE ensures coherent safety checking, preserving the fidelity, quality, and safety of the output. SAFREE achieves SOTA performance in suppressing unsafe content in T2I generation compared to training-free baselines and effectively filters targeted concepts while maintaining high-quality images. It also shows competitive results against training-based methods. We extend SAFREE to various T2I backbones and T2V tasks, showcasing its flexibility and generalization. SAFREE provides a robust and adaptable safeguard for ensuring safe visual generation.\"\\n      },\\n      {\\n        \"paperId\": \"37e1bc43d22da21d6be616522e3fe217bf7f3d8e\",\\n        \"url\": \"https://www.semanticscholar.org/paper/37e1bc43d22da21d6be616522e3fe217bf7f3d8e\",\\n        \"title\": \"Adapting to Distribution Shift by Visual Domain Prompt Generation\",\\n        \"citationCount\": 19,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2405.02797, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work proposes a domain-aware contrastive loss and employ meta-learning to facilitate domain knowledge extraction and outperforms previous work on 5 large-scale benchmarks including WILDS and DomainNet.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-05-05\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"35793956\",\\n            \"name\": \"Zhixiang Chi\"\\n          },\\n          {\\n            \"authorId\": \"2300096585\",\\n            \"name\": \"Li Gu\"\\n          },\\n          {\\n            \"authorId\": \"2300089295\",\\n            \"name\": \"Tao Zhong\"\\n          },\\n          {\\n            \"authorId\": \"2277793919\",\\n            \"name\": \"Huan Liu\"\\n          },\\n          {\\n            \"authorId\": \"1787848\",\\n            \"name\": \"Yuanhao Yu\"\\n          },\\n          {\\n            \"authorId\": \"2277598061\",\\n            \"name\": \"Konstantinos N. Plataniotis\"\\n          },\\n          {\\n            \"authorId\": \"2277695392\",\\n            \"name\": \"Yang Wang\"\\n          }\\n        ],\\n        \"abstract\": \"In this paper, we aim to adapt a model at test-time using a few unlabeled data to address distribution shifts. To tackle the challenges of extracting domain knowledge from a limited amount of data, it is crucial to utilize correlated information from pre-trained backbones and source domains. Previous studies fail to utilize recent foundation models with strong out-of-distribution generalization. Additionally, domain-centric designs are not flavored in their works. Furthermore, they employ the process of modelling source domains and the process of learning to adapt independently into disjoint training stages. In this work, we propose an approach on top of the pre-computed features of the foundation model. Specifically, we build a knowledge bank to learn the transferable knowledge from source domains. Conditioned on few-shot target data, we introduce a domain prompt generator to condense the knowledge bank into a domain-specific prompt. The domain prompt then directs the visual features towards a particular domain via a guidance module. Moreover, we propose a domain-aware contrastive loss and employ meta-learning to facilitate domain knowledge extraction. Extensive experiments are conducted to validate the domain knowledge extraction. The proposed method outperforms previous work on 5 large-scale benchmarks including WILDS and DomainNet.\"\\n      },\\n      {\\n        \"paperId\": \"ac4dc5ff917141f30b53314f765d30fb4b83c8b7\",\\n        \"url\": \"https://www.semanticscholar.org/paper/ac4dc5ff917141f30b53314f765d30fb4b83c8b7\",\\n        \"title\": \"EviPrompt: A Training-Free Evidential Prompt Generation Method for Adapting Segment Anything Model in Medical Images\",\\n        \"citationCount\": 14,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {\\'tldr\\'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/TIP.2024.3482175?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/TIP.2024.3482175, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": null,\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-10-22\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2145205189\",\\n            \"name\": \"Yinsong Xu\"\\n          },\\n          {\\n            \"authorId\": \"2266873357\",\\n            \"name\": \"Jiaqi Tang\"\\n          },\\n          {\\n            \"authorId\": \"2266387679\",\\n            \"name\": \"Aidong Men\"\\n          },\\n          {\\n            \"authorId\": \"2266583142\",\\n            \"name\": \"Qingchao Chen\"\\n          }\\n        ],\\n        \"abstract\": \"Medical image segmentation is a critical task in clinical applications. Recently, the Segment Anything Model (SAM) has demonstrated potential for natural image segmentation. However, the requirement for expert labour to provide prompts, and the domain gap between natural and medical images pose significant obstacles in adapting SAM to medical images. To overcome these challenges, this paper introduces a novel prompt generation method named EviPrompt. The proposed method requires only a single reference image-annotation pair, making it a training-free solution that significantly reduces the need for extensive labelling and computational resources. First, prompts are automatically generated based on the similarity between features of the reference and target images, and evidential learning is introduced to improve reliability. Then, to mitigate the impact of the domain gap, committee voting and inference-guided in-context learning are employed, generating prompts primarily based on human prior knowledge and reducing reliance on extracted semantic information. EviPrompt represents an efficient and robust approach to medical image segmentation. We evaluate it across a broad range of tasks and modalities, confirming its efficacy. The source code is available at https://github.com/SPIresearch/EviPrompt.\"\\n      },\\n      {\\n        \"paperId\": \"6ae06f2bf66f19637adc695bc47256eeb1635b10\",\\n        \"url\": \"https://www.semanticscholar.org/paper/6ae06f2bf66f19637adc695bc47256eeb1635b10\",\\n        \"title\": \"A Structured Narrative Prompt for Prompting Narratives from Large Language Models: Sentiment Assessment of ChatGPT-Generated Narratives and Real Tweets\",\\n        \"citationCount\": 22,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://www.mdpi.com/1999-5903/15/12/375/pdf?version=1700746546\",\\n          \"status\": \"GOLD\",\\n          \"license\": \"CCBY\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.3390/fi15120375?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.3390/fi15120375, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A structured narrative prompt for sending queries to LLMs, an experiment with the narrative generation process using OpenAI\\'s ChatGPT, and an assessment of statistical significance in the sentiment prevalence of the generated narratives compared to real tweets indicate significant promise in the utilization of LLMs for helping to connect a simulated agent\\\\u2019s experiences with real people.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2023-11-23\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2253939386\",\\n            \"name\": \"Christopher J. Lynch\"\\n          },\\n          {\\n            \"authorId\": \"2222695010\",\\n            \"name\": \"Erik J. Jensen\"\\n          },\\n          {\\n            \"authorId\": \"2268131942\",\\n            \"name\": \"Virginia Zamponi\"\\n          },\\n          {\\n            \"authorId\": \"2059262010\",\\n            \"name\": \"Kevin O\\'Brien\"\\n          },\\n          {\\n            \"authorId\": \"144563993\",\\n            \"name\": \"Erika F. Frydenlund\"\\n          },\\n          {\\n            \"authorId\": \"1442241538\",\\n            \"name\": \"Ross J. Gore\"\\n          }\\n        ],\\n        \"abstract\": \"Large language models (LLMs) excel in providing natural language responses that sound authoritative, reflect knowledge of the context area, and can present from a range of varied perspectives. Agent-based models and simulations consist of simulated agents that interact within a simulated environment to explore societal, social, and ethical, among other, problems. Simulated agents generate large volumes of data and discerning useful and relevant content is an onerous task. LLMs can help in communicating agents\\\\u2019 perspectives on key life events by providing natural language narratives. However, these narratives should be factual, transparent, and reproducible. Therefore, we present a structured narrative prompt for sending queries to LLMs, we experiment with the narrative generation process using OpenAI\\\\u2019s ChatGPT, and we assess statistically significant differences across 11 Positive and Negative Affect Schedule (PANAS) sentiment levels between the generated narratives and real tweets using chi-squared tests and Fisher\\\\u2019s exact tests. The narrative prompt structure effectively yields narratives with the desired components from ChatGPT. In four out of forty-four categories, ChatGPT generated narratives which have sentiment scores that were not discernibly different, in terms of statistical significance (alpha level \\\\u03b1=0.05), from the sentiment expressed in real tweets. Three outcomes are provided: (1) a list of benefits and challenges for LLMs in narrative generation; (2) a structured prompt for requesting narratives of an LLM chatbot based on simulated agents\\\\u2019 information; (3) an assessment of statistical significance in the sentiment prevalence of the generated narratives compared to real tweets. This indicates significant promise in the utilization of LLMs for helping to connect a simulated agent\\\\u2019s experiences with real people.\"\\n      },\\n      {\\n        \"paperId\": \"a05b33072a8330413947eab0833ed2fd21de4963\",\\n        \"url\": \"https://www.semanticscholar.org/paper/a05b33072a8330413947eab0833ed2fd21de4963\",\\n        \"title\": \"Compound Text-Guided Prompt Tuning via Image-Adaptive Cues\",\\n        \"citationCount\": 12,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2312.06401, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work introduces text supervision to the optimization of prompts, which enables two benefits: 1) releasing the model reliance on the pre-defined category names during inference, thereby enabling more flexible prompt generation; 2) reducing the number of inputs to the text encoder, which decreases GPU memory consumption significantly.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2023-12-11\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2273557458\",\\n            \"name\": \"Hao Tan\"\\n          },\\n          {\\n            \"authorId\": \"2273559489\",\\n            \"name\": \"Jun Li\"\\n          },\\n          {\\n            \"authorId\": \"2118764798\",\\n            \"name\": \"Yizhuang Zhou\"\\n          },\\n          {\\n            \"authorId\": \"2273589717\",\\n            \"name\": \"Jun Wan\"\\n          },\\n          {\\n            \"authorId\": \"2113457514\",\\n            \"name\": \"Zhen Lei\"\\n          },\\n          {\\n            \"authorId\": \"2274088311\",\\n            \"name\": \"Xiangyu Zhang\"\\n          }\\n        ],\\n        \"abstract\": \"Vision-Language Models (VLMs) such as CLIP have demonstrated remarkable generalization capabilities to downstream tasks. However, existing prompt tuning based frameworks need to parallelize learnable textual inputs for all categories, suffering from massive GPU memory consumption when there is a large number of categories in the target dataset. Moreover, previous works require to include category names within prompts, exhibiting subpar performance when dealing with ambiguous category names. To address these shortcomings, we propose Compound Text-Guided Prompt Tuning (TGP-T) that significantly reduces resource demand while achieving superior performance. We introduce text supervision to the optimization of prompts, which enables two benefits: 1) releasing the model reliance on the pre-defined category names during inference, thereby enabling more flexible prompt generation; 2) reducing the number of inputs to the text encoder, which decreases GPU memory consumption significantly. Specifically, we found that compound text supervisions, i.e., category-wise and content-wise, is highly effective, since they provide inter-class separability and capture intra-class variations, respectively. Moreover, we condition the prompt generation on visual features through a module called Bonder, which facilitates the alignment between prompts and visual features. Extensive experiments on few-shot recognition and domain generalization demonstrate that TGP-T achieves superior performance with consistently lower training costs. It reduces GPU memory usage by 93% and attains a 2.5% performance gain on 16-shot ImageNet. The code is available at https://github.com/EricTan7/TGP-T.\"\\n      },\\n      {\\n        \"paperId\": \"b53dba04b2518ebed943daa9ab58f19af81e2012\",\\n        \"url\": \"https://www.semanticscholar.org/paper/b53dba04b2518ebed943daa9ab58f19af81e2012\",\\n        \"title\": \"Efficient Model Personalization in Federated Learning via Client-Specific Prompt Generation\",\\n        \"citationCount\": 98,\\n        \"openAccessPdf\": {\\n          \"url\": \"http://arxiv.org/pdf/2308.15367\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2308.15367, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work proposes a novel personalized FL framework of client-specific Prompt Generation (pFedPG), which learns to deploy a personalized prompt generator at the server for producing client- specific visual prompts that efficiently adapts frozen backbones to local data distributions.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2023-08-29\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"41015732\",\\n            \"name\": \"Fu-En Yang\"\\n          },\\n          {\\n            \"authorId\": \"48586406\",\\n            \"name\": \"Chien-Yi Wang\"\\n          },\\n          {\\n            \"authorId\": \"2218628989\",\\n            \"name\": \"Yu-Chiang Frank Wang\"\\n          }\\n        ],\\n        \"abstract\": \"Federated learning (FL) emerges as a decentralized learning framework which trains models from multiple distributed clients without sharing their data to preserve privacy. Recently, large-scale pre-trained models (e.g., Vision Transformer) have shown a strong capability of deriving robust representations. However, the data heterogeneity among clients, the limited computation resources, and the communication bandwidth restrict the deployment of large-scale models in FL frameworks. To leverage robust representations from large-scale models while enabling efficient model personalization for heterogeneous clients, we propose a novel personalized FL framework of client-specific Prompt Generation (pFedPG), which learns to deploy a personalized prompt generator at the server for producing client-specific visual prompts that efficiently adapts frozen backbones to local data distributions. Our proposed framework jointly optimizes the stages of personalized prompt adaptation locally and personalized prompt generation globally. The former aims to train visual prompts that adapt foundation models to each client, while the latter observes local optimization directions to generate personalized prompts for all clients. Through extensive experiments on benchmark datasets, we show that our pFedPG is favorable against state-of-the-art personalized FL methods under various types of data heterogeneity, allowing computation and communication efficient model personalization.\"\\n      },\\n      {\\n        \"paperId\": \"d92a423e09804595c8a2e241f890f5a24d326bb5\",\\n        \"url\": \"https://www.semanticscholar.org/paper/d92a423e09804595c8a2e241f890f5a24d326bb5\",\\n        \"title\": \"Prompt-based Code Completion via Multi-Retrieval Augmented Generation\",\\n        \"citationCount\": 17,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://arxiv.org/pdf/2405.07530\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2405.07530, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"ProCC, a code completion framework leveraging prompt engineering and the contextual multi-armed bandits algorithm to flexibly incorporate and adapt to multiple perspectives of code, is proposed.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-05-13\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2176503078\",\\n            \"name\": \"Hanzhuo Tan\"\\n          },\\n          {\\n            \"authorId\": \"2290488730\",\\n            \"name\": \"Qi Luo\"\\n          },\\n          {\\n            \"authorId\": \"51181043\",\\n            \"name\": \"Lingixao Jiang\"\\n          },\\n          {\\n            \"authorId\": \"2301156520\",\\n            \"name\": \"Zizheng Zhan\"\\n          },\\n          {\\n            \"authorId\": \"2174027344\",\\n            \"name\": \"Jing Li\"\\n          },\\n          {\\n            \"authorId\": \"1557360433\",\\n            \"name\": \"Haotian Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2290557105\",\\n            \"name\": \"Yuqun Zhang\"\\n          }\\n        ],\\n        \"abstract\": \"Automated code completion, aiming at generating subsequent tokens from unfinished code, has significantly benefited from recent progress in pre-trained Large Language Models (LLMs). However, these models often suffer from coherence issues and hallucinations when dealing with complex code logic or extrapolating beyond their training data. Existing Retrieval Augmented Generation (RAG) techniques partially address these issues by retrieving relevant code with a separate encoding model where the retrieved snippet serves as contextual reference for code completion. However, their retrieval scope is subject to a singular perspective defined by the encoding model, which largely overlooks the complexity and diversity inherent in code semantics. To address this limitation, we propose ProCC, a code completion framework leveraging prompt engineering and the contextual multi-armed bandits algorithm to flexibly incorporate and adapt to multiple perspectives of code. ProCC first employs a prompt-based multi-retriever system which crafts prompt templates to elicit LLM knowledge to understand code semantics with multiple retrieval perspectives. Then, it adopts the adaptive retrieval selection algorithm to incorporate code similarity into the decision-making process to determine the most suitable retrieval perspective for the LLM to complete the code. Experimental results demonstrate that ProCC outperforms a widely-studied code completion technique RepoCoder by 7.92% on the public benchmark CCEval, 3.19% in HumanEval-Infilling, 2.80% on our collected open-source benchmark suite, and 4.48% on the private-domain benchmark suite collected from Kuaishou Technology in terms of Exact Match. ProCC also allows augmenting fine-tuned techniques in a plug-and-play manner, yielding an averaged 6.5% improvement over the fine-tuned model.\"\\n      },\\n      {\\n        \"paperId\": \"47b2e5c3e7a907d2b65b3f9b918f83532d12f41a\",\\n        \"url\": \"https://www.semanticscholar.org/paper/47b2e5c3e7a907d2b65b3f9b918f83532d12f41a\",\\n        \"title\": \"Automatic counter-narrative generation for hate speech in Spanish\",\\n        \"citationCount\": 16,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"The article shows that the use of GPT-3 outperforms other models in generating non-offensive and informative counter-narratives, which some-times present compelling arguments.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": null,\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2229887611\",\\n            \"name\": \"Mar\\\\u00eda Estrella Vallecillo Rodrguez\"\\n          },\\n          {\\n            \"authorId\": \"1799967\",\\n            \"name\": \"Arturo Montejo-R\\\\u00e1ez\"\\n          },\\n          {\\n            \"authorId\": \"51183850\",\\n            \"name\": \"M. T. M. Valdivia\"\\n          }\\n        ],\\n        \"abstract\": null\\n      },\\n      {\\n        \"paperId\": \"bc9f1025246efeed568650934b6e183415aca279\",\\n        \"url\": \"https://www.semanticscholar.org/paper/bc9f1025246efeed568650934b6e183415aca279\",\\n        \"title\": \"PREADD: Prefix-Adaptive Decoding for Controlled Text Generation\",\\n        \"citationCount\": 29,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://arxiv.org/pdf/2307.03214\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2307.03214, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"The proposed Prefix-Adaptive Decoding (PREADD), a flexible method for controlled text generation, outperforms not only prompting baselines, but also an auxiliary-expert control method, by 12% or more in relative gain on the authors\\' main metrics for each task.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2023-07-06\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2221313264\",\\n            \"name\": \"Jonathan Pei\"\\n          },\\n          {\\n            \"authorId\": \"1410652795\",\\n            \"name\": \"Kevin Yang\"\\n          },\\n          {\\n            \"authorId\": \"38666915\",\\n            \"name\": \"D. Klein\"\\n          }\\n        ],\\n        \"abstract\": \"We propose Prefix-Adaptive Decoding (PREADD), a flexible method for controlled text generation. Unlike existing methods that use auxiliary expert models to control for attributes, PREADD does not require an external model, instead relying on linearly combining output logits from multiple prompts. Specifically, PREADD contrasts the output logits generated using a raw prompt against those generated using a prefix-prepended prompt, enabling both positive and negative control with respect to any attribute encapsulated by the prefix. We evaluate PREADD on three tasks -- toxic output mitigation, gender bias reduction, and sentiment control -- and find that PREADD outperforms not only prompting baselines, but also an auxiliary-expert control method, by 12% or more in relative gain on our main metrics for each task.\"\\n      },\\n      {\\n        \"paperId\": \"5a4d1add108f2e6d8ccd9f6ac94ab3e0335db540\",\\n        \"url\": \"https://www.semanticscholar.org/paper/5a4d1add108f2e6d8ccd9f6ac94ab3e0335db540\",\\n        \"title\": \"Agents\\' Room: Narrative Generation through Multi-step Collaboration\",\\n        \"citationCount\": 31,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2410.02603, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"It is shown that Agents\\' Room generates stories that are preferred by expert evaluators over those produced by baseline systems by leveraging collaboration and specialization to decompose the complex story writing task into tractable components.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-10-03\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2174667321\",\\n            \"name\": \"Fantine Huot\"\\n          },\\n          {\\n            \"authorId\": \"23181472\",\\n            \"name\": \"Reinald Kim Amplayo\"\\n          },\\n          {\\n            \"authorId\": \"52578817\",\\n            \"name\": \"J. Palomaki\"\\n          },\\n          {\\n            \"authorId\": \"1411178896\",\\n            \"name\": \"Alice Shoshana Jakobovits\"\\n          },\\n          {\\n            \"authorId\": \"2324056615\",\\n            \"name\": \"Elizabeth Clark\"\\n          },\\n          {\\n            \"authorId\": \"1747893\",\\n            \"name\": \"Mirella Lapata\"\\n          }\\n        ],\\n        \"abstract\": \"Writing compelling fiction is a multifaceted process combining elements such as crafting a plot, developing interesting characters, and using evocative language. While large language models (LLMs) show promise for story writing, they currently rely heavily on intricate prompting, which limits their use. We propose Agents\\' Room, a generation framework inspired by narrative theory, that decomposes narrative writing into subtasks tackled by specialized agents. To illustrate our method, we introduce Tell Me A Story, a high-quality dataset of complex writing prompts and human-written stories, and a novel evaluation framework designed specifically for assessing long narratives. We show that Agents\\' Room generates stories that are preferred by expert evaluators over those produced by baseline systems by leveraging collaboration and specialization to decompose the complex story writing task into tractable components. We provide extensive analysis with automated and human-based metrics of the generated output.\"\\n      },\\n      {\\n        \"paperId\": \"ff3edac95dcc82eae72cd25519e5dff075fc58be\",\\n        \"url\": \"https://www.semanticscholar.org/paper/ff3edac95dcc82eae72cd25519e5dff075fc58be\",\\n        \"title\": \"Retrieval Augmented Generation in Prompt-based Text-to-Speech Synthesis with Context-Aware Contrastive Language-Audio Pretraining\",\\n        \"citationCount\": 14,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2406.03714, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper adapt retrieval augmented generation (RAG) from LLMs to prompt-based TTS, and presents a Context-Aware Contrastive Language-Audio Pre-training (CA-CLAP) model to extract context-aware, style-related features.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-06-06\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2159562819\",\\n            \"name\": \"Jinlong Xue\"\\n          },\\n          {\\n            \"authorId\": \"2111214299\",\\n            \"name\": \"Yayue Deng\"\\n          },\\n          {\\n            \"authorId\": \"2261912430\",\\n            \"name\": \"Yingming Gao\"\\n          },\\n          {\\n            \"authorId\": \"2161324824\",\\n            \"name\": \"Ya Li\"\\n          }\\n        ],\\n        \"abstract\": \"Recent prompt-based text-to-speech (TTS) models can clone an unseen speaker using only a short speech prompt. They leverage a strong in-context ability to mimic the speech prompts, including speaker style, prosody, and emotion. Therefore, the selection of a speech prompt greatly influences the generated speech, akin to the importance of a prompt in large language models (LLMs). However, current prompt-based TTS models choose the speech prompt manually or simply at random. Hence, in this paper, we adapt retrieval augmented generation (RAG) from LLMs to prompt-based TTS. Unlike traditional RAG methods, we additionally consider contextual information during the retrieval process and present a Context-Aware Contrastive Language-Audio Pre-training (CA-CLAP) model to extract context-aware, style-related features. The objective and subjective evaluations demonstrate that our proposed RAG method outperforms baselines, and our CA-CLAP achieves better results than text-only retrieval methods.\"\\n      },\\n      {\\n        \"paperId\": \"3a6d9e0d5896491dbdb192ea1a9032e9940abe54\",\\n        \"url\": \"https://www.semanticscholar.org/paper/3a6d9e0d5896491dbdb192ea1a9032e9940abe54\",\\n        \"title\": \"Text-driven Prompt Generation for Vision-Language Models in Federated Learning\",\\n        \"citationCount\": 17,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://arxiv.org/pdf/2310.06123\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2310.06123, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work proposes Federated Text-driven Prompt Generation (FedTPG), which learns a unified prompt generation network across multiple remote clients in a scalable manner and is superior to existing federated prompt learning methods, that achieve overall better generalization on both seen and unseen classes.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2023-10-09\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2256983385\",\\n            \"name\": \"Chen Qiu\"\\n          },\\n          {\\n            \"authorId\": \"2257324808\",\\n            \"name\": \"Xingyu Li\"\\n          },\\n          {\\n            \"authorId\": \"29359383\",\\n            \"name\": \"Chaithanya Kumar Mummadi\"\\n          },\\n          {\\n            \"authorId\": \"144487556\",\\n            \"name\": \"M. Ganesh\"\\n          },\\n          {\\n            \"authorId\": \"2257091754\",\\n            \"name\": \"Zhenzhen Li\"\\n          },\\n          {\\n            \"authorId\": \"2257130661\",\\n            \"name\": \"Lu Peng\"\\n          },\\n          {\\n            \"authorId\": \"2257132255\",\\n            \"name\": \"Wan-Yi Lin\"\\n          }\\n        ],\\n        \"abstract\": \"Prompt learning for vision-language models, e.g., CoOp, has shown great success in adapting CLIP to different downstream tasks, making it a promising solution for federated learning due to computational reasons. Existing prompt learning techniques replace hand-crafted text prompts with learned vectors that offer improvements on seen classes, but struggle to generalize to unseen classes. Our work addresses this challenge by proposing Federated Text-driven Prompt Generation (FedTPG), which learns a unified prompt generation network across multiple remote clients in a scalable manner. The prompt generation network is conditioned on task-related text input, thus is context-aware, making it suitable to generalize for both seen and unseen classes. Our comprehensive empirical evaluations on nine diverse image classification datasets show that our method is superior to existing federated prompt learning methods, that achieve overall better generalization on both seen and unseen classes and is also generalizable to unseen datasets.\"\\n      },\\n      {\\n        \"paperId\": \"d6d6b2c88bee4004428435c7e070733709df71a6\",\\n        \"url\": \"https://www.semanticscholar.org/paper/d6d6b2c88bee4004428435c7e070733709df71a6\",\\n        \"title\": \"Socrates: Data Story Generation via Adaptive Machine-Guided Elicitation of User Feedback\",\\n        \"citationCount\": 16,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {\\'tldr\\'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/TVCG.2023.3327363?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/TVCG.2023.3327363, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": null,\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2023-11-03\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2153300135\",\\n            \"name\": \"Guande Wu\"\\n          },\\n          {\\n            \"authorId\": \"30518075\",\\n            \"name\": \"Shunan Guo\"\\n          },\\n          {\\n            \"authorId\": \"1890683\",\\n            \"name\": \"J. Hoffswell\"\\n          },\\n          {\\n            \"authorId\": \"51192588\",\\n            \"name\": \"G. Chan\"\\n          },\\n          {\\n            \"authorId\": \"2176092943\",\\n            \"name\": \"R. Rossi\"\\n          },\\n          {\\n            \"authorId\": \"2176108906\",\\n            \"name\": \"E. Koh\"\\n          }\\n        ],\\n        \"abstract\": \"Visual data stories can effectively convey insights from data, yet their creation often necessitates intricate data exploration, insight discovery, narrative organization, and customization to meet the communication objectives of the storyteller. Existing automated data storytelling techniques, however, tend to overlook the importance of user customization during the data story authoring process, limiting the system\\'s ability to create tailored narratives that reflect the user\\'s intentions. We present a novel data story generation workflow that leverages adaptive machine-guided elicitation of user feedback to customize the story. Our approach employs an adaptive plug-in module for existing story generation systems, which incorporates user feedback through interactive questioning based on the conversation history and dataset. This adaptability refines the system\\'s understanding of the user\\'s intentions, ensuring the final narrative aligns with their goals. We demonstrate the feasibility of our approach through the implementation of an interactive prototype: Socrates. Through a quantitative user study with 18 participants that compares our method to a state-of-the-art data story generation algorithm, we show that Socrates produces more relevant stories with a larger overlap of insights compared to human-generated stories. We also demonstrate the usability of Socrates via interviews with three data analysts and highlight areas of future work.\"\\n      },\\n      {\\n        \"paperId\": \"57fca8aba9760ed8001be7ad074554ee4515d258\",\\n        \"url\": \"https://www.semanticscholar.org/paper/57fca8aba9760ed8001be7ad074554ee4515d258\",\\n        \"title\": \"Adaptive Radiotherapy: Next-Generation Radiotherapy\",\\n        \"citationCount\": 54,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://www.mdpi.com/2072-6694/16/6/1206/pdf?version=1710899842\",\\n          \"status\": \"GOLD\",\\n          \"license\": \"CCBY\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC10968833, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"Adaptive radiotherapy (ART), a notable innovation that addresses anatomy changes and optimizes the therapeutic ratio, is introduced, highlighting the ongoing development of imaging technologies and AI and emphasizing their contribution to enhancing the applicability and effectiveness of ART.\"\\n        },\\n        \"publicationTypes\": [\\n          \"Review\",\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-03-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2283275224\",\\n            \"name\": \"Olga Dona Lemus\"\\n          },\\n          {\\n            \"authorId\": \"2251160990\",\\n            \"name\": \"Minsong Cao\"\\n          },\\n          {\\n            \"authorId\": \"2054380388\",\\n            \"name\": \"Bin Cai\"\\n          },\\n          {\\n            \"authorId\": \"2292572199\",\\n            \"name\": \"Michael Cummings\"\\n          },\\n          {\\n            \"authorId\": \"2248857176\",\\n            \"name\": \"Dandan Zheng\"\\n          }\\n        ],\\n        \"abstract\": \"Simple Summary Radiotherapy, a crucial cancer treatment, has evolved significantly over the years. Traditionally, treatment plans were based on initial scans used throughout the treatment course, accounting for changes in the patient\\\\u2019s anatomy by additional margins to targets. However, the field has moved towards decreasing margins with the advancement of delivery and targeting accuracy in order to decrease toxicity, and the increasing use of image guidance has illuminated patient anatomical changes such as organ deformation, weight loss, tumor shrinkage, and even biological changes that are unaccounted for by the conventional approach. Adaptive radiotherapy (ART) addresses this by adjusting treatment plans according to these changes. ART can be conducted in two ways: online (adjustments made during treatment sessions) and offline (adjustments made between treatment sessions). Advances in technology, especially in medical imaging (CT, MRI, and PET scans) and artificial intelligence, have made ART more feasible and efficient. ART offers more precise cancer treatment by adapting to changes in the patient\\\\u2019s body, leading to better outcomes with fewer side effects. Abstract Radiotherapy, a crucial technique in cancer therapy, has traditionally relied on the premise of largely unchanging patient anatomy during the treatment course and encompassing uncertainties by target margins. This review introduces adaptive radiotherapy (ART), a notable innovation that addresses anatomy changes and optimizes the therapeutic ratio. ART utilizes advanced imaging techniques such as CT, MRI, and PET to modify the treatment plan based on observed anatomical changes and even biological changes during the course of treatment. The narrative review provides a comprehensive guide on ART for healthcare professionals and trainees in radiation oncology and anyone else interested in the topic. The incorporation of artificial intelligence in ART has played a crucial role in improving effectiveness, particularly in contour segmentation, treatment planning, and quality assurance. This has expedited the process to render online ART feasible, lowered the burden for radiation oncology practitioners, and enhanced the precision of dynamically personalized treatment. Current technical and clinical progress on ART is discussed in this review, highlighting the ongoing development of imaging technologies and AI and emphasizing their contribution to enhancing the applicability and effectiveness of ART.\"\\n      },\\n      {\\n        \"paperId\": \"62d4fcfb2f6717d4eb562f4e51f37004fc5d7109\",\\n        \"url\": \"https://www.semanticscholar.org/paper/62d4fcfb2f6717d4eb562f4e51f37004fc5d7109\",\\n        \"title\": \"Adaptive Inference-Time Compute: LLMs Can Predict if They Can Do Better, Even Mid-Generation\",\\n        \"citationCount\": 47,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2410.02725, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A new generative self-evaluation scheme designed to adaptively reduce the number of generated samples while maintaining or even improving performance and enabling more efficient and scalable compute utilization during inference for LLMs is introduced.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-10-03\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2161717022\",\\n            \"name\": \"Rohin Manvi\"\\n          },\\n          {\\n            \"authorId\": \"2324336057\",\\n            \"name\": \"Anikait Singh\"\\n          },\\n          {\\n            \"authorId\": \"2269095529\",\\n            \"name\": \"Stefano Ermon\"\\n          }\\n        ],\\n        \"abstract\": \"Inference-time computation is a powerful paradigm to enhance the performance of large language models (LLMs), with Best-of-N sampling being a widely used technique. However, this method is computationally expensive, requiring both (1) an external reward model and (2) the generation of multiple samples. In this work, we introduce a new generative self-evaluation scheme designed to adaptively reduce the number of generated samples while maintaining or even improving performance. We use a generative reward model formulation, allowing the LLM to predict mid-generation the probability that restarting the generation will yield a better response. These predictions are obtained without an external reward model and can be used to decide whether or not to generate more samples, prune unpromising samples early on, or to pick the best sample. This capability is very inexpensive as it involves generating a single predefined token. Trained using a dataset constructed with real unfiltered LMSYS user prompts, Llama 3.1 8B\\'s win rate against GPT-4 on AlpacaEval increases from 21% to 34% with 16 samples and math performance on GSM8K improves from 84% to 91%. By sampling only when the LLM determines that it is beneficial to do so and adaptively adjusting temperature annealing, we demonstrate that 74% of the improvement from using 16 samples can be achieved with only 1.2 samples on average. We further demonstrate that 50-75% of samples can be pruned early in generation with minimal degradation in performance. Overall, our methods enable more efficient and scalable compute utilization during inference for LLMs.\"\\n      },\\n      {\\n        \"paperId\": \"c0e24f98323c7114b9229ac17b8b63581e3e5914\",\\n        \"url\": \"https://www.semanticscholar.org/paper/c0e24f98323c7114b9229ac17b8b63581e3e5914\",\\n        \"title\": \"Dynamic and Adaptive Feature Generation with LLM\",\\n        \"citationCount\": 29,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2406.03505, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This research introduces a novel approach adopting large language models and feature-generating prompts to address challenges of explainability, applicability, and inflexible strategy and proposes a dynamic and adaptive feature generation method that enhances the interpretability of the feature generation process.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-06-04\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2130031226\",\\n            \"name\": \"XinHao Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2108045855\",\\n            \"name\": \"Jinghan Zhang\"\\n          },\\n          {\\n            \"authorId\": \"1966492\",\\n            \"name\": \"Banafsheh Rekabdar\"\\n          },\\n          {\\n            \"authorId\": \"2145108199\",\\n            \"name\": \"Yuanchun Zhou\"\\n          },\\n          {\\n            \"authorId\": \"2301248160\",\\n            \"name\": \"Pengfei Wang\"\\n          },\\n          {\\n            \"authorId\": \"2293571072\",\\n            \"name\": \"Kunpeng Liu\"\\n          }\\n        ],\\n        \"abstract\": \"The representation of feature space is a crucial environment where data points get vectorized and embedded for subsequent modeling. Thus, the efficacy of machine learning (ML) algorithms is closely related to the quality of feature engineering. As one of the most important techniques, feature generation transforms raw data into an optimized feature space conducive to model training and further refines the space. Despite the advancements in automated feature engineering and feature generation, current methodologies often suffer from three fundamental issues: lack of explainability, limited applicability, and inflexible strategy. These shortcomings frequently hinder and limit the deployment of ML models across varied scenarios. Our research introduces a novel approach adopting large language models (LLMs) and feature-generating prompts to address these challenges. We propose a dynamic and adaptive feature generation method that enhances the interpretability of the feature generation process. Our approach broadens the applicability across various data types and tasks and offers advantages in terms of strategic flexibility. A broad range of experiments showcases that our approach is significantly superior to existing methods.\"\\n      },\\n      {\\n        \"paperId\": \"a314b4c385e9732794a48d1d34f637b13245c71d\",\\n        \"url\": \"https://www.semanticscholar.org/paper/a314b4c385e9732794a48d1d34f637b13245c71d\",\\n        \"title\": \"RetrievalQA: Assessing Adaptive Retrieval-Augmented Generation for Short-form Open-Domain Question Answering\",\\n        \"citationCount\": 21,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2402.16457, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work presents a benchmark, RetrievalQA, comprising 1,271 short-form questions covering new world and long-tail knowledge, and proposes Time-Aware Adaptive Retrieval (TA-ARE), a simple yet effective method that helps LLMs assess the necessity of retrieval without calibration or additional training.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-02-26\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2144370264\",\\n            \"name\": \"Zihan Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2257039084\",\\n            \"name\": \"Meng Fang\"\\n          },\\n          {\\n            \"authorId\": \"2261455899\",\\n            \"name\": \"Ling Chen\"\\n          }\\n        ],\\n        \"abstract\": \"Adaptive retrieval-augmented generation (ARAG) aims to dynamically determine the necessity of retrieval for queries instead of retrieving indiscriminately to enhance the efficiency and relevance of the sourced information. However, previous works largely overlook the evaluation of ARAG approaches, leading to their effectiveness being understudied. This work presents a benchmark, RetrievalQA, comprising 1,271 short-form questions covering new world and long-tail knowledge. The knowledge necessary to answer the questions is absent from LLMs; therefore, external information must be retrieved to answer correctly. This makes RetrievalQA a suitable testbed to evaluate existing ARAG methods. We observe that calibration-based methods heavily rely on threshold tuning, while vanilla prompting is inadequate for guiding LLMs to make reliable retrieval decisions. Based on our findings, we propose Time-Aware Adaptive Retrieval (TA-ARE), a simple yet effective method that helps LLMs assess the necessity of retrieval without calibration or additional training. The dataset and code will be available at https://github.com/hyintell/RetrievalQA\"\\n      },\\n      {\\n        \"paperId\": \"c958736c08a86816c588fd8fe12f39cec8a64bf8\",\\n        \"url\": \"https://www.semanticscholar.org/paper/c958736c08a86816c588fd8fe12f39cec8a64bf8\",\\n        \"title\": \"SGDM: An Adaptive Style-Guided Diffusion Model for Personalized Text to Image Generation\",\\n        \"citationCount\": 21,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {\\'tldr\\'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/TMM.2024.3399075?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/TMM.2024.3399075, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": null,\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": null,\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2300989129\",\\n            \"name\": \"Yifei Xu\"\\n          },\\n          {\\n            \"authorId\": \"2238891797\",\\n            \"name\": \"Xiaolong Xu\"\\n          },\\n          {\\n            \"authorId\": \"2112666114\",\\n            \"name\": \"Honghao Gao\"\\n          },\\n          {\\n            \"authorId\": \"2166050115\",\\n            \"name\": \"Fu Xiao\"\\n          }\\n        ],\\n        \"abstract\": \"The existing personalized text-to-image generation models face issues such as repeated training and insufficient generalization capabilities. We present an adaptive Style-Guided Diffusion Model (SGDM). When provided with a set of stylistically consistent images and prompts as inputs, SGDM can generate images that align with the prompts while maintaining style consistency with the input images. SGDM first extracts features from the input style image and then combines style features from different depths. Last, style features are injected into the noise generation process of the original Stable Diffusion (SD) model by the style-guided module we propose. This strategy fully leverages the generative and generalization capabilities of the pre-trained text-to-image model to ensure the accuracy of the generated image\\'s content. We present a dataset construction method suitable for style personalized generation tasks of this kind, enabling the trained model to generate stylized images adaptively instead of re-training for each style. We also present an evaluation metric, StySim, to measure the style similarity between two images, and this metric shows that the style personalization capability of SGDM is the best. And metrics such as FID, KID, and CLIPSIM indicate that SGDM maintains good performance in text-to-image generation.\"\\n      },\\n      {\\n        \"paperId\": \"cf56a7b28fb27279b1c94fb920b5722cf50c8852\",\\n        \"url\": \"https://www.semanticscholar.org/paper/cf56a7b28fb27279b1c94fb920b5722cf50c8852\",\\n        \"title\": \"AdvPrompter: Fast Adaptive Adversarial Prompting for LLMs\",\\n        \"citationCount\": 113,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2404.16873, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A novel method that uses another LLM, called AdvPrompter, to generate human-readable adversarial prompts in seconds, and shows that training on adversarial suffixes generated by AdvPrompter is a promising strategy for improving the robustness of LLMs to jailbreaking attacks.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-04-21\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"1443432623\",\\n            \"name\": \"Anselm Paulus\"\\n          },\\n          {\\n            \"authorId\": \"3461866\",\\n            \"name\": \"Arman Zharmagambetov\"\\n          },\\n          {\\n            \"authorId\": \"2298951327\",\\n            \"name\": \"Chuan Guo\"\\n          },\\n          {\\n            \"authorId\": \"2298758184\",\\n            \"name\": \"Brandon Amos\"\\n          },\\n          {\\n            \"authorId\": \"2253746559\",\\n            \"name\": \"Yuandong Tian\"\\n          }\\n        ],\\n        \"abstract\": \"Large Language Models (LLMs) are vulnerable to jailbreaking attacks that lead to generation of inappropriate or harmful content. Manual red-teaming requires a time-consuming search for adversarial prompts, whereas automatic adversarial prompt generation often leads to semantically meaningless attacks that do not scale well. In this paper, we present a novel method that uses another LLM, called AdvPrompter, to generate human-readable adversarial prompts in seconds. AdvPrompter, which is trained using an alternating optimization algorithm, generates suffixes that veil the input instruction without changing its meaning, such that the TargetLLM is lured to give a harmful response. Experimental results on popular open source TargetLLMs show highly competitive results on the AdvBench and HarmBench datasets, that also transfer to closed-source black-box LLMs. We also show that training on adversarial suffixes generated by AdvPrompter is a promising strategy for improving the robustness of LLMs to jailbreaking attacks.\"\\n      },\\n      {\\n        \"paperId\": \"1a41d449f2ed4051ac4c763fb8b1b07357eaf5f4\",\\n        \"url\": \"https://www.semanticscholar.org/paper/1a41d449f2ed4051ac4c763fb8b1b07357eaf5f4\",\\n        \"title\": \"WavLLM: Towards Robust and Adaptive Speech Large Language Model\",\\n        \"citationCount\": 104,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2404.00656, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"WavLLM is introduced, a robust and adaptive speech large language model with dual encoders, and a prompt-aware LoRA weight adapter, optimized by a two-stage curriculum learning approach, exhibiting robust generalization capabilities in executing complex tasks using CoT approach.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-03-31\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2277450543\",\\n            \"name\": \"Shujie Hu\"\\n          },\\n          {\\n            \"authorId\": \"2135918679\",\\n            \"name\": \"Long Zhou\"\\n          },\\n          {\\n            \"authorId\": \"2107983441\",\\n            \"name\": \"Shujie Liu\"\\n          },\\n          {\\n            \"authorId\": \"2107970655\",\\n            \"name\": \"Sanyuan Chen\"\\n          },\\n          {\\n            \"authorId\": \"2294360053\",\\n            \"name\": \"Hongkun Hao\"\\n          },\\n          {\\n            \"authorId\": \"2258308585\",\\n            \"name\": \"Jing Pan\"\\n          },\\n          {\\n            \"authorId\": \"2274190703\",\\n            \"name\": \"Xunying Liu\"\\n          },\\n          {\\n            \"authorId\": \"2280887661\",\\n            \"name\": \"Jinyu Li\"\\n          },\\n          {\\n            \"authorId\": \"9075412\",\\n            \"name\": \"S. Sivasankaran\"\\n          },\\n          {\\n            \"authorId\": \"2294832157\",\\n            \"name\": \"Linquan Liu\"\\n          },\\n          {\\n            \"authorId\": \"2277299355\",\\n            \"name\": \"Furu Wei\"\\n          }\\n        ],\\n        \"abstract\": \"The recent advancements in large language models (LLMs) have revolutionized the field of natural language processing, progressively broadening their scope to multimodal perception and generation. However, effectively integrating listening capabilities into LLMs poses significant challenges, particularly with respect to generalizing across varied contexts and executing complex auditory tasks. In this work, we introduce WavLLM, a robust and adaptive speech large language model with dual encoders, and a prompt-aware LoRA weight adapter, optimized by a two-stage curriculum learning approach. Leveraging dual encoders, we decouple different types of speech information, utilizing a Whisper encoder to process the semantic content of speech, and a WavLM encoder to capture the unique characteristics of the speaker\\'s identity. Within the curriculum learning framework, WavLLM first builds its foundational capabilities by optimizing on mixed elementary single tasks, followed by advanced multi-task training on more complex tasks such as combinations of the elementary tasks. To enhance the flexibility and adherence to different tasks and instructions, a prompt-aware LoRA weight adapter is introduced in the second advanced multi-task training stage. We validate the proposed model on universal speech benchmarks including tasks such as ASR, ST, SV, ER, and also apply it to specialized datasets like Gaokao English listening comprehension set for SQA, and speech Chain-of-Thought (CoT) evaluation set. Experiments demonstrate that the proposed model achieves state-of-the-art performance across a range of speech tasks on the same model size, exhibiting robust generalization capabilities in executing complex tasks using CoT approach. Furthermore, our model successfully completes Gaokao tasks without specialized training. The codes, models, audio, and Gaokao evaluation set can be accessed at \\\\\\\\url{aka.ms/wavllm}.\"\\n      },\\n      {\\n        \"paperId\": \"5cacb35e7bd86e64e5aca126e5011b64630007d8\",\\n        \"url\": \"https://www.semanticscholar.org/paper/5cacb35e7bd86e64e5aca126e5011b64630007d8\",\\n        \"title\": \"OmniAvatar: Efficient Audio-Driven Avatar Video Generation with Adaptive Body Animation\",\\n        \"citationCount\": 12,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2506.18866, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"OmniAvatar is introduced, an innovative audio-driven full-body video generation model that enhances human animation with improved lip-sync accuracy and natural movements, and introduces a pixel-wise multi-hierarchical audio embedding strategy to better capture audio features in the latent space.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2025-06-23\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2293272991\",\\n            \"name\": \"Qijun Gan\"\\n          },\\n          {\\n            \"authorId\": \"2294510159\",\\n            \"name\": \"Ruizi Yang\"\\n          },\\n          {\\n            \"authorId\": \"2314648986\",\\n            \"name\": \"Jianke Zhu\"\\n          },\\n          {\\n            \"authorId\": \"2370937157\",\\n            \"name\": \"Shaofei Xue\"\\n          },\\n          {\\n            \"authorId\": \"2370937932\",\\n            \"name\": \"Steven Hoi\"\\n          }\\n        ],\\n        \"abstract\": \"Significant progress has been made in audio-driven human animation, while most existing methods focus mainly on facial movements, limiting their ability to create full-body animations with natural synchronization and fluidity. They also struggle with precise prompt control for fine-grained generation. To tackle these challenges, we introduce OmniAvatar, an innovative audio-driven full-body video generation model that enhances human animation with improved lip-sync accuracy and natural movements. OmniAvatar introduces a pixel-wise multi-hierarchical audio embedding strategy to better capture audio features in the latent space, enhancing lip-syncing across diverse scenes. To preserve the capability for prompt-driven control of foundation models while effectively incorporating audio features, we employ a LoRA-based training approach. Extensive experiments show that OmniAvatar surpasses existing models in both facial and semi-body video generation, offering precise text-based control for creating videos in various domains, such as podcasts, human interactions, dynamic scenes, and singing. Our project page is https://omni-avatar.github.io/.\"\\n      },\\n      {\\n        \"paperId\": \"e8a897a12ce666a898582c3454a3e49acae40422\",\\n        \"url\": \"https://www.semanticscholar.org/paper/e8a897a12ce666a898582c3454a3e49acae40422\",\\n        \"title\": \"SceneCraft: Automating Interactive Narrative Scene Generation in Digital Games with Large Language Models\",\\n        \"citationCount\": 56,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://ojs.aaai.org/index.php/AIIDE/article/download/27504/27277\",\\n          \"status\": \"BRONZE\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1609/aiide.v19i1.27504?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1609/aiide.v19i1.27504, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper proposes SCENECRAFT, a narrative scene generation framework that automates NPC interaction crucial to unfolding plot events and demonstrates its effectiveness in creating narrative experiences based on creativity, adaptability, and alignment with intended author instructions.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2023-10-06\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2071942775\",\\n            \"name\": \"Vikram Kumaran\"\\n          },\\n          {\\n            \"authorId\": \"2256208513\",\\n            \"name\": \"Jonathan Rowe\"\\n          },\\n          {\\n            \"authorId\": \"9808011\",\\n            \"name\": \"Bradford W. Mott\"\\n          },\\n          {\\n            \"authorId\": \"2238471264\",\\n            \"name\": \"James C. Lester\"\\n          }\\n        ],\\n        \"abstract\": \"Creating engaging interactive story-based experiences dynamically responding to individual player choices poses significant challenges for narrative-centered games. Recent advances in pre-trained large language models (LLMs) have the potential to revolutionize procedural content generation for narrative-centered games. Historically, interactive narrative generation has specified pivotal events in the storyline, often utilizing planning-based approaches toward achieving narrative coherence and maintaining the story arc. However, manual authorship is typically used to create detail and variety in non-player character (NPC) interaction to specify and instantiate plot events. This paper proposes SCENECRAFT, a narrative scene generation framework that automates NPC interaction crucial to unfolding plot events. SCENECRAFT interprets natural language instructions about scene objectives, NPC traits, location, and narrative variations. It then employs large language models to generate game scenes aligned with authorial intent. It generates branching conversation paths that adapt to player choices while adhering to the author\\\\u2019s interaction goals. LLMs generate interaction scripts, semantically extract character emotions and gestures to align with the script, and convert dialogues into a game scripting language. The generated script can then be played utilizing an existing narrative-centered game framework. Through empirical evaluation using automated and human assessments, we demonstrate SCENECRAFT\\\\u2019s effectiveness in creating narrative experiences based on creativity, adaptability, and alignment with intended author instructions.\"\\n      },\\n      {\\n        \"paperId\": \"d599dc40c9cb8d6d76554ee7d21d20c22cc7cdb5\",\\n        \"url\": \"https://www.semanticscholar.org/paper/d599dc40c9cb8d6d76554ee7d21d20c22cc7cdb5\",\\n        \"title\": \"ConsistI2V: Enhancing Visual Consistency for Image-to-Video Generation\",\\n        \"citationCount\": 99,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2402.04324, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work proposes ConsistI2V, a diffusion-based method to enhance visual consistency for I2V generation that introduces spatiotemporal attention over the first frame to maintain spatial and motion consistency and noise initialization from the low-frequency band of the first frame to enhance layout consistency.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-02-06\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2268493042\",\\n            \"name\": \"Weiming Ren\"\\n          },\\n          {\\n            \"authorId\": \"2283183497\",\\n            \"name\": \"Harry Yang\"\\n          },\\n          {\\n            \"authorId\": \"2143853895\",\\n            \"name\": \"Ge Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2268683835\",\\n            \"name\": \"Cong Wei\"\\n          },\\n          {\\n            \"authorId\": \"2279346001\",\\n            \"name\": \"Xinrun Du\"\\n          },\\n          {\\n            \"authorId\": \"2283188391\",\\n            \"name\": \"Stephen W. Huang\"\\n          },\\n          {\\n            \"authorId\": \"2253811180\",\\n            \"name\": \"Wenhu Chen\"\\n          }\\n        ],\\n        \"abstract\": \"Image-to-video (I2V) generation aims to use the initial frame (alongside a text prompt) to create a video sequence. A grand challenge in I2V generation is to maintain visual consistency throughout the video: existing methods often struggle to preserve the integrity of the subject, background, and style from the first frame, as well as ensure a fluid and logical progression within the video narrative. To mitigate these issues, we propose ConsistI2V, a diffusion-based method to enhance visual consistency for I2V generation. Specifically, we introduce (1) spatiotemporal attention over the first frame to maintain spatial and motion consistency, (2) noise initialization from the low-frequency band of the first frame to enhance layout consistency. These two approaches enable ConsistI2V to generate highly consistent videos. We also extend the proposed approaches to show their potential to improve consistency in auto-regressive long video generation and camera motion control. To verify the effectiveness of our method, we propose I2V-Bench, a comprehensive evaluation benchmark for I2V generation. Our automatic and human evaluation results demonstrate the superiority of ConsistI2V over existing methods.\"\\n      },\\n      {\\n        \"paperId\": \"f00a8162648403f32c9dfb937b9220b234c82531\",\\n        \"url\": \"https://www.semanticscholar.org/paper/f00a8162648403f32c9dfb937b9220b234c82531\",\\n        \"title\": \"viz2viz: Prompt-driven stylized visualization generation using a diffusion model\",\\n        \"citationCount\": 13,\\n        \"openAccessPdf\": {\\n          \"url\": \"http://arxiv.org/pdf/2304.01919\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2304.01919, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": null\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2023-04-04\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2305625074\",\\n            \"name\": \"Jiaqi Wu\"\\n          },\\n          {\\n            \"authorId\": \"152836325\",\\n            \"name\": \"John Joon Young Chung\"\\n          },\\n          {\\n            \"authorId\": \"2630700\",\\n            \"name\": \"Eytan Adar\"\\n          }\\n        ],\\n        \"abstract\": \"Creating stylized visualization requires going beyond the limited, abstract, geometric marks produced by most tools. Rather, the designer builds stylized idioms where the marks are both transformed (e.g., photographs of candles instead of bars) and also synthesized into a \\'scene\\' that pushes the boundaries of traditional visualizations. To support this, we introduce viz2viz, a system for transforming visualizations with a textual prompt to a stylized form. The system follows a high-level recipe that leverages various generative methods to produce new visualizations that retain the properties of the original dataset. While the base recipe is consistent across many visualization types, we demonstrate how it can be specifically adapted to the creation of different visualization types (bar charts, area charts, pie charts, and network visualizations). Our approach introduces techniques for using different prompts for different marks (i.e., each bar can be something completely different) while still retaining image\\\\\"coherence.\\\\\"We conclude with an evaluation of the approach and discussion on extensions and limitations.\"\\n      },\\n      {\\n        \"paperId\": \"80b8d03f910bb252cb251f7f74ceaaf8f4c4aabe\",\\n        \"url\": \"https://www.semanticscholar.org/paper/80b8d03f910bb252cb251f7f74ceaaf8f4c4aabe\",\\n        \"title\": \"Adaptive Test Generation Using a Large Language Model\",\\n        \"citationCount\": 69,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://arxiv.org/pdf/2302.06527\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {\\'abstract\\'}. Paper or abstract available at https://api.unpaywall.org/v2/10.48550/arXiv.2302.06527?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.48550/arXiv.2302.06527, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"T EST P ILOT uses Codex, an off-the-shelf LLM, to automatically generate unit tests for a given program without requiring additional training or few-shot learning on examples of existing tests, and does not generate memorized tests.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": null,\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2257193192\",\\n            \"name\": \"Max Sch\\\\u00e4fer\"\\n          },\\n          {\\n            \"authorId\": \"2317114000\",\\n            \"name\": \"Sarah Nadi\"\\n          },\\n          {\\n            \"authorId\": \"46235560\",\\n            \"name\": \"A. Eghbali\"\\n          },\\n          {\\n            \"authorId\": \"2257181137\",\\n            \"name\": \"Frank Tip\"\\n          }\\n        ],\\n        \"abstract\": null\\n      },\\n      {\\n        \"paperId\": \"9038f40c43e7d62d8f1dc4819093083090911f7a\",\\n        \"url\": \"https://www.semanticscholar.org/paper/9038f40c43e7d62d8f1dc4819093083090911f7a\",\\n        \"title\": \"Novelty Controlled Paraphrase Generation with Retrieval Augmented Conditional Prompt Tuning\",\\n        \"citationCount\": 39,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://ojs.aaai.org/index.php/AAAI/article/download/21297/21046\",\\n          \"status\": \"GOLD\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2202.00535, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"The effectiveness of the proposed approaches for retaining the semantic content of the original text while inducing lexical novelty in the generation of paraphrase generation is demonstrated.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2022-02-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"123467107\",\\n            \"name\": \"Jishnu Ray Chowdhury\"\\n          },\\n          {\\n            \"authorId\": \"2152482425\",\\n            \"name\": \"Yong Zhuang\"\\n          },\\n          {\\n            \"authorId\": \"2152375661\",\\n            \"name\": \"Shuyi Wang\"\\n          }\\n        ],\\n        \"abstract\": \"Paraphrase generation is a fundamental and long-standing task in natural language processing. In this paper, we concentrate on two contributions to the task: (1) we propose Retrieval Augmented Prompt Tuning (RAPT) as a parameter-efficient method to adapt large pre-trained language models for paraphrase generation; (2) we propose Novelty Conditioned RAPT (NC-RAPT) as a simple model-agnostic method of using specialized prompt tokens for controlled paraphrase generation with varying levels of lexical novelty. By conducting extensive experiments on four datasets, we demonstrate the effectiveness of the proposed approaches for retaining the semantic content of the original text while inducing lexical novelty in the generation.\"\\n      },\\n      {\\n        \"paperId\": \"df19bd0bb788474a578fac33d6cff9867af3eead\",\\n        \"url\": \"https://www.semanticscholar.org/paper/df19bd0bb788474a578fac33d6cff9867af3eead\",\\n        \"title\": \"FABULA: Intelligence Report Generation Using Retrieval-Augmented Narrative Construction\",\\n        \"citationCount\": 16,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://dl.acm.org/doi/pdf/10.1145/3625007.3627505\",\\n          \"status\": \"BRONZE\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2310.13848, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A Retrieval Augmented Generation (RAG) approach to augment prompting of an autoregressive decoder by retrieving structured information asserted in a knowledge graph to generate targeted information based on a narrative plot model is introduced.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Book\"\\n        ],\\n        \"publicationDate\": \"2023-10-20\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"51092885\",\\n            \"name\": \"P. Ranade\"\\n          },\\n          {\\n            \"authorId\": \"2269936199\",\\n            \"name\": \"Anupam Joshi\"\\n          }\\n        ],\\n        \"abstract\": \"Narrative construction is the process of representing disparate event information into a logical plot structure that models an end to end story. Intelligence analysis is an example of a domain that can benefit tremendously from narrative construction techniques, particularly in aiding analysts during the largely manual and costly process of synthesizing event information into comprehensive intelligence reports. Manual intelligence report generation is often prone to challenges such as integrating dynamic event information, writing fine-grained queries, and closing information gaps. This motivates the development of a system that retrieves and represents critical aspects of events in a form that aids in automatic generation of intelligence reports. We introduce a Retrieval Augmented Generation (RAG) approach to augment prompting of an autoregressive decoder by retrieving structured information asserted in a knowledge graph to generate targeted information based on a narrative plot model. We apply our approach to the problem of neural intelligence report generation and introduce FABULA, framework to augment intelligence analysis workflows using RAG. An analyst can use FABULA to query an Event Plot Graph (EPG) to retrieve relevant event plot points, which can be used to augment prompting of a Large Language Model (LLM) during intelligence report generation. Our evaluation studies show that the plot points included in the generated intelligence reports have high semantic relevance, high coherency, and low data redundancy.\"\\n      },\\n      {\\n        \"paperId\": \"bb2ef694e8b5a99e1f7ceb014968b4d1dc2e122a\",\\n        \"url\": \"https://www.semanticscholar.org/paper/bb2ef694e8b5a99e1f7ceb014968b4d1dc2e122a\",\\n        \"title\": \"EIPE-text: Evaluation-Guided Iterative Plan Extraction for Long-Form Narrative Text Generation\",\\n        \"citationCount\": 11,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://arxiv.org/pdf/2310.08185\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2310.08185, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A new framework called Evaluation-guided Iterative Plan Extraction for long-form narrative text generation (EIPE-text), which extracts plans from the corpus of narratives and utilizes the extracted plans to construct a better planner.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2023-10-12\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2214585980\",\\n            \"name\": \"Wang You\"\\n          },\\n          {\\n            \"authorId\": \"51198241\",\\n            \"name\": \"Wenshan Wu\"\\n          },\\n          {\\n            \"authorId\": \"3887469\",\\n            \"name\": \"Yaobo Liang\"\\n          },\\n          {\\n            \"authorId\": \"35374367\",\\n            \"name\": \"Shaoguang Mao\"\\n          },\\n          {\\n            \"authorId\": \"2151101534\",\\n            \"name\": \"Chenfei Wu\"\\n          },\\n          {\\n            \"authorId\": \"2257345975\",\\n            \"name\": \"Maosong Cao\"\\n          },\\n          {\\n            \"authorId\": \"2257125576\",\\n            \"name\": \"Yuzhe Cai\"\\n          },\\n          {\\n            \"authorId\": \"2214448244\",\\n            \"name\": \"Yiduo Guo\"\\n          },\\n          {\\n            \"authorId\": \"2258547658\",\\n            \"name\": \"Yan Xia\"\\n          },\\n          {\\n            \"authorId\": \"2249539478\",\\n            \"name\": \"Furu Wei\"\\n          },\\n          {\\n            \"authorId\": \"2072609829\",\\n            \"name\": \"Nan Duan\"\\n          }\\n        ],\\n        \"abstract\": \"Plan-and-Write is a common hierarchical approach in long-form narrative text generation, which first creates a plan to guide the narrative writing. Following this approach, several studies rely on simply prompting large language models for planning, which often yields suboptimal results. In this paper, we propose a new framework called Evaluation-guided Iterative Plan Extraction for long-form narrative text generation (EIPE-text), which extracts plans from the corpus of narratives and utilizes the extracted plans to construct a better planner. EIPE-text has three stages: plan extraction, learning, and inference. In the plan extraction stage, it iteratively extracts and improves plans from the narrative corpus and constructs a plan corpus. We propose a question answer (QA) based evaluation mechanism to automatically evaluate the plans and generate detailed plan refinement instructions to guide the iterative improvement. In the learning stage, we build a better planner by fine-tuning with the plan corpus or in-context learning with examples in the plan corpus. Finally, we leverage a hierarchical approach to generate long-form narratives. We evaluate the effectiveness of EIPE-text in the domains of novels and storytelling. Both GPT-4-based evaluations and human evaluations demonstrate that our method can generate more coherent and relevant long-form narratives. Our code will be released in the future.\"\\n      },\\n      {\\n        \"paperId\": \"02747bc59ddf832b0b7d04a1f491d9fa519066fe\",\\n        \"url\": \"https://www.semanticscholar.org/paper/02747bc59ddf832b0b7d04a1f491d9fa519066fe\",\\n        \"title\": \"PANGeA: Procedural Artificial Narrative Using Generative AI for Turn-Based, Role-Playing Video Games\",\\n        \"citationCount\": 15,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://ojs.aaai.org/index.php/AIIDE/article/download/31876/34043\",\\n          \"status\": \"BRONZE\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1609/aiide.v20i1.31876?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1609/aiide.v20i1.31876, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"PANGeA can help game designers generate narrative-consistent content while leveraging LLMs of different sizes, suitable for various devices, and is supported by a novel validation system for handling free-form text input during game development and gameplay.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-11-15\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2286339849\",\\n            \"name\": \"Stephanie Buongiorno\"\\n          },\\n          {\\n            \"authorId\": \"2132867162\",\\n            \"name\": \"Lawrence J. Klinkert\"\\n          },\\n          {\\n            \"authorId\": \"2298971047\",\\n            \"name\": \"Zixin Zhuang\"\\n          },\\n          {\\n            \"authorId\": \"2298969170\",\\n            \"name\": \"Tanishq Chawla\"\\n          },\\n          {\\n            \"authorId\": \"2299097774\",\\n            \"name\": \"Corey Clark\"\\n          }\\n        ],\\n        \"abstract\": \"Large language models (LLMs) offer unprecedented flexibility in procedural generation, enabling the creation of dynamic video game storylines that evolve with user input. A critical aspect of realizing this potential is allowing players and developers to provide dynamic or free-form text to drive generation. Ingesting free-form text for a video game poses challenges, however, as it can prompt the LLM to generate content beyond the intended narrative scope. In response to this challenge, this research introduces Procedural Artificial Narrative using Generative AI (PANGeA) for leveraging LLMs to create narrative content for turn-based, role-playing games (RPGs). PANGeA is an approach comprised of components including a memory system, validation system, a Unity game engine plug-in, and a server with a RESTful interface that enables connecting PANGeA components with any game engine as well as accessing local and private LLMs. PANGeA procedurally generates level data like setting, key items, non-playable characters (NPCs)), and dialogue based on a set of configuration and design rules provided by the game designer. This process is supported by a novel validation system for handling free-form text input during game development and gameplay, which aligns LLM generation with the narrative. It does this by evoking the LLM\\'s capabilities to dynamically evaluate the text input against game rules that reinforce the designer\\'s initial criteria. To enrich player-NPC interactions, PANGeA uses the Big Five Personality model to shape NPC responses. To explore its broad application, PANGeA is evaluated across two studies. First, this research presents a narrative test scenario of the prototype game, Dark Shadows, which was developed using PANGeA within the Unity game engine. This is followed by an ablation study that tests PANGeA\\'s performance across 10 different role-playing game scenarios\\\\u2013from western to science fiction\\\\u2013and across three model sizes: Llama-3 (8B), GPT-3.5, and GPT-4. These evaluations demonstrate that PANGeA\\'s NPCs can hold dynamic, narrative-consistent conversations that, without the memory system, would exceed the LLM\\'s context length. In addition, the results demonstrate PANGeA\\'s validation system not only aligns LLM responses with the game narrative but also improves the performance of Llama-3 (8B), enabling it to perform comparably to large-scale foundational models like GPT-4. With the validation system, Llama-3 (8B)\\'s performance improved from 28% accuracy to 98%, and GPT-4\\'s from 71% to 99%. These findings indicate PANGeA can help game designers generate narrative-consistent content while leveraging LLMs of different sizes, suitable for various devices.\"\\n      },\\n      {\\n        \"paperId\": \"22a0bfac8cc0cb9c01123d8a898e3235ddcab269\",\\n        \"url\": \"https://www.semanticscholar.org/paper/22a0bfac8cc0cb9c01123d8a898e3235ddcab269\",\\n        \"title\": \"Weigh Your Own Words: Improving Hate Speech Counter Narrative Generation via Attention Regularization\",\\n        \"citationCount\": 11,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://arxiv.org/pdf/2309.02311\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2309.02311, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"Novel attention regularization methodologies to improve the generalization capabilities of Pretrained Transformer-based Language Models for counter narratives generation and paves the way for better and more flexible counter-speech generation models.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2023-09-05\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2161343118\",\\n            \"name\": \"Helena Bonaldi\"\\n          },\\n          {\\n            \"authorId\": \"1481857041\",\\n            \"name\": \"Giuseppe Attanasio\"\\n          },\\n          {\\n            \"authorId\": \"2101317501\",\\n            \"name\": \"Debora Nozza\"\\n          },\\n          {\\n            \"authorId\": \"1912357\",\\n            \"name\": \"Marco Guerini\"\\n          }\\n        ],\\n        \"abstract\": \"Recent computational approaches for combating online hate speech involve the automatic generation of counter narratives by adapting Pretrained Transformer-based Language Models (PLMs) with human-curated data. This process, however, can produce in-domain overfitting, resulting in models generating acceptable narratives only for hatred similar to training data, with little portability to other targets or to real-world toxic language. This paper introduces novel attention regularization methodologies to improve the generalization capabilities of PLMs for counter narratives generation. Overfitting to training-specific terms is then discouraged, resulting in more diverse and richer narratives. We experiment with two attention-based regularization techniques on a benchmark English dataset. Regularized models produce better counter narratives than state-of-the-art approaches in most cases, both in terms of automatic metrics and human evaluation, especially when hateful targets are not present in the training data. This work paves the way for better and more flexible counter-speech generation models, a task for which datasets are highly challenging to produce.\"\\n      },\\n      {\\n        \"paperId\": \"919e7ce11dd6d6605259b7d176d8cebaec9e7d06\",\\n        \"url\": \"https://www.semanticscholar.org/paper/919e7ce11dd6d6605259b7d176d8cebaec9e7d06\",\\n        \"title\": \"AI Illustrator: Translating Raw Descriptions into Images by Prompt-based Cross-Modal Generation\",\\n        \"citationCount\": 10,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://arxiv.org/pdf/2209.03160\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2209.03160, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A Prompt-based Cross-Modal Generation Framework (PCM-Frame) to leverage two powerful pre-trained models, including CLIP and StyleGAN, and conducts a user study to demonstrate its superiority over the competing methods of text-to-image translation with complicated semantics.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Book\"\\n        ],\\n        \"publicationDate\": \"2022-09-07\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"144987142\",\\n            \"name\": \"Y. Ma\"\\n          },\\n          {\\n            \"authorId\": \"46402216\",\\n            \"name\": \"Huan Yang\"\\n          },\\n          {\\n            \"authorId\": \"2127734772\",\\n            \"name\": \"Bei Liu\"\\n          },\\n          {\\n            \"authorId\": \"3247966\",\\n            \"name\": \"Jianlong Fu\"\\n          },\\n          {\\n            \"authorId\": \"2168547810\",\\n            \"name\": \"Jiaying Liu\"\\n          }\\n        ],\\n        \"abstract\": \"AI illustrator aims to automatically design visually appealing images for books to provoke rich thoughts and emotions. To achieve this goal, we propose a framework for translating raw descriptions with complex semantics into semantically corresponding images. The main challenge lies in the complexity of the semantics of raw descriptions, which may be hard to be visualized e.g., \\\\\"gloomy\\\\\" or \\\\\"Asian\\\\\"). It usually poses challenges for existing methods to handle such descriptions. To address this issue, we propose a Prompt-based Cross-Modal Generation Framework (PCM-Frame) to leverage two powerful pre-trained models, including CLIP and StyleGAN. Our framework consists of two components: a projection module from Text Embeddings to Image Embeddings based on prompts, and an adapted image generation module built on StyleGAN which takes Image Embeddings as inputs and is trained by combined semantic consistency losses. To bridge the gap between realistic images and illustration designs, we further adopt a stylization model as post-processing in our framework for better visual effects. Benefiting from the pre-trained models, our method can handle complex descriptions and does not require external paired data for training. Furthermore, we have built a benchmark that consists of 200 descriptions from literature books or online resources. We conduct a user study to demonstrate our superiority over the competing methods of text-to-image translation with complicated semantics.\"\\n      },\\n      {\\n        \"paperId\": \"8e37dc1215681aa153a51c07078ba8befd6a6e01\",\\n        \"url\": \"https://www.semanticscholar.org/paper/8e37dc1215681aa153a51c07078ba8befd6a6e01\",\\n        \"title\": \"AdaPlanner: Adaptive Planning from Feedback with Language Models\",\\n        \"citationCount\": 174,\\n        \"openAccessPdf\": {\\n          \"url\": \"http://arxiv.org/pdf/2305.16653\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2305.16653, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A closed-loop approach, AdaPlanner, is proposed, which allows the LLM agent to refine its self-generated plan adaptively in response to environmental feedback, and develops a code-style LLM prompt structure that facilitates plan generation across a variety of tasks, environments, and agent capabilities.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2023-05-26\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2118180896\",\\n            \"name\": \"Haotian Sun\"\\n          },\\n          {\\n            \"authorId\": \"8103389\",\\n            \"name\": \"Yuchen Zhuang\"\\n          },\\n          {\\n            \"authorId\": \"2865034\",\\n            \"name\": \"Lingkai Kong\"\\n          },\\n          {\\n            \"authorId\": \"2218437288\",\\n            \"name\": \"Bo Dai\"\\n          },\\n          {\\n            \"authorId\": \"145657504\",\\n            \"name\": \"Chao Zhang\"\\n          }\\n        ],\\n        \"abstract\": \"Large language models (LLMs) have recently demonstrated the potential in acting as autonomous agents for sequential decision-making tasks. However, most existing methods either take actions greedily without planning or rely on static plans that are not adaptable to environmental feedback. Consequently, the sequential decision-making performance of LLM agents degenerates with problem complexity and plan horizons increase. We propose a closed-loop approach, AdaPlanner, which allows the LLM agent to refine its self-generated plan adaptively in response to environmental feedback. In AdaPlanner, the LLM agent adaptively refines its plan from feedback with both in-plan and out-of-plan refinement strategies. To mitigate hallucination, we develop a code-style LLM prompt structure that facilitates plan generation across a variety of tasks, environments, and agent capabilities. Furthermore, we propose a skill discovery mechanism that leverages successful plans as few-shot exemplars, enabling the agent to plan and refine with fewer task demonstrations. Our experiments in the ALFWorld and MiniWoB++ environments demonstrate that AdaPlanner outperforms state-of-the-art baselines by 3.73% and 4.11% while utilizing 2x and 600x fewer samples, respectively.\"\\n      },\\n      {\\n        \"paperId\": \"3c23a892605e55f260f647234eb6b5108c84ab84\",\\n        \"url\": \"https://www.semanticscholar.org/paper/3c23a892605e55f260f647234eb6b5108c84ab84\",\\n        \"title\": \"Decoding Methods for Neural Narrative Generation\",\\n        \"citationCount\": 27,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://aclanthology.org/2021.gem-1.16.pdf\",\\n          \"status\": \"HYBRID\",\\n          \"license\": \"CCBY\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2010.07375, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work employs GPT-2 and performs ablations across nucleus sampling thresholds and diverse decoding hyperparameters and analyses results over multiple criteria with automatic and human evaluation, finding that nucleus sampling is generally best with thresholds between 0.7 and 0.9 and a maximum mutual information objective can improve the quality of generated stories.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2020-10-14\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"34989844\",\\n            \"name\": \"Alexandra DeLucia\"\\n          },\\n          {\\n            \"authorId\": \"49355602\",\\n            \"name\": \"Aaron Mueller\"\\n          },\\n          {\\n            \"authorId\": \"32551341\",\\n            \"name\": \"Xiang Lisa Li\"\\n          },\\n          {\\n            \"authorId\": \"2319137716\",\\n            \"name\": \"Jo\\\\u00e3o Sedoc\"\\n          }\\n        ],\\n        \"abstract\": \"Narrative generation is an open-ended NLP task in which a model generates a story given a prompt. The task is similar to neural response generation for chatbots; however, innovations in response generation are often not applied to narrative generation, despite the similarity between these tasks. We aim to bridge this gap by applying and evaluating advances in decoding methods for neural response generation to neural narrative generation. In particular, we employ GPT-2 and perform ablations across nucleus sampling thresholds and diverse decoding hyperparameters\\\\u2014specifically, maximum mutual information\\\\u2014analyzing results over multiple criteria with automatic and human evaluation. We find that (1) nucleus sampling is generally best with thresholds between 0.7 and 0.9; (2) a maximum mutual information objective can improve the quality of generated stories; and (3) established automatic metrics do not correlate well with human judgments of narrative quality on any qualitative metric.\"\\n      },\\n      {\\n        \"paperId\": \"08b85bce712168998004ee80ce4e475390413c74\",\\n        \"url\": \"https://www.semanticscholar.org/paper/08b85bce712168998004ee80ce4e475390413c74\",\\n        \"title\": \"A Prompt Pattern Catalog to Enhance Prompt Engineering with ChatGPT\",\\n        \"citationCount\": 1442,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2302.11382, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A catalog of prompt engineering techniques presented in pattern form that have been applied to solve common problems when conversing with LLMs to improve the outputs of LLM conversations is described.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2023-02-21\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2111231491\",\\n            \"name\": \"Jules White\"\\n          },\\n          {\\n            \"authorId\": \"31669889\",\\n            \"name\": \"Quchen Fu\"\\n          },\\n          {\\n            \"authorId\": \"2159003474\",\\n            \"name\": \"Sam Hays\"\\n          },\\n          {\\n            \"authorId\": \"2087444219\",\\n            \"name\": \"Michael Sandborn\"\\n          },\\n          {\\n            \"authorId\": \"2088026203\",\\n            \"name\": \"Carlos Olea\"\\n          },\\n          {\\n            \"authorId\": \"2180248126\",\\n            \"name\": \"Henry Gilbert\"\\n          },\\n          {\\n            \"authorId\": \"2065484415\",\\n            \"name\": \"Ashraf Elnashar\"\\n          },\\n          {\\n            \"authorId\": \"1405594772\",\\n            \"name\": \"Jesse Spencer-Smith\"\\n          },\\n          {\\n            \"authorId\": \"2064746796\",\\n            \"name\": \"Douglas C. Schmidt\"\\n          }\\n        ],\\n        \"abstract\": \"Prompt engineering is an increasingly important skill set needed to converse effectively with large language models (LLMs), such as ChatGPT. Prompts are instructions given to an LLM to enforce rules, automate processes, and ensure specific qualities (and quantities) of generated output. Prompts are also a form of programming that can customize the outputs and interactions with an LLM. This paper describes a catalog of prompt engineering techniques presented in pattern form that have been applied to solve common problems when conversing with LLMs. Prompt patterns are a knowledge transfer method analogous to software patterns since they provide reusable solutions to common problems faced in a particular context, i.e., output generation and interaction when working with LLMs. This paper provides the following contributions to research on prompt engineering that apply LLMs to automate software development tasks. First, it provides a framework for documenting patterns for structuring prompts to solve a range of problems so that they can be adapted to different domains. Second, it presents a catalog of patterns that have been applied successfully to improve the outputs of LLM conversations. Third, it explains how prompts can be built from multiple patterns and illustrates prompt patterns that benefit from combination with other prompt patterns.\"\\n      },\\n      {\\n        \"paperId\": \"177d8c5d0b172cb1da3f2694949acfd840485a4b\",\\n        \"url\": \"https://www.semanticscholar.org/paper/177d8c5d0b172cb1da3f2694949acfd840485a4b\",\\n        \"title\": \"Manage Real Time Power Imbalance with Renewable Energy: Fast Generation Dispatch or Adaptive Frequency Regulation?\",\\n        \"citationCount\": 11,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {\\'tldr\\'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/pesgm52003.2023.10252632?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/pesgm52003.2023.10252632, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": null,\\n        \"publicationTypes\": null,\\n        \"publicationDate\": \"2023-07-16\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"30562087\",\\n            \"name\": \"Ningchao Gao\"\\n          },\\n          {\\n            \"authorId\": \"145870971\",\\n            \"name\": \"X. Fang\"\\n          }\\n        ],\\n        \"abstract\": \"The carbon neutrality objective requires a large amount of renewable energy integrated into power systems. The rapid deployment of variable renewable energy (VRE), such as solar photovoltaic (PV) generation, increases the system realtime power imbalance because of the random variation and uncertainty of VRE power generation. To manage this, system operators have two potential options: The first is a fast real-time generation dispatch to schedule generation resources with a small time interval to promptly follow the load and renewable power change. The second is to procure adaptive frequency regulation services, such as secondary frequency regulation (SFR), based on the system variation and imbalance conditions to reduce the power imbalance and maintain the stable intra-interval frequency. This paper proposes an integrated alternating current optimal power flow-based generation scheduling and time domain simulation framework to investigate the economic and reliability perspectives of these two options. The impacts of the SFR requirements on the generation cost and frequency response performance are analyzed. In addition, the SFR provided by the PV generation is investigated. The uncertainty of the PV power output is considered using chance constraints to guarantee the real-time delivery of its frequency regulation services. The framework is tested in the IEEE 39-bus network integrated with a large-scale PV power plant. The simulation results demonstrate that the small dispatch interval does not necessarily improve the system frequency response and cannot maintain stable frequency in real time. An adaptive frequency regulation with a 5-minute economic dispatch interval is more appropriate and efficient to reduce the generation cost and improve the frequency response with renewable energy.\"\\n      },\\n      {\\n        \"paperId\": \"6b7f54a8451977a5d24ed1cf93a5ad71b32a9828\",\\n        \"url\": \"https://www.semanticscholar.org/paper/6b7f54a8451977a5d24ed1cf93a5ad71b32a9828\",\\n        \"title\": \"Adaptive In-Context Learning with Large Language Models for Bundle Generation\",\\n        \"citationCount\": 11,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2312.16262, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"An adaptive in-context learning paradigm is proposed, which allows LLMs to draw tailored lessons from related sessions as demonstrations, enhancing the performance on target sessions, and introduces a self-correction strategy promoting mutual improvements of the two tasks without supervision signals.\"\\n        },\\n        \"publicationTypes\": [\\n          \"Book\",\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2023-12-26\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2274082438\",\\n            \"name\": \"Zhu Sun\"\\n          },\\n          {\\n            \"authorId\": \"2273938691\",\\n            \"name\": \"Kaidong Feng\"\\n          },\\n          {\\n            \"authorId\": \"2276986037\",\\n            \"name\": \"Jie Yang\"\\n          },\\n          {\\n            \"authorId\": \"40507824\",\\n            \"name\": \"Xinghua Qu\"\\n          },\\n          {\\n            \"authorId\": \"2276798784\",\\n            \"name\": \"Hui Fang\"\\n          },\\n          {\\n            \"authorId\": \"8748397\",\\n            \"name\": \"Y. Ong\"\\n          },\\n          {\\n            \"authorId\": \"2276742821\",\\n            \"name\": \"Wenyuan Liu\"\\n          }\\n        ],\\n        \"abstract\": \"Most existing bundle generation approaches fall short in generating fixed-size bundles. Furthermore, they often neglect the underlying user intents reflected by the bundles in the generation process, resulting in less intelligible bundles. This paper addresses these limitations through the exploration of two interrelated tasks, i.e., personalized bundle generation and the underlying intent inference, based on different user sessions. Inspired by the reasoning capabilities of large language models (LLMs), we propose an adaptive in-context learning paradigm, which allows LLMs to draw tailored lessons from related sessions as demonstrations, enhancing the performance on target sessions. Specifically, we first employ retrieval augmented generation to identify nearest neighbor sessions, and then carefully design prompts to guide LLMs in executing both tasks on these neighbor sessions. To tackle reliability and hallucination challenges, we further introduce (1) a self-correction strategy promoting mutual improvements of the two tasks without supervision signals and (2) an auto-feedback mechanism for adaptive supervision based on the distinct mistakes made by LLMs on different neighbor sessions. Thereby, the target session can gain customized lessons for improved performance by observing the demonstrations of its neighbor sessions. Experiments on three real-world datasets demonstrate the effectiveness of our proposed method.\"\\n      },\\n      {\\n        \"paperId\": \"5b33c0ab87c532261435c93e38819185f0132d99\",\\n        \"url\": \"https://www.semanticscholar.org/paper/5b33c0ab87c532261435c93e38819185f0132d99\",\\n        \"title\": \"Text Prompt with Normality Guidance for Weakly Supervised Video Anomaly Detection\",\\n        \"citationCount\": 61,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://arxiv.org/pdf/2404.08531\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2404.08531, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A novel pseudo-label generation and self-training framework based on Text Prompt with Normality Guidance (TPWNG) for WSVAD is proposed and achieves state-of-the-art performance on two benchmark datasets, UCF-Crime and XD-Violence.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-04-12\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2296889795\",\\n            \"name\": \"Zhiwei Yang\"\\n          },\\n          {\\n            \"authorId\": \"2163063860\",\\n            \"name\": \"Jing Liu\"\\n          },\\n          {\\n            \"authorId\": \"2678268\",\\n            \"name\": \"Peng Wu\"\\n          }\\n        ],\\n        \"abstract\": \"Weakly supervised video anomaly detection (WSVAD) is a challenging task. Generating fine-grained pseudo-labels based on weak-label and then self-training a classifier is currently a promising solution. However, since the existing methods use only RGB visual modality and the utilization of category text information is neglected, thus limiting the generation of more accurate pseudo-labels and affecting the performance of self-training. Inspired by the manual labeling process based on the event description, in this paper, we propose a novel pseudo-label generation and self-training framework based on Text Prompt with Normality Guidance (TPWNG) for WSVAD. Our idea is to transfer the rich language-visual knowledge of the contrastive language-image pre-training (CLIP) model for aligning the video event description text and corresponding video frames to generate pseudo-labels. Specifically, We first fine-tune the CLIP for domain adaptation by designing two ranking losses and a distributional inconsistency loss. Further, we propose a learnable text prompt mechanism with the assist of a normality visual prompt to further improve the matching accuracy of video event description text and video frames. Then, we design a pseudo-label generation module based on the normality guidance to infer reliable frame-level pseudo-labels. Finally, we introduce a temporal context self-adaptive learning module to learn the temporal dependencies of different video events more flexibly and accurately. Extensive experiments show that our method achieves state-of-the-art performance on two benchmark datasets, UCF-Crime and XD-Violence, demonstrating the effectiveness of our proposed method.\"\\n      },\\n      {\\n        \"paperId\": \"9689acb6cb760e8bc21c16f368368b37dee977f9\",\\n        \"url\": \"https://www.semanticscholar.org/paper/9689acb6cb760e8bc21c16f368368b37dee977f9\",\\n        \"title\": \"Adaptive Machine Translation with Large Language Models\",\\n        \"citationCount\": 105,\\n        \"openAccessPdf\": {\\n          \"url\": \"http://arxiv.org/pdf/2301.13294\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2301.13294, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"It is observed that the translation quality with few-shot in-context learning can surpass that of strong encoder-decoder MT systems, especially for high-resource languages.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2023-01-30\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"9400076\",\\n            \"name\": \"Yasmin Moslem\"\\n          },\\n          {\\n            \"authorId\": \"1748844\",\\n            \"name\": \"Rejwanul Haque\"\\n          },\\n          {\\n            \"authorId\": \"144315616\",\\n            \"name\": \"Andy Way\"\\n          }\\n        ],\\n        \"abstract\": \"Consistency is a key requirement of high-quality translation. It is especially important to adhere to pre-approved terminology and adapt to corrected translations in domain-specific projects. Machine translation (MT) has achieved significant progress in the area of domain adaptation. However, real-time adaptation remains challenging. Large-scale language models (LLMs) have recently shown interesting capabilities of in-context learning, where they learn to replicate certain input-output text generation patterns, without further fine-tuning. By feeding an LLM at inference time with a prompt that consists of a list of translation pairs, it can then simulate the domain and style characteristics. This work aims to investigate how we can utilize in-context learning to improve real-time adaptive MT. Our extensive experiments show promising results at translation time. For example, GPT-3.5 can adapt to a set of in-domain sentence pairs and/or terminology while translating a new sentence. We observe that the translation quality with few-shot in-context learning can surpass that of strong encoder-decoder MT systems, especially for high-resource languages. Moreover, we investigate whether we can combine MT from strong encoder-decoder models with fuzzy matches, which can further improve translation quality, especially for less supported languages. We conduct our experiments across five diverse language pairs, namely English-to-Arabic (EN-AR), English-to-Chinese (EN-ZH), English-to-French (EN-FR), English-to-Kinyarwanda (EN-RW), and English-to-Spanish (EN-ES).\"\\n      },\\n      {\\n        \"paperId\": \"7665642af9e682e012bec045102a4d009421067c\",\\n        \"url\": \"https://www.semanticscholar.org/paper/7665642af9e682e012bec045102a4d009421067c\",\\n        \"title\": \"HumanGaussian: Text-Driven 3D Human Generation with Gaussian Splatting\",\\n        \"citationCount\": 97,\\n        \"openAccessPdf\": {\\n          \"url\": \"http://arxiv.org/pdf/2311.17061\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2311.17061, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"An efficient yet effective framework that generates high-quality 3D humans with fine-grained geometry and realistic appearance, based on the insight that 3D Gaussian Splatting is an efficient renderer with peri-odic Gaussian shrinkage or growing, where such adaptive density control can be naturally guided by intrinsic human structures.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2023-11-28\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2257708193\",\\n            \"name\": \"Xian Liu\"\\n          },\\n          {\\n            \"authorId\": \"2260342453\",\\n            \"name\": \"Xiaohang Zhan\"\\n          },\\n          {\\n            \"authorId\": \"1397711601\",\\n            \"name\": \"Jiaxiang Tang\"\\n          },\\n          {\\n            \"authorId\": \"2260340529\",\\n            \"name\": \"Ying Shan\"\\n          },\\n          {\\n            \"authorId\": \"2247995148\",\\n            \"name\": \"Gang Zeng\"\\n          },\\n          {\\n            \"authorId\": \"2258618427\",\\n            \"name\": \"Dahua Lin\"\\n          },\\n          {\\n            \"authorId\": \"2257370021\",\\n            \"name\": \"Xihui Liu\"\\n          },\\n          {\\n            \"authorId\": \"2249080787\",\\n            \"name\": \"Ziwei Liu\"\\n          }\\n        ],\\n        \"abstract\": \"Realistic 3D human generationfrom text prompts is a de-sirable yet challenging task. Existing methods optimize 3D representations like mesh or neural fields via score distil-lation sampling (SDS), which suffers from inadequate fine details or excessive training time. In this paper, we pro-pose an efficient yet effective framework, HumanGaussian, that generates high-quality 3D humans with fine-grained geometry and realistic appearance. Our key insight is that 3D Gaussian Splatting is an efficient renderer with peri-odic Gaussian shrinkage or growing, where such adaptive density control can be naturally guided by intrinsic human structures. Specifically, 1) we first propose a Structure-Aware SDS that simultaneously optimizes human appear-ance and geometry. The multi-modal score function from both RGB and depth space is leveraged to distill the Gaus-sian densification and pruning process. 2) Moreover, we devise an Annealed Negative Prompt Guidance by decom-posing SDS into a noisier generative score and a cleaner classifier score, which well addresses the over-saturation issue. The floating artifacts are further eliminated based on Gaussian size in a prune-only phase to enhance generation smoothness. Extensive experiments demonstrate the supe-rior efficiency and competitive quality of our framework, rendering vivid 3D humans under diverse scenarios.\"\\n      },\\n      {\\n        \"paperId\": \"5888f7aba5c601a668c290bf57addf79cc1518f1\",\\n        \"url\": \"https://www.semanticscholar.org/paper/5888f7aba5c601a668c290bf57addf79cc1518f1\",\\n        \"title\": \"ChatUniTest: a ChatGPT-based automated unit test generation tool\",\\n        \"citationCount\": 84,\\n        \"openAccessPdf\": {\\n          \"url\": \"http://arxiv.org/pdf/2305.04764\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {\\'abstract\\'}. Paper or abstract available at https://api.unpaywall.org/v2/10.48550/arXiv.2305.04764?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.48550/arXiv.2305.04764, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper introduces ChatUniTest, a ChatGPT-based automated unit test generation tool developed under the Generation-Validation-Repair framework that outperforms EvoSuite in branch and line coverage, surpasses AthenaTest and A3Test in focal method coverage, and generates assertions while utilizing mock objects and re\\\\ufb02ection to achieve test objectives.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": null,\\n        \"authors\": [\\n          {\\n            \"authorId\": \"1387638000\",\\n            \"name\": \"Zhuo-Qi Xie\"\\n          },\\n          {\\n            \"authorId\": \"2300176046\",\\n            \"name\": \"Yinghao Chen\"\\n          },\\n          {\\n            \"authorId\": \"2064478633\",\\n            \"name\": \"Chen Zhi\"\\n          },\\n          {\\n            \"authorId\": \"145590434\",\\n            \"name\": \"Shuiguang Deng\"\\n          },\\n          {\\n            \"authorId\": \"2116398505\",\\n            \"name\": \"Jianwei Yin\"\\n          }\\n        ],\\n        \"abstract\": null\\n      },\\n      {\\n        \"paperId\": \"107dce4ffbdd8d83b75492216646269d8c037ab5\",\\n        \"url\": \"https://www.semanticscholar.org/paper/107dce4ffbdd8d83b75492216646269d8c037ab5\",\\n        \"title\": \"The Future of Learning in the Age of Generative AI: Automated Question Generation and Assessment with Large Language Models\",\\n        \"citationCount\": 10,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2410.09576, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"The discussion underscores the potential of LLMs to replace costly, time-consuming human assessments when appropriately guided, showcasing their advanced understanding and reasoning capabilities in streamlining educational processes.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-10-12\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"1847858\",\\n            \"name\": \"Subhankar Maity\"\\n          },\\n          {\\n            \"authorId\": \"2144085844\",\\n            \"name\": \"Aniket Deroy\"\\n          }\\n        ],\\n        \"abstract\": \"In recent years, large language models (LLMs) and generative AI have revolutionized natural language processing (NLP), offering unprecedented capabilities in education. This chapter explores the transformative potential of LLMs in automated question generation and answer assessment. It begins by examining the mechanisms behind LLMs, emphasizing their ability to comprehend and generate human-like text. The chapter then discusses methodologies for creating diverse, contextually relevant questions, enhancing learning through tailored, adaptive strategies. Key prompting techniques, such as zero-shot and chain-of-thought prompting, are evaluated for their effectiveness in generating high-quality questions, including open-ended and multiple-choice formats in various languages. Advanced NLP methods like fine-tuning and prompt-tuning are explored for their role in generating task-specific questions, despite associated costs. The chapter also covers the human evaluation of generated questions, highlighting quality variations across different methods and areas for improvement. Furthermore, it delves into automated answer assessment, demonstrating how LLMs can accurately evaluate responses, provide constructive feedback, and identify nuanced understanding or misconceptions. Examples illustrate both successful assessments and areas needing improvement. The discussion underscores the potential of LLMs to replace costly, time-consuming human assessments when appropriately guided, showcasing their advanced understanding and reasoning capabilities in streamlining educational processes.\"\\n      },\\n      {\\n        \"paperId\": \"c0a526a448a5d0d6b17c5b08fc5920c2d22fcb74\",\\n        \"url\": \"https://www.semanticscholar.org/paper/c0a526a448a5d0d6b17c5b08fc5920c2d22fcb74\",\\n        \"title\": \"UN-SAM: Universal Prompt-Free Segmentation for Generalized Nuclei Images\",\\n        \"citationCount\": 32,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2402.16663, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work proposes the Universal prompt-free SAM framework for Nuclei segmentation (UN-SAM), by providing a fully automated solution with remarkable generalization capabilities, and devise a multi-scale Self-Prompt Generation (SPGen) module to revolutionize clinical workflow.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-02-26\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2117102192\",\\n            \"name\": \"Zhen Chen\"\\n          },\\n          {\\n            \"authorId\": \"2287826940\",\\n            \"name\": \"Qing Xu\"\\n          },\\n          {\\n            \"authorId\": \"2275332982\",\\n            \"name\": \"Xinyu Liu\"\\n          },\\n          {\\n            \"authorId\": \"2275284236\",\\n            \"name\": \"Yixuan Yuan\"\\n          }\\n        ],\\n        \"abstract\": \"In digital pathology, precise nuclei segmentation is pivotal yet challenged by the diversity of tissue types, staining protocols, and imaging conditions. Recently, the segment anything model (SAM) revealed overwhelming performance in natural scenarios and impressive adaptation to medical imaging. Despite these advantages, the reliance of labor-intensive manual annotation as segmentation prompts severely hinders their clinical applicability, especially for nuclei image analysis containing massive cells where dense manual prompts are impractical. To overcome the limitations of current SAM methods while retaining the advantages, we propose the Universal prompt-free SAM framework for Nuclei segmentation (UN-SAM), by providing a fully automated solution with remarkable generalization capabilities. Specifically, to eliminate the labor-intensive requirement of per-nuclei annotations for prompt, we devise a multi-scale Self-Prompt Generation (SPGen) module to revolutionize clinical workflow by automatically generating high-quality mask hints to guide the segmentation tasks. Moreover, to unleash the generalization capability of SAM across a variety of nuclei images, we devise a Domain-adaptive Tuning Encoder (DT-Encoder) to seamlessly harmonize visual features with domain-common and domain-specific knowledge, and further devise a Domain Query-enhanced Decoder (DQ-Decoder) by leveraging learnable domain queries for segmentation decoding in different nuclei domains. Extensive experiments prove that UN-SAM with exceptional performance surpasses state-of-the-arts in nuclei instance and semantic segmentation, especially the generalization capability in zero-shot scenarios. The source code is available at https://github.com/CUHK-AIM-Group/UN-SAM.\"\\n      },\\n      {\\n        \"paperId\": \"3c38d98583bdfc0d02b618d94394161f65d4dc96\",\\n        \"url\": \"https://www.semanticscholar.org/paper/3c38d98583bdfc0d02b618d94394161f65d4dc96\",\\n        \"title\": \"Adapting LLMs for Efficient Context Processing through Soft Prompt Compression\",\\n        \"citationCount\": 35,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://arxiv.org/pdf/2404.04997\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2404.04997, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This investigation presents an innovative framework that strategically tailors LLMs for streamlined context processing by harnessing the synergies among natural language summarization, soft prompt com- pression, and augmented utility preservation mechanisms.\"\\n        },\\n        \"publicationTypes\": [\\n          \"Book\",\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-04-07\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2295677749\",\\n            \"name\": \"Cangqing Wang\"\\n          },\\n          {\\n            \"authorId\": \"2295682977\",\\n            \"name\": \"Yutian Yang\"\\n          },\\n          {\\n            \"authorId\": \"2303231934\",\\n            \"name\": \"Ruisi Li\"\\n          },\\n          {\\n            \"authorId\": \"2295684775\",\\n            \"name\": \"Dan Sun\"\\n          },\\n          {\\n            \"authorId\": \"2295667065\",\\n            \"name\": \"Ruicong Cai\"\\n          },\\n          {\\n            \"authorId\": \"2295678151\",\\n            \"name\": \"Yuzhu Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2295659472\",\\n            \"name\": \"Chengqian Fu\"\\n          }\\n        ],\\n        \"abstract\": \"The rapid advancement of Large Language Models (LLMs) has inaugurated a transformative epoch in natural language processing, fostering unprecedented proficiency in text generation, comprehension, and contextual scrutiny. Nevertheless, effectively handling extensive contexts, crucial for myriad applications, poses a formidable obstacle owing to the intrinsic constraints of the models\\\\u2019 context window sizes and the computational burdens entailed by their operations. This investigation presents an innovative framework that strategically tailors LLMs for streamlined context processing by harnessing the synergies among natural language summarization, soft prompt com- pression, and augmented utility preservation mechanisms. Our methodology, dubbed SoftPromptComp, amalgamates natural language prompts extracted from summarization methodologies with dynamically generated soft prompts to forge a concise yet semantically robust depiction of protracted contexts. This depiction undergoes further refinement via a weighting mechanism optimizing information retention and utility for subsequent tasks. We substantiate that our framework markedly diminishes computational overhead and enhances LLMs\\\\u2019 efficacy across various benchmarks, while upholding or even augmenting the caliber of the produced content. By amalgamating soft prompt compression with sophisticated summarization, SoftPromptComp confronts the dual challenges of managing lengthy contexts and ensuring model scalability. Our findings point towards a propitious trajectory for augmenting LLMs\\\\u2019 applicability and efficiency, rendering them more versatile and pragmatic for real- world applications. This research enriches the ongoing discourse on optimizing language models, providing insights into the potency of soft prompts and summarization techniques as pivotal instruments for the forthcoming generation of NLP solutions.\"\\n      },\\n      {\\n        \"paperId\": \"836e3069a83f455f916114e7265e00187e511838\",\\n        \"url\": \"https://www.semanticscholar.org/paper/836e3069a83f455f916114e7265e00187e511838\",\\n        \"title\": \"Locally Differentially Private Document Generation Using Zero Shot Prompting\",\\n        \"citationCount\": 55,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2310.16111, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work proposes a locally differentially private mechanism called DP-Prompt, which leverages the power of pretrained large language models and zero-shot prompting to counter author de-anonymization attacks while minimizing the impact on downstream utility.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2023-10-24\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"1512255229\",\\n            \"name\": \"Saiteja Utpala\"\\n          },\\n          {\\n            \"authorId\": \"2261493078\",\\n            \"name\": \"Sara Hooker\"\\n          },\\n          {\\n            \"authorId\": \"2261697074\",\\n            \"name\": \"Pin Yu Chen\"\\n          }\\n        ],\\n        \"abstract\": \"Numerous studies have highlighted the privacy risks associated with pretrained large language models. In contrast, our research offers a unique perspective by demonstrating that pretrained large language models can effectively contribute to privacy preservation. We propose a locally differentially private mechanism called DP-Prompt, which leverages the power of pretrained large language models and zero-shot prompting to counter author de-anonymization attacks while minimizing the impact on downstream utility. When DP-Prompt is used with a powerful language model like ChatGPT (gpt-3.5), we observe a notable reduction in the success rate of de-anonymization attacks, showing that it surpasses existing approaches by a considerable margin despite its simpler design. For instance, in the case of the IMDB dataset, DP-Prompt (with ChatGPT) perfectly recovers the clean sentiment F1 score while achieving a 46\\\\\\\\% reduction in author identification F1 score against static attackers and a 26\\\\\\\\% reduction against adaptive attackers. We conduct extensive experiments across six open-source large language models, ranging up to 7 billion parameters, to analyze various effects of the privacy-utility tradeoff.\"\\n      },\\n      {\\n        \"paperId\": \"f087d0175346f143e190fc39f4ec4ceb5b3cc093\",\\n        \"url\": \"https://www.semanticscholar.org/paper/f087d0175346f143e190fc39f4ec4ceb5b3cc093\",\\n        \"title\": \"Fine-Grained Visual Prompt Learning of Vision-Language Models for Image Recognition\",\\n        \"citationCount\": 17,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1145/3581783.3612403?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/3581783.3612403, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"Fine-grained visual prompt is introduced into the image encoder of the vision-language model for focusing on the target object and conducting information interaction within the object, which facilitates generating discriminative visual features for image recognition.\"\\n        },\\n        \"publicationTypes\": [\\n          \"Book\",\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2023-10-26\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2261902040\",\\n            \"name\": \"Hongbo Sun\"\\n          },\\n          {\\n            \"authorId\": \"153003087\",\\n            \"name\": \"Xiangteng He\"\\n          },\\n          {\\n            \"authorId\": \"2261798088\",\\n            \"name\": \"Jiahuan Zhou\"\\n          },\\n          {\\n            \"authorId\": \"143753918\",\\n            \"name\": \"Yuxin Peng\"\\n          }\\n        ],\\n        \"abstract\": \"Large-scale pre-trained vision-language (VL) models have shown powerful generic representation capabilities for adapting to downstream tasks with limited training data, which are data-efficient solutions to various applications such as image recognition. In order to enhance the adaption performance, most existing methods attempt to introduce learnable vectors into the text prompt to generate adaptive classification weights for the class in the downstream task. However, they generally focus on the text side while neglecting adaptive visual feature generation on the image side, which is insufficient to fit the downstream task data. In this paper, we propose fine-grained visual prompt learning (FG-VPL) of vision-language models for image recognition with few training samples, and the main contributions are: (1) Fine-grained visual prompt is introduced into the image encoder of the vision-language model for focusing on the target object and conducting information interaction within the object, which facilitates generating discriminative visual features for image recognition. (2) A two-pathway adaptive recognition module is proposed to narrow the domain gap and utilize both the cross-modal knowledge of the vision-language model and the visual information of the few-sample training set for classifying images with the help of feature adapters. We conduct extensive experiments on 11 image recognition benchmark datasets under the few training samples setting, which demonstrate that our proposed approach can achieve state-of-the-art performance. The code is available at https://github.com/PKU-ICST-MIPL/FG-VPL_ACMMM2023.\"\\n      },\\n      {\\n        \"paperId\": \"06d8562831c32844285a691c5250d04726df3c61\",\\n        \"url\": \"https://www.semanticscholar.org/paper/06d8562831c32844285a691c5250d04726df3c61\",\\n        \"title\": \"A Systematic Survey of Prompt Engineering on Vision-Language Foundation Models\",\\n        \"citationCount\": 206,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://arxiv.org/pdf/2307.12980\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {\\'abstract\\'}. Paper or abstract available at https://api.unpaywall.org/v2/10.48550/arXiv.2307.12980?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.48550/arXiv.2307.12980, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper aims to provide a comprehensive survey of cutting-edge research in prompt engineering on three types of vision-language models: multimodal-to-text generation models, image-text matching models, and text- to-image generation models.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Review\"\\n        ],\\n        \"publicationDate\": null,\\n        \"authors\": [\\n          {\\n            \"authorId\": \"52203056\",\\n            \"name\": \"Jindong Gu\"\\n          },\\n          {\\n            \"authorId\": \"2223193538\",\\n            \"name\": \"Zhen Han\"\\n          },\\n          {\\n            \"authorId\": \"2116572341\",\\n            \"name\": \"Shuo Chen\"\\n          },\\n          {\\n            \"authorId\": \"1791052\",\\n            \"name\": \"Ahmad Beirami\"\\n          },\\n          {\\n            \"authorId\": \"2147293727\",\\n            \"name\": \"Bailan He\"\\n          },\\n          {\\n            \"authorId\": \"2143853643\",\\n            \"name\": \"Gengyuan Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2072387342\",\\n            \"name\": \"Ruotong Liao\"\\n          },\\n          {\\n            \"authorId\": \"2219078907\",\\n            \"name\": \"Yao Qin\"\\n          },\\n          {\\n            \"authorId\": \"1742501819\",\\n            \"name\": \"Volker Tresp\"\\n          },\\n          {\\n            \"authorId\": \"143635540\",\\n            \"name\": \"Philip H. S. Torr\"\\n          }\\n        ],\\n        \"abstract\": null\\n      },\\n      {\\n        \"paperId\": \"b013c9eb1284554ae696fba02bd4d7fc599890b6\",\\n        \"url\": \"https://www.semanticscholar.org/paper/b013c9eb1284554ae696fba02bd4d7fc599890b6\",\\n        \"title\": \"StyleCrafter: Enhancing Stylized Text-to-Video Generation with Style Adapter\",\\n        \"citationCount\": 34,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {\\'abstract\\'}. Paper or abstract available at https://api.unpaywall.org/v2/10.48550/arXiv.2312.00330?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.48550/arXiv.2312.00330, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"StyleCrafter is introduced, a generic method that enhances pre-trained T2V models with a style control adapter, enabling video generation in any style by providing a reference image, and designs a scale-adaptive fusion module to balance the influences of text-based content features and image-based style features.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": null,\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2269171464\",\\n            \"name\": \"Gongye Liu\"\\n          },\\n          {\\n            \"authorId\": \"2257035878\",\\n            \"name\": \"Menghan Xia\"\\n          },\\n          {\\n            \"authorId\": \"2257199953\",\\n            \"name\": \"Yong Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2149052351\",\\n            \"name\": \"Haoxin Chen\"\\n          },\\n          {\\n            \"authorId\": \"2087273800\",\\n            \"name\": \"Jinbo Xing\"\\n          },\\n          {\\n            \"authorId\": \"2253795356\",\\n            \"name\": \"Xintao Wang\"\\n          },\\n          {\\n            \"authorId\": \"3001727\",\\n            \"name\": \"Yujiu Yang\"\\n          },\\n          {\\n            \"authorId\": \"2257019659\",\\n            \"name\": \"Ying Shan\"\\n          }\\n        ],\\n        \"abstract\": null\\n      }\\n    ]\\n  },\\n  \"iterative context update\": {\\n    \"total\": 9050,\\n    \"offset\": 0,\\n    \"next\": 50,\\n    \"data\": [\\n      {\\n        \"paperId\": \"a0d2ea210c9bd21676605682a76cec1a4004320a\",\\n        \"url\": \"https://www.semanticscholar.org/paper/a0d2ea210c9bd21676605682a76cec1a4004320a\",\\n        \"title\": \"Iterative Context-Aware Graph Inference for Visual Dialog\",\\n        \"citationCount\": 51,\\n        \"openAccessPdf\": {\\n          \"url\": \"http://arxiv.org/pdf/2004.02194\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2004.02194, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work proposes a novel Context-Aware Graph (CAG) neural network, where each node in the graph corresponds to a joint semantic feature, including both object-based (visual) and history-related (textual) context representations.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2020-04-05\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"144713153\",\\n            \"name\": \"Dan Guo\"\\n          },\\n          {\\n            \"authorId\": \"46507139\",\\n            \"name\": \"Haibo Wang\"\\n          },\\n          {\\n            \"authorId\": \"5462268\",\\n            \"name\": \"Hanwang Zhang\"\\n          },\\n          {\\n            \"authorId\": \"143962510\",\\n            \"name\": \"Zhengjun Zha\"\\n          },\\n          {\\n            \"authorId\": \"47446553\",\\n            \"name\": \"Meng Wang\"\\n          }\\n        ],\\n        \"abstract\": \"Visual dialog is a challenging task that requires the comprehension of the semantic dependencies among implicit visual and textual contexts. This task can refer to the relation inference in a graphical model with sparse contexts and unknown graph structure (relation descriptor), and how to model the underlying context-aware relation inference is critical. To this end, we propose a novel Context-Aware Graph (CAG) neural network. Each node in the graph corresponds to a joint semantic feature, including both object-based (visual) and history-related (textual) context representations. The graph structure (relations in dialog) is iteratively updated using an adaptive top-K message passing mechanism. Specifically, in every message passing step, each node selects the most K relevant nodes, and only receives messages from them. Then, after the update, we impose graph attention on all the nodes to get the final graph embedding and infer the answer. In CAG, each node has dynamic relations in the graph (different related K neighbor nodes), and only the most relevant nodes are attributive to the context-aware relational graph inference. Experimental results on VisDial v0.9 and v1.0 datasets show that CAG outperforms comparative methods. Visualization results further validate the interpretability of our method.\"\\n      },\\n      {\\n        \"paperId\": \"697e176d66a17c0b24613b8513ab951dc4112c34\",\\n        \"url\": \"https://www.semanticscholar.org/paper/697e176d66a17c0b24613b8513ab951dc4112c34\",\\n        \"title\": \"Iterative Geometry Encoding Volume for Stereo Matching\",\\n        \"citationCount\": 280,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://arxiv.org/pdf/2303.06615\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2303.06615, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper proposes Iterative Geometry Encoding Volume (IGEV-Stereo), a new deep network architecture for stereo matching that builds a combined geometry encoding volume that encodes geometry and context information as well as local matching details, and iteratively indexes it to update the disparity map.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2023-03-12\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2158317969\",\\n            \"name\": \"Gangwei Xu\"\\n          },\\n          {\\n            \"authorId\": \"2107956900\",\\n            \"name\": \"Xianqi Wang\"\\n          },\\n          {\\n            \"authorId\": \"2117436225\",\\n            \"name\": \"Xiao-Hua Ding\"\\n          },\\n          {\\n            \"authorId\": \"2150441002\",\\n            \"name\": \"Xin Yang\"\\n          }\\n        ],\\n        \"abstract\": \"Recurrent All-Pairs Field Transforms (RAFT) has shown great potentials in matching tasks. However, all-pairs correlations lack non-local geometry knowledge and have difficulties tackling local ambiguities in ill-posed regions. In this paper, we propose Iterative Geometry Encoding Volume (IGEV-Stereo), a new deep network architecture for stereo matching. The proposed IGEV-Stereo builds a combined geometry encoding volume that encodes geometry and context information as well as local matching details, and iteratively indexes it to update the disparity map. To speed up the convergence, we exploit GEV to regress an accurate starting point for ConvGRUs iterations. Our IGEV-Stereo ranks 1st on KITTI 2015 and 2012 (Reflective) among all published methods and is the fastest among the top 10 methods. In addition, IGEV-Stereo has strong cross-dataset generalization as well as high inference efficiency. We also extend our IGEV to multi-view stereo (MVS), i.e. IGEV-MVS, which achieves competitive accuracy on DTU benchmark. Code is available at https://github.com/gangweiX/IGEV.\"\\n      },\\n      {\\n        \"paperId\": \"8c3a6d134aa3374ed8e1de2c47bcc73e3f9ee15e\",\\n        \"url\": \"https://www.semanticscholar.org/paper/8c3a6d134aa3374ed8e1de2c47bcc73e3f9ee15e\",\\n        \"title\": \"Proximal Stabilized Interior Point Methods and Low-Frequency-Update Preconditioning Techniques\",\\n        \"citationCount\": 13,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://link.springer.com/content/pdf/10.1007/s10957-023-02194-4.pdf\",\\n          \"status\": \"HYBRID\",\\n          \"license\": \"CCBY\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1007/s10957-023-02194-4?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1007/s10957-023-02194-4, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work considers Primal Dual Regularized Interior Point Methods in the framework of the Proximal Point Method, and shows experimentally that low-frequency-update of the preconditioners are allowed, paving the path for an alternative class of second order methods characterized by reduced computational effort.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2023-04-05\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"15569181\",\\n            \"name\": \"S. Cipolla\"\\n          },\\n          {\\n            \"authorId\": \"51043392\",\\n            \"name\": \"J. Gondzio\"\\n          }\\n        ],\\n        \"abstract\": \"In this work, in the context of Linear and convex Quadratic Programming, we consider Primal Dual Regularized Interior Point Methods (PDR-IPMs) in the framework of the Proximal Point Method. The resulting Proximal Stabilized IPM (PS-IPM) is strongly supported by theoretical results concerning convergence and the rate of convergence, and can handle degenerate problems. Moreover, in the second part of this work, we analyse the interactions between the regularization parameters and the computational footprint of the linear algebra routines used to solve the Newton linear systems. In particular, when these systems are solved using an iterative Krylov method, we are able to show\\\\u2014using a new rearrangement of the Schur complement which exploits regularization\\\\u2014that general purposes preconditioners remain attractive for a series of subsequent IPM iterations. Indeed, if on the one hand a series of theoretical results underpin the fact that the approach here presented allows a better re-use of such computed preconditioners, on the other, we show experimentally that such (re)computations are needed only in a fraction of the total IPM iterations. The resulting regularized second order methods, for which low-frequency-update of the preconditioners are allowed, pave the path for an alternative class of second order methods characterized by reduced computational effort.\"\\n      },\\n      {\\n        \"paperId\": \"8fd4d5762de7c6861c841bc54208ebeec76b6213\",\\n        \"url\": \"https://www.semanticscholar.org/paper/8fd4d5762de7c6861c841bc54208ebeec76b6213\",\\n        \"title\": \"IMENet: Joint 3D Semantic Scene Completion and 2D Semantic Segmentation through Iterative Mutual Enhancement\",\\n        \"citationCount\": 13,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://www.ijcai.org/proceedings/2021/0110.pdf\",\\n          \"status\": \"BRONZE\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2106.15413, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work argues that this sequential scheme does not ensure these two tasks fully benefit each other, and presents an Iterative Mutual Enhancement Network (IMENet) to solve them jointly, which interactively refines the two tasks at the late prediction stage.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2021-06-29\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"1490486462\",\\n            \"name\": \"Jie Li\"\\n          },\\n          {\\n            \"authorId\": \"1518268141\",\\n            \"name\": \"Laiyan Ding\"\\n          },\\n          {\\n            \"authorId\": \"1516136186\",\\n            \"name\": \"Rui Huang\"\\n          }\\n        ],\\n        \"abstract\": \"3D semantic scene completion and 2D semantic segmentation are two tightly correlated tasks that are both essential for indoor scene understanding, because they predict the same semantic classes, using positively correlated high-level features. Current methods use 2D features extracted from early-fused RGB-D images for 2D segmentation to improve 3D scene completion. We argue that this sequential scheme does not ensure these two tasks fully benefit each other, and present an Iterative Mutual Enhancement Network (IMENet) to solve them jointly, which interactively refines the two tasks at the late prediction stage. Specifically, two refinement modules are developed under a unified framework for the two tasks. The first is a 2D Deformable Context Pyramid (DCP) module, which receives the projection from the current 3D predictions to refine the 2D predictions. In turn, a 3D Deformable Depth Attention (DDA) module is proposed to leverage the reprojected results from 2D predictions to update the coarse 3D predictions. This iterative fusion happens to the stable high-level features of both tasks at a late stage. Extensive experiments on NYU and NYUCAD datasets verify the effectiveness of the proposed iterative late fusion scheme, and our approach outperforms the state of the art on both 3D semantic scene completion and 2D semantic segmentation.\"\\n      },\\n      {\\n        \"paperId\": \"be089bc2e9983223068b4eb0d67b4b2f5fc30321\",\\n        \"url\": \"https://www.semanticscholar.org/paper/be089bc2e9983223068b4eb0d67b4b2f5fc30321\",\\n        \"title\": \"Iterative Privileged Learning\",\\n        \"citationCount\": 17,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {\\'tldr\\'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/TNNLS.2018.2889906?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/TNNLS.2018.2889906, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": null,\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2020-08-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2116383294\",\\n            \"name\": \"Xue Li\"\\n          },\\n          {\\n            \"authorId\": \"145728792\",\\n            \"name\": \"Bo Du\"\\n          },\\n          {\\n            \"authorId\": \"2760404\",\\n            \"name\": \"Yipeng Zhang\"\\n          },\\n          {\\n            \"authorId\": null,\\n            \"name\": \"Chang Xu\"\\n          },\\n          {\\n            \"authorId\": \"143719920\",\\n            \"name\": \"D. Tao\"\\n          }\\n        ],\\n        \"abstract\": \"While in the learning using privileged information paradigm, privileged information may not be as informative as example features in the context of making accurate label predictions, it may be able to provide some effective comments (e.g., the values of the auxiliary function) like a human teacher on the efficacy of the learned model. In a departure from conventional static manipulations of privileged information within the support vector machine framework, this paper investigates iterative privileged learning within the context of gradient boosted decision trees (GBDTs). As the learned model evolves, the comments learned from privileged information to assess the model should also be actively upgraded instead of remaining static and passive. During the learning phase of the GBDT method, new DTs are discovered to enhance the performance of the model, and iteratively update the comments generated from the privileged information to accurately assess and coach the up-to-date model. The resulting objective function can be efficiently solved within the gradient boosting framework. Experimental results on real-world data sets demonstrate the benefits of studying privileged information in an iterative manner, as well as the effectiveness of the proposed algorithm.\"\\n      },\\n      {\\n        \"paperId\": \"22ae025a4f0b644369d05a3e13c0021f868a8372\",\\n        \"url\": \"https://www.semanticscholar.org/paper/22ae025a4f0b644369d05a3e13c0021f868a8372\",\\n        \"title\": \"Hunyuan-DiT: A Powerful Multi-Resolution Diffusion Transformer with Fine-Grained Chinese Understanding\",\\n        \"citationCount\": 201,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2405.08748, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"Huyuan-DiT, a text-to-image diffusion transformer with fine-grained understanding of both English and Chinese, sets a new state-of-the-art in Chinese-to-image generation compared with other open-source models.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-05-14\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2286959603\",\\n            \"name\": \"Zhimin Li\"\\n          },\\n          {\\n            \"authorId\": \"2301232669\",\\n            \"name\": \"Jianwei Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2301456725\",\\n            \"name\": \"Qin Lin\"\\n          },\\n          {\\n            \"authorId\": \"2291439620\",\\n            \"name\": \"Jiangfeng Xiong\"\\n          },\\n          {\\n            \"authorId\": \"2301272172\",\\n            \"name\": \"Yanxin Long\"\\n          },\\n          {\\n            \"authorId\": \"2291203760\",\\n            \"name\": \"Xinchi Deng\"\\n          },\\n          {\\n            \"authorId\": \"2301231346\",\\n            \"name\": \"Yingfang Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2278003955\",\\n            \"name\": \"Xingchao Liu\"\\n          },\\n          {\\n            \"authorId\": \"2162225343\",\\n            \"name\": \"Minbin Huang\"\\n          },\\n          {\\n            \"authorId\": \"2301268000\",\\n            \"name\": \"Zedong Xiao\"\\n          },\\n          {\\n            \"authorId\": \"2301264655\",\\n            \"name\": \"Dayou Chen\"\\n          },\\n          {\\n            \"authorId\": \"2268375443\",\\n            \"name\": \"Jiajun He\"\\n          },\\n          {\\n            \"authorId\": \"2276668269\",\\n            \"name\": \"Jiahao Li\"\\n          },\\n          {\\n            \"authorId\": \"2301265720\",\\n            \"name\": \"Wenyue Li\"\\n          },\\n          {\\n            \"authorId\": \"2279868541\",\\n            \"name\": \"Chen Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2194482010\",\\n            \"name\": \"Rongwei Quan\"\\n          },\\n          {\\n            \"authorId\": \"2301417687\",\\n            \"name\": \"Jianxiang Lu\"\\n          },\\n          {\\n            \"authorId\": \"2301265144\",\\n            \"name\": \"Jiabin Huang\"\\n          },\\n          {\\n            \"authorId\": \"2302916549\",\\n            \"name\": \"Xiaoyan Yuan\"\\n          },\\n          {\\n            \"authorId\": \"2282541108\",\\n            \"name\": \"Xiao-Ting Zheng\"\\n          },\\n          {\\n            \"authorId\": \"2238393614\",\\n            \"name\": \"Yixuan Li\"\\n          },\\n          {\\n            \"authorId\": \"2290433340\",\\n            \"name\": \"Jihong Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2256776298\",\\n            \"name\": \"Chao Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2279065846\",\\n            \"name\": \"Mengxi Chen\"\\n          },\\n          {\\n            \"authorId\": \"2290446584\",\\n            \"name\": \"Jie Liu\"\\n          },\\n          {\\n            \"authorId\": \"2180527302\",\\n            \"name\": \"Zheng Fang\"\\n          },\\n          {\\n            \"authorId\": \"2280852403\",\\n            \"name\": \"Weiyan Wang\"\\n          },\\n          {\\n            \"authorId\": \"2067731583\",\\n            \"name\": \"J. Xue\"\\n          },\\n          {\\n            \"authorId\": \"2267016579\",\\n            \"name\": \"Yang-Dan Tao\"\\n          },\\n          {\\n            \"authorId\": \"2141509708\",\\n            \"name\": \"Jianchen Zhu\"\\n          },\\n          {\\n            \"authorId\": \"2278809013\",\\n            \"name\": \"Kai Liu\"\\n          },\\n          {\\n            \"authorId\": \"2107932778\",\\n            \"name\": \"Si-Da Lin\"\\n          },\\n          {\\n            \"authorId\": \"2373974424\",\\n            \"name\": \"Yifu Sun\"\\n          },\\n          {\\n            \"authorId\": \"2261466479\",\\n            \"name\": \"Yun Li\"\\n          },\\n          {\\n            \"authorId\": \"2195078271\",\\n            \"name\": \"Dongdong Wang\"\\n          },\\n          {\\n            \"authorId\": \"2393630397\",\\n            \"name\": \"Mingtao Chen\"\\n          },\\n          {\\n            \"authorId\": \"2297268600\",\\n            \"name\": \"Zhichao Hu\"\\n          },\\n          {\\n            \"authorId\": \"2290971075\",\\n            \"name\": \"Xiao Xiao\"\\n          },\\n          {\\n            \"authorId\": \"2266017750\",\\n            \"name\": \"Yan Chen\"\\n          },\\n          {\\n            \"authorId\": \"2265797902\",\\n            \"name\": \"Yuhong Liu\"\\n          },\\n          {\\n            \"authorId\": \"2283172530\",\\n            \"name\": \"Wei Liu\"\\n          },\\n          {\\n            \"authorId\": \"2256286591\",\\n            \"name\": \"Dingyong Wang\"\\n          },\\n          {\\n            \"authorId\": \"2284490220\",\\n            \"name\": \"Yong Yang\"\\n          },\\n          {\\n            \"authorId\": \"2280399696\",\\n            \"name\": \"Jie Jiang\"\\n          },\\n          {\\n            \"authorId\": \"2268722917\",\\n            \"name\": \"Qinglin Lu\"\\n          }\\n        ],\\n        \"abstract\": \"We present Hunyuan-DiT, a text-to-image diffusion transformer with fine-grained understanding of both English and Chinese. To construct Hunyuan-DiT, we carefully design the transformer structure, text encoder, and positional encoding. We also build from scratch a whole data pipeline to update and evaluate data for iterative model optimization. For fine-grained language understanding, we train a Multimodal Large Language Model to refine the captions of the images. Finally, Hunyuan-DiT can perform multi-turn multimodal dialogue with users, generating and refining images according to the context. Through our holistic human evaluation protocol with more than 50 professional human evaluators, Hunyuan-DiT sets a new state-of-the-art in Chinese-to-image generation compared with other open-source models. Code and pretrained models are publicly available at github.com/Tencent/HunyuanDiT\"\\n      },\\n      {\\n        \"paperId\": \"7d2f92bfcded0fe63cb3926155d769263b9580c9\",\\n        \"url\": \"https://www.semanticscholar.org/paper/7d2f92bfcded0fe63cb3926155d769263b9580c9\",\\n        \"title\": \"RLCoder: Reinforcement Learning for Repository-Level Code Completion\",\\n        \"citationCount\": 39,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2407.19487, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work proposes RLCoder, a novel reinforcement learning framework, which can enable the retriever to learn to retrieve useful content for code completion without the need for labeled data and introduces a stop signal mechanism.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-07-28\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2279774134\",\\n            \"name\": \"Yanlin Wang\"\\n          },\\n          {\\n            \"authorId\": \"2279774134\",\\n            \"name\": \"Yanlin Wang\"\\n          },\\n          {\\n            \"authorId\": \"2314374754\",\\n            \"name\": \"Daya Guo\"\\n          },\\n          {\\n            \"authorId\": \"2254800142\",\\n            \"name\": \"Jiachi Chen\"\\n          },\\n          {\\n            \"authorId\": \"2305869100\",\\n            \"name\": \"Ruikai Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2305694096\",\\n            \"name\": \"Yuchi Ma\"\\n          },\\n          {\\n            \"authorId\": \"2267902535\",\\n            \"name\": \"Zibin Zheng\"\\n          }\\n        ],\\n        \"abstract\": \"Repository-level code completion aims to generate code for unfinished code snippets within the context of a specified repository. Existing approaches mainly rely on retrievalaugmented generation strategies due to limitations in input sequence length. However, traditional lexical-based retrieval methods like BM25 struggle to capture code semantics, while model-based retrieval methods face challenges due to the lack of labeled data for training. Therefore, we propose RLCoder, a novel reinforcement learning framework, which can enable the retriever to learn to retrieve useful content for code completion without the need for labeled data. Specifically, we iteratively evaluate the usefulness of retrieved content based on the perplexity of the target code when provided with the retrieved content as additional context, and provide feedback to update the retriever parameters. This iterative process enables the retriever to learn from its successes and failures, gradually improving its ability to retrieve relevant and high-quality content. Considering that not all situations require information beyond code files and not all retrieved context is helpful for generation, we also introduce a stop signal mechanism, allowing the retriever to decide when to retrieve and which candidates to retain autonomously. Extensive experimental results demonstrate that RLCoder consistently outperforms state-of-the-art methods on CrossCodeEval and RepoEval, achieving 12.2% EM improvement over previous methods. Moreover, experiments show that our framework can generalize across different programming languages and further improve previous methods like RepoCoder. We provide the code and data at https://github.com/DeepSoftwareAnalytics/RLCoder.\"\\n      },\\n      {\\n        \"paperId\": \"69ec41a25a4ead52bab62ea220103fdde06f7126\",\\n        \"url\": \"https://www.semanticscholar.org/paper/69ec41a25a4ead52bab62ea220103fdde06f7126\",\\n        \"title\": \"Logic-in-Frames: Dynamic Keyframe Search via Visual Semantic-Logical Verification for Long Video Understanding\",\\n        \"citationCount\": 15,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2503.13139, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work introduces a semantics-driven search framework that reformulates keyframe selection under the paradigm of Visual Semantic-Logical Search, validating its effectiveness in bridging the logical gap between textual queries and visual-temporal reasoning.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2025-03-17\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2257320765\",\\n            \"name\": \"Weiyu Guo\"\\n          },\\n          {\\n            \"authorId\": \"2347655949\",\\n            \"name\": \"Ziyang Chen\"\\n          },\\n          {\\n            \"authorId\": \"2347552474\",\\n            \"name\": \"Shaoguang Wang\"\\n          },\\n          {\\n            \"authorId\": \"2347022035\",\\n            \"name\": \"Jianxiang He\"\\n          },\\n          {\\n            \"authorId\": \"2294806563\",\\n            \"name\": \"Yijie Xu\"\\n          },\\n          {\\n            \"authorId\": \"2348899671\",\\n            \"name\": \"Jinhui Ye\"\\n          },\\n          {\\n            \"authorId\": \"2257321422\",\\n            \"name\": \"Ying Sun\"\\n          },\\n          {\\n            \"authorId\": \"2346984163\",\\n            \"name\": \"Hui Xiong\"\\n          }\\n        ],\\n        \"abstract\": \"Understanding long video content is a complex endeavor that often relies on densely sampled frame captions or end-to-end feature selectors, yet these techniques commonly overlook the logical relationships between textual queries and visual elements. In practice, computational constraints necessitate coarse frame subsampling, a challenge analogous to\\\\\"finding a needle in a haystack.\\\\\"To address this issue, we introduce a semantics-driven search framework that reformulates keyframe selection under the paradigm of Visual Semantic-Logical Search. Specifically, we systematically define four fundamental logical dependencies: 1) spatial co-occurrence, 2) temporal proximity, 3) attribute dependency, and 4) causal order. These relations dynamically update frame sampling distributions through an iterative refinement process, enabling context-aware identification of semantically critical frames tailored to specific query requirements. Our method establishes new SOTA performance on the manually annotated benchmark in key-frame selection metrics. Furthermore, when applied to downstream video question-answering tasks, the proposed approach demonstrates the best performance gains over existing methods on LongVideoBench and Video-MME, validating its effectiveness in bridging the logical gap between textual queries and visual-temporal reasoning. The code will be publicly available.\"\\n      },\\n      {\\n        \"paperId\": \"c5dae02bb601106a51bec4497f0581affd6af8a7\",\\n        \"url\": \"https://www.semanticscholar.org/paper/c5dae02bb601106a51bec4497f0581affd6af8a7\",\\n        \"title\": \"Zero-Shot Class Unlearning in CLIP with Synthetic Samples\",\\n        \"citationCount\": 10,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2407.07485, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work focuses on unlearning within CLIP, a dual vision-language encoder model trained on a massive dataset of image-text pairs using contrastive loss, and expands the application of Lipschitz regularization to the multimodal context of CLIP to achieve forgetting.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-07-10\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2310610834\",\\n            \"name\": \"Alexey Kravets\"\\n          },\\n          {\\n            \"authorId\": \"2310607749\",\\n            \"name\": \"Vinay Namboodiri\"\\n          }\\n        ],\\n        \"abstract\": \"Machine unlearning is a crucial area of research. It is driven by the need to remove sensitive information from models to safeguard individuals\\' right to be forgotten under rigorous regulations such as GDPR. In this work, we focus on unlearning within CLIP, a dual vision-language encoder model trained on a massive dataset of image-text pairs using contrastive loss. To achieve forgetting we expand the application of Lipschitz regularization to the multimodal context of CLIP. Specifically, we smooth both visual and textual embeddings associated with the class intended to be forgotten relative to the perturbation introduced to the samples from that class. Additionally, importantly, we remove the necessity for real forgetting data by generating synthetic samples via gradient ascent maximizing the target class. Our forgetting procedure is iterative, where we track accuracy on a synthetic forget set and stop when accuracy falls below a chosen threshold. We employ a selective layers update strategy based on their average absolute gradient value to mitigate over-forgetting. We validate our approach on several standard datasets and provide thorough ablation analysis and comparisons with previous work.\"\\n      },\\n      {\\n        \"paperId\": \"096df636db478a4eb7ab9c74f8a6cae97efed149\",\\n        \"url\": \"https://www.semanticscholar.org/paper/096df636db478a4eb7ab9c74f8a6cae97efed149\",\\n        \"title\": \"Real-time parameter updating for nonlinear digital twins using inverse mapping models and transient-based features\",\\n        \"citationCount\": 20,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://link.springer.com/content/pdf/10.1007/s11071-023-08354-5.pdf\",\\n          \"status\": \"HYBRID\",\\n          \"license\": \"CCBY\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1007/s11071-023-08354-5?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1007/s11071-023-08354-5, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"Various types of transient response features are introduced to update parameter values of nonlinear dynamical systems with increased computational efficiency and accuracy and it is shown that a smart selection of features, based on the frequency content of the transient response, can improve the accuracy of the estimated parameter values, leading to more accurate updated models.\"\\n        },\\n        \"publicationTypes\": null,\\n        \"publicationDate\": \"2023-03-26\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2133908735\",\\n            \"name\": \"B. M. Kessels\"\\n          },\\n          {\\n            \"authorId\": \"102993518\",\\n            \"name\": \"R. Fey\"\\n          },\\n          {\\n            \"authorId\": \"2246982557\",\\n            \"name\": \"N. van de Wouw\"\\n          }\\n        ],\\n        \"abstract\": \"In the context of digital twins, it is essential that a model gives an accurate description of the (controlled) dynamic behavior of a physical system during the system\\\\u2019s entire operational life. Therefore, model updating techniques are required that enable real-time updating of physically interpretable parameter values and are applicable to a wide range of (nonlinear) dynamical systems. As traditional, iterative, parameter updating methods may be computationally too expensive for real-time updating, the inverse mapping parameter updating (IMPU) method is proposed as an alternative. For this method, first, an artificial neural network (ANN) is trained offline using novel features of simulated transient response data. Then, in the online phase, this ANN maps, with little computational cost, a set of measured output response features to parameter estimates enabling real-time model updating. In this paper, various types of transient response features are introduced to update parameter values of nonlinear dynamical systems with increased computational efficiency and accuracy. To analyze the efficacy of these features, the IMPU method is applied to a (simulated) nonlinear multibody system. It is shown that a smart selection of features, based on, e.g., the frequency content of the transient response, can improve the accuracy of the estimated parameter values, leading to more accurate updated models. Furthermore, the generalization capabilities of the ANNs are analyzed for these feature types, by varying the number of training samples and assessing the effect of incomplete training data. It is shown that the IMPU method can predict parameter values that are not part of the training data with acceptable accuracy as well.\"\\n      },\\n      {\\n        \"paperId\": \"82b7f2ab4faec66f16a62b9b5c610c7a5aa1e410\",\\n        \"url\": \"https://www.semanticscholar.org/paper/82b7f2ab4faec66f16a62b9b5c610c7a5aa1e410\",\\n        \"title\": \"Meta-AF: Meta-Learning for Adaptive Filters\",\\n        \"citationCount\": 31,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://ieeexplore.ieee.org/ielx7/6570655/9970249/09961879.pdf\",\\n          \"status\": \"HYBRID\",\\n          \"license\": \"CCBYNCND\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2204.11942, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work seeks to improve upon hand-derived adaptive filter algorithms and present a comprehensive framework for learning online, adaptive signal processing algorithms or update rules directly from data and frames the development of adaptive filters as a meta-learning problem in the context of deep learning and uses a form of self-supervision to learn online iterative update rules for adaptive filters.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2022-04-25\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"50823831\",\\n            \"name\": \"Jonah Casebeer\"\\n          },\\n          {\\n            \"authorId\": \"32125125\",\\n            \"name\": \"Nicholas J. Bryan\"\\n          },\\n          {\\n            \"authorId\": \"1718742\",\\n            \"name\": \"Paris Smaragdis\"\\n          }\\n        ],\\n        \"abstract\": \"Adaptive filtering algorithms are pervasive throughout signal processing and have had a material impact on a wide variety of domains including audio processing, telecommunications, biomedical sensing, astrophysics and cosmology, seismology, and many more. Adaptive filters typically operate via specialized online, iterative optimization methods such as least-mean squares or recursive least squares and aim to process signals in unknown or nonstationary environments. Such algorithms, however, can be slow and laborious to develop, require domain expertise to create, and necessitate mathematical insight for improvement. In this work, we seek to improve upon hand-derived adaptive filter algorithms and present a comprehensive framework for learning online, adaptive signal processing algorithms or update rules directly from data. To do so, we frame the development of adaptive filters as a meta-learning problem in the context of deep learning and use a form of self-supervision to learn online iterative update rules for adaptive filters. To demonstrate our approach, we focus on audio applications and systematically develop meta-learned adaptive filters for five canonical audio problems including system identification, acoustic echo cancellation, blind equalization, multi-channel dereverberation, and beamforming.We compare our approach against common baselines and/or recent state-of-the-art methods. We show we can learn high-performing adaptive filters that operate in real-time and, in most cases, significantly outperform each method we compare against \\\\u2013 all using a single general-purpose configuration of our approach.\"\\n      },\\n      {\\n        \"paperId\": \"f55c7f2156ecc389132acf5c3fbaca4c3f832abf\",\\n        \"url\": \"https://www.semanticscholar.org/paper/f55c7f2156ecc389132acf5c3fbaca4c3f832abf\",\\n        \"title\": \"Flexible Task Scheduling in Data Relay Satellite Networks\",\\n        \"citationCount\": 22,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {\\'tldr\\'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/TAES.2021.3115587?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/TAES.2021.3115587, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": null,\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2022-04-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"46532001\",\\n            \"name\": \"Guohua Wu\"\\n          },\\n          {\\n            \"authorId\": \"30176488\",\\n            \"name\": \"Qizhang Luo\"\\n          },\\n          {\\n            \"authorId\": \"2153095510\",\\n            \"name\": \"Yanqi Zhu\"\\n          },\\n          {\\n            \"authorId\": \"2049670966\",\\n            \"name\": \"Xinjiang Chen\"\\n          },\\n          {\\n            \"authorId\": \"2115387723\",\\n            \"name\": \"Yang Feng\"\\n          },\\n          {\\n            \"authorId\": \"1731634\",\\n            \"name\": \"W. Pedrycz\"\\n          }\\n        ],\\n        \"abstract\": \"The task-schedulingalgorithm is a key module to satisfy various complex user requirements, and improve the usage flexibility and efficiency of data relay satellites networks (DRSN). In this context, we first propose a novel application mode for DRSN, in which users are allowed to submit multiple optional service time windows and specify a preferred antenna as well as an expected execution duration for each task. Meanwhile, the start time of a service time window can be adjusted within a specified range. A mathematical programming model that maximizes the completion ratio of tasks and the expectation satisfaction of users is established. Moreover, a conflict resolution-assisted iterative task-scheduling algorithm (CRITS) is designed, composing of five closely dependent operators: resource matching, service durations generation, conflict evaluation, conflict resolution, and solution update. To verify the effectiveness of the proposed CRITS, extensive experiments are carried out. The experimental results demonstrate the competitive performance of CRITS in addressing the DRSN scheduling problem. In comparison with two heuristic algorithms (heuristic algorithm based on time-freedom degree and a heuristic algorithm based on task priority) and a meta-heuristic algorithm (adaptive variable neighborhood descent combined with a tabu list), the proposed CRITS increases the overall completion ratio of tasks by 6.65, 10.26, and 10.96%, respectively.\"\\n      },\\n      {\\n        \"paperId\": \"b12f40443c189e665520811f99f0ec2b47d90194\",\\n        \"url\": \"https://www.semanticscholar.org/paper/b12f40443c189e665520811f99f0ec2b47d90194\",\\n        \"title\": \"RSS-Based Byzantine Fault-Tolerant Localization Algorithm Under NLOS Environment\",\\n        \"citationCount\": 28,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {\\'tldr\\'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/LCOMM.2020.3027904?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/LCOMM.2020.3027904, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": null,\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2021-02-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"26411201\",\\n            \"name\": \"Xiaojun Mei\"\\n          },\\n          {\\n            \"authorId\": \"35250883\",\\n            \"name\": \"Huafeng Wu\"\\n          },\\n          {\\n            \"authorId\": \"144922818\",\\n            \"name\": \"J. Xian\"\\n          },\\n          {\\n            \"authorId\": \"150041356\",\\n            \"name\": \"Bowen Chen\"\\n          }\\n        ],\\n        \"abstract\": \"Localization is one of the most critical tasks in wireless sensor networks, but achieving a relatively accurate location estimation is challenging when there have Byzantine fault and non-line-of-sight (NLOS) bias simultaneously. In this context, a localization method, based on received signal strength (RSS), is proposed in this letter to mitigate the impact of Byzantine fault and NLOS bias on the localization accuracy of wireless sensor networks. The proposed method relies on a Byzantine fault-tolerant localization algorithm (BFLA), which converts the localization problem into a generalized trust-region subproblem (GTRS) by applying certain approximations. In order to obtain a feasible solution to the GTRS, a block-coordinate update (BCU) function with a regularization term is used to divide the localization problem into two subproblems. An iterative method, whose start-point is obtained by an unconstrained squared-range (USR) algorithm, is then used to obtain a solution. Numerical simulations are carried out to show the effectiveness of the proposed method, compared with the state-of-the-art approaches in different scenarios.\"\\n      },\\n      {\\n        \"paperId\": \"c04e9454a696086ea69f37f59d45347f5c6d438c\",\\n        \"url\": \"https://www.semanticscholar.org/paper/c04e9454a696086ea69f37f59d45347f5c6d438c\",\\n        \"title\": \"Enhancing Transferability of Adversarial Examples with Spatial Momentum\",\\n        \"citationCount\": 10,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2203.13479, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A novel method named Spatial Momentum Iterative FGSM attack (SMI-FGSM), which introduces the mechanism of momentum accumulation from temporal domain to spatial domain by considering the context information from different regions within the image to enhance adversarial transferability.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2022-03-25\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2445322\",\\n            \"name\": \"Guoqiu Wang\"\\n          },\\n          {\\n            \"authorId\": \"27066021\",\\n            \"name\": \"Huanqian Yan\"\\n          },\\n          {\\n            \"authorId\": \"2769710\",\\n            \"name\": \"Xingxing Wei\"\\n          }\\n        ],\\n        \"abstract\": \"Many adversarial attack methods achieve satisfactory attack success rates under the white-box setting, but they usually show poor transferability when attacking other DNN models. Momentum-based attack is one effective method to improve transferability. It integrates the momentum term into the iterative process, which can stabilize the update directions by adding the gradients\\' temporal correlation for each pixel. We argue that only this temporal momentum is not enough, the gradients from the spatial domain within an image, i.e. gradients from the context pixels centered on the target pixel are also important to the stabilization. For that, we propose a novel method named Spatial Momentum Iterative FGSM attack (SMI-FGSM), which introduces the mechanism of momentum accumulation from temporal domain to spatial domain by considering the context information from different regions within the image. SMI-FGSM is then integrated with temporal momentum to simultaneously stabilize the gradients\\' update direction from both the temporal and spatial domains. Extensive experiments show that our method indeed further enhances adversarial transferability. It achieves the best transferability success rate for multiple mainstream undefended and defended models, which outperforms the state-of-the-art attack methods by a large margin of 10\\\\\\\\% on average.\"\\n      },\\n      {\\n        \"paperId\": \"8a1f15ef8875b2f2a80a5ddd52ea41967203cd26\",\\n        \"url\": \"https://www.semanticscholar.org/paper/8a1f15ef8875b2f2a80a5ddd52ea41967203cd26\",\\n        \"title\": \"xERTE: Explainable Reasoning on Temporal Knowledge Graphs for Forecasting Future Links\",\\n        \"citationCount\": 35,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2012.15537, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A future link forecasting framework that reasons over query-relevant subgraphs of temporal KGs and jointly models the graph structures and the temporal context information and conducts a survey to show that the reasoning arguments extracted by the model for link forecasting are aligned with human understanding.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Review\"\\n        ],\\n        \"publicationDate\": \"2020-12-31\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2765914\",\\n            \"name\": \"Zhen Han\"\\n          },\\n          {\\n            \"authorId\": \"2158172225\",\\n            \"name\": \"Peng Chen\"\\n          },\\n          {\\n            \"authorId\": \"10684484\",\\n            \"name\": \"Yunpu Ma\"\\n          },\\n          {\\n            \"authorId\": \"1742501819\",\\n            \"name\": \"Volker Tresp\"\\n          }\\n        ],\\n        \"abstract\": \"Interest has been rising lately towards modeling time-evolving knowledge graphs (KGs). Recently, graph representation learning approaches have become the dominant paradigm for link prediction on temporal KGs. However, the embedding-based approaches largely operate in a black-box fashion, lacking the ability to judge the results\\' reliability. This paper provides a future link forecasting framework that reasons over query-relevant subgraphs of temporal KGs and jointly models the graph structures and the temporal context information. Especially, we propose a temporal relational attention mechanism and a novel reverse representation update scheme to guide the extraction of an enclosing subgraph around the query. The subgraph is expanded by an iterative sampling of temporal neighbors and attention propagation. As a result, our approach provides human-understandable arguments for the prediction. We evaluate our model on four benchmark temporal knowledge graphs for the link forecasting task. While being more explainable, our model also obtains a relative improvement of up to 17.7 $\\\\\\\\%$ on MRR compared to the previous best KG forecasting methods. We also conduct a survey with 53 respondents, and the results show that the reasoning arguments extracted by the model for link forecasting are aligned with human understanding.\"\\n      },\\n      {\\n        \"paperId\": \"51458bf945a446271cf857016582d3fc881f8f9a\",\\n        \"url\": \"https://www.semanticscholar.org/paper/51458bf945a446271cf857016582d3fc881f8f9a\",\\n        \"title\": \"Joint User Grouping, Scheduling, and Precoding for Multicast Energy Efficiency in Multigroup Multicast Systems\",\\n        \"citationCount\": 17,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://doi.org/10.1109/twc.2020.3020081\",\\n          \"status\": \"HYBRID\",\\n          \"license\": \"CCBY\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2005.06843, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper studies the joint design of user grouping, scheduling and precoding to optimize energy efficiency (EE) for multigroup multicast scenarios in single-cell multiuser MISO downlink channels and proposes a convex-concave procedure framework based iterative algorithm for each optimization criteria.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2020-05-14\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"40370059\",\\n            \"name\": \"Ashok Bandi\"\\n          },\\n          {\\n            \"authorId\": \"153604607\",\\n            \"name\": \"Bhavani Shankar\"\\n          },\\n          {\\n            \"authorId\": \"1760292\",\\n            \"name\": \"S. Chatzinotas\"\\n          },\\n          {\\n            \"authorId\": \"102896981\",\\n            \"name\": \"B. Ottersten\"\\n          }\\n        ],\\n        \"abstract\": \"This paper studies the joint design of user grouping, scheduling (or admission control) and precoding to optimize energy efficiency (EE) for multigroup multicast scenarios in single-cell multiuser MISO downlink channels. Noticing that the existing definition of EE fails to account for group sizes, a new metric called multicast energy efficiency (MEE) is proposed. In this context, the joint design is considered for the maximization of MEE, EE, and scheduled users. Firstly, with the help of binary variables (associated with grouping and scheduling) the joint design problem is formulated as a mixed-Boolean fractional programming problem such that it facilitates the joint update of grouping, scheduling and precoding variables. Further, several novel optimization formulations are proposed to reveal the hidden difference of convex/ concave structure in the objective and associated constraints. Thereafter, we propose a convex-concave procedure framework based iterative algorithm for each optimization criteria where grouping, scheduling, and precoding variables are updated jointly in each iteration. Finally, we compare the performance of the three design criteria concerning three performance metrics namely MEE, EE, and scheduled users through Monte-Carlo simulations. These simulations establish the need for MEE and the improvement from the system optimization.\"\\n      },\\n      {\\n        \"paperId\": \"8c8595a050845a8926aa52b22998b2ca609a5d96\",\\n        \"url\": \"https://www.semanticscholar.org/paper/8c8595a050845a8926aa52b22998b2ca609a5d96\",\\n        \"title\": \"Automatic Distortion Rectification of Wide-Angle Images Using Outlier Refinement for Streamlining Vision Tasks\",\\n        \"citationCount\": 16,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://www.mdpi.com/1424-8220/20/3/894/pdf?version=1581513374\",\\n          \"status\": \"GOLD\",\\n          \"license\": \"CCBY\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC7039389, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"The proposed algorithm was able to rectify the distortions of wide-angle and fish-eye cameras even in extreme conditions such as heavy illumination changes and severe lens distortions.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2020-02-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"97740251\",\\n            \"name\": \"Vijay Kakani\"\\n          },\\n          {\\n            \"authorId\": \"2380060\",\\n            \"name\": \"Hakil Kim\"\\n          },\\n          {\\n            \"authorId\": \"2108780171\",\\n            \"name\": \"Jongso Lee\"\\n          },\\n          {\\n            \"authorId\": \"34630628\",\\n            \"name\": \"Choonwoo Ryu\"\\n          },\\n          {\\n            \"authorId\": \"14653761\",\\n            \"name\": \"Mahendar Kumbham\"\\n          }\\n        ],\\n        \"abstract\": \"The study proposes an outlier refinement methodology for automatic distortion rectification of wide-angle and fish-eye lens camera models in the context of streamlining vision-based tasks. The line-members sets are estimated in a scene through accumulation of line candidates emerging from the same edge source. An iterative optimization with an outlier refinement scheme was applied to the loss value, to simultaneously remove the extremely curved outliers from the line-members set and update the robust line members as well as estimating the best-fit distortion parameters with lowest possible loss. The proposed algorithm was able to rectify the distortions of wide-angle and fish-eye cameras even in extreme conditions such as heavy illumination changes and severe lens distortions. Experiments were conducted using various evaluation metrics both at the pixel-level (image quality, edge stretching effects, pixel-point error) as well as higher-level use-cases (object detection, height estimation) with respect to real and synthetic data from publicly available, privately acquired sources. The performance evaluations of the proposed algorithm have been investigated using an ablation study on various datasets in correspondence to the significance analysis of the refinement scheme and loss function. Several quantitative and qualitative comparisons were carried out on the proposed approach against various self-calibration approaches.\"\\n      },\\n      {\\n        \"paperId\": \"07503b6fa32459203e176ce880c2a5ba23f0f8e5\",\\n        \"url\": \"https://www.semanticscholar.org/paper/07503b6fa32459203e176ce880c2a5ba23f0f8e5\",\\n        \"title\": \"Learning to Decode: Reinforcement Learning for Decoding of Sparse Graph-Based Channel Codes\",\\n        \"citationCount\": 11,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2010.05637, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work model the CN update process as a multi-armed bandit process with dependent arms and employ a Q-learning scheme for optimizing the CN scheduling policy to improve decoding performance and reduce the decoding complexity dramatically once the model is learned.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2020-10-12\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"49916135\",\\n            \"name\": \"Salman Habib\"\\n          },\\n          {\\n            \"authorId\": \"93037496\",\\n            \"name\": \"Allison Beemer\"\\n          },\\n          {\\n            \"authorId\": \"2064748249\",\\n            \"name\": \"J. Kliewer\"\\n          }\\n        ],\\n        \"abstract\": \"We show in this work that reinforcement learning can be successfully applied to decoding short to moderate length sparse graph-based channel codes. Specifically, we focus on low-density parity check (LDPC) codes, which for example have been standardized in the context of 5G cellular communication systems due to their excellent error correcting performance. These codes are typically decoded via belief propagation iterative decoding on the corresponding bipartite (Tanner) graph of the code via flooding, i.e., all check and variable nodes in the Tanner graph are updated at once. In contrast, in this paper we utilize a sequential update policy which selects the optimum check node (CN) scheduling in order to improve decoding performance. In particular, we model the CN update process as a multi-armed bandit process with dependent arms and employ a Q-learning scheme for optimizing the CN scheduling policy. In order to reduce the learning complexity, we propose a novel graph-induced CN clustering approach to partition the state space in such a way that dependencies between clusters are minimized. Our results show that compared to other decoding approaches from the literature, the proposed reinforcement learning scheme not only significantly improves the decoding performance, but also reduces the decoding complexity dramatically once the model is learned.\"\\n      },\\n      {\\n        \"paperId\": \"9aa6a885754a27fe42a87e4dfaed87d618fd8518\",\\n        \"url\": \"https://www.semanticscholar.org/paper/9aa6a885754a27fe42a87e4dfaed87d618fd8518\",\\n        \"title\": \"Iterative Refinement of Project-Level Code Context for Precise Code Generation with Compiler Feedback\",\\n        \"citationCount\": 37,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2403.16792, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"CoGen is presented, a new code generation approach that uses compiler feedback to improve the LLM-generated code and consistently outperforms the existing retrieval-based code generation baselines.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-03-25\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2170260327\",\\n            \"name\": \"Zhangqian Bi\"\\n          },\\n          {\\n            \"authorId\": \"2273871682\",\\n            \"name\": \"Yao Wan\"\\n          },\\n          {\\n            \"authorId\": \"2293395709\",\\n            \"name\": \"Zheng Wang\"\\n          },\\n          {\\n            \"authorId\": \"2273385018\",\\n            \"name\": \"Hongyu Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2293312291\",\\n            \"name\": \"Batu Guan\"\\n          },\\n          {\\n            \"authorId\": \"2293769071\",\\n            \"name\": \"Fangxin Lu\"\\n          },\\n          {\\n            \"authorId\": \"2293399901\",\\n            \"name\": \"Zili Zhang\"\\n          },\\n          {\\n            \"authorId\": \"34296085\",\\n            \"name\": \"Yulei Sui\"\\n          },\\n          {\\n            \"authorId\": \"1678835\",\\n            \"name\": \"Xuanhua Shi\"\\n          },\\n          {\\n            \"authorId\": \"2277587874\",\\n            \"name\": \"Hai Jin\"\\n          }\\n        ],\\n        \"abstract\": \"Large Language Models (LLMs) have shown remarkable progress in automated code generation. Yet, LLM-generated code may contain errors in API usage, class, data structure, or missing project-specific information. As much of this project-specific context cannot fit into the prompts of LLMs, we must find ways to allow the model to explore the project-level code context. We present CoCoGen, a new code generation approach that uses compiler feedback to improve the LLM-generated code. CoCoGen first leverages static analysis to identify mismatches between the generated code and the project\\'s context. It then iteratively aligns and fixes the identified errors using information extracted from the code repository. We integrate CoCoGen with two representative LLMs, i.e., GPT-3.5-Turbo and Code Llama (13B), and apply it to Python code generation. Experimental results show that CoCoGen significantly improves the vanilla LLMs by over 80% in generating code dependent on the project context and consistently outperforms the existing retrieval-based code generation baselines.\"\\n      },\\n      {\\n        \"paperId\": \"3e43429b24beb5b93513775a259c23ce3a133f67\",\\n        \"url\": \"https://www.semanticscholar.org/paper/3e43429b24beb5b93513775a259c23ce3a133f67\",\\n        \"title\": \"Addressing Background Context Bias in Few-Shot Segmentation Through Iterative Modulation\",\\n        \"citationCount\": 23,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {\\'tldr\\', \\'abstract\\'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/CVPR52733.2024.00324?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/CVPR52733.2024.00324, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": null,\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-06-16\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"151101398\",\\n            \"name\": \"Lanyun Zhu\"\\n          },\\n          {\\n            \"authorId\": \"2261909262\",\\n            \"name\": \"Tianrun Chen\"\\n          },\\n          {\\n            \"authorId\": \"2510538\",\\n            \"name\": \"Jianxiong Yin\"\\n          },\\n          {\\n            \"authorId\": \"2250849553\",\\n            \"name\": \"Simon See\"\\n          },\\n          {\\n            \"authorId\": \"2321778822\",\\n            \"name\": \"Jun Liu\"\\n          }\\n        ],\\n        \"abstract\": null\\n      },\\n      {\\n        \"paperId\": \"8745157f991013b23fbb79d300ba560f9005c8d4\",\\n        \"url\": \"https://www.semanticscholar.org/paper/8745157f991013b23fbb79d300ba560f9005c8d4\",\\n        \"title\": \"Energy-Based Cross Attention for Bayesian Context Update in Text-to-Image Diffusion Models\",\\n        \"citationCount\": 29,\\n        \"openAccessPdf\": {\\n          \"url\": \"http://arxiv.org/pdf/2306.09869\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2306.09869, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A novel energy-based model (EBM) framework is presented that first formulate EBMs of latent image representations and text embeddings in each cross-attention layer of the denoising autoencoder, thereby implicitly minimizing a nested hierarchy of energy functions.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2023-06-16\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"153118937\",\\n            \"name\": \"Geon Yeong Park\"\\n          },\\n          {\\n            \"authorId\": \"2109216792\",\\n            \"name\": \"Jeongsol Kim\"\\n          },\\n          {\\n            \"authorId\": \"3332270\",\\n            \"name\": \"Beomsu Kim\"\\n          },\\n          {\\n            \"authorId\": \"2152577026\",\\n            \"name\": \"Sang Wan Lee\"\\n          },\\n          {\\n            \"authorId\": \"30547794\",\\n            \"name\": \"Jong-Chul Ye\"\\n          }\\n        ],\\n        \"abstract\": \"Despite the remarkable performance of text-to-image diffusion models in image generation tasks, recent studies have raised the issue that generated images sometimes cannot capture the intended semantic contents of the text prompts, which phenomenon is often called semantic misalignment. To address this, here we present a novel energy-based model (EBM) framework. Specifically, we first formulate EBMs of latent image representations and text embeddings in each cross-attention layer of the denoising autoencoder. Then, we obtain the gradient of the log posterior of context vectors, which can be updated and transferred to the subsequent cross-attention layer, thereby implicitly minimizing a nested hierarchy of energy functions. Our latent EBMs further allow zero-shot compositional generation as a linear combination of cross-attention outputs from different contexts. Using extensive experiments, we demonstrate that the proposed method is highly effective in handling various image generation tasks, including multi-concept generation, text-guided image inpainting, and real and synthetic image editing.\"\\n      },\\n      {\\n        \"paperId\": \"b217b6bc340af9a10bebbf8acc36ea30871769bd\",\\n        \"url\": \"https://www.semanticscholar.org/paper/b217b6bc340af9a10bebbf8acc36ea30871769bd\",\\n        \"title\": \"In-Context Learning with Iterative Demonstration Selection\",\\n        \"citationCount\": 69,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2310.09881, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"Iterative Demonstration Selection (IDS) iteratively selects examples that are diverse but still strongly correlated with the test sample as ICL demonstrations, and can consistently outperform existing ICL demonstration selection methods.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2023-10-15\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2084609980\",\\n            \"name\": \"Chengwei Qin\"\\n          },\\n          {\\n            \"authorId\": \"2258754564\",\\n            \"name\": \"Aston Zhang\"\\n          },\\n          {\\n            \"authorId\": \"1627060158\",\\n            \"name\": \"Anirudh Dagar\"\\n          },\\n          {\\n            \"authorId\": \"2258719488\",\\n            \"name\": \"Wenming Ye\"\\n          }\\n        ],\\n        \"abstract\": \"Spurred by advancements in scale, large language models (LLMs) have demonstrated strong few-shot learning ability via in-context learning (ICL). However, the performance of ICL has been shown to be highly sensitive to the selection of few-shot demonstrations. Selecting the most suitable examples as context remains an ongoing challenge and an open problem. Existing literature has highlighted the importance of selecting examples that are diverse or semantically similar to the test sample while ignoring the fact that the optimal selection dimension, i.e., diversity or similarity, is task-specific. Based on how the test sample is answered, we propose Iterative Demonstration Selection (IDS) to leverage the merits of both dimensions. Using zero-shot chain-of-thought reasoning (Zero-shot-CoT), IDS iteratively selects examples that are diverse but still strongly correlated with the test sample as ICL demonstrations. Specifically, IDS applies Zero-shot-CoT to the test sample before demonstration selection. The output reasoning path is then used to choose demonstrations that are prepended to the test sample for inference. The generated answer is followed by its corresponding reasoning path for extracting a new set of demonstrations in the next iteration. After several iterations, IDS adopts majority voting to obtain the final result. Through extensive experiments on tasks including reasoning, question answering, and topic classification, we demonstrate that IDS can consistently outperform existing ICL demonstration selection methods.\"\\n      },\\n      {\\n        \"paperId\": \"43ffae1c1c6a7371dfe89ac84ae45f3456fdc5a0\",\\n        \"url\": \"https://www.semanticscholar.org/paper/43ffae1c1c6a7371dfe89ac84ae45f3456fdc5a0\",\\n        \"title\": \"Iterative Forward Tuning Boosts In-context Learning in Language Models\",\\n        \"citationCount\": 19,\\n        \"openAccessPdf\": {\\n          \"url\": \"http://arxiv.org/pdf/2305.13016\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2305.13016, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A novel two-stage framework to boost ICL in LLMs is introduced, which delineates the ICL process into two distinct stages: Deep-Thinking and test stages, and incorporates a unique attention mechanism, i.e., iterative enhanced attention, which enables multiple rounds of information accumulation.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2023-05-22\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2135964855\",\\n            \"name\": \"Jiaxi Yang\"\\n          },\\n          {\\n            \"authorId\": \"151471590\",\\n            \"name\": \"Binyuan Hui\"\\n          },\\n          {\\n            \"authorId\": \"2144399900\",\\n            \"name\": \"Min Yang\"\\n          },\\n          {\\n            \"authorId\": \"66200440\",\\n            \"name\": \"Binhua Li\"\\n          },\\n          {\\n            \"authorId\": \"2087380523\",\\n            \"name\": \"Fei Huang\"\\n          },\\n          {\\n            \"authorId\": \"1527090216\",\\n            \"name\": \"Yongbin Li\"\\n          }\\n        ],\\n        \"abstract\": \"Despite the advancements in in-context learning (ICL) for large language models (LLMs), current research centers on specific prompt engineering, such as demonstration selection, with the expectation that a single iteration of demonstrations processing can generalize effectively to a given test sample. However, this perspective overlooks the potential benefits derived from multiple iterations involving demonstrations, a practice aligning more closely with the iterative decision-making process exhibited by humans, who often learn through analogy. In this study, we introduce a novel two-stage framework to boost ICL in LLMs. Specifically, our framework delineates the ICL process into two distinct stages: Deep-Thinking and test stages. The Deep-Thinking stage incorporates a unique attention mechanism, i.e., iterative enhanced attention, which enables multiple rounds of information accumulation. This mechanism operates by manipulating the Key-Value matrices without training, fostering enhanced understanding capabilities in LLMs by thinking demonstrations multiple times. We evaluated Deep-Thinking across a range of benchmarks and LLMs, showing its superior performance over vanilla ICL methods and its effectiveness in challenging tasks where demonstration selection is infeasible.\"\\n      },\\n      {\\n        \"paperId\": \"fa4ea8b773ffd6706ba5cf427f9671f81b04dcdd\",\\n        \"url\": \"https://www.semanticscholar.org/paper/fa4ea8b773ffd6706ba5cf427f9671f81b04dcdd\",\\n        \"title\": \"ALLURE: Auditing and Improving LLM-based Evaluation of Text using Iterative In-Context-Learning\",\\n        \"citationCount\": 17,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://arxiv.org/pdf/2309.13701\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2309.13701, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"Allure, a systematic approach to Auditing Large Language Models Understanding and Reasoning Errors, involves comparing LLM-generated evaluations with annotated data, and iteratively incorporating instances of significant deviation into the evaluator, which leverages in-context learning (ICL) to enhance and improve robust evaluation of text by LLMs.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2023-09-24\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2245381886\",\\n            \"name\": \"Hosein Hasanbeig\"\\n          },\\n          {\\n            \"authorId\": \"20013278\",\\n            \"name\": \"Hiteshi Sharma\"\\n          },\\n          {\\n            \"authorId\": \"15449757\",\\n            \"name\": \"Leo Betthauser\"\\n          },\\n          {\\n            \"authorId\": \"1844283112\",\\n            \"name\": \"F. Frujeri\"\\n          },\\n          {\\n            \"authorId\": \"2248289111\",\\n            \"name\": \"Ida Momennejad\"\\n          }\\n        ],\\n        \"abstract\": \"From grading papers to summarizing medical documents, large language models (LLMs) are evermore used for evaluation of text generated by humans and AI alike. However, despite their extensive utility, LLMs exhibit distinct failure modes, necessitating a thorough audit and improvement of their text evaluation capabilities. Here we introduce ALLURE, a systematic approach to Auditing Large Language Models Understanding and Reasoning Errors. ALLURE involves comparing LLM-generated evaluations with annotated data, and iteratively incorporating instances of significant deviation into the evaluator, which leverages in-context learning (ICL) to enhance and improve robust evaluation of text by LLMs. Through this iterative process, we refine the performance of the evaluator LLM, ultimately reducing reliance on human annotators in the evaluation process. We anticipate ALLURE to serve diverse applications of LLMs in various domains related to evaluation of textual data, such as medical summarization, education, and and productivity.\"\\n      },\\n      {\\n        \"paperId\": \"860bafe8a3aa0e3a981951f0757996272276b54e\",\\n        \"url\": \"https://www.semanticscholar.org/paper/860bafe8a3aa0e3a981951f0757996272276b54e\",\\n        \"title\": \"ALLURE: A Systematic Protocol for Auditing and Improving LLM-based Evaluation of Text using Iterative In-Context-Learning\",\\n        \"citationCount\": 12,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"AllURE, a systematic approach to Auditing Large Language Models Understanding and Reasoning Errors, involves comparing LLM-generated evaluations with annotated data, and iteratively incorporating instances of significant deviation into the evaluator, which leverages in-context learning (ICL) to enhance and improve robust evaluation of text by LLMs.\"\\n        },\\n        \"publicationTypes\": null,\\n        \"publicationDate\": null,\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2245381886\",\\n            \"name\": \"Hosein Hasanbeig\"\\n          },\\n          {\\n            \"authorId\": \"2258705787\",\\n            \"name\": \"Microsoft Usa HITESHI SHARMA\"\\n          },\\n          {\\n            \"authorId\": \"2258705783\",\\n            \"name\": \"Microsoft Usa LEO BETTHAUSER\"\\n          },\\n          {\\n            \"authorId\": \"2258705381\",\\n            \"name\": \"Microsoft Usa FELIPE VIEIRA FRUJERI\"\\n          },\\n          {\\n            \"authorId\": \"2248289111\",\\n            \"name\": \"Ida Momennejad\"\\n          }\\n        ],\\n        \"abstract\": null\\n      },\\n      {\\n        \"paperId\": \"3f27df9f8b30a8c35fa3e7df7fdf7ec1a5cc28cd\",\\n        \"url\": \"https://www.semanticscholar.org/paper/3f27df9f8b30a8c35fa3e7df7fdf7ec1a5cc28cd\",\\n        \"title\": \"Enhancing Iterative Learning Control With Fractional Power Update Law\",\\n        \"citationCount\": 14,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {\\'tldr\\'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/JAS.2023.123525?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/JAS.2023.123525, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": null,\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2023-05-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2118272992\",\\n            \"name\": \"Zihan Li\"\\n          },\\n          {\\n            \"authorId\": \"144986261\",\\n            \"name\": \"D. Shen\"\\n          },\\n          {\\n            \"authorId\": \"2156506994\",\\n            \"name\": \"Xinghuo Yu\"\\n          }\\n        ],\\n        \"abstract\": \"The P-type update law has been the mainstream technique used in iterative learning control (ILC) systems, which resembles linear feedback control with asymptotical convergence. In recent years, finite-time control strategies such as terminal sliding mode control have been shown to be effective in ramping up convergence speed by introducing fractional power with feedback. In this paper, we show that such mechanism can equally ramp up the learning speed in ILC systems. We first propose a fractional power update rule for ILC of single-input-single-output linear systems. A nonlinear error dynamics is constructed along the iteration axis to illustrate the evolutionary converging process. Using the nonlinear mapping approach, fast convergence towards the limit cycles of tracking errors inherently existing in ILC systems is proven. The limit cycles are shown to be tunable to determine the steady states. Numerical simulations are provided to verify the theoretical results.\"\\n      },\\n      {\\n        \"paperId\": \"38333f6e8f0388968edc4b2ea7a683ce69677e69\",\\n        \"url\": \"https://www.semanticscholar.org/paper/38333f6e8f0388968edc4b2ea7a683ce69677e69\",\\n        \"title\": \"Monte Carlo Tree Search Boosts Reasoning via Iterative Preference Learning\",\\n        \"citationCount\": 182,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2405.00451, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work leverages Monte Carlo Tree Search (MCTS) to iteratively collect preference data, utilizing its look-ahead ability to break down instance-level rewards into more granular step-level signals, to enhance consistency in intermediate steps.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-05-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2299483768\",\\n            \"name\": \"Yuxi Xie\"\\n          },\\n          {\\n            \"authorId\": \"1996705\",\\n            \"name\": \"Anirudh Goyal\"\\n          },\\n          {\\n            \"authorId\": \"2289857220\",\\n            \"name\": \"Wenyue Zheng\"\\n          },\\n          {\\n            \"authorId\": \"2257033898\",\\n            \"name\": \"Min-Yen Kan\"\\n          },\\n          {\\n            \"authorId\": \"2542999\",\\n            \"name\": \"T. Lillicrap\"\\n          },\\n          {\\n            \"authorId\": \"2266466003\",\\n            \"name\": \"Kenji Kawaguchi\"\\n          },\\n          {\\n            \"authorId\": \"2289844602\",\\n            \"name\": \"Michael Shieh\"\\n          }\\n        ],\\n        \"abstract\": \"We introduce an approach aimed at enhancing the reasoning capabilities of Large Language Models (LLMs) through an iterative preference learning process inspired by the successful strategy employed by AlphaZero. Our work leverages Monte Carlo Tree Search (MCTS) to iteratively collect preference data, utilizing its look-ahead ability to break down instance-level rewards into more granular step-level signals. To enhance consistency in intermediate steps, we combine outcome validation and stepwise self-evaluation, continually updating the quality assessment of newly generated data. The proposed algorithm employs Direct Preference Optimization (DPO) to update the LLM policy using this newly generated step-level preference data. Theoretical analysis reveals the importance of using on-policy sampled data for successful self-improving. Extensive evaluations on various arithmetic and commonsense reasoning tasks demonstrate remarkable performance improvements over existing models. For instance, our approach outperforms the Mistral-7B Supervised Fine-Tuning (SFT) baseline on GSM8K, MATH, and ARC-C, with substantial increases in accuracy to $81.8\\\\\\\\%$ (+$5.9\\\\\\\\%$), $34.7\\\\\\\\%$ (+$5.8\\\\\\\\%$), and $76.4\\\\\\\\%$ (+$15.8\\\\\\\\%$), respectively. Additionally, our research delves into the training and inference compute tradeoff, providing insights into how our method effectively maximizes performance gains. Our code is publicly available at https://github.com/YuxiXie/MCTS-DPO.\"\\n      },\\n      {\\n        \"paperId\": \"a1675f47125aa409525c5f759b5e6bcc1c8831aa\",\\n        \"url\": \"https://www.semanticscholar.org/paper/a1675f47125aa409525c5f759b5e6bcc1c8831aa\",\\n        \"title\": \"Enhancing Retrieval-Augmented Large Language Models with Iterative Retrieval-Generation Synergy\",\\n        \"citationCount\": 360,\\n        \"openAccessPdf\": {\\n          \"url\": \"http://arxiv.org/pdf/2305.15294\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2305.15294, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper shows that strong performance can be achieved by a method, called Iter-RetGen, which synergizes retrieval and generation in an iterative manner, and shows that it can flexibly leverage parametric knowledge and non-parametric knowledge, and is superior to or competitive with state-of-the-art retrieval-augmented baselines while causing fewer overheads of retrieval andgeneration.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2023-05-24\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"144485528\",\\n            \"name\": \"Zhihong Shao\"\\n          },\\n          {\\n            \"authorId\": \"2171182\",\\n            \"name\": \"Yeyun Gong\"\\n          },\\n          {\\n            \"authorId\": \"1752875\",\\n            \"name\": \"Yelong Shen\"\\n          },\\n          {\\n            \"authorId\": \"1730108\",\\n            \"name\": \"Minlie Huang\"\\n          },\\n          {\\n            \"authorId\": \"46429989\",\\n            \"name\": \"Nan Duan\"\\n          },\\n          {\\n            \"authorId\": \"2109136147\",\\n            \"name\": \"Weizhu Chen\"\\n          }\\n        ],\\n        \"abstract\": \"Large language models are powerful text processors and reasoners, but are still subject to limitations including outdated knowledge and hallucinations, which necessitates connecting them to the world. Retrieval-augmented large language models have raised extensive attention for grounding model generation on external knowledge. However, retrievers struggle to capture relevance, especially for queries with complex information needs. Recent work has proposed to improve relevance modeling by having large language models actively involved in retrieval, i.e., to improve retrieval with generation. In this paper, we show that strong performance can be achieved by a method we call Iter-RetGen, which synergizes retrieval and generation in an iterative manner. A model output shows what might be needed to finish a task, and thus provides an informative context for retrieving more relevant knowledge which in turn helps generate a better output in the next iteration. Compared with recent work which interleaves retrieval with generation when producing an output, Iter-RetGen processes all retrieved knowledge as a whole and largely preserves the flexibility in generation without structural constraints. We evaluate Iter-RetGen on multi-hop question answering, fact verification, and commonsense reasoning, and show that it can flexibly leverage parametric knowledge and non-parametric knowledge, and is superior to or competitive with state-of-the-art retrieval-augmented baselines while causing fewer overheads of retrieval and generation. We can further improve performance via generation-augmented retrieval adaptation.\"\\n      },\\n      {\\n        \"paperId\": \"af5c7848417882012203ac21399977ebda695a2b\",\\n        \"url\": \"https://www.semanticscholar.org/paper/af5c7848417882012203ac21399977ebda695a2b\",\\n        \"title\": \"RepoCoder: Repository-Level Code Completion Through Iterative Retrieval and Generation\",\\n        \"citationCount\": 325,\\n        \"openAccessPdf\": {\\n          \"url\": \"http://arxiv.org/pdf/2303.12570\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2303.12570, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"RepoCoder streamlines the repository-level code completion process by incorporating a similarity-based retriever and a pre-trained code language model in an iterative retrieval-generation pipeline and has the ability to generate code at various levels of granularity.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2023-03-22\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2158120018\",\\n            \"name\": \"Fengji Zhang\"\\n          },\\n          {\\n            \"authorId\": \"143876723\",\\n            \"name\": \"B. Chen\"\\n          },\\n          {\\n            \"authorId\": \"2211964951\",\\n            \"name\": \"Yue Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2155352529\",\\n            \"name\": \"Jin Liu\"\\n          },\\n          {\\n            \"authorId\": \"2134434187\",\\n            \"name\": \"Daoguang Zan\"\\n          },\\n          {\\n            \"authorId\": \"145469202\",\\n            \"name\": \"Yi Mao\"\\n          },\\n          {\\n            \"authorId\": \"153249455\",\\n            \"name\": \"Jian-Guang Lou\"\\n          },\\n          {\\n            \"authorId\": \"2109136147\",\\n            \"name\": \"Weizhu Chen\"\\n          }\\n        ],\\n        \"abstract\": \"The task of repository-level code completion is to continue writing the unfinished code based on a broader context of the repository. While for automated code completion tools, it is difficult to utilize the useful information scattered in different files. We propose RepoCoder, a simple, generic, and effective framework to address the challenge. It streamlines the repository-level code completion process by incorporating a similarity-based retriever and a pre-trained code language model in an iterative retrieval-generation pipeline. RepoCoder makes effective utilization of repository-level information for code completion and has the ability to generate code at various levels of granularity. Moreover, we propose a new benchmark RepoEval, which consists of the latest and high-quality real-world repositories covering line, API invocation, and function body completion scenarios. Experimental results indicate that RepoCoder significantly improves the In-File completion baseline by over 10% in all settings and consistently outperforms the vanilla retrieval-augmented code completion approach. Furthermore, we validate the effectiveness of RepoCoder through comprehensive analysis, providing valuable insights for future research. Our source code and benchmark are publicly available: https://github.com/microsoft/CodeT/tree/main/RepoCoder\"\\n      },\\n      {\\n        \"paperId\": \"cc26cab4d4e0c6e3dddbae96abd65ee2d1f87f69\",\\n        \"url\": \"https://www.semanticscholar.org/paper/cc26cab4d4e0c6e3dddbae96abd65ee2d1f87f69\",\\n        \"title\": \"An iterative method to compute conformal mappings and their inverses in the context of water waves over topographies\",\\n        \"citationCount\": 13,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://arxiv.org/pdf/2103.02022\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2103.02022, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"An iterative numerical method to compute the conformal mapping in the context of propagating water waves over uneven topographies using the MATLAB Schwarz-Christoffel toolbox mapping as a benchmark is investigated.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2021-03-02\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"104491233\",\\n            \"name\": \"M. Flamarion\"\\n          },\\n          {\\n            \"authorId\": \"1429030643\",\\n            \"name\": \"R. Ribeiro-Jr\"\\n          }\\n        ],\\n        \"abstract\": \"An iterative numerical method to compute the conformal mapping in the context of propagating water waves over uneven topographies is investigated. The map flattens the fluid domain onto a canonical strip in which computations are performed. The accuracy of the method is tested by using the MATLAB Schwarz\\\\u2013Christoffel toolbox mapping as a benchmark. Besides, we give a numerical alternative to compute the inverse of the conformal map.\"\\n      },\\n      {\\n        \"paperId\": \"edf23ce5689d0acf18cc7ff8556c696f9231644d\",\\n        \"url\": \"https://www.semanticscholar.org/paper/edf23ce5689d0acf18cc7ff8556c696f9231644d\",\\n        \"title\": \"Five-Precision GMRES-Based Iterative Refinement\",\\n        \"citationCount\": 36,\\n        \"openAccessPdf\": {\\n          \"url\": \"http://eprints.maths.manchester.ac.uk/2807/1/paper.pdf\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1137/23m1549079?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1137/23m1549079, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"GMRES-IR5 has the potential to solve relatively badly conditioned problems in less time and memory than GMRES-IR3, thanks to the use of lower precision arithmetic in the GMRES iterations.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-02-08\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"38318287\",\\n            \"name\": \"P. Amestoy\"\\n          },\\n          {\\n            \"authorId\": \"1760511\",\\n            \"name\": \"A. Buttari\"\\n          },\\n          {\\n            \"authorId\": \"1699285\",\\n            \"name\": \"N. Higham\"\\n          },\\n          {\\n            \"authorId\": \"1398545502\",\\n            \"name\": \"J. L\\\\u2019Excellent\"\\n          },\\n          {\\n            \"authorId\": \"144615299\",\\n            \"name\": \"Th\\\\u00e9o Mary\"\\n          },\\n          {\\n            \"authorId\": \"1720746305\",\\n            \"name\": \"Bastien Vieubl\\\\u00e9\"\\n          }\\n        ],\\n        \"abstract\": \". GMRES-based iterative re\\\\ufb01nement in three precisions (GMRES-IR3) uses a low pre- cision LU factorization to accelerate the solution of a linear system without compromising numerical stability or robustness. GMRES-IR3 solves the update equation using GMRES preconditioned by the LU factors, where all operations within GMRES are carried out in the working precision u , except for the matrix-vector products and the application of the preconditioner, which require the use of extra precision u 2 . The use of extra precision can be expensive, and is especially unattractive if it is not available in hardware; for this reason, existing implementations have not used extra precision, despite the absence of an error analysis for this approach. We relax the requirements on the precisions used within GMRES, allowing the use of arbitrary precisions u p (for applying the preconditioner) and u g (for the rest of the operations). We obtain the \\\\ufb01ve-precision GMRES-based iterative re\\\\ufb01nement (GMRES-IR5) algorithm. We carry out a rounding error analysis that generalizes that of GMRES-IR3, obtaining conditions under which the forward and backward errors converge to their limiting values. Our analysis makes use of a new result on the backward stability of MGS-GMRES in two precisions. On hardware where up to \\\\ufb01ve arithmetics are available, the number of possible combinations of precisions in GMRES-IR5 is extremely large, but our analysis identi\\\\ufb01es a small subset of relevant combinations. By choosing from within this subset one can achieve di\\\\ufb00erent levels of tradeo\\\\ufb00 between cost and robustness, which allows for a \\\\ufb01ner choice of precisions depending on the problem di\\\\ufb03culty and the available hardware. Our numerical experiments on both random dense matrices and real-life sparse matrices from a wide range of applications show that the practical behavior of GMRES-IR5 is in good agreement with our theoretical analysis. GMRES-IR5 therefore has the potential to solve relatively badly conditioned problems in less time and memory than GMRES-IR3, thanks to the use of lower precision arithmetic in the GMRES iterations.\"\\n      },\\n      {\\n        \"paperId\": \"dc256e179d4e8eff48879a40ddc414b15b0b2300\",\\n        \"url\": \"https://www.semanticscholar.org/paper/dc256e179d4e8eff48879a40ddc414b15b0b2300\",\\n        \"title\": \"RAT: Retrieval Augmented Thoughts Elicit Context-Aware Reasoning in Long-Horizon Generation\",\\n        \"citationCount\": 69,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2403.05313, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work explores how iterative revising a chain of thoughts with the help of information retrieval significantly improves large language models\\' reasoning and generation ability in long-horizon generation tasks, while hugely mitigating hallucination.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-03-08\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"47196237\",\\n            \"name\": \"Zihao Wang\"\\n          },\\n          {\\n            \"authorId\": \"70097297\",\\n            \"name\": \"Anji Liu\"\\n          },\\n          {\\n            \"authorId\": \"2257447835\",\\n            \"name\": \"Haowei Lin\"\\n          },\\n          {\\n            \"authorId\": \"2290567624\",\\n            \"name\": \"Jiaqi Li\"\\n          },\\n          {\\n            \"authorId\": \"2257636629\",\\n            \"name\": \"Xiaojian Ma\"\\n          },\\n          {\\n            \"authorId\": \"2257367774\",\\n            \"name\": \"Yitao Liang\"\\n          }\\n        ],\\n        \"abstract\": \"We explore how iterative revising a chain of thoughts with the help of information retrieval significantly improves large language models\\' reasoning and generation ability in long-horizon generation tasks, while hugely mitigating hallucination. In particular, the proposed method -- *retrieval-augmented thoughts* (RAT) -- revises each thought step one by one with retrieved information relevant to the task query, the current and the past thought steps, after the initial zero-shot CoT is generated. Applying RAT to GPT-3.5, GPT-4, and CodeLLaMA-7b substantially improves their performances on various long-horizon generation tasks; on average of relatively increasing rating scores by 13.63% on code generation, 16.96% on mathematical reasoning, 19.2% on creative writing, and 42.78% on embodied task planning. The demo page can be found at https://craftjarvis.github.io/RAT\"\\n      },\\n      {\\n        \"paperId\": \"ca5cfff2fcc87e1fb88c74b6eca3f7ee2c638bde\",\\n        \"url\": \"https://www.semanticscholar.org/paper/ca5cfff2fcc87e1fb88c74b6eca3f7ee2c638bde\",\\n        \"title\": \"Learned Two-Step Iterative Shrinkage Thresholding Algorithm for Deep Compressive Sensing\",\\n        \"citationCount\": 31,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {\\'tldr\\'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/TCSVT.2023.3325340?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/TCSVT.2023.3325340, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": null,\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-05-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"35361812\",\\n            \"name\": \"Hongping Gan\"\\n          },\\n          {\\n            \"authorId\": \"2260282205\",\\n            \"name\": \"Xiaoyang Wang\"\\n          },\\n          {\\n            \"authorId\": \"2260279661\",\\n            \"name\": \"Lijun He\"\\n          },\\n          {\\n            \"authorId\": \"2260264613\",\\n            \"name\": \"Jie Liu\"\\n          }\\n        ],\\n        \"abstract\": \"Deep unrolling architectures have revitalized compressive sensing (CS) by seamlessly blending deep neural networks with traditional optimization-based reconstruction algorithms. In pursuit of an efficient and deep interpretable approach, we propose LTwIST for CS problem, a novel deep unrolling framework that draws inspiration from the well-known two-step iterative shrinkage thresholding (TwIST) algorithm. LTwIST uses a trainable sensing matrix to adaptively learn structural information in images, and introduces a customized U-block architecture to solve the proximal mapping of nonlinear transformations connected with the sparsity-inducing regularizer. Specifically, each iteration recovery step of LTwIST corresponds to an iterative update step of the traditional TwIST algorithm. Moreover, the proposed method is designed to learn all the parameters end-to-end without manual tuning such as shrinkable thresholds, step sizes, etc. As a result, LTwIST obviates the need for manual parameter optimization, allows for high-quality image recovery and provides unambiguous interpretability. Moreover, our proposed LTwIST is also applicable to CS-based magnetic resonance imaging and exhibits a strong reconstruction performance. Extensive experiments on several public benchmark datasets demonstrate that the proposed LTwIST outperforms existing state-of-the-art deep CS methods by considerable margins in terms of quality evaluation metrics and visual performance. Our code is available on LTwIST.\"\\n      },\\n      {\\n        \"paperId\": \"53dcf71b6be11d543f1bc7753e8ff8e9945eff77\",\\n        \"url\": \"https://www.semanticscholar.org/paper/53dcf71b6be11d543f1bc7753e8ff8e9945eff77\",\\n        \"title\": \"Iterative and Scenario-based Requirements Specification in a System of Systems Context\",\\n        \"citationCount\": 10,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2102.05400, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper describes an extension for the scenario-modeling language for Kotlin (SMLK) to continuously and formally model requirements on SoS and CS level and combines SMLK with agile development techniques to support the iterative requirements specification and modeling.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2021-02-10\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"1423730807\",\\n            \"name\": \"Carsten Wiecher\"\\n          },\\n          {\\n            \"authorId\": \"2909345\",\\n            \"name\": \"Joel Greenyer\"\\n          },\\n          {\\n            \"authorId\": \"40500399\",\\n            \"name\": \"Carsten Wolff\"\\n          },\\n          {\\n            \"authorId\": \"152617022\",\\n            \"name\": \"H. Anacker\"\\n          },\\n          {\\n            \"authorId\": \"3261846\",\\n            \"name\": \"R. Dumitrescu\"\\n          }\\n        ],\\n        \"abstract\": \"[Context&Motivation]Due to the managerial ,operational and evolutionary independence of constituent systems (CSs) in a System of Systems (SoS) context, top-down and linear requirements engineering (RE) approaches are insufficient. RE techniques for SoS must support iterating, changing, synchronizing, and communicating requirements across different abstraction and hierarchy levels as well as scopes of responsibility. [Question/Problem] We address the challenge of SoS requirements specification, where requirements can describe the SoS behavior, but also the behavior of CSs that are developed independently. [Principal Ideas] To support the requirements specification in an SoS environment, we propose a scenario-based and iterative specification technique. This allows requirements engineers to continuously model and jointly execute and test the system behavior for the SoS and the CS in order to detect contradictions in the requirement specifications at an early stage. [Contribution] In this paper, we describe an extension for the scenario-modeling language for Kotlin (SMLK) to continuously and formally model requirements on SoS and CS level. To support the iterative requirements specification and modeling we combine SMLK with agile development techniques. We demonstrate the applicability of our approach with the help of an example from the field of e-mobility.\"\\n      },\\n      {\\n        \"paperId\": \"f6ca551535a92b669ebd97b246ac582e73af30ee\",\\n        \"url\": \"https://www.semanticscholar.org/paper/f6ca551535a92b669ebd97b246ac582e73af30ee\",\\n        \"title\": \"An attempt to analyse Iterative Data Snooping and L1-norm based on Monte Carlo simulation in the context of leveling networks\",\\n        \"citationCount\": 11,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://riunet.upv.es/bitstream/10251/193468/3/KleinSuracide%20-%20An%20attempt%20to%20analyse%20Iterative%20Data%20Snooping%20and%20L1-norm%20based%20on%20Monte%20Carlo%20si....pdf\",\\n          \"status\": \"GREEN\",\\n          \"license\": \"CCBYNC\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1080/00396265.2021.1878338?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1080/00396265.2021.1878338, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"Evaluating the outlier identification performance of iterative Data Snooping (IDS) and L1-norm in levelling networks by considering the redundancy of the network, number and size of the outliers shows that L 1-norm performs better than IDS for the case of networks with low redundancy.\"\\n        },\\n        \"publicationTypes\": null,\\n        \"publicationDate\": \"2021-02-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"31183250\",\\n            \"name\": \"I. Klein\"\\n          },\\n          {\\n            \"authorId\": \"1834987\",\\n            \"name\": \"S. Suraci\"\\n          },\\n          {\\n            \"authorId\": \"2125043152\",\\n            \"name\": \"Leonardo Castro de Oliveira\"\\n          },\\n          {\\n            \"authorId\": \"72414897\",\\n            \"name\": \"V. F. Rofatto\"\\n          },\\n          {\\n            \"authorId\": \"2059636056\",\\n            \"name\": \"M. T. Matsuoka\"\\n          },\\n          {\\n            \"authorId\": \"1691074\",\\n            \"name\": \"S. Baselga\"\\n          }\\n        ],\\n        \"abstract\": \"The goal of this paper is to evaluate the outlier identification performance of iterative Data Snooping (IDS) and L1-norm in levelling networks by considering the redundancy of the network, number and size of the outliers. For this purpose, several Monte-Carlo experiments were conducted into three different levelling networks configurations. In addition, a new way to compare the results of IDS based on Least Squares (LS) residuals and robust estimators such as the L1-norm has also been developed and presented. From the perspective of analysis only according to the success rate, it is shown that L1-norm performs better than IDS for the case of networks with low redundancy , especially for cases where more than one outlier is present in the dataset. In the relationship between false positive rate and outlier identification success rate, however, IDS performs better than L1-norm, independently of the levelling network configuration, number and size of outliers.\"\\n      },\\n      {\\n        \"paperId\": \"2a5e4419c4bf16178a441c975b970fa33a9f5361\",\\n        \"url\": \"https://www.semanticscholar.org/paper/2a5e4419c4bf16178a441c975b970fa33a9f5361\",\\n        \"title\": \"Nash Equilibria for Linear Quadratic Discrete-Time Dynamic Games via Iterative and Data-Driven Algorithms\",\\n        \"citationCount\": 15,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://ieeexplore.ieee.org/ielx7/9/4601496/10463100.pdf\",\\n          \"status\": \"HYBRID\",\\n          \"license\": \"CCBY\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/TAC.2024.3375249?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/TAC.2024.3375249, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This article proposes four different iterative algorithms to find Nash equilibrium strategies for discrete-time linear quadratic games based on the solution of either Lyapunov or Riccati equations for each player.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-10-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2046956775\",\\n            \"name\": \"Benita Nortmann\"\\n          },\\n          {\\n            \"authorId\": \"2267944139\",\\n            \"name\": \"Andrea Monti\"\\n          },\\n          {\\n            \"authorId\": \"1764871\",\\n            \"name\": \"M. Sassano\"\\n          },\\n          {\\n            \"authorId\": \"2695624\",\\n            \"name\": \"T. Mylvaganam\"\\n          }\\n        ],\\n        \"abstract\": \"Determining feedback Nash equilibrium solutions of nonzero-sum dynamic games is generally challenging. In this article, we propose four different iterative algorithms to find Nash equilibrium strategies for discrete-time linear quadratic games. The strategy update laws are based on the solution of either Lyapunov or Riccati equations for each player. Local convergence criteria are discussed. Motivated by the fact that in many practical scenarios each player in the game may have access to different (incomplete) information, we also introduce purely data-driven implementations of the algorithms. This allows the players to reach a Nash equilibrium solution of the game via scheduled experiments and without knowledge of each other\\'s performance criteria or of the system dynamics. The efficacy of the presented algorithms is illustrated via numerical examples and a practical example involving human\\\\u2013robot interaction.\"\\n      },\\n      {\\n        \"paperId\": \"d47ecf36e7e67d9349ecac29dfb981feaf229229\",\\n        \"url\": \"https://www.semanticscholar.org/paper/d47ecf36e7e67d9349ecac29dfb981feaf229229\",\\n        \"title\": \"iMSCGnet: Iterative Multi-Scale Context-Guided Segmentation of Skin Lesion in Dermoscopic Images\",\\n        \"citationCount\": 23,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://ieeexplore.ieee.org/ielx7/6287639/8948470/09007375.pdf\",\\n          \"status\": \"GOLD\",\\n          \"license\": \"CCBY\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ACCESS.2020.2974512?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ACCESS.2020.2974512, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A multi-scale context-guided network named as MSCGnet is proposed to segment the skin lesions accurately and generally outperforms the state-of-the-art methods.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": null,\\n        \"authors\": [\\n          {\\n            \"authorId\": \"5691344\",\\n            \"name\": \"Yujiao Tang\"\\n          },\\n          {\\n            \"authorId\": \"39271955\",\\n            \"name\": \"Zhiwen Fang\"\\n          },\\n          {\\n            \"authorId\": \"153431402\",\\n            \"name\": \"Shaofeng Yuan\"\\n          },\\n          {\\n            \"authorId\": \"30380386\",\\n            \"name\": \"C. Zhan\"\\n          },\\n          {\\n            \"authorId\": \"2072728505\",\\n            \"name\": \"Yanyan Xing\"\\n          },\\n          {\\n            \"authorId\": \"10638646\",\\n            \"name\": \"Joey Tianyi Zhou\"\\n          },\\n          {\\n            \"authorId\": \"2115412895\",\\n            \"name\": \"Feng Yang\"\\n          }\\n        ],\\n        \"abstract\": \"Despite much effort has been devoted to skin lesion segmentation, the performance of existing methods is still not satisfactory enough for practical applications. The challenges may include fuzzy lesion boundary, uneven and low contrast, and variation of colors across space, which often lead to fragmentary segmentation and inaccurate boundary. To alleviate this problem, we propose a multi-scale context-guided network named as MSCGnet to segment the skin lesions accurately. In MSCGnet, the context information is utilized to guide the feature encoding procedure. Moreover, because of the information loss in spatial down-sampling, a context-based attention structure (CAs) is designed to select effective context features in the decoding path. Furthermore, we boost the performance of MSCGnet with iterations and term this upgraded version as iterative MSCGnet, denoted as iMSCGnet. To supervise the training of iMSCGnet in an end-to-end fashion, a novel objective function of deep supervision, which consists of the terms of each encoding layers and the terms from each MSCGnet output of iMSCGnet, is employed. Our method is evaluated extensively on the four publicly available datasets, including ISBI2016 <xref ref-type=\\\\\"bibr\\\\\" rid=\\\\\"ref1\\\\\">[1]</xref>, ISBI2017 <xref ref-type=\\\\\"bibr\\\\\" rid=\\\\\"ref2\\\\\">[2]</xref>, ISIC2018 <xref ref-type=\\\\\"bibr\\\\\" rid=\\\\\"ref3\\\\\">[3]</xref> and PH2 <xref ref-type=\\\\\"bibr\\\\\" rid=\\\\\"ref4\\\\\">[4]</xref> datasets. The experimental results prove the effectiveness of proposed components and show that our method generally outperforms the state-of-the-art methods.\"\\n      },\\n      {\\n        \"paperId\": \"29528d8cb030a65f62a35b1237f1f5483077ad0a\",\\n        \"url\": \"https://www.semanticscholar.org/paper/29528d8cb030a65f62a35b1237f1f5483077ad0a\",\\n        \"title\": \"Inference Scaling for Long-Context Retrieval Augmented Generation\",\\n        \"citationCount\": 47,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2410.04343, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work investigates inference scaling for retrieval augmented generation (RAG), exploring the combination of multiple strategies beyond simply increasing the quantity of knowledge, including in-context learning and iterative prompting, and reveals that increasing inference computation leads to nearly linear gains in RAG performance when optimally allocated.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-10-06\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2028213158\",\\n            \"name\": \"Zhenrui Yue\"\\n          },\\n          {\\n            \"authorId\": \"39371343\",\\n            \"name\": \"Honglei Zhuang\"\\n          },\\n          {\\n            \"authorId\": \"2324782053\",\\n            \"name\": \"Aijun Bai\"\\n          },\\n          {\\n            \"authorId\": \"2261281337\",\\n            \"name\": \"Kai Hui\"\\n          },\\n          {\\n            \"authorId\": \"1886219\",\\n            \"name\": \"R. Jagerman\"\\n          },\\n          {\\n            \"authorId\": \"2324910979\",\\n            \"name\": \"Hansi Zeng\"\\n          },\\n          {\\n            \"authorId\": \"2099586642\",\\n            \"name\": \"Zhen Qin\"\\n          },\\n          {\\n            \"authorId\": \"2325158655\",\\n            \"name\": \"Dong Wang\"\\n          },\\n          {\\n            \"authorId\": \"2261356664\",\\n            \"name\": \"Xuanhui Wang\"\\n          },\\n          {\\n            \"authorId\": \"1815447\",\\n            \"name\": \"Michael Bendersky\"\\n          }\\n        ],\\n        \"abstract\": \"The scaling of inference computation has unlocked the potential of long-context large language models (LLMs) across diverse settings. For knowledge-intensive tasks, the increased compute is often allocated to incorporate more external knowledge. However, without effectively utilizing such knowledge, solely expanding context does not always enhance performance. In this work, we investigate inference scaling for retrieval augmented generation (RAG), exploring the combination of multiple strategies beyond simply increasing the quantity of knowledge, including in-context learning and iterative prompting. These strategies provide additional flexibility to scale test-time computation (e.g., by increasing retrieved documents or generation steps), thereby enhancing LLMs\\' ability to effectively acquire and utilize contextual information. We address two key questions: (1) How does RAG performance benefit from the scaling of inference computation when optimally configured? (2) Can we predict the optimal test-time compute allocation for a given budget by modeling the relationship between RAG performance and inference parameters? Our observations reveal that increasing inference computation leads to nearly linear gains in RAG performance when optimally allocated, a relationship we describe as the inference scaling laws for RAG. Building on this, we further develop the computation allocation model to estimate RAG performance across different inference configurations. The model predicts optimal inference parameters under various computation constraints, which align closely with the experimental results. By applying these optimal configurations, we demonstrate that scaling inference compute on long-context LLMs achieves up to 58.9% gains on benchmark datasets compared to standard RAG.\"\\n      },\\n      {\\n        \"paperId\": \"ca47c63a97848e60389037c93a4feb2daf849c3e\",\\n        \"url\": \"https://www.semanticscholar.org/paper/ca47c63a97848e60389037c93a4feb2daf849c3e\",\\n        \"title\": \"Iterative Data Smoothing: Mitigating Reward Overfitting and Overoptimization in RLHF\",\\n        \"citationCount\": 41,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2401.16335, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"The core idea is that during each training epoch, the model is updated with the data, but also update the date using the model, replacing hard labels with soft labels, and the empirical findings highlight the superior performance of this approach over the traditional methods.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-01-29\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"23111704\",\\n            \"name\": \"Banghua Zhu\"\\n          },\\n          {\\n            \"authorId\": \"2273930444\",\\n            \"name\": \"Michael I. Jordan\"\\n          },\\n          {\\n            \"authorId\": \"2258657022\",\\n            \"name\": \"Jiantao Jiao\"\\n          }\\n        ],\\n        \"abstract\": \"Reinforcement Learning from Human Feedback (RLHF) is a pivotal technique that aligns language models closely with human-centric values. The initial phase of RLHF involves learning human values using a reward model from ranking data. It is observed that the performance of the reward model degrades after one epoch of training, and optimizing too much against the learned reward model eventually hinders the true objective. This paper delves into these issues, leveraging the theoretical insights to design improved reward learning algorithm termed \\'Iterative Data Smoothing\\' (IDS). The core idea is that during each training epoch, we not only update the model with the data, but also update the date using the model, replacing hard labels with soft labels. Our empirical findings highlight the superior performance of this approach over the traditional methods.\"\\n      },\\n      {\\n        \"paperId\": \"8051818817a9a3815be6623a679d4a7f5a7b7964\",\\n        \"url\": \"https://www.semanticscholar.org/paper/8051818817a9a3815be6623a679d4a7f5a7b7964\",\\n        \"title\": \"Retrieve, Summarize, Plan: Advancing Multi-hop Question Answering with an Iterative Approach\",\\n        \"citationCount\": 36,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2407.13101, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A novel iterative RAG method called ReSP is proposed, equipped with a dual-function summarizer that compresses information from retrieved documents, targeting both the overarching question and the current sub-question concurrently.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Book\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-07-18\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2305310515\",\\n            \"name\": \"Zhouyu Jiang\"\\n          },\\n          {\\n            \"authorId\": \"2273904059\",\\n            \"name\": \"Mengshu Sun\"\\n          },\\n          {\\n            \"authorId\": \"2303947668\",\\n            \"name\": \"Lei Liang\"\\n          },\\n          {\\n            \"authorId\": \"2290024603\",\\n            \"name\": \"Zhiqiang Zhang\"\\n          }\\n        ],\\n        \"abstract\": \"Multi-hop question answering is a challenging task with distinct industrial relevance, and Retrieval-Augmented Generation (RAG) methods based on large language models (LLMs) have become a popular approach to tackle this task. Owing to the potential inability to retrieve all necessary information in a single iteration, a series of iterative RAG methods has been recently developed, showing significant performance improvements. However, existing methods still face two critical challenges: context overload resulting from multiple rounds of retrieval, and over-planning and repetitive planning due to the lack of a recorded retrieval trajectory. In this paper, we propose a novel iterative RAG method called ReSP, equipped with a dual-function summarizer. This summarizer compresses information from retrieved documents, targeting both the overarching question and the current sub-question concurrently. Experimental results on the multi-hop question-answering datasets HotpotQA and 2WikiMultihopQA demonstrate that our method significantly outperforms the state-of-the-art, and exhibits excellent robustness concerning context length.\"\\n      },\\n      {\\n        \"paperId\": \"9a1b5eb2a3e17621feab02c0f7f8c4df65ebf71c\",\\n        \"url\": \"https://www.semanticscholar.org/paper/9a1b5eb2a3e17621feab02c0f7f8c4df65ebf71c\",\\n        \"title\": \"CTIF-Net: A CNN-Transformer Iterative Fusion Network for Salient Object Detection\",\\n        \"citationCount\": 36,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {\\'tldr\\'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/TCSVT.2023.3321190?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/TCSVT.2023.3321190, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": null,\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-05-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2116395954\",\\n            \"name\": \"Junbin Yuan\"\\n          },\\n          {\\n            \"authorId\": \"2254058034\",\\n            \"name\": \"Aiqing Zhu\"\\n          },\\n          {\\n            \"authorId\": \"2575087\",\\n            \"name\": \"Qingzhen Xu\"\\n          },\\n          {\\n            \"authorId\": \"2757120\",\\n            \"name\": \"Kanoksak Wattanachote\"\\n          },\\n          {\\n            \"authorId\": \"2240697074\",\\n            \"name\": \"Yongyi Gong\"\\n          }\\n        ],\\n        \"abstract\": \"Capturing sufficient global context and rich spatial structure information is critical for dense prediction tasks. Convolutional Neural Network (CNN) is particularly adept at modeling fine-grained local features, while Transformer excels at modeling global context information. It is evident that CNN and Transformer exhibit complementary characteristics. Exploring the design of a network, that efficiently fuses these two models to leverage their strengths fully and achieve more accurate detection, represents a promising and worthwhile research topic. In this paper, we introduce a novel CNN-Transformer Iterative Fusion Network (CTIF-Net) for salient object detection. It efficiently combines CNN and Transformer to achieve superior performance by using a parallel dual encoder structure and a feature iterative fusion module. Firstly, CTIF-Net extracts features from the image using the CNN and the Transformer, respectively. Secondly, two feature convertors and a feature iterative fusion module are employed to combine and iteratively refine the two sets of features. The experimental results on multiple SOD datasets show that CTIF-Net outperforms 17 state-of-the-art methods, achieving higher performance in various mainstream evaluation metrics such as F-measure, S-measure, and MAE value. Code can be found at https://github.com/danielfaster/CTIF-Net/.\"\\n      },\\n      {\\n        \"paperId\": \"c811bedbe8f4c21d0cba9f9175f7c2eb203284a7\",\\n        \"url\": \"https://www.semanticscholar.org/paper/c811bedbe8f4c21d0cba9f9175f7c2eb203284a7\",\\n        \"title\": \"Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context\",\\n        \"citationCount\": 2813,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2403.05530, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"Gemini 1.5 models achieve near-perfect recall on long-context retrieval tasks across modalities, improve the state-of-the-art in long-document QA, long-video QA and long-context ASR, and match or surpass Gemini 1.0 Ultra\\'s state-of-the-art performance across a broad set of benchmarks.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-03-08\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"1557386977\",\\n            \"name\": \"Machel Reid\"\\n          },\\n          {\\n            \"authorId\": \"2417003\",\\n            \"name\": \"Nikolay Savinov\"\\n          },\\n          {\\n            \"authorId\": \"3035073\",\\n            \"name\": \"Denis Teplyashin\"\\n          },\\n          {\\n            \"authorId\": \"150077954\",\\n            \"name\": \"Dmitry Lepikhin\"\\n          },\\n          {\\n            \"authorId\": \"2542999\",\\n            \"name\": \"T. Lillicrap\"\\n          },\\n          {\\n            \"authorId\": \"2285263\",\\n            \"name\": \"Jean-Baptiste Alayrac\"\\n          },\\n          {\\n            \"authorId\": \"1737285\",\\n            \"name\": \"Radu Soricut\"\\n          },\\n          {\\n            \"authorId\": \"2672644\",\\n            \"name\": \"Angeliki Lazaridou\"\\n          },\\n          {\\n            \"authorId\": \"2273534960\",\\n            \"name\": \"Orhan Firat\"\\n          },\\n          {\\n            \"authorId\": \"4337102\",\\n            \"name\": \"Julian Schrittwieser\"\\n          },\\n          {\\n            \"authorId\": \"2460849\",\\n            \"name\": \"Ioannis Antonoglou\"\\n          },\\n          {\\n            \"authorId\": \"1508890387\",\\n            \"name\": \"Rohan Anil\"\\n          },\\n          {\\n            \"authorId\": \"148016269\",\\n            \"name\": \"Sebastian Borgeaud\"\\n          },\\n          {\\n            \"authorId\": \"2273563615\",\\n            \"name\": \"Andrew M. Dai\"\\n          },\\n          {\\n            \"authorId\": \"2143434227\",\\n            \"name\": \"Katie Millican\"\\n          },\\n          {\\n            \"authorId\": \"2275180676\",\\n            \"name\": \"Ethan Dyer\"\\n          },\\n          {\\n            \"authorId\": \"2143471164\",\\n            \"name\": \"Mia Glaese\"\\n          },\\n          {\\n            \"authorId\": \"2070364520\",\\n            \"name\": \"Thibault Sottiaux\"\\n          },\\n          {\\n            \"authorId\": \"2275292292\",\\n            \"name\": \"Ben-jamin Lee\"\\n          },\\n          {\\n            \"authorId\": \"2290484786\",\\n            \"name\": \"Fabio Viola\"\\n          },\\n          {\\n            \"authorId\": \"47447264\",\\n            \"name\": \"Malcolm Reynolds\"\\n          },\\n          {\\n            \"authorId\": \"2145139570\",\\n            \"name\": \"Yuanzhong Xu\"\\n          },\\n          {\\n            \"authorId\": \"2065370007\",\\n            \"name\": \"James Molloy\"\\n          },\\n          {\\n            \"authorId\": \"2249566095\",\\n            \"name\": \"Jilin Chen\"\\n          },\\n          {\\n            \"authorId\": \"2090818\",\\n            \"name\": \"M. Isard\"\\n          },\\n          {\\n            \"authorId\": \"152399055\",\\n            \"name\": \"P. Barham\"\\n          },\\n          {\\n            \"authorId\": \"2146532222\",\\n            \"name\": \"Tom Hennigan\"\\n          },\\n          {\\n            \"authorId\": \"2275176102\",\\n            \"name\": \"Ross Mcilroy\"\\n          },\\n          {\\n            \"authorId\": \"2275525680\",\\n            \"name\": \"Melvin Johnson\"\\n          },\\n          {\\n            \"authorId\": \"1698491\",\\n            \"name\": \"J. Schalkwyk\"\\n          },\\n          {\\n            \"authorId\": \"2275181648\",\\n            \"name\": \"Eli Collins\"\\n          },\\n          {\\n            \"authorId\": \"2143538252\",\\n            \"name\": \"Eliza Rutherford\"\\n          },\\n          {\\n            \"authorId\": \"2275185558\",\\n            \"name\": \"Erica Moreira\"\\n          },\\n          {\\n            \"authorId\": \"34122449\",\\n            \"name\": \"Kareem W. Ayoub\"\\n          },\\n          {\\n            \"authorId\": \"2275186741\",\\n            \"name\": \"Megha Goel\"\\n          },\\n          {\\n            \"authorId\": \"1406288863\",\\n            \"name\": \"Clemens Meyer\"\\n          },\\n          {\\n            \"authorId\": \"2005813\",\\n            \"name\": \"Gregory Thornton\"\\n          },\\n          {\\n            \"authorId\": \"2275729730\",\\n            \"name\": \"Zhen Yang\"\\n          },\\n          {\\n            \"authorId\": \"47407464\",\\n            \"name\": \"H. Michalewski\"\\n          },\\n          {\\n            \"authorId\": \"2275185143\",\\n            \"name\": \"Zaheer Abbas\"\\n          },\\n          {\\n            \"authorId\": \"74530494\",\\n            \"name\": \"Nathan Schucher\"\\n          },\\n          {\\n            \"authorId\": \"12679121\",\\n            \"name\": \"Ankesh Anand\"\\n          },\\n          {\\n            \"authorId\": \"2275185509\",\\n            \"name\": \"Richard Ives\"\\n          },\\n          {\\n            \"authorId\": \"2058168486\",\\n            \"name\": \"James Keeling\"\\n          },\\n          {\\n            \"authorId\": \"3257286\",\\n            \"name\": \"Karel Lenc\"\\n          },\\n          {\\n            \"authorId\": \"40269586\",\\n            \"name\": \"S. Haykal\"\\n          },\\n          {\\n            \"authorId\": \"2944868\",\\n            \"name\": \"Siamak Shakeri\"\\n          },\\n          {\\n            \"authorId\": \"67311962\",\\n            \"name\": \"Pranav Shyam\"\\n          },\\n          {\\n            \"authorId\": \"2841893\",\\n            \"name\": \"A. Chowdhery\"\\n          },\\n          {\\n            \"authorId\": \"81387328\",\\n            \"name\": \"Roman Ring\"\\n          },\\n          {\\n            \"authorId\": \"2135383313\",\\n            \"name\": \"Stephen Spencer\"\\n          },\\n          {\\n            \"authorId\": \"1413718981\",\\n            \"name\": \"Eren Sezener\"\\n          },\\n          {\\n            \"authorId\": \"2289035179\",\\n            \"name\": \"Luke Vilnis\"\\n          },\\n          {\\n            \"authorId\": \"2275186577\",\\n            \"name\": \"Os-car Chang\"\\n          },\\n          {\\n            \"authorId\": \"2288269273\",\\n            \"name\": \"Nobuyuki Morioka\"\\n          },\\n          {\\n            \"authorId\": \"2275183383\",\\n            \"name\": \"George Tucker\"\\n          },\\n          {\\n            \"authorId\": \"2276211400\",\\n            \"name\": \"Ce Zheng\"\\n          },\\n          {\\n            \"authorId\": \"2275186839\",\\n            \"name\": \"Oliver Woodman\"\\n          },\\n          {\\n            \"authorId\": \"80930649\",\\n            \"name\": \"Nithya Attaluri\"\\n          },\\n          {\\n            \"authorId\": \"2367821\",\\n            \"name\": \"Tom\\\\u00e1s Kocisk\\\\u00fd\"\\n          },\\n          {\\n            \"authorId\": \"2275187189\",\\n            \"name\": \"Evgenii Eltyshev\"\\n          },\\n          {\\n            \"authorId\": \"2275535939\",\\n            \"name\": \"Xi Chen\"\\n          },\\n          {\\n            \"authorId\": \"2275188933\",\\n            \"name\": \"Timothy Chung\"\\n          },\\n          {\\n            \"authorId\": \"1394635460\",\\n            \"name\": \"Vittorio Selo\"\\n          },\\n          {\\n            \"authorId\": \"1791585\",\\n            \"name\": \"Siddhartha Brahma\"\\n          },\\n          {\\n            \"authorId\": \"1737522\",\\n            \"name\": \"Petko Georgiev\"\\n          },\\n          {\\n            \"authorId\": \"133666998\",\\n            \"name\": \"Ambrose Slone\"\\n          },\\n          {\\n            \"authorId\": \"2275539055\",\\n            \"name\": \"Zhenkai Zhu\"\\n          },\\n          {\\n            \"authorId\": \"2266398107\",\\n            \"name\": \"James Lottes\"\\n          },\\n          {\\n            \"authorId\": \"2275178766\",\\n            \"name\": \"Siyuan Qiao\"\\n          },\\n          {\\n            \"authorId\": \"2290484287\",\\n            \"name\": \"Ben Caine\"\\n          },\\n          {\\n            \"authorId\": \"2287841795\",\\n            \"name\": \"Sebastian Riedel\"\\n          },\\n          {\\n            \"authorId\": \"2275176047\",\\n            \"name\": \"Alex Tomala\"\\n          },\\n          {\\n            \"authorId\": \"2159545857\",\\n            \"name\": \"Martin Chadwick\"\\n          },\\n          {\\n            \"authorId\": \"2253158807\",\\n            \"name\": \"J Christopher Love\"\\n          },\\n          {\\n            \"authorId\": \"2070068655\",\\n            \"name\": \"Peter Choy\"\\n          },\\n          {\\n            \"authorId\": \"2073395505\",\\n            \"name\": \"Sid Mittal\"\\n          },\\n          {\\n            \"authorId\": \"2815290\",\\n            \"name\": \"N. Houlsby\"\\n          },\\n          {\\n            \"authorId\": \"2269752766\",\\n            \"name\": \"Yunhao Tang\"\\n          },\\n          {\\n            \"authorId\": \"2289446231\",\\n            \"name\": \"Matthew Lamm\"\\n          },\\n          {\\n            \"authorId\": \"2275159462\",\\n            \"name\": \"Libin Bai\"\\n          },\\n          {\\n            \"authorId\": \"2197671266\",\\n            \"name\": \"Qiao Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2253917827\",\\n            \"name\": \"Luheng He\"\\n          },\\n          {\\n            \"authorId\": \"2275287219\",\\n            \"name\": \"Yong Cheng\"\\n          },\\n          {\\n            \"authorId\": \"2275186692\",\\n            \"name\": \"Peter Humphreys\"\\n          },\\n          {\\n            \"authorId\": \"2275290025\",\\n            \"name\": \"Yujia Li\"\\n          },\\n          {\\n            \"authorId\": \"1786259\",\\n            \"name\": \"Sergey Brin\"\\n          },\\n          {\\n            \"authorId\": \"51042571\",\\n            \"name\": \"Albin Cassirer\"\\n          },\\n          {\\n            \"authorId\": \"2283231534\",\\n            \"name\": \"Ying-Qi Miao\"\\n          },\\n          {\\n            \"authorId\": \"1780245\",\\n            \"name\": \"Luk\\\\u00e1s Zilka\"\\n          },\\n          {\\n            \"authorId\": \"2275189014\",\\n            \"name\": \"Taylor Tobin\"\\n          },\\n          {\\n            \"authorId\": \"2266735761\",\\n            \"name\": \"Kelvin Xu\"\\n          },\\n          {\\n            \"authorId\": \"2161966573\",\\n            \"name\": \"Lev Proleev\"\\n          },\\n          {\\n            \"authorId\": \"2275175792\",\\n            \"name\": \"Daniel Sohn\"\\n          },\\n          {\\n            \"authorId\": \"2275182383\",\\n            \"name\": \"Al-berto Magni\"\\n          },\\n          {\\n            \"authorId\": \"2258347245\",\\n            \"name\": \"L. Hendricks\"\\n          },\\n          {\\n            \"authorId\": \"2290513267\",\\n            \"name\": \"Isabel Gao\"\\n          },\\n          {\\n            \"authorId\": \"2217756237\",\\n            \"name\": \"Santiago Ontan\\'on\"\\n          },\\n          {\\n            \"authorId\": \"2275177720\",\\n            \"name\": \"Oskar Bunyan\"\\n          },\\n          {\\n            \"authorId\": \"2275185667\",\\n            \"name\": \"Nathan Byrd\"\\n          },\\n          {\\n            \"authorId\": \"2275537981\",\\n            \"name\": \"Abhanshu Sharma\"\\n          },\\n          {\\n            \"authorId\": \"48335426\",\\n            \"name\": \"Biao Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2290662953\",\\n            \"name\": \"Mario Pinto\"\\n          },\\n          {\\n            \"authorId\": \"2275170606\",\\n            \"name\": \"Rishika Sinha\"\\n          },\\n          {\\n            \"authorId\": \"18138802\",\\n            \"name\": \"Harsh Mehta\"\\n          },\\n          {\\n            \"authorId\": \"2275186531\",\\n            \"name\": \"Dawei Jia\"\\n          },\\n          {\\n            \"authorId\": \"1413064976\",\\n            \"name\": \"Sergi Caelles\"\\n          },\\n          {\\n            \"authorId\": \"1991019030\",\\n            \"name\": \"Albert Webson\"\\n          },\\n          {\\n            \"authorId\": \"2275162692\",\\n            \"name\": \"Alex Morris\"\\n          },\\n          {\\n            \"authorId\": \"2080504963\",\\n            \"name\": \"Becca Roelofs\"\\n          },\\n          {\\n            \"authorId\": \"2290634593\",\\n            \"name\": \"Yifan Ding\"\\n          },\\n          {\\n            \"authorId\": \"86898863\",\\n            \"name\": \"Robin Strudel\"\\n          },\\n          {\\n            \"authorId\": \"2275193471\",\\n            \"name\": \"Xuehan Xiong\"\\n          },\\n          {\\n            \"authorId\": \"39687627\",\\n            \"name\": \"Marvin Ritter\"\\n          },\\n          {\\n            \"authorId\": \"2256989598\",\\n            \"name\": \"Mostafa Dehghani\"\\n          },\\n          {\\n            \"authorId\": \"1706980\",\\n            \"name\": \"R. Chaabouni\"\\n          },\\n          {\\n            \"authorId\": \"2078909017\",\\n            \"name\": \"Abhijit Karmarkar\"\\n          },\\n          {\\n            \"authorId\": \"2290484705\",\\n            \"name\": \"Guangda Lai\"\\n          },\\n          {\\n            \"authorId\": \"3468078\",\\n            \"name\": \"Fabian Mentzer\"\\n          },\\n          {\\n            \"authorId\": \"2290664586\",\\n            \"name\": \"Bibo Xu\"\\n          },\\n          {\\n            \"authorId\": \"2261797906\",\\n            \"name\": \"YaGuang Li\"\\n          },\\n          {\\n            \"authorId\": \"2275534739\",\\n            \"name\": \"Yujing Zhang\"\\n          },\\n          {\\n            \"authorId\": \"40470211\",\\n            \"name\": \"T. Paine\"\\n          },\\n          {\\n            \"authorId\": \"40034895\",\\n            \"name\": \"Alex Goldin\"\\n          },\\n          {\\n            \"authorId\": \"3007442\",\\n            \"name\": \"Behnam Neyshabur\"\\n          },\\n          {\\n            \"authorId\": \"1734809439\",\\n            \"name\": \"Kate Baumli\"\\n          },\\n          {\\n            \"authorId\": \"6639036\",\\n            \"name\": \"Anselm Levskaya\"\\n          },\\n          {\\n            \"authorId\": \"2274104519\",\\n            \"name\": \"Michael Laskin\"\\n          },\\n          {\\n            \"authorId\": \"2275193644\",\\n            \"name\": \"Wenhao Jia\"\\n          },\\n          {\\n            \"authorId\": \"34269227\",\\n            \"name\": \"Jack W. Rae\"\\n          },\\n          {\\n            \"authorId\": \"2268673324\",\\n            \"name\": \"Kefan Xiao\"\\n          },\\n          {\\n            \"authorId\": \"2290485493\",\\n            \"name\": \"Antoine He\"\\n          },\\n          {\\n            \"authorId\": \"2290487747\",\\n            \"name\": \"Skye Giordano\"\\n          },\\n          {\\n            \"authorId\": \"2307454258\",\\n            \"name\": \"Lakshman Yagati\"\\n          },\\n          {\\n            \"authorId\": \"143783339\",\\n            \"name\": \"Jean-Baptiste Lespiau\"\\n          },\\n          {\\n            \"authorId\": \"122704930\",\\n            \"name\": \"Paul Natsev\"\\n          },\\n          {\\n            \"authorId\": \"2275185831\",\\n            \"name\": \"Sanjay Ganapathy\"\\n          },\\n          {\\n            \"authorId\": \"144097210\",\\n            \"name\": \"Fangyu Liu\"\\n          },\\n          {\\n            \"authorId\": \"2290487616\",\\n            \"name\": \"Danilo Martins\"\\n          },\\n          {\\n            \"authorId\": \"2249840944\",\\n            \"name\": \"Nanxin Chen\"\\n          },\\n          {\\n            \"authorId\": \"2275191526\",\\n            \"name\": \"Yunhan Xu\"\\n          },\\n          {\\n            \"authorId\": \"2275180117\",\\n            \"name\": \"Megan Barnes\"\\n          },\\n          {\\n            \"authorId\": \"2268760156\",\\n            \"name\": \"Rhys May\"\\n          },\\n          {\\n            \"authorId\": \"2275188533\",\\n            \"name\": \"Arpi Vezer\"\\n          },\\n          {\\n            \"authorId\": \"2275114643\",\\n            \"name\": \"Junhyuk Oh\"\\n          },\\n          {\\n            \"authorId\": \"2118834006\",\\n            \"name\": \"Ken Franko\"\\n          },\\n          {\\n            \"authorId\": \"2273670422\",\\n            \"name\": \"Sophie Bridgers\"\\n          },\\n          {\\n            \"authorId\": \"2275832693\",\\n            \"name\": \"Ruizhe Zhao\"\\n          },\\n          {\\n            \"authorId\": \"2275291909\",\\n            \"name\": \"Boxi Wu\"\\n          },\\n          {\\n            \"authorId\": \"40608942\",\\n            \"name\": \"Basil Mustafa\"\\n          },\\n          {\\n            \"authorId\": \"2290487447\",\\n            \"name\": \"Sean Sechrist\"\\n          },\\n          {\\n            \"authorId\": \"3166516\",\\n            \"name\": \"Emilio Parisotto\"\\n          },\\n          {\\n            \"authorId\": \"2598683\",\\n            \"name\": \"Thanumalayan Sankaranarayana Pillai\"\\n          },\\n          {\\n            \"authorId\": \"2290487293\",\\n            \"name\": \"Chris Larkin\"\\n          },\\n          {\\n            \"authorId\": \"2275149073\",\\n            \"name\": \"Chenjie Gu\"\\n          },\\n          {\\n            \"authorId\": \"2275186804\",\\n            \"name\": \"Christina Sorokin\"\\n          },\\n          {\\n            \"authorId\": \"2048712\",\\n            \"name\": \"M. Krikun\"\\n          },\\n          {\\n            \"authorId\": \"2275182203\",\\n            \"name\": \"Alexey Guseynov\"\\n          },\\n          {\\n            \"authorId\": \"2065404873\",\\n            \"name\": \"Jessica Landon\"\\n          },\\n          {\\n            \"authorId\": \"2275187147\",\\n            \"name\": \"Romina Datta\"\\n          },\\n          {\\n            \"authorId\": \"1863250\",\\n            \"name\": \"A. Pritzel\"\\n          },\\n          {\\n            \"authorId\": \"2151245633\",\\n            \"name\": \"Phoebe Thacker\"\\n          },\\n          {\\n            \"authorId\": \"2275801132\",\\n            \"name\": \"Fan Yang\"\\n          },\\n          {\\n            \"authorId\": \"2290487874\",\\n            \"name\": \"Kevin Hui\"\\n          },\\n          {\\n            \"authorId\": \"119556335\",\\n            \"name\": \"A.E. Hauth\"\\n          },\\n          {\\n            \"authorId\": \"2273556813\",\\n            \"name\": \"C. Yeh\"\\n          },\\n          {\\n            \"authorId\": \"2290481818\",\\n            \"name\": \"David Barker\"\\n          },\\n          {\\n            \"authorId\": \"1423275766\",\\n            \"name\": \"J. Mao-Jones\"\\n          },\\n          {\\n            \"authorId\": \"2166051497\",\\n            \"name\": \"Sophia Austin\"\\n          },\\n          {\\n            \"authorId\": \"2307453241\",\\n            \"name\": \"Hannah Sheahan\"\\n          },\\n          {\\n            \"authorId\": \"2620528\",\\n            \"name\": \"Parker Schuh\"\\n          },\\n          {\\n            \"authorId\": \"2275188153\",\\n            \"name\": \"James Svensson\"\\n          },\\n          {\\n            \"authorId\": \"2275193365\",\\n            \"name\": \"Rohan Jain\"\\n          },\\n          {\\n            \"authorId\": \"96641652\",\\n            \"name\": \"V. Ramasesh\"\\n          },\\n          {\\n            \"authorId\": \"2275185833\",\\n            \"name\": \"Anton Briukhov\"\\n          },\\n          {\\n            \"authorId\": \"2275180366\",\\n            \"name\": \"D. Chung\"\\n          },\\n          {\\n            \"authorId\": \"51029932\",\\n            \"name\": \"Tamara von Glehn\"\\n          },\\n          {\\n            \"authorId\": \"2275166845\",\\n            \"name\": \"Christina Butterfield\"\\n          },\\n          {\\n            \"authorId\": \"2275184551\",\\n            \"name\": \"Priya Jhakra\"\\n          },\\n          {\\n            \"authorId\": \"2275252154\",\\n            \"name\": \"Matt Wiethoff\"\\n          },\\n          {\\n            \"authorId\": \"2275193725\",\\n            \"name\": \"Justin Frye\"\\n          },\\n          {\\n            \"authorId\": \"2275175432\",\\n            \"name\": \"Jordan Grimstad\"\\n          },\\n          {\\n            \"authorId\": \"2158369306\",\\n            \"name\": \"Beer Changpinyo\"\\n          },\\n          {\\n            \"authorId\": \"153892869\",\\n            \"name\": \"Charline Le Lan\"\\n          },\\n          {\\n            \"authorId\": \"2275181572\",\\n            \"name\": \"Anna Bortsova\"\\n          },\\n          {\\n            \"authorId\": \"2275892922\",\\n            \"name\": \"Yonghui Wu\"\\n          },\\n          {\\n            \"authorId\": \"2767859\",\\n            \"name\": \"P. Voigtlaender\"\\n          },\\n          {\\n            \"authorId\": \"2279918122\",\\n            \"name\": \"Tara N. Sainath\"\\n          },\\n          {\\n            \"authorId\": \"2275575378\",\\n            \"name\": \"Charlotte Smith\"\\n          },\\n          {\\n            \"authorId\": \"2191689971\",\\n            \"name\": \"Will Hawkins\"\\n          },\\n          {\\n            \"authorId\": \"2275191626\",\\n            \"name\": \"Kris Cao\"\\n          },\\n          {\\n            \"authorId\": \"2275186515\",\\n            \"name\": \"James Besley\"\\n          },\\n          {\\n            \"authorId\": \"2059763226\",\\n            \"name\": \"S. Srinivasan\"\\n          },\\n          {\\n            \"authorId\": \"3175815\",\\n            \"name\": \"Mark Omernick\"\\n          },\\n          {\\n            \"authorId\": \"2160887964\",\\n            \"name\": \"Colin Gaffney\"\\n          },\\n          {\\n            \"authorId\": \"1956049835\",\\n            \"name\": \"G. Surita\"\\n          },\\n          {\\n            \"authorId\": \"2290484991\",\\n            \"name\": \"Ryan Burnell\"\\n          },\\n          {\\n            \"authorId\": \"2143374656\",\\n            \"name\": \"Bogdan Damoc\"\\n          },\\n          {\\n            \"authorId\": \"2275220028\",\\n            \"name\": \"Junwhan Ahn\"\\n          },\\n          {\\n            \"authorId\": \"2285740851\",\\n            \"name\": \"Andrew Brock\"\\n          },\\n          {\\n            \"authorId\": \"2146532125\",\\n            \"name\": \"Mantas Pajarskas\"\\n          },\\n          {\\n            \"authorId\": \"2275187155\",\\n            \"name\": \"Anastasia Petrushkina\"\\n          },\\n          {\\n            \"authorId\": \"30155667\",\\n            \"name\": \"Seb Noury\"\\n          },\\n          {\\n            \"authorId\": \"2275186192\",\\n            \"name\": \"Lorenzo Blanco\"\\n          },\\n          {\\n            \"authorId\": \"1754860\",\\n            \"name\": \"Kevin Swersky\"\\n          },\\n          {\\n            \"authorId\": \"2275185727\",\\n            \"name\": \"Arun Ahuja\"\\n          },\\n          {\\n            \"authorId\": \"2261737895\",\\n            \"name\": \"Thi Avrahami\"\\n          },\\n          {\\n            \"authorId\": \"40055795\",\\n            \"name\": \"Vedant Misra\"\\n          },\\n          {\\n            \"authorId\": \"2275184736\",\\n            \"name\": \"Raoul de Liedekerke\"\\n          },\\n          {\\n            \"authorId\": \"2275181534\",\\n            \"name\": \"Mariko Iinuma\"\\n          },\\n          {\\n            \"authorId\": \"144703404\",\\n            \"name\": \"A. Polozov\"\\n          },\\n          {\\n            \"authorId\": \"143981350\",\\n            \"name\": \"Sarah York\"\\n          },\\n          {\\n            \"authorId\": \"47568983\",\\n            \"name\": \"George van den Driessche\"\\n          },\\n          {\\n            \"authorId\": \"2275185732\",\\n            \"name\": \"Paul Michel\"\\n          },\\n          {\\n            \"authorId\": \"2273650801\",\\n            \"name\": \"Justin Chiu\"\\n          },\\n          {\\n            \"authorId\": \"46901218\",\\n            \"name\": \"Rory Blevins\"\\n          },\\n          {\\n            \"authorId\": \"2275185661\",\\n            \"name\": \"Zach Gleicher\"\\n          },\\n          {\\n            \"authorId\": \"39257069\",\\n            \"name\": \"Adri\\\\u00e0 Recasens\"\\n          },\\n          {\\n            \"authorId\": \"2275186093\",\\n            \"name\": \"Alban Rrustemi\"\\n          },\\n          {\\n            \"authorId\": \"1980809\",\\n            \"name\": \"E. Gribovskaya\"\\n          },\\n          {\\n            \"authorId\": \"2275277736\",\\n            \"name\": \"Au-rko Roy\"\\n          },\\n          {\\n            \"authorId\": \"2290487054\",\\n            \"name\": \"Wiktor Gworek\"\\n          },\\n          {\\n            \"authorId\": \"2275186656\",\\n            \"name\": \"S\\\\u00e9bastien M. R. Arnold\"\\n          },\\n          {\\n            \"authorId\": \"2275291886\",\\n            \"name\": \"Lisa Lee\"\\n          },\\n          {\\n            \"authorId\": \"2267341862\",\\n            \"name\": \"James Lee-Thorp\"\\n          },\\n          {\\n            \"authorId\": \"2090812426\",\\n            \"name\": \"M. Maggioni\"\\n          },\\n          {\\n            \"authorId\": \"2275183119\",\\n            \"name\": \"Enrique Piqueras\"\\n          },\\n          {\\n            \"authorId\": \"2051018967\",\\n            \"name\": \"Kartikeya Badola\"\\n          },\\n          {\\n            \"authorId\": \"2425230\",\\n            \"name\": \"S. Vikram\"\\n          },\\n          {\\n            \"authorId\": \"2275585027\",\\n            \"name\": \"Lucas Gonzalez\"\\n          },\\n          {\\n            \"authorId\": \"2275186584\",\\n            \"name\": \"Anirudh Baddepudi\"\\n          },\\n          {\\n            \"authorId\": \"2268665228\",\\n            \"name\": \"Evan Senter\"\\n          },\\n          {\\n            \"authorId\": \"2261961752\",\\n            \"name\": \"J. Devlin\"\\n          },\\n          {\\n            \"authorId\": \"47901308\",\\n            \"name\": \"James Qin\"\\n          },\\n          {\\n            \"authorId\": \"2275148073\",\\n            \"name\": \"Michael Azzam\"\\n          },\\n          {\\n            \"authorId\": \"1994939814\",\\n            \"name\": \"Maja Trebacz\"\\n          },\\n          {\\n            \"authorId\": \"35930544\",\\n            \"name\": \"M. Polacek\"\\n          },\\n          {\\n            \"authorId\": \"2290485355\",\\n            \"name\": \"Kashyap Krishnakumar\"\\n          },\\n          {\\n            \"authorId\": \"2275193337\",\\n            \"name\": \"Shuo-Yiin Chang\"\\n          },\\n          {\\n            \"authorId\": \"2275176212\",\\n            \"name\": \"Matthew Tung\"\\n          },\\n          {\\n            \"authorId\": \"2275187196\",\\n            \"name\": \"Ivo Penchev\"\\n          },\\n          {\\n            \"authorId\": \"2258551072\",\\n            \"name\": \"Rishabh Joshi\"\\n          },\\n          {\\n            \"authorId\": \"2275180557\",\\n            \"name\": \"Kate Olszewska\"\\n          },\\n          {\\n            \"authorId\": \"2275184985\",\\n            \"name\": \"Carrie Muir\"\\n          },\\n          {\\n            \"authorId\": \"2275185968\",\\n            \"name\": \"Mateo Wirth\"\\n          },\\n          {\\n            \"authorId\": \"2275184113\",\\n            \"name\": \"A. Hartman\"\\n          },\\n          {\\n            \"authorId\": \"2160888100\",\\n            \"name\": \"Joshua Newlan\"\\n          },\\n          {\\n            \"authorId\": \"2252586080\",\\n            \"name\": \"S. Kashem\"\\n          },\\n          {\\n            \"authorId\": \"2218882489\",\\n            \"name\": \"Vijay Bolina\"\\n          },\\n          {\\n            \"authorId\": \"2290484418\",\\n            \"name\": \"Elahe Dabir\"\\n          },\\n          {\\n            \"authorId\": \"3038326\",\\n            \"name\": \"Joost R. van Amersfoort\"\\n          },\\n          {\\n            \"authorId\": \"2275130349\",\\n            \"name\": \"Zafarali Ahmed\"\\n          },\\n          {\\n            \"authorId\": \"2275185511\",\\n            \"name\": \"James Cobon-Kerr\"\\n          },\\n          {\\n            \"authorId\": \"2269391198\",\\n            \"name\": \"Aishwarya B Kamath\"\\n          },\\n          {\\n            \"authorId\": \"2259962018\",\\n            \"name\": \"A. M. Hrafnkelsson\"\\n          },\\n          {\\n            \"authorId\": \"2274787555\",\\n            \"name\": \"Le Hou\"\\n          },\\n          {\\n            \"authorId\": \"2290485798\",\\n            \"name\": \"Ian Mackinnon\"\\n          },\\n          {\\n            \"authorId\": \"2156930381\",\\n            \"name\": \"Alexandre Frechette\"\\n          },\\n          {\\n            \"authorId\": \"51210148\",\\n            \"name\": \"Eric Noland\"\\n          },\\n          {\\n            \"authorId\": \"2275182246\",\\n            \"name\": \"Xi-ance Si\"\\n          },\\n          {\\n            \"authorId\": \"2779842\",\\n            \"name\": \"Emanuel Taropa\"\\n          },\\n          {\\n            \"authorId\": \"2350430090\",\\n            \"name\": \"Dong Li\"\\n          },\\n          {\\n            \"authorId\": \"2275183277\",\\n            \"name\": \"Phil Crone\"\\n          },\\n          {\\n            \"authorId\": \"4478284\",\\n            \"name\": \"Anmol Gulati\"\\n          },\\n          {\\n            \"authorId\": \"2275180682\",\\n            \"name\": \"S\\'ebastien Cevey\"\\n          },\\n          {\\n            \"authorId\": \"2275173231\",\\n            \"name\": \"Jonas Adler\"\\n          },\\n          {\\n            \"authorId\": \"2275786213\",\\n            \"name\": \"Ada Ma\"\\n          },\\n          {\\n            \"authorId\": \"2275185813\",\\n            \"name\": \"David Silver\"\\n          },\\n          {\\n            \"authorId\": \"148152480\",\\n            \"name\": \"Simon Tokumine\"\\n          },\\n          {\\n            \"authorId\": \"2067745837\",\\n            \"name\": \"Richard Powell\"\\n          },\\n          {\\n            \"authorId\": \"2275280377\",\\n            \"name\": \"Stephan Lee\"\\n          },\\n          {\\n            \"authorId\": \"2275571997\",\\n            \"name\": \"Michael B. Chang\"\\n          },\\n          {\\n            \"authorId\": \"2275396476\",\\n            \"name\": \"Samer Hassan\"\\n          },\\n          {\\n            \"authorId\": \"2007712128\",\\n            \"name\": \"Diana Mincu\"\\n          },\\n          {\\n            \"authorId\": \"2064599701\",\\n            \"name\": \"Antoine Yang\"\\n          },\\n          {\\n            \"authorId\": \"153898744\",\\n            \"name\": \"Nir Levine\"\\n          },\\n          {\\n            \"authorId\": \"2275186701\",\\n            \"name\": \"Jenny Brennan\"\\n          },\\n          {\\n            \"authorId\": \"2249764807\",\\n            \"name\": \"Mingqiu Wang\"\\n          },\\n          {\\n            \"authorId\": \"2265053608\",\\n            \"name\": \"Sarah Hodkinson\"\\n          },\\n          {\\n            \"authorId\": \"2144551262\",\\n            \"name\": \"Jeffrey Zhao\"\\n          },\\n          {\\n            \"authorId\": \"2290487597\",\\n            \"name\": \"Josh Lipschultz\"\\n          },\\n          {\\n            \"authorId\": \"20702300\",\\n            \"name\": \"Aedan Pope\"\\n          },\\n          {\\n            \"authorId\": \"2275571997\",\\n            \"name\": \"Michael B. Chang\"\\n          },\\n          {\\n            \"authorId\": \"2275767067\",\\n            \"name\": \"Cheng Li\"\\n          },\\n          {\\n            \"authorId\": \"2121764\",\\n            \"name\": \"Laurent El Shafey\"\\n          },\\n          {\\n            \"authorId\": \"2264591527\",\\n            \"name\": \"M. Paganini\"\\n          },\\n          {\\n            \"authorId\": \"2269733876\",\\n            \"name\": \"Sholto Douglas\"\\n          },\\n          {\\n            \"authorId\": \"2266464503\",\\n            \"name\": \"Bernd Bohnet\"\\n          },\\n          {\\n            \"authorId\": \"2274107421\",\\n            \"name\": \"Fabio Pardo\"\\n          },\\n          {\\n            \"authorId\": \"2275182230\",\\n            \"name\": \"Seth Odoom\"\\n          },\\n          {\\n            \"authorId\": \"2269541835\",\\n            \"name\": \"Mihaela Ro\\\\u0219ca\"\\n          },\\n          {\\n            \"authorId\": \"2267546965\",\\n            \"name\": \"Cicero Nogueira dos Santos\"\\n          },\\n          {\\n            \"authorId\": \"2275185640\",\\n            \"name\": \"Kedar Soparkar\"\\n          },\\n          {\\n            \"authorId\": \"35099444\",\\n            \"name\": \"A. Guez\"\\n          },\\n          {\\n            \"authorId\": \"2275187110\",\\n            \"name\": \"Tom Hudson\"\\n          },\\n          {\\n            \"authorId\": \"2275188563\",\\n            \"name\": \"Steven Hansen\"\\n          },\\n          {\\n            \"authorId\": \"50844587\",\\n            \"name\": \"Chulayuth Asawaroengchai\"\\n          },\\n          {\\n            \"authorId\": \"104000494\",\\n            \"name\": \"Ravichandra Addanki\"\\n          },\\n          {\\n            \"authorId\": \"2290486855\",\\n            \"name\": \"Tianhe Yu\"\\n          },\\n          {\\n            \"authorId\": \"3448463\",\\n            \"name\": \"Wojciech Stokowiec\"\\n          },\\n          {\\n            \"authorId\": \"2258793616\",\\n            \"name\": \"Mina Khan\"\\n          },\\n          {\\n            \"authorId\": \"2243002880\",\\n            \"name\": \"Justin Gilmer\"\\n          },\\n          {\\n            \"authorId\": \"2253808003\",\\n            \"name\": \"Jaehoon Lee\"\\n          },\\n          {\\n            \"authorId\": \"2290485108\",\\n            \"name\": \"Carrie Grimes Bostock\"\\n          },\\n          {\\n            \"authorId\": \"1996199677\",\\n            \"name\": \"Keran Rong\"\\n          },\\n          {\\n            \"authorId\": \"2263289033\",\\n            \"name\": \"Jonathan Caton\"\\n          },\\n          {\\n            \"authorId\": \"2275184275\",\\n            \"name\": \"Pedram Pejman\"\\n          },\\n          {\\n            \"authorId\": \"1696719\",\\n            \"name\": \"Filip Pavetic\"\\n          },\\n          {\\n            \"authorId\": \"2259937157\",\\n            \"name\": \"Geoff Brown\"\\n          },\\n          {\\n            \"authorId\": \"2290595918\",\\n            \"name\": \"Vivek Sharma\"\\n          },\\n          {\\n            \"authorId\": \"2170162986\",\\n            \"name\": \"Mario Luvci\\'c\"\\n          },\\n          {\\n            \"authorId\": \"2275176205\",\\n            \"name\": \"Rajku-mar Samuel\"\\n          },\\n          {\\n            \"authorId\": \"2941141\",\\n            \"name\": \"J. Djolonga\"\\n          },\\n          {\\n            \"authorId\": \"2063800905\",\\n            \"name\": \"Amol Mandhane\"\\n          },\\n          {\\n            \"authorId\": \"2275187845\",\\n            \"name\": \"Lars Lowe Sjosund\"\\n          },\\n          {\\n            \"authorId\": \"118801223\",\\n            \"name\": \"Elena Buchatskaya\"\\n          },\\n          {\\n            \"authorId\": \"2275158927\",\\n            \"name\": \"Elspeth White\"\\n          },\\n          {\\n            \"authorId\": \"2201776471\",\\n            \"name\": \"Natalie Clay\"\\n          },\\n          {\\n            \"authorId\": \"2260169185\",\\n            \"name\": \"Jiepu Jiang\"\\n          },\\n          {\\n            \"authorId\": \"2275798209\",\\n            \"name\": \"Hyeontaek Lim\"\\n          },\\n          {\\n            \"authorId\": \"38637384\",\\n            \"name\": \"Ross Hemsley\"\\n          },\\n          {\\n            \"authorId\": \"2275184618\",\\n            \"name\": \"Jane Labanowski\"\\n          },\\n          {\\n            \"authorId\": \"41019080\",\\n            \"name\": \"Nicola De Cao\"\\n          },\\n          {\\n            \"authorId\": \"2275188258\",\\n            \"name\": \"David Steiner\"\\n          },\\n          {\\n            \"authorId\": \"3362306\",\\n            \"name\": \"Sayed Hadi Hashemi\"\\n          },\\n          {\\n            \"authorId\": \"2288056644\",\\n            \"name\": \"Jacob Austin\"\\n          },\\n          {\\n            \"authorId\": \"2105841261\",\\n            \"name\": \"Anita Gergely\"\\n          },\\n          {\\n            \"authorId\": \"2221119859\",\\n            \"name\": \"Tim Blyth\"\\n          },\\n          {\\n            \"authorId\": \"2275190309\",\\n            \"name\": \"Joe Stanton\"\\n          },\\n          {\\n            \"authorId\": \"2272718153\",\\n            \"name\": \"K. Shivakumar\"\\n          },\\n          {\\n            \"authorId\": \"9356387\",\\n            \"name\": \"Aditya Siddhant\"\\n          },\\n          {\\n            \"authorId\": \"39552848\",\\n            \"name\": \"Anders Andreassen\"\\n          },\\n          {\\n            \"authorId\": \"2279996944\",\\n            \"name\": \"Carlos L. Araya\"\\n          },\\n          {\\n            \"authorId\": \"2275187415\",\\n            \"name\": \"Nikhil Sethi\"\\n          },\\n          {\\n            \"authorId\": \"2934334\",\\n            \"name\": \"Rakesh Shivanna\"\\n          },\\n          {\\n            \"authorId\": \"2275161833\",\\n            \"name\": \"Steven Hand\"\\n          },\\n          {\\n            \"authorId\": \"12295226\",\\n            \"name\": \"Ankur Bapna\"\\n          },\\n          {\\n            \"authorId\": \"2402489\",\\n            \"name\": \"A. Khodaei\"\\n          },\\n          {\\n            \"authorId\": \"19200186\",\\n            \"name\": \"Antoine Miech\"\\n          },\\n          {\\n            \"authorId\": \"2287809580\",\\n            \"name\": \"Garrett Tanzer\"\\n          },\\n          {\\n            \"authorId\": \"1394189636\",\\n            \"name\": \"Andy Swing\"\\n          },\\n          {\\n            \"authorId\": \"41037204\",\\n            \"name\": \"S. Thakoor\"\\n          },\\n          {\\n            \"authorId\": \"2291169360\",\\n            \"name\": \"Zhufeng Pan\"\\n          },\\n          {\\n            \"authorId\": \"81408931\",\\n            \"name\": \"Zachary Nado\"\\n          },\\n          {\\n            \"authorId\": \"2218062983\",\\n            \"name\": \"Stephanie Winkler\"\\n          },\\n          {\\n            \"authorId\": \"2256337021\",\\n            \"name\": \"Dian Yu\"\\n          },\\n          {\\n            \"authorId\": \"144413479\",\\n            \"name\": \"Mohammad Saleh\"\\n          },\\n          {\\n            \"authorId\": \"108173905\",\\n            \"name\": \"Lorenzo Maggiore\"\\n          },\\n          {\\n            \"authorId\": \"2159207795\",\\n            \"name\": \"Iain Barr\"\\n          },\\n          {\\n            \"authorId\": \"2275187490\",\\n            \"name\": \"Minh Giang\"\\n          },\\n          {\\n            \"authorId\": \"2275186582\",\\n            \"name\": \"Thais Kagohara\"\\n          },\\n          {\\n            \"authorId\": \"1841008\",\\n            \"name\": \"Ivo Danihelka\"\\n          },\\n          {\\n            \"authorId\": \"2275176043\",\\n            \"name\": \"Amit Marathe\"\\n          },\\n          {\\n            \"authorId\": \"2275181199\",\\n            \"name\": \"Vladimir Feinberg\"\\n          },\\n          {\\n            \"authorId\": \"2275176049\",\\n            \"name\": \"Mohamed Elhawaty\"\\n          },\\n          {\\n            \"authorId\": \"3404697\",\\n            \"name\": \"Nimesh Ghelani\"\\n          },\\n          {\\n            \"authorId\": \"48257711\",\\n            \"name\": \"Dan Horgan\"\\n          },\\n          {\\n            \"authorId\": \"2275121046\",\\n            \"name\": \"Helen Miller\"\\n          },\\n          {\\n            \"authorId\": \"2275184334\",\\n            \"name\": \"Lexi Walker\"\\n          },\\n          {\\n            \"authorId\": \"1825728\",\\n            \"name\": \"Richard Tanburn\"\\n          },\\n          {\\n            \"authorId\": \"2275180099\",\\n            \"name\": \"Mukarram Tariq\"\\n          },\\n          {\\n            \"authorId\": \"2275113487\",\\n            \"name\": \"Disha Shrivastava\"\\n          },\\n          {\\n            \"authorId\": \"2290487337\",\\n            \"name\": \"Fei Xia\"\\n          },\\n          {\\n            \"authorId\": \"2284761701\",\\n            \"name\": \"Chung-Cheng Chiu\"\\n          },\\n          {\\n            \"authorId\": \"2333511945\",\\n            \"name\": \"Zoe Ashwood\"\\n          },\\n          {\\n            \"authorId\": \"2290486431\",\\n            \"name\": \"Khuslen Baatarsukh\"\\n          },\\n          {\\n            \"authorId\": \"2412073\",\\n            \"name\": \"Sina Samangooei\"\\n          },\\n          {\\n            \"authorId\": \"2275177971\",\\n            \"name\": \"Fred Alcober\"\\n          },\\n          {\\n            \"authorId\": \"2163521750\",\\n            \"name\": \"Axel Stjerngren\"\\n          },\\n          {\\n            \"authorId\": \"2258235140\",\\n            \"name\": \"P. Komarek\"\\n          },\\n          {\\n            \"authorId\": \"2275185589\",\\n            \"name\": \"Katerina Tsihlas\"\\n          },\\n          {\\n            \"authorId\": \"11167300\",\\n            \"name\": \"Anudhyan Boral\"\\n          },\\n          {\\n            \"authorId\": \"89066101\",\\n            \"name\": \"R. Comanescu\"\\n          },\\n          {\\n            \"authorId\": \"2275275439\",\\n            \"name\": \"Jeremy Chen\"\\n          },\\n          {\\n            \"authorId\": \"7247867\",\\n            \"name\": \"Ruibo Liu\"\\n          },\\n          {\\n            \"authorId\": \"2275185808\",\\n            \"name\": \"Dawn Bloxwich\"\\n          },\\n          {\\n            \"authorId\": \"2182971260\",\\n            \"name\": \"Charlie Chen\"\\n          },\\n          {\\n            \"authorId\": \"2265240845\",\\n            \"name\": \"Yanhua Sun\"\\n          },\\n          {\\n            \"authorId\": \"2275173841\",\\n            \"name\": \"Fangxi-aoyu Feng\"\\n          },\\n          {\\n            \"authorId\": \"2251517316\",\\n            \"name\": \"M. Mauger\"\\n          },\\n          {\\n            \"authorId\": \"1404332584\",\\n            \"name\": \"Xerxes Dotiwalla\"\\n          },\\n          {\\n            \"authorId\": \"2297847306\",\\n            \"name\": \"V. Hellendoorn\"\\n          },\\n          {\\n            \"authorId\": \"2275184531\",\\n            \"name\": \"Michael Sharman\"\\n          },\\n          {\\n            \"authorId\": \"2275187038\",\\n            \"name\": \"Ivy Zheng\"\\n          },\\n          {\\n            \"authorId\": \"2256873459\",\\n            \"name\": \"Krishna Haridasan\"\\n          },\\n          {\\n            \"authorId\": \"1403998955\",\\n            \"name\": \"Gabriel Barth-Maron\"\\n          },\\n          {\\n            \"authorId\": \"2275181554\",\\n            \"name\": \"Craig Swanson\"\\n          },\\n          {\\n            \"authorId\": \"2275184739\",\\n            \"name\": \"Dominika Rogozi\\'nska\"\\n          },\\n          {\\n            \"authorId\": \"2290741315\",\\n            \"name\": \"Alek Andreev\"\\n          },\\n          {\\n            \"authorId\": \"2249760524\",\\n            \"name\": \"P. Rubenstein\"\\n          },\\n          {\\n            \"authorId\": \"2275189194\",\\n            \"name\": \"Ruoxin Sang\"\\n          },\\n          {\\n            \"authorId\": \"2265528853\",\\n            \"name\": \"Dan Hurt\"\\n          },\\n          {\\n            \"authorId\": \"2275189864\",\\n            \"name\": \"Gamaleldin Elsayed\"\\n          },\\n          {\\n            \"authorId\": \"2290529512\",\\n            \"name\": \"Ren-shen Wang\"\\n          },\\n          {\\n            \"authorId\": \"2290485332\",\\n            \"name\": \"Dave Lacey\"\\n          },\\n          {\\n            \"authorId\": \"2279830514\",\\n            \"name\": \"Anastasija Ili\\'c\"\\n          },\\n          {\\n            \"authorId\": \"2275112414\",\\n            \"name\": \"Yao Zhao\"\\n          },\\n          {\\n            \"authorId\": \"2215449616\",\\n            \"name\": \"Woohyun Han\"\\n          },\\n          {\\n            \"authorId\": \"2257256357\",\\n            \"name\": \"Lora Aroyo\"\\n          },\\n          {\\n            \"authorId\": \"2275177173\",\\n            \"name\": \"Chimezie Iwuanyanwu\"\\n          },\\n          {\\n            \"authorId\": \"48942032\",\\n            \"name\": \"Vitaly Nikolaev\"\\n          },\\n          {\\n            \"authorId\": \"40627523\",\\n            \"name\": \"Balaji Lakshminarayanan\"\\n          },\\n          {\\n            \"authorId\": \"2290484919\",\\n            \"name\": \"Sadegh Jazayeri\"\\n          },\\n          {\\n            \"authorId\": \"31713635\",\\n            \"name\": \"Raphael Lopez Kaufman\"\\n          },\\n          {\\n            \"authorId\": \"2150348369\",\\n            \"name\": \"Mani Varadarajan\"\\n          },\\n          {\\n            \"authorId\": \"118505443\",\\n            \"name\": \"Chetan Tekur\"\\n          },\\n          {\\n            \"authorId\": \"2275187305\",\\n            \"name\": \"Doug Fritz\"\\n          },\\n          {\\n            \"authorId\": \"2140488873\",\\n            \"name\": \"Misha Khalman\"\\n          },\\n          {\\n            \"authorId\": \"2257286979\",\\n            \"name\": \"David Reitter\"\\n          },\\n          {\\n            \"authorId\": \"2290487762\",\\n            \"name\": \"Kingshuk Dasgupta\"\\n          },\\n          {\\n            \"authorId\": \"1658856741\",\\n            \"name\": \"Shourya Sarcar\"\\n          },\\n          {\\n            \"authorId\": \"103861813\",\\n            \"name\": \"T. Ornduff\"\\n          },\\n          {\\n            \"authorId\": \"2265527968\",\\n            \"name\": \"Javier Snaider\"\\n          },\\n          {\\n            \"authorId\": \"2174667321\",\\n            \"name\": \"Fantine Huot\"\\n          },\\n          {\\n            \"authorId\": \"2275694953\",\\n            \"name\": \"Johnson Jia\"\\n          },\\n          {\\n            \"authorId\": \"2275186180\",\\n            \"name\": \"Rupert Kemp\"\\n          },\\n          {\\n            \"authorId\": \"1702423\",\\n            \"name\": \"Nejc Trdin\"\\n          },\\n          {\\n            \"authorId\": \"2275186554\",\\n            \"name\": \"Anitha Vijayakumar\"\\n          },\\n          {\\n            \"authorId\": \"2290490486\",\\n            \"name\": \"Lucy Kim\"\\n          },\\n          {\\n            \"authorId\": \"2269460640\",\\n            \"name\": \"Christof Angermueller\"\\n          },\\n          {\\n            \"authorId\": \"2290485653\",\\n            \"name\": \"Li Lao\"\\n          },\\n          {\\n            \"authorId\": \"2275249023\",\\n            \"name\": \"Tianqi Liu\"\\n          },\\n          {\\n            \"authorId\": \"2290556567\",\\n            \"name\": \"Haibin Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2290485958\",\\n            \"name\": \"David Engel\"\\n          },\\n          {\\n            \"authorId\": \"2275190069\",\\n            \"name\": \"Somer Greene\"\\n          },\\n          {\\n            \"authorId\": \"2275169305\",\\n            \"name\": \"Anais White\"\\n          },\\n          {\\n            \"authorId\": \"2290488307\",\\n            \"name\": \"Jessica Austin\"\\n          },\\n          {\\n            \"authorId\": \"2290666013\",\\n            \"name\": \"Lilly Taylor\"\\n          },\\n          {\\n            \"authorId\": \"2275181309\",\\n            \"name\": \"Shereen Ashraf\"\\n          },\\n          {\\n            \"authorId\": \"2290539499\",\\n            \"name\": \"Dangyi Liu\"\\n          },\\n          {\\n            \"authorId\": \"2280669236\",\\n            \"name\": \"Maria Georgaki\"\\n          },\\n          {\\n            \"authorId\": \"2290485789\",\\n            \"name\": \"Irene Cai\"\\n          },\\n          {\\n            \"authorId\": \"2275177999\",\\n            \"name\": \"Yana Kulizhskaya\"\\n          },\\n          {\\n            \"authorId\": \"2096063076\",\\n            \"name\": \"Sonam Goenka\"\\n          },\\n          {\\n            \"authorId\": \"4125424\",\\n            \"name\": \"Brennan Saeta\"\\n          },\\n          {\\n            \"authorId\": \"4529644\",\\n            \"name\": \"Kiran Vodrahalli\"\\n          },\\n          {\\n            \"authorId\": \"2290488254\",\\n            \"name\": \"Christian Frank\"\\n          },\\n          {\\n            \"authorId\": \"47182967\",\\n            \"name\": \"D. Cesare\"\\n          },\\n          {\\n            \"authorId\": \"2275186837\",\\n            \"name\": \"Brona Robenek\"\\n          },\\n          {\\n            \"authorId\": \"2290487825\",\\n            \"name\": \"Harry Richardson\"\\n          },\\n          {\\n            \"authorId\": \"2275186342\",\\n            \"name\": \"Mah-moud Alnahlawi\"\\n          },\\n          {\\n            \"authorId\": \"2275187959\",\\n            \"name\": \"Christo-pher Yew\"\\n          },\\n          {\\n            \"authorId\": \"2275181434\",\\n            \"name\": \"Priya Ponnapalli\"\\n          },\\n          {\\n            \"authorId\": \"1749128\",\\n            \"name\": \"M. Tagliasacchi\"\\n          },\\n          {\\n            \"authorId\": \"2275188906\",\\n            \"name\": \"Alex Korchemniy\"\\n          },\\n          {\\n            \"authorId\": \"2275999038\",\\n            \"name\": \"Yelin Kim\"\\n          },\\n          {\\n            \"authorId\": \"2275195333\",\\n            \"name\": \"Dinghua Li\"\\n          },\\n          {\\n            \"authorId\": \"2080520726\",\\n            \"name\": \"B. Rosgen\"\\n          },\\n          {\\n            \"authorId\": \"2275186260\",\\n            \"name\": \"Kyle Levin\"\\n          },\\n          {\\n            \"authorId\": \"2275187022\",\\n            \"name\": \"Jeremy Wiesner\"\\n          },\\n          {\\n            \"authorId\": \"2275187506\",\\n            \"name\": \"Praseem Banzal\"\\n          },\\n          {\\n            \"authorId\": \"2275182960\",\\n            \"name\": \"Praveen Srinivasan\"\\n          },\\n          {\\n            \"authorId\": \"2254035020\",\\n            \"name\": \"Hongkun Yu\"\\n          },\\n          {\\n            \"authorId\": \"2275186671\",\\n            \"name\": \"cCauglar Unlu\"\\n          },\\n          {\\n            \"authorId\": \"2275179889\",\\n            \"name\": \"David Reid\"\\n          },\\n          {\\n            \"authorId\": \"9941702\",\\n            \"name\": \"Zora Tung\"\\n          },\\n          {\\n            \"authorId\": \"2591720\",\\n            \"name\": \"D. Finchelstein\"\\n          },\\n          {\\n            \"authorId\": \"2290629265\",\\n            \"name\": \"Ravin Kumar\"\\n          },\\n          {\\n            \"authorId\": \"2288791213\",\\n            \"name\": \"A. Elisseeff\"\\n          },\\n          {\\n            \"authorId\": \"2290557316\",\\n            \"name\": \"Jin Huang\"\\n          },\\n          {\\n            \"authorId\": \"2290594698\",\\n            \"name\": \"Ming Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2070271342\",\\n            \"name\": \"Rui Zhu\"\\n          },\\n          {\\n            \"authorId\": \"2275185644\",\\n            \"name\": \"Ricardo Aguilar\"\\n          },\\n          {\\n            \"authorId\": \"2275181171\",\\n            \"name\": \"Mai Gim\\'enez\"\\n          },\\n          {\\n            \"authorId\": \"2275552322\",\\n            \"name\": \"Jiawei Xia\"\\n          },\\n          {\\n            \"authorId\": \"2770149\",\\n            \"name\": \"Olivier Dousse\"\\n          },\\n          {\\n            \"authorId\": \"145556052\",\\n            \"name\": \"W. Gierke\"\\n          },\\n          {\\n            \"authorId\": \"1735318\",\\n            \"name\": \"S. Yeganeh\"\\n          },\\n          {\\n            \"authorId\": \"2290486731\",\\n            \"name\": \"Damion Yates\"\\n          },\\n          {\\n            \"authorId\": \"153776147\",\\n            \"name\": \"Komal Jalan\"\\n          },\\n          {\\n            \"authorId\": \"2275716550\",\\n            \"name\": \"Lu Li\"\\n          },\\n          {\\n            \"authorId\": \"2275189350\",\\n            \"name\": \"Eri Latorre-Chimoto\"\\n          },\\n          {\\n            \"authorId\": \"2112293680\",\\n            \"name\": \"D. D. Nguyen\"\\n          },\\n          {\\n            \"authorId\": \"2275187515\",\\n            \"name\": \"Ken Durden\"\\n          },\\n          {\\n            \"authorId\": \"2561675\",\\n            \"name\": \"Praveen Kallakuri\"\\n          },\\n          {\\n            \"authorId\": \"2290524803\",\\n            \"name\": \"Yaxin Liu\"\\n          },\\n          {\\n            \"authorId\": \"2275221227\",\\n            \"name\": \"Matthew Johnson\"\\n          },\\n          {\\n            \"authorId\": \"2275175372\",\\n            \"name\": \"Tomy Tsai\"\\n          },\\n          {\\n            \"authorId\": \"2275188417\",\\n            \"name\": \"Alice Talbert\"\\n          },\\n          {\\n            \"authorId\": \"2275539011\",\\n            \"name\": \"Jasmine Liu\"\\n          },\\n          {\\n            \"authorId\": \"40390373\",\\n            \"name\": \"Alexander Neitz\"\\n          },\\n          {\\n            \"authorId\": \"2217508229\",\\n            \"name\": \"C. Elkind\"\\n          },\\n          {\\n            \"authorId\": \"2269473701\",\\n            \"name\": \"Marco Selvi\"\\n          },\\n          {\\n            \"authorId\": \"2275188903\",\\n            \"name\": \"Mimi Jasarevic\"\\n          },\\n          {\\n            \"authorId\": \"2258550407\",\\n            \"name\": \"Livio Baldini Soares\"\\n          },\\n          {\\n            \"authorId\": \"7353832\",\\n            \"name\": \"Livio Baldini Soares\"\\n          },\\n          {\\n            \"authorId\": \"2164862499\",\\n            \"name\": \"Pidong Wang\"\\n          },\\n          {\\n            \"authorId\": \"2290580195\",\\n            \"name\": \"A. Wang\"\\n          },\\n          {\\n            \"authorId\": \"2181807096\",\\n            \"name\": \"Xinyu Ye\"\\n          },\\n          {\\n            \"authorId\": \"2214770531\",\\n            \"name\": \"Krystal Kallarackal\"\\n          },\\n          {\\n            \"authorId\": \"2275188993\",\\n            \"name\": \"Lucia Loher\"\\n          },\\n          {\\n            \"authorId\": \"2290486901\",\\n            \"name\": \"Hoi Lam\"\\n          },\\n          {\\n            \"authorId\": \"2290485721\",\\n            \"name\": \"Josef Broder\"\\n          },\\n          {\\n            \"authorId\": \"1404655176\",\\n            \"name\": \"D. Holtmann-Rice\"\\n          },\\n          {\\n            \"authorId\": \"2275150753\",\\n            \"name\": \"Nina Martin\"\\n          },\\n          {\\n            \"authorId\": \"2257926827\",\\n            \"name\": \"Bramandia Ramadhana\"\\n          },\\n          {\\n            \"authorId\": \"1393948967\",\\n            \"name\": \"Daniel Toyama\"\\n          },\\n          {\\n            \"authorId\": \"2290488378\",\\n            \"name\": \"Mrinal Shukla\"\\n          },\\n          {\\n            \"authorId\": \"2266467648\",\\n            \"name\": \"Sujoy Basu\"\\n          },\\n          {\\n            \"authorId\": \"2290784246\",\\n            \"name\": \"Abhi Mohan\"\\n          }\\n        ],\\n        \"abstract\": \"In this report, we introduce the Gemini 1.5 family of models, representing the next generation of highly compute-efficient multimodal models capable of recalling and reasoning over fine-grained information from millions of tokens of context, including multiple long documents and hours of video and audio. The family includes two new models: (1) an updated Gemini 1.5 Pro, which exceeds the February version on the great majority of capabilities and benchmarks; (2) Gemini 1.5 Flash, a more lightweight variant designed for efficiency with minimal regression in quality. Gemini 1.5 models achieve near-perfect recall on long-context retrieval tasks across modalities, improve the state-of-the-art in long-document QA, long-video QA and long-context ASR, and match or surpass Gemini 1.0 Ultra\\'s state-of-the-art performance across a broad set of benchmarks. Studying the limits of Gemini 1.5\\'s long-context ability, we find continued improvement in next-token prediction and near-perfect retrieval (>99%) up to at least 10M tokens, a generational leap over existing models such as Claude 3.0 (200k) and GPT-4 Turbo (128k). Finally, we highlight real-world use cases, such as Gemini 1.5 collaborating with professionals on completing their tasks achieving 26 to 75% time savings across 10 different job categories, as well as surprising new capabilities of large language models at the frontier; when given a grammar manual for Kalamang, a language with fewer than 200 speakers worldwide, the model learns to translate English to Kalamang at a similar level to a person who learned from the same content.\"\\n      },\\n      {\\n        \"paperId\": \"ac4c1c56a196523f42643f0722fe2fde776fd1fa\",\\n        \"url\": \"https://www.semanticscholar.org/paper/ac4c1c56a196523f42643f0722fe2fde776fd1fa\",\\n        \"title\": \"Context Dependant Iterative Parameter Optimisation for Robust Robot Navigation\",\\n        \"citationCount\": 17,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {\\'tldr\\'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ICRA40945.2020.9196550?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ICRA40945.2020.9196550, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": null,\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2020-05-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"144621045\",\\n            \"name\": \"Adam Binch\"\\n          },\\n          {\\n            \"authorId\": \"145785386\",\\n            \"name\": \"Gautham P. Das\"\\n          },\\n          {\\n            \"authorId\": \"3291831\",\\n            \"name\": \"J. P. Fentanes\"\\n          },\\n          {\\n            \"authorId\": \"1728609\",\\n            \"name\": \"Marc Hanheide\"\\n          }\\n        ],\\n        \"abstract\": \"Progress in autonomous mobile robotics has seen significant advances in the development of many algorithms for motion control and path planning. However, robust performance from these algorithms can often only be expected if the parameters controlling them are tuned specifically for the respective robot model, and optimised for specific scenarios in the environment the robot is working in. Such parameter tuning can, depending on the underlying algorithm, amount to a substantial combinatorial challenge, often rendering extensive manual tuning of these parameters intractable. In this paper, we present a framework that permits the use of different navigation actions and/or parameters depending on the spatial context of the navigation task. We consider the respective navigation algorithms themselves mostly as a \\\\\"black box\\\\\", and find suitable parameters by means of an iterative optimisation, improving for performance metrics in simulated environments. We present a genetic algorithm incorporated into the framework, and empirically show that the resulting parameter sets lead to substantial performance improvements in both simulated and real-world environments in the domain of agricultural robots.\"\\n      },\\n      {\\n        \"paperId\": \"1d82f61ee52331a94141a50b59b186ccf105f6a9\",\\n        \"url\": \"https://www.semanticscholar.org/paper/1d82f61ee52331a94141a50b59b186ccf105f6a9\",\\n        \"title\": \"Building Math Agents with Multi-Turn Iterative Preference Learning\",\\n        \"citationCount\": 58,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2409.02392, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work introduces a multi-turn direct preference learning framework, tailored for this context, that leverages feedback from code interpreters and optimizes trajectory-level preferences and includes multi-turn DPO and multi-turn KTO as specific implementations.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-09-04\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2275119437\",\\n            \"name\": \"Wei Xiong\"\\n          },\\n          {\\n            \"authorId\": \"1382573171\",\\n            \"name\": \"Chengshuai Shi\"\\n          },\\n          {\\n            \"authorId\": \"2266463492\",\\n            \"name\": \"Jiaming Shen\"\\n          },\\n          {\\n            \"authorId\": \"2302798001\",\\n            \"name\": \"Aviv Rosenberg\"\\n          },\\n          {\\n            \"authorId\": \"2266819166\",\\n            \"name\": \"Zhen Qin\"\\n          },\\n          {\\n            \"authorId\": \"2439765\",\\n            \"name\": \"Daniele Calandriello\"\\n          },\\n          {\\n            \"authorId\": \"2140488873\",\\n            \"name\": \"Misha Khalman\"\\n          },\\n          {\\n            \"authorId\": \"2258551072\",\\n            \"name\": \"Rishabh Joshi\"\\n          },\\n          {\\n            \"authorId\": \"1808897\",\\n            \"name\": \"Bilal Piot\"\\n          },\\n          {\\n            \"authorId\": \"2316576401\",\\n            \"name\": \"Mohammad Saleh\"\\n          },\\n          {\\n            \"authorId\": \"2319960151\",\\n            \"name\": \"Chi Jin\"\\n          },\\n          {\\n            \"authorId\": \"2301173100\",\\n            \"name\": \"Tong Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2239381730\",\\n            \"name\": \"Tianqi Liu\"\\n          }\\n        ],\\n        \"abstract\": \"Recent studies have shown that large language models\\' (LLMs) mathematical problem-solving capabilities can be enhanced by integrating external tools, such as code interpreters, and employing multi-turn Chain-of-Thought (CoT) reasoning. While current methods focus on synthetic data generation and Supervised Fine-Tuning (SFT), this paper studies the complementary direct preference learning approach to further improve model performance. However, existing direct preference learning algorithms are originally designed for the single-turn chat task, and do not fully address the complexities of multi-turn reasoning and external tool integration required for tool-integrated mathematical reasoning tasks. To fill in this gap, we introduce a multi-turn direct preference learning framework, tailored for this context, that leverages feedback from code interpreters and optimizes trajectory-level preferences. This framework includes multi-turn DPO and multi-turn KTO as specific implementations. The effectiveness of our framework is validated through training of various language models using an augmented prompt set from the GSM8K and MATH datasets. Our results demonstrate substantial improvements: a supervised fine-tuned Gemma-1.1-it-7B model\\'s performance increased from 77.5% to 83.9% on GSM8K and from 46.1% to 51.2% on MATH. Similarly, a Gemma-2-it-9B model improved from 84.1% to 86.3% on GSM8K and from 51.0% to 54.5% on MATH.\"\\n      },\\n      {\\n        \"paperId\": \"a8176838a636651324b9bac1b3443c803b44e1b3\",\\n        \"url\": \"https://www.semanticscholar.org/paper/a8176838a636651324b9bac1b3443c803b44e1b3\",\\n        \"title\": \"Online Iterative Reinforcement Learning from Human Feedback with General Preference Model\",\\n        \"citationCount\": 29,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2402.07314, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work considers a standard mathematical formulation, the reverse-KL regularized minimax game between two LLMs for RLHF under general preference oracle, and proposes sample-efficient algorithms for both the offline learning from a pre-collected preference dataset and online learning where the preference oracle is queried along the way of training.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-02-11\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2105550407\",\\n            \"name\": \"Chen Ye\"\\n          },\\n          {\\n            \"authorId\": \"2275119437\",\\n            \"name\": \"Wei Xiong\"\\n          },\\n          {\\n            \"authorId\": \"2283877837\",\\n            \"name\": \"Yuheng Zhang\"\\n          },\\n          {\\n            \"authorId\": \"35279146\",\\n            \"name\": \"Hanze Dong\"\\n          },\\n          {\\n            \"authorId\": \"2275167899\",\\n            \"name\": \"Nan Jiang\"\\n          },\\n          {\\n            \"authorId\": \"2260473361\",\\n            \"name\": \"Tong Zhang\"\\n          }\\n        ],\\n        \"abstract\": \"We investigate Reinforcement Learning from Human Feedback (RLHF) in the context of a general preference oracle. In particular, we do not assume the existence of a reward function and an oracle preference signal drawn from the Bradley-Terry model as most of the prior works do. We consider a standard mathematical formulation, the reverse-KL regularized minimax game between two LLMs for RLHF under general preference oracle. The learning objective of this formulation is to find a policy so that it is consistently preferred by the KL-regularized preference oracle over any competing LLMs. We show that this framework is strictly more general than the reward-based one, and propose sample-efficient algorithms for both the offline learning from a pre-collected preference dataset and online learning where we can query the preference oracle along the way of training. Empirical studies verify the effectiveness of the proposed framework.\"\\n      },\\n      {\\n        \"paperId\": \"0145bde24bb4e8482c2e69dd28efe5ead1fe7d8c\",\\n        \"url\": \"https://www.semanticscholar.org/paper/0145bde24bb4e8482c2e69dd28efe5ead1fe7d8c\",\\n        \"title\": \"Generalized Iterative Bayesian Update and Applications to Mechanisms for Privacy Protection\",\\n        \"citationCount\": 13,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {\\'tldr\\'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/EuroSP48549.2020.00038?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/EuroSP48549.2020.00038, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": null,\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2020-09-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2292169\",\\n            \"name\": \"Ehab ElSalamouny\"\\n          },\\n          {\\n            \"authorId\": \"1722055\",\\n            \"name\": \"C. Palamidessi\"\\n          }\\n        ],\\n        \"abstract\": \"The iterative Bayesian update (IBU) and the matrix inversion (INV) are the main methods to retrieve the original distribution from noisy data resulting from the application of privacy protection mechanisms. We show that the foundations of IBU established in the literature are flawed, as they rely on an assumption that in general is not satisfied in typical datasets. We then propose an extension of the method, covering a more general privacy model, where different users are allowed to apply different privacy mechanisms. We call our algorithm GIBU, for Generalized IBU, and we prove its convergence to the maximum likelihood estimate, constructing a proof that does not rely on the problematic assumption, thus fixing also the theory of IBU. Finally we evaluate the precision of GIBU on data sanitized with k-RR, Rappor, geo-indistinguishability and exponential mechanisms. We show that, while GIBU and INV are comparable in the first two cases, the performance of GIBU is definitely superior in the latter cases.\"\\n      },\\n      {\\n        \"paperId\": \"7d49f330d114ec3d65ffb67eb9dc79aea7b8befc\",\\n        \"url\": \"https://www.semanticscholar.org/paper/7d49f330d114ec3d65ffb67eb9dc79aea7b8befc\",\\n        \"title\": \"Spontaneous Reward Hacking in Iterative Self-Refinement\",\\n        \"citationCount\": 15,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2407.04549, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"Using an essay editing task, it is shown that iterative self-refinement leads to deviation between the language model evaluator and human judgment, demonstrating that reward hacking can occur spontaneously in-context with the use of iterative self-refinement.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-07-05\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2311112294\",\\n            \"name\": \"Jane Pan\"\\n          },\\n          {\\n            \"authorId\": \"2321875898\",\\n            \"name\": \"He He\"\\n          },\\n          {\\n            \"authorId\": \"2297768298\",\\n            \"name\": \"Samuel R. Bowman\"\\n          },\\n          {\\n            \"authorId\": \"2297816489\",\\n            \"name\": \"Shi Feng\"\\n          }\\n        ],\\n        \"abstract\": \"Language models are capable of iteratively improving their outputs based on natural language feedback, thus enabling in-context optimization of user preference. In place of human users, a second language model can be used as an evaluator, providing feedback along with numerical ratings which the generator attempts to optimize. However, because the evaluator is an imperfect proxy of user preference, this optimization can lead to reward hacking, where the evaluator\\'s ratings improve while the generation quality remains stagnant or even decreases as judged by actual user preference. The concern of reward hacking is heightened in iterative self-refinement where the generator and the evaluator use the same underlying language model, in which case the optimization pressure can drive them to exploit shared vulnerabilities. Using an essay editing task, we show that iterative self-refinement leads to deviation between the language model evaluator and human judgment, demonstrating that reward hacking can occur spontaneously in-context with the use of iterative self-refinement. In addition, we study conditions under which reward hacking occurs and observe two factors that affect reward hacking severity: model size and context sharing between the generator and the evaluator.\"\\n      },\\n      {\\n        \"paperId\": \"21ed545a3b02bbcba6a62f5858bca948c2ed4641\",\\n        \"url\": \"https://www.semanticscholar.org/paper/21ed545a3b02bbcba6a62f5858bca948c2ed4641\",\\n        \"title\": \"Explainable Benchmarking for Iterative Optimization Heuristics\",\\n        \"citationCount\": 19,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://arxiv.org/pdf/2401.17842\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2401.17842, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"The IOHxplainer software library is introduced, for systematic analysing the performance of various optimization algorithms and the impact of their different components and hyperparameters, aiming to improve future benchmarking and algorithm design practices.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-01-31\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2218156728\",\\n            \"name\": \"N. V. Stein\"\\n          },\\n          {\\n            \"authorId\": \"108119673\",\\n            \"name\": \"Diederick Vermetten\"\\n          },\\n          {\\n            \"authorId\": \"3160375\",\\n            \"name\": \"Anna V. Kononova\"\\n          },\\n          {\\n            \"authorId\": \"2237990304\",\\n            \"name\": \"T. Back\"\\n          }\\n        ],\\n        \"abstract\": \"Benchmarking heuristic algorithms is vital to understand under which conditions and on what kind of problems certain algorithms perform well. In most current research into heuristic optimization algorithms, only a very limited number of scenarios, algorithm configurations and hyper-parameter settings are explored, leading to incomplete and often biased insights and results. This article presents a novel approach that we call explainable benchmarking. We introduce the IOHxplainer software library, for systematic analysing the performance of various optimization algorithms and the impact of their different components and hyperparameters. We showcase the methodology in the context of two modular optimization implementations. Through this library, we examine the impact of different algorithmic components and configurations, offering insights into their performance across diverse scenarios. We provide a systematic method for evaluating and interpreting the behaviour and efficiency of iterative optimization heuristics in a more transparent and comprehensible manner, aiming to improve future benchmarking and algorithm design practices.\"\\n      },\\n      {\\n        \"paperId\": \"0019cb24ec04498836b8215e9495b968f2c01666\",\\n        \"url\": \"https://www.semanticscholar.org/paper/0019cb24ec04498836b8215e9495b968f2c01666\",\\n        \"title\": \"IGEV++: Iterative Multi-Range Geometry Encoding Volumes for Stereo Matching\",\\n        \"citationCount\": 41,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2409.00638, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper introduces an adaptive patch matching module that efficiently and effectively computes matching costs for large disparity ranges and/or ill-posed regions, and proposes a selective geometry feature fusion module to adaptively fuse multi-range and multi-granularity geometry features in MGEV.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-09-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2158317969\",\\n            \"name\": \"Gangwei Xu\"\\n          },\\n          {\\n            \"authorId\": \"2289775621\",\\n            \"name\": \"Xianqi Wang\"\\n          },\\n          {\\n            \"authorId\": \"2319408912\",\\n            \"name\": \"Zhaoxing Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2049033628\",\\n            \"name\": \"Junda Cheng\"\\n          },\\n          {\\n            \"authorId\": \"2319622516\",\\n            \"name\": \"Chunyuan Liao\"\\n          },\\n          {\\n            \"authorId\": \"2265575132\",\\n            \"name\": \"Xin Yang\"\\n          }\\n        ],\\n        \"abstract\": \"Stereo matching is a core component in many computer vision and robotics systems. Despite significant advances over the last decade, handling matching ambiguities in ill-posed regions and large disparities remains an open challenge. In this paper, we propose a new deep network architecture, called IGEV++, for stereo matching. The proposed IGEV++ constructs Multi-range Geometry Encoding Volumes (MGEV), which encode coarse-grained geometry information for ill-posed regions and large disparities, while preserving fine-grained geometry information for details and small disparities. To construct MGEV, we introduce an adaptive patch matching module that efficiently and effectively computes matching costs for large disparity ranges and/or ill-posed regions. We further propose a selective geometry feature fusion module to adaptively fuse multi-range and multi-granularity geometry features in MGEV. Then, we input the fused geometry features into ConvGRUs to iteratively update the disparity map. MGEV allows to efficiently handle large disparities and ill-posed regions, such as occlusions and textureless regions, and enjoys rapid convergence during iterations. Our IGEV++ achieves the best performance on the Scene Flow test set across all disparity ranges, up to 768px. Our IGEV++ also achieves state-of-the-art accuracy on the Middlebury, ETH3D, KITTI 2012, and 2015 benchmarks. Specifically, IGEV++ achieves a 3.23% 2-pixel outlier rate (Bad 2.0) on the large disparity benchmark, Middlebury, representing error reductions of 31.9% and 54.8% compared to RAFT-Stereo and GMStereo, respectively. We also present a real-time version of IGEV++ that achieves the best performance among all published real-time methods on the KITTI benchmarks.\"\\n      },\\n      {\\n        \"paperId\": \"f305fddaf0aa92cd19b8714db6ef67ade6ece1ee\",\\n        \"url\": \"https://www.semanticscholar.org/paper/f305fddaf0aa92cd19b8714db6ef67ade6ece1ee\",\\n        \"title\": \"De-Hallucinator: Iterative Grounding for LLM-Based Code Completion\",\\n        \"citationCount\": 29,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {\\'abstract\\'}. Paper or abstract available at https://api.unpaywall.org/v2/10.48550/arXiv.2401.01701?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.48550/arXiv.2401.01701, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"De-Hallucinator is presented, an LLM-based code completion technique that grounds the predictions of a model through a novel combination of retrieving suitable API references and iteratively querying the model with increasingly suitable context information in the prompt.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": null,\\n        \"authors\": [\\n          {\\n            \"authorId\": \"46235560\",\\n            \"name\": \"A. Eghbali\"\\n          },\\n          {\\n            \"authorId\": \"2282536979\",\\n            \"name\": \"Michael Pradel\"\\n          }\\n        ],\\n        \"abstract\": null\\n      }\\n    ]\\n  },\\n  \"thematic coherence prompting\": {\\n    \"total\": 692,\\n    \"offset\": 0,\\n    \"next\": 50,\\n    \"data\": [\\n      {\\n        \"paperId\": \"f50be995ae7bc8b5889df68be77bf89d1d253a50\",\\n        \"url\": \"https://www.semanticscholar.org/paper/f50be995ae7bc8b5889df68be77bf89d1d253a50\",\\n        \"title\": \"Supporting best practice in reflexive thematic analysis reporting in Palliative Medicine: A review of published research and introduction to the Reflexive Thematic Analysis Reporting Guidelines (RTARG)\",\\n        \"citationCount\": 350,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://journals.sagepub.com/doi/pdf/10.1177/02692163241234800\",\\n          \"status\": \"HYBRID\",\\n          \"license\": \"CCBY\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC11157981, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"The Reflexive Thematic Analysis Reporting Guidelines (the RTARG) are developed, informed by this review, other reviews the authors have done and their values and experience as qualitative researchers, to support qualitative researchers in producing coherent and reflexively open reports of reflexive thematic analysis.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Review\"\\n        ],\\n        \"publicationDate\": \"2024-03-12\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2290884456\",\\n            \"name\": \"Virginia Braun\"\\n          },\\n          {\\n            \"authorId\": \"2287216787\",\\n            \"name\": \"Victoria Clarke\"\\n          }\\n        ],\\n        \"abstract\": \"Background: Reflexive thematic analysis is widely used in qualitative research published in Palliative Medicine, and in the broader field of health research. However, this approach is often not used well. Common problems in published reflexive thematic analysis in general include assuming thematic analysis is a singular approach, rather than a family of methods, confusing themes and topics, and treating and reporting reflexive thematic analysis as if it is atheoretical. Purpose: We reviewed 20 papers published in Palliative Medicine between 2014 and 2022 that cited Braun and Clarke, identified using the search term \\\\u2018thematic analysis\\\\u2019 and the default \\\\u2018relevance\\\\u2019 setting on the journal webpage. The aim of the review was to identify common problems and instances of good practice. Problems centred around a lack of methodological coherence, and a lack of reflexive openness, clarity and detail in reporting. We considered contributors to these common problems, including the use of reporting checklists that are not coherent with the values of reflexive thematic analysis. To support qualitative researchers in producing coherent and reflexively open reports of reflexive thematic analysis we have developed the Reflexive Thematic Analysis Reporting Guidelines (the RTARG; in Supplemental Materials) informed by this review, other reviews we have done and our values and experience as qualitative researchers. The RTARG is also intended for use by peer reviewers to encourage methodologically coherent reviewing. Key learning points: Methodological incoherence and a lack of transparency are common problems in reflexive thematic analysis research published in Palliative Medicine. Coherence can be facilitated by researchers and reviewers striving to be knowing \\\\u2013 thoughtful, deliberative, reflexive and theoretically aware \\\\u2013 practitioners and appraisers of reflexive thematic analysis and developing an understanding of the diversity within the thematic analysis family of methods.\"\\n      },\\n      {\\n        \"paperId\": \"8b12b59aef23566a7a3461f69230c83ab632a918\",\\n        \"url\": \"https://www.semanticscholar.org/paper/8b12b59aef23566a7a3461f69230c83ab632a918\",\\n        \"title\": \"Weak central coherence in adults with ASD: Evidence from eye-tracking and thematic content analysis of social scenes\",\\n        \"citationCount\": 11,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://figshare.com/articles/journal_contribution/Weak_central_coherence_in_adults_with_ASD_Evidence_from_eye-tracking_and_thematic_content_analysis_of_social_scenes/19633669/1/files/34870696.pdf\",\\n          \"status\": \"GREEN\",\\n          \"license\": \"CCBY\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1080/23279095.2022.2060105?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1080/23279095.2022.2060105, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": null,\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2021-08-24\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"1677019029\",\\n            \"name\": \"S. Tassini\"\\n          },\\n          {\\n            \"authorId\": \"39763547\",\\n            \"name\": \"M. C. Melo\"\\n          },\\n          {\\n            \"authorId\": \"2922008\",\\n            \"name\": \"O. Bueno\"\\n          },\\n          {\\n            \"authorId\": \"2222542539\",\\n            \"name\": \"Claudia Berlim de Mello\"\\n          }\\n        ],\\n        \"abstract\": \"Abstract Central Coherence Weakness has been defined as a tendency for local rather than global processing that may underlie core deficits in Autism Spectrum Disorder (ASD). In social contexts it may be expressed in difficulties to integrate social cues arising from the recognition of emotions in faces or from the environment in order to understand people\\'s interactions. A sample of 28 adults diagnosed with ASD Level 1 and 25 controls was submitted to a cartoon-like task with the instruction to describe social scenes and to Navon letter stimuli. Both quantitative measures and qualitative (thematic content analysis) procedures were used to assess performance. Heatmap and fixation preferences according to the stimuli quadrants were used to investigate eye-tracking patterns. A tendency to local processing, independently of the stimuli type, in the ASD participants was observed. Data from visual tracking by quadrants and from verbal reports suggest loss of social cues important for understanding context. Their reaction time and response duration were increased in relation to controls. The findings corroborate the idea that weak central coherence may be part of the cognitive phenotype in ASD. LAY ABSTRACT Autistic adults often report difficulties in interpreting social situations. These difficulties are commonly associated with a tendency to visually focus on specific parts of the situation (known as local processing) to the detriment of the whole situation. This way of looking at things has been given the name \\\\u201cweak central coherence,\\\\u201d and may result in difficulties in understanding a situation or other people\\\\u2019s behaviors. A group of ASD and controls were asked to describe two different types of image, one showing a common social situation, the other Navon figure. Eye-tracking technology was used to analyze how the participants looked at the images (which part of the image and for how long) and asked about what they had seen. The results showed that in the group of autistic participants there was a tendency to focus on the details in both types of images. The analysis of the verbal reports revealed that the interpretation of the social contexts by those with ASD was not what was expected, associated with a specific focus on details. These findings may be useful for a better understanding of some difficulties experienced by ASD in social contexts and contribute to therapeutic treatments.\"\\n      },\\n      {\\n        \"paperId\": \"fff1e9eb32f8591aea3f2db54035465093381b89\",\\n        \"url\": \"https://www.semanticscholar.org/paper/fff1e9eb32f8591aea3f2db54035465093381b89\",\\n        \"title\": \"Selective Prompting Tuning for Personalized Conversations with LLMs\",\\n        \"citationCount\": 13,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2406.18187, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work initializes a set of soft prompts and uses a trainable dense retriever to adaptively select suitable soft prompts for LLMs according to different input contexts, where the prompt retriever is dynamically updated through feedback from the LLMs.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-06-26\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2274015208\",\\n            \"name\": \"Qiushi Huang\"\\n          },\\n          {\\n            \"authorId\": \"2110814131\",\\n            \"name\": \"Xubo Liu\"\\n          },\\n          {\\n            \"authorId\": \"2273320275\",\\n            \"name\": \"Tom Ko\"\\n          },\\n          {\\n            \"authorId\": \"49814531\",\\n            \"name\": \"Boyong Wu\"\\n          },\\n          {\\n            \"authorId\": \"2239051433\",\\n            \"name\": \"Wenwu Wang\"\\n          },\\n          {\\n            \"authorId\": \"2273525536\",\\n            \"name\": \"Yu Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2189113746\",\\n            \"name\": \"Lilian Tang\"\\n          }\\n        ],\\n        \"abstract\": \"In conversational AI, personalizing dialogues with persona profiles and contextual understanding is essential. Despite large language models\\' (LLMs) improved response coherence, effective persona integration remains a challenge. In this work, we first study two common approaches for personalizing LLMs: textual prompting and direct fine-tuning. We observed that textual prompting often struggles to yield responses that are similar to the ground truths in datasets, while direct fine-tuning tends to produce repetitive or overly generic replies. To alleviate those issues, we propose \\\\\\\\textbf{S}elective \\\\\\\\textbf{P}rompt \\\\\\\\textbf{T}uning (SPT), which softly prompts LLMs for personalized conversations in a selective way. Concretely, SPT initializes a set of soft prompts and uses a trainable dense retriever to adaptively select suitable soft prompts for LLMs according to different input contexts, where the prompt retriever is dynamically updated through feedback from the LLMs. Additionally, we propose context-prompt contrastive learning and prompt fusion learning to encourage the SPT to enhance the diversity of personalized conversations. Experiments on the CONVAI2 dataset demonstrate that SPT significantly enhances response diversity by up to 90\\\\\\\\%, along with improvements in other critical performance indicators. Those results highlight the efficacy of SPT in fostering engaging and personalized dialogue generation. The SPT model code (https://github.com/hqsiswiliam/SPT) is publicly available for further exploration.\"\\n      },\\n      {\\n        \"paperId\": \"71e294c3b503f141b11cad8e3ebc033a667d2b3b\",\\n        \"url\": \"https://www.semanticscholar.org/paper/71e294c3b503f141b11cad8e3ebc033a667d2b3b\",\\n        \"title\": \"Exploring Learner Prompting Behavior and Its Effect on ChatGPT-Assisted English Writing Revision\",\\n        \"citationCount\": 12,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {\\'abstract\\'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1007/s40299-024-00930-6?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1007/s40299-024-00930-6, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"Findings underscore the necessity of incorporating prompt-writing instruction into English writing pedagogy to enhance learners\\\\u2019 ability to craft specific, goal-aligned prompts, leading to more productive feedback from AI tools like ChatGPT and facilitating meaningful improvements in higher-order writing skills.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-10-21\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"116938583\",\\n            \"name\": \"M. Hwang\"\\n          },\\n          {\\n            \"authorId\": \"121076013\",\\n            \"name\": \"Robert D. Jeens\"\\n          },\\n          {\\n            \"authorId\": \"2314224171\",\\n            \"name\": \"Hee-Kyung Lee\"\\n          }\\n        ],\\n        \"abstract\": null\\n      },\\n      {\\n        \"paperId\": \"b908824639d18f11883abcab21efeb22e315ab9c\",\\n        \"url\": \"https://www.semanticscholar.org/paper/b908824639d18f11883abcab21efeb22e315ab9c\",\\n        \"title\": \"Multimodal Procedural Planning via Dual Text-Image Prompting\",\\n        \"citationCount\": 51,\\n        \"openAccessPdf\": {\\n          \"url\": \"http://arxiv.org/pdf/2305.01795\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2305.01795, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"Text-Image Prompting (TIP) is proposed, a dual-modality prompting method that jointly leverages zero-shot reasoning ability in large language models (LLMs) and compelling text-to-image generation ability from diffusion-based models to tackle the key challenges of MPP.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2023-05-02\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"47006228\",\\n            \"name\": \"Yujie Lu\"\\n          },\\n          {\\n            \"authorId\": \"2887562\",\\n            \"name\": \"Pan Lu\"\\n          },\\n          {\\n            \"authorId\": \"2142370346\",\\n            \"name\": \"Zhiyu Chen\"\\n          },\\n          {\\n            \"authorId\": \"51439692\",\\n            \"name\": \"Wanrong Zhu\"\\n          },\\n          {\\n            \"authorId\": \"47120131\",\\n            \"name\": \"X. Wang\"\\n          },\\n          {\\n            \"authorId\": \"1682479\",\\n            \"name\": \"William Yang Wang\"\\n          }\\n        ],\\n        \"abstract\": \"Embodied agents have achieved prominent performance in following human instructions to complete tasks. However, the potential of providing instructions informed by texts and images to assist humans in completing tasks remains underexplored. To uncover this capability, we present the multimodal procedural planning (MPP) task, in which models are given a high-level goal and generate plans of paired text-image steps, providing more complementary and informative guidance than unimodal plans. The key challenges of MPP are to ensure the informativeness, temporal coherence,and accuracy of plans across modalities. To tackle this, we propose Text-Image Prompting (TIP), a dual-modality prompting method that jointly leverages zero-shot reasoning ability in large language models (LLMs) and compelling text-to-image generation ability from diffusion-based models. TIP improves the interaction in the dual modalities using Text-to-Image Bridge and Image-to-Text Bridge, allowing LLMs to guide the textual-grounded image plan generation and leveraging the descriptions of image plans to ground the textual plan reversely. To address the lack of relevant datasets, we collect WIKIPLAN and RECIPEPLAN as a testbed for MPP. Our results show compelling human preferences and automatic scores against unimodal and multimodal baselines on WIKIPLAN and RECIPEPLAN in terms of informativeness, temporal coherence, and plan accuracy. Our code and data: https://github.com/YujieLu10/MPP.\"\\n      },\\n      {\\n        \"paperId\": \"5e490270c0fd49418e6bc5732c375867f253a20f\",\\n        \"url\": \"https://www.semanticscholar.org/paper/5e490270c0fd49418e6bc5732c375867f253a20f\",\\n        \"title\": \"Fine-Grained Modeling of Narrative Context: A Coherence Perspective via Retrospective Questions\",\\n        \"citationCount\": 10,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://arxiv.org/pdf/2402.13551\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2402.13551, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work introduces an original and practical paradigm for narrative comprehension, stemming from the characteristics that individual passages within narratives tend to be more cohesively related than isolated, by formulating a graph dubbed NarCo, which explicitly depicts task-agnostic coherence dependencies that are ready to be consumed by various downstream tasks.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-02-21\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2276317005\",\\n            \"name\": \"Liyan Xu\"\\n          },\\n          {\\n            \"authorId\": \"153154545\",\\n            \"name\": \"JiangNan Li\"\\n          },\\n          {\\n            \"authorId\": \"2265525656\",\\n            \"name\": \"Mo Yu\"\\n          },\\n          {\\n            \"authorId\": \"2283871195\",\\n            \"name\": \"Jie Zhou\"\\n          }\\n        ],\\n        \"abstract\": \"This work introduces an original and practical paradigm for narrative comprehension, stemming from the characteristics that individual passages within narratives tend to be more cohesively related than isolated. Complementary to the common end-to-end paradigm, we propose a fine-grained modeling of narrative context, by formulating a graph dubbed NarCo, which explicitly depicts task-agnostic coherence dependencies that are ready to be consumed by various downstream tasks. In particular, edges in NarCo encompass free-form retrospective questions between context snippets, inspired by human cognitive perception that constantly reinstates relevant events from prior context. Importantly, our graph formalism is practically instantiated by LLMs without human annotations, through our designed two-stage prompting scheme. To examine the graph properties and its utility, we conduct three studies in narratives, each from a unique angle: edge relation efficacy, local context enrichment, and broader application in QA. All tasks could benefit from the explicit coherence captured by NarCo.\"\\n      },\\n      {\\n        \"paperId\": \"e47bce9eaa7d0cce48c64d614f11105d48fb3881\",\\n        \"url\": \"https://www.semanticscholar.org/paper/e47bce9eaa7d0cce48c64d614f11105d48fb3881\",\\n        \"title\": \"Conceptual and design thinking for thematic analysis\",\\n        \"citationCount\": 2681,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {\\'abstract\\'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1037/QUP0000196?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1037/QUP0000196, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": null,\\n        \"publicationTypes\": null,\\n        \"publicationDate\": \"2021-02-22\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"40465786\",\\n            \"name\": \"Virginia Braun\"\\n          },\\n          {\\n            \"authorId\": \"145881640\",\\n            \"name\": \"Victoria Clarke\"\\n          }\\n        ],\\n        \"abstract\": null\\n      },\\n      {\\n        \"paperId\": \"fb449d114d7d3550fb2fff424be6b5d8434d7924\",\\n        \"url\": \"https://www.semanticscholar.org/paper/fb449d114d7d3550fb2fff424be6b5d8434d7924\",\\n        \"title\": \"Prompting Large Language Models for Topic Modeling\",\\n        \"citationCount\": 45,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2312.09693, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper proposes PromptTopic, a novel topic modeling approach that harnesses the advanced language understanding of large language models (LLMs) to address challenges of short text datasets that lack co-occurring words.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2023-12-15\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2262111802\",\\n            \"name\": \"Han Wang\"\\n          },\\n          {\\n            \"authorId\": \"2218633063\",\\n            \"name\": \"Nirmalendu Prakash\"\\n          },\\n          {\\n            \"authorId\": \"2261746844\",\\n            \"name\": \"N. Hoang\"\\n          },\\n          {\\n            \"authorId\": \"72043108\",\\n            \"name\": \"Ming Shan Hee\"\\n          },\\n          {\\n            \"authorId\": \"2339777541\",\\n            \"name\": \"Usman Naseem\"\\n          },\\n          {\\n            \"authorId\": \"2266006388\",\\n            \"name\": \"Roy Ka-Wei Lee\"\\n          }\\n        ],\\n        \"abstract\": \"Topic modeling is a widely used technique for revealing underlying thematic structures within textual data. However, existing models have certain limitations, particularly when dealing with short text datasets that lack co-occurring words. Moreover, these models often neglect sentence-level semantics, focusing primarily on token-level semantics. In this paper, we propose PromptTopic, a novel topic modeling approach that harnesses the advanced language understanding of large language models (LLMs) to address these challenges. It involves extracting topics at the sentence level from individual documents, then aggregating and condensing these topics into a predefined quantity, ultimately providing coherent topics for texts of varying lengths. This approach eliminates the need for manual parameter tuning and improves the quality of extracted topics. We benchmark PromptTopic against the state-of-the-art baselines on three vastly diverse datasets, establishing its proficiency in discovering meaningful topics. Furthermore, qualitative analysis showcases PromptTopic\\\\u2019s ability to uncover relevant topics in multiple datasets.\"\\n      },\\n      {\\n        \"paperId\": \"3f5bc00fb6d1de56e3a4902df74ef305adfe4dcd\",\\n        \"url\": \"https://www.semanticscholar.org/paper/3f5bc00fb6d1de56e3a4902df74ef305adfe4dcd\",\\n        \"title\": \"Improved prompting and process for writing user personas with LLMs, using qualitative interviews: Capturing behaviour and personality traits of users\",\\n        \"citationCount\": 13,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://arxiv.org/pdf/2310.06391\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2310.06391, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This draft paper presents a workflow for creating User Personas with Large Language Models, using the results of a Thematic Analysis of qualitative interviews, using a recently released LLM which allows the processing of 16 thousand tokens and the possibility to offer a refined prompting for the creation of Personas.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2023-10-10\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"1774708\",\\n            \"name\": \"S. Paoli\"\\n          }\\n        ],\\n        \"abstract\": \"This draft paper presents a workflow for creating User Personas with Large Language Models, using the results of a Thematic Analysis of qualitative interviews. The proposed workflow uses improved prompting and a larger pool of Themes, compared to previous work conducted by the author for the same task. This is possible due to the capabilities of a recently released LLM which allows the processing of 16 thousand tokens (GPT3.5-Turbo-16k) and also due to the possibility to offer a refined prompting for the creation of Personas. The paper offers details of performing Phase 2 and 3 of Thematic Analysis, and then discusses the improved workflow for creating Personas. The paper also offers some reflections on the relationship between the proposed process and existing approaches to Personas such as the data-driven and qualitative Personas. Moreover, the paper offers reflections on the capacity of LLMs to capture user behaviours and personality traits, from the underlying dataset of qualitative interviews used for the analysis.\"\\n      },\\n      {\\n        \"paperId\": \"234e31a541d5d4d1f64497cdd98dfde5131a342a\",\\n        \"url\": \"https://www.semanticscholar.org/paper/234e31a541d5d4d1f64497cdd98dfde5131a342a\",\\n        \"title\": \"BERTopic Modeling of Natural Language Processing Abstracts: Thematic Structure and Trajectory\",\\n        \"citationCount\": 16,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://doi.org/10.30865/mib.v7i3.6426\",\\n          \"status\": \"GOLD\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.30865/mib.v7i3.6426?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.30865/mib.v7i3.6426, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This research aimed to apply unsupervised clustering techniques to 13,027 Scopus abstracts to uncover structure and themes in natural language processing (NLP) publications to uncover structure and themes in research trends.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2023-07-31\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2091271718\",\\n            \"name\": \"S. Samsir\"\\n          },\\n          {\\n            \"authorId\": \"2313547470\",\\n            \"name\": \"Reagan Surbakti Saragih\"\\n          },\\n          {\\n            \"authorId\": \"2313548926\",\\n            \"name\": \"S. Subagio\"\\n          },\\n          {\\n            \"authorId\": \"2200502353\",\\n            \"name\": \"Rahmad Aditiya\"\\n          },\\n          {\\n            \"authorId\": \"2091271983\",\\n            \"name\": \"Ronal Watrianthos\"\\n          }\\n        ],\\n        \"abstract\": \"The rapid growth in the academic literature presents challenges in identifying relevant studies. This research aimed to apply unsupervised clustering techniques to 13,027 Scopus abstracts to uncover structure and themes in natural language processing (NLP) publications. Abstracts were pre-processed with tokenization, lemmatization, and vectorization. The BERTopic algorithm was used for clustering, using the MiniLM-L6-v2 embedding model and a minimum topic size of 50. Quantitative analysis revealed eight main topics, with sizes ranging from 205 to 4089 abstracts per topic. The language models topic was most prominent with 4089 abstracts. The topics were evaluated using coherence scores between 0.42 and 0.58, indicating meaningful themes. Keywords and sample documents provided interpretable topic representations. The results showcase the ability to produce coherent topics and capture connections between NLP studies. Clustering supports focused browsing and identification of relevant literature. Unlike human-curated classifications, the unsupervised data-driven approach prevents bias. Given the need to understand research trends, clustering abstracts enables efficient knowledge discovery from scientific corpora. This methodology can be applied to various datasets and fields to uncover overlooked patterns. The ability to adjust parameters allows for customized analysis. In general, unsupervised clustering provides a versatile framework for navigating, summarizing, and analyzing academic literature as volumes expand exponentially.\"\\n      },\\n      {\\n        \"paperId\": \"baeb8b5869eb94a2e43ea8cd0df3b8077ec8e91c\",\\n        \"url\": \"https://www.semanticscholar.org/paper/baeb8b5869eb94a2e43ea8cd0df3b8077ec8e91c\",\\n        \"title\": \"SAM-OCTA: Prompting Segment-Anything for OCTA Image Segmentation\",\\n        \"citationCount\": 10,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://arxiv.org/pdf/2310.07183\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2310.07183, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"From comprehensive experimental results with the OCTA-500 dataset, the SAM-OCTA method has achieved state-of-the-art performance in common OCTA segmentation tasks related to RV and FAZ, and it also performs accurate segmentation of artery-vein and local vessels.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2023-10-11\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2243298334\",\\n            \"name\": \"Xinrun Chen\"\\n          },\\n          {\\n            \"authorId\": \"2243437545\",\\n            \"name\": \"Chengliang Wang\"\\n          },\\n          {\\n            \"authorId\": \"2216447644\",\\n            \"name\": \"Haojian Ning\"\\n          },\\n          {\\n            \"authorId\": \"2257137905\",\\n            \"name\": \"Shiying Li\"\\n          }\\n        ],\\n        \"abstract\": \"Segmenting specific targets or biomarkers is necessary to analyze optical coherence tomography angiography (OCTA) images. Previous methods typically segment all the targets in an OCTA sample, such as retinal vessels (RVs). Although these methods perform well in accuracy and precision, OCTA analyses often focusing local information within the images which has not been fulfilled. In this paper, we propose a method called SAM-OCTA for local segmentation in OCTA images. The method fine-tunes a pre-trained segment anything model (SAM) using low-rank adaptation (LoRA) and utilizes prompt points for local RVs, arteries, and veins segmentation in OCTA. To explore the effect and mechanism of prompt points, we set up global and local segmentation modes with two prompt point generation strategies, namely random selection and special annotation. Considering practical usage, we conducted extended experiments with different model scales and analyzed the model performance before and after fine-tuning besides the general segmentation task. From comprehensive experimental results with the OCTA-500 dataset, our SAM-OCTA method has achieved state-of-the-art performance in common OCTA segmentation tasks related to RV and FAZ, and it also performs accurate segmentation of artery-vein and local vessels. The code is available at https://github.com/ShellRedia/SAM-OCTA-extend.\"\\n      },\\n      {\\n        \"paperId\": \"78164b272db4afa960e3e0e62e3331dc36a48cd2\",\\n        \"url\": \"https://www.semanticscholar.org/paper/78164b272db4afa960e3e0e62e3331dc36a48cd2\",\\n        \"title\": \"Analyzable Chain-of-Musical-Thought Prompting for High-Fidelity Music Generation\",\\n        \"citationCount\": 12,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2503.19611, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"MusiCoT empowers the AR model to first outline an overall music structure before generating audio tokens, thereby enhancing the coherence and creativity of the resulting compositions, producing music quality that rivals state-of-the-art generation models.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2025-03-25\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2316946541\",\\n            \"name\": \"Max W. Y. Lam\"\\n          },\\n          {\\n            \"authorId\": \"2358193518\",\\n            \"name\": \"Yijin Xing\"\\n          },\\n          {\\n            \"authorId\": \"2352015696\",\\n            \"name\": \"Weiya You\"\\n          },\\n          {\\n            \"authorId\": \"2287919309\",\\n            \"name\": \"Jingcheng Wu\"\\n          },\\n          {\\n            \"authorId\": \"2118932090\",\\n            \"name\": \"Zongyu Yin\"\\n          },\\n          {\\n            \"authorId\": \"2352404752\",\\n            \"name\": \"Fuqiang Jiang\"\\n          },\\n          {\\n            \"authorId\": \"2319220700\",\\n            \"name\": \"Hangyu Liu\"\\n          },\\n          {\\n            \"authorId\": \"2319238631\",\\n            \"name\": \"Feng Liu\"\\n          },\\n          {\\n            \"authorId\": \"2292777550\",\\n            \"name\": \"Xingda Li\"\\n          },\\n          {\\n            \"authorId\": \"2352448900\",\\n            \"name\": \"Wei-Tsung Lu\"\\n          },\\n          {\\n            \"authorId\": \"2352227661\",\\n            \"name\": \"Hanyu Chen\"\\n          },\\n          {\\n            \"authorId\": \"2352042214\",\\n            \"name\": \"Tong Feng\"\\n          },\\n          {\\n            \"authorId\": \"2352908525\",\\n            \"name\": \"Tianwei Zhao\"\\n          },\\n          {\\n            \"authorId\": \"2352289761\",\\n            \"name\": \"Chien-Hung Liu\"\\n          },\\n          {\\n            \"authorId\": \"2300130251\",\\n            \"name\": \"Xuchen Song\"\\n          },\\n          {\\n            \"authorId\": \"2353891852\",\\n            \"name\": \"Yang Li\"\\n          },\\n          {\\n            \"authorId\": \"2353423689\",\\n            \"name\": \"Yahui Zhou\"\\n          }\\n        ],\\n        \"abstract\": \"Autoregressive (AR) models have demonstrated impressive capabilities in generating high-fidelity music. However, the conventional next-token prediction paradigm in AR models does not align with the human creative process in music composition, potentially compromising the musicality of generated samples. To overcome this limitation, we introduce MusiCoT, a novel chain-of-thought (CoT) prompting technique tailored for music generation. MusiCoT empowers the AR model to first outline an overall music structure before generating audio tokens, thereby enhancing the coherence and creativity of the resulting compositions. By leveraging the contrastive language-audio pretraining (CLAP) model, we establish a chain of\\\\\"musical thoughts\\\\\", making MusiCoT scalable and independent of human-labeled data, in contrast to conventional CoT methods. Moreover, MusiCoT allows for in-depth analysis of music structure, such as instrumental arrangements, and supports music referencing -- accepting variable-length audio inputs as optional style references. This innovative approach effectively addresses copying issues, positioning MusiCoT as a vital practical method for music prompting. Our experimental results indicate that MusiCoT consistently achieves superior performance across both objective and subjective metrics, producing music quality that rivals state-of-the-art generation models. Our samples are available at https://MusiCoT.github.io/.\"\\n      },\\n      {\\n        \"paperId\": \"8d7971734b904ece00f8c830b3ddd684deca50a3\",\\n        \"url\": \"https://www.semanticscholar.org/paper/8d7971734b904ece00f8c830b3ddd684deca50a3\",\\n        \"title\": \"ChatGPT in thematic analysis: Can AI become a research assistant in qualitative research?\",\\n        \"citationCount\": 18,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {\\'abstract\\'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1007/s11135-025-02165-z?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1007/s11135-025-02165-z, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This article evaluates how GenAI can support thematic analysis using a publicly available interview dataset from Lumivero and introduces Guided AI Thematic Analysis (GAITA), an adaptation of King et al.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2025-06-02\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"1450826804\",\\n            \"name\": \"Kien Nguyen-Trung\"\\n          }\\n        ],\\n        \"abstract\": null\\n      },\\n      {\\n        \"paperId\": \"94617fc3593e87c4ff72ea0a3f5456fde5896801\",\\n        \"url\": \"https://www.semanticscholar.org/paper/94617fc3593e87c4ff72ea0a3f5456fde5896801\",\\n        \"title\": \"Deductive Closure Training of Language Models for Coherence, Accuracy, and Updatability\",\\n        \"citationCount\": 21,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2401.08574, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"DCT shows that LMs\\' reasoning capabilities during inference can be leveraged during training to improve their reliability, and shows that LMs\\' reasoning capabilities during inference can be leveraged during training to improve their reliability.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-01-16\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"1754469865\",\\n            \"name\": \"Afra Feyza Aky\\\\u00fcrek\"\\n          },\\n          {\\n            \"authorId\": \"1992708068\",\\n            \"name\": \"Ekin Aky\\\\u00fcrek\"\\n          },\\n          {\\n            \"authorId\": \"41019330\",\\n            \"name\": \"Leshem Choshen\"\\n          },\\n          {\\n            \"authorId\": \"2129412\",\\n            \"name\": \"Derry Tanti Wijaya\"\\n          },\\n          {\\n            \"authorId\": \"2257268279\",\\n            \"name\": \"Jacob Andreas\"\\n          }\\n        ],\\n        \"abstract\": \"While language models (LMs) can sometimes generate factually correct text and estimate truth values of individual claims, these generally do not reflect a globally coherent, manipulable model of the world. As a consequence, current LMs also generate incorrect or nonsensical content, and are difficult to edit and bring up to date. We present a method called Deductive Closure Training (DCT) that uses LMs themselves to identify implications of (and contradictions within) the text that they generate, yielding an efficient self-supervised procedure for improving LM factuality. Given a collection of seed documents, DCT prompts LMs to generate additional text implied by these documents, reason globally about the correctness of this generated text, and finally fine-tune on text inferred to be correct. Given seed documents from a trusted source, DCT provides a tool for supervised model updating; if seed documents are sampled from the LM itself, DCT enables fully unsupervised fine-tuning for improved coherence and accuracy. Across the CREAK, MQUaKE, and Reversal Curse datasets, supervised DCT improves LM fact verification and text generation accuracy by 3-26%; on CREAK fully unsupervised DCT improves verification accuracy by 12%. These results show that LMs\\' reasoning capabilities during inference can be leveraged during training to improve their reliability.\"\\n      },\\n      {\\n        \"paperId\": \"e3f68363ec2c5b1472dad31d8551dd516861c134\",\\n        \"url\": \"https://www.semanticscholar.org/paper/e3f68363ec2c5b1472dad31d8551dd516861c134\",\\n        \"title\": \"Performance of Artificial Intelligence in Detecting Diabetic Macular Edema From Fundus Photography and Optical Coherence Tomography Images: A Systematic Review and Meta-analysis.\",\\n        \"citationCount\": 20,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://diabetesjournals.figshare.com/articles/figure/Performance_of_Artificial_Intelligence_in_Detecting_Diabetic_Macular_Edema_from_Fundus_Photographs_and_Optical_Coherence_Tomography_Images_A_Systematic_Review_and_Meta-analysis/24518287/1/files/43067464.pdf\",\\n          \"status\": \"GREEN\",\\n          \"license\": \"CCBYNCSA\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.2337/dc23-0993?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.2337/dc23-0993, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This meta-analysis demonstrates satisfactory performance of AI in detecting DME from FP or OCT images and indicates external validation is warranted for future studies to evaluate model generalizability.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Review\"\\n        ],\\n        \"publicationDate\": \"2024-01-19\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2279977400\",\\n            \"name\": \"Ching Lam\"\\n          },\\n          {\\n            \"authorId\": \"2279974752\",\\n            \"name\": \"Yiu Lun Wong\"\\n          },\\n          {\\n            \"authorId\": \"1758777204\",\\n            \"name\": \"Ziqi Tang\"\\n          },\\n          {\\n            \"authorId\": \"2191364808\",\\n            \"name\": \"Xiaoyan Hu\"\\n          },\\n          {\\n            \"authorId\": \"2191377385\",\\n            \"name\": \"Truong X. Nguyen\"\\n          },\\n          {\\n            \"authorId\": \"2277830890\",\\n            \"name\": \"Dawei Yang\"\\n          },\\n          {\\n            \"authorId\": \"2280039410\",\\n            \"name\": \"Shuyi Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2280058989\",\\n            \"name\": \"J. Ding\"\\n          },\\n          {\\n            \"authorId\": \"34785499\",\\n            \"name\": \"Simon K H Szeto\"\\n          },\\n          {\\n            \"authorId\": \"108011531\",\\n            \"name\": \"A. Ran\"\\n          },\\n          {\\n            \"authorId\": \"2249532719\",\\n            \"name\": \"C. Y. Cheung\"\\n          }\\n        ],\\n        \"abstract\": \"BACKGROUND\\\\nDiabetic macular edema (DME) is the leading cause of vision loss in people with diabetes. Application of artificial intelligence (AI) in interpreting fundus photography (FP) and optical coherence tomography (OCT) images allows prompt detection and intervention.\\\\n\\\\n\\\\nPURPOSE\\\\nTo evaluate the performance of AI in detecting DME from FP or OCT images and identify potential factors affecting model performances.\\\\n\\\\n\\\\nDATA SOURCES\\\\nWe searched seven electronic libraries up to 12 February 2023.\\\\n\\\\n\\\\nSTUDY SELECTION\\\\nWe included studies using AI to detect DME from FP or OCT images.\\\\n\\\\n\\\\nDATA EXTRACTION\\\\nWe extracted study characteristics and performance parameters.\\\\n\\\\n\\\\nDATA SYNTHESIS\\\\nFifty-three studies were included in the meta-analysis. FP-based algorithms of 25 studies yielded pooled area under the receiver operating characteristic curve (AUROC), sensitivity, and specificity of 0.964, 92.6%, and 91.1%, respectively. OCT-based algorithms of 28 studies yielded pooled AUROC, sensitivity, and specificity of 0.985, 95.9%, and 97.9%, respectively. Potential factors improving model performance included deep learning techniques, larger size, and more diversity in training data sets. Models demonstrated better performance when validated internally than externally, and those trained with multiple data sets showed better results upon external validation.\\\\n\\\\n\\\\nLIMITATIONS\\\\nAnalyses were limited by unstandardized algorithm outcomes and insufficient data in patient demographics, OCT volumetric scans, and external validation.\\\\n\\\\n\\\\nCONCLUSIONS\\\\nThis meta-analysis demonstrates satisfactory performance of AI in detecting DME from FP or OCT images. External validation is warranted for future studies to evaluate model generalizability. Further investigations may estimate optimal sample size, effect of class balance, patient demographics, and additional benefits of OCT volumetric scans.\"\\n      },\\n      {\\n        \"paperId\": \"9aa5a10147b893f13f55f7500c89ba0a5e5d065b\",\\n        \"url\": \"https://www.semanticscholar.org/paper/9aa5a10147b893f13f55f7500c89ba0a5e5d065b\",\\n        \"title\": \"Vision-Language Models for Feature Detection of Macular Diseases on Optical Coherence Tomography.\",\\n        \"citationCount\": 19,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1001/jamaophthalmol.2024.1165?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1001/jamaophthalmol.2024.1165, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"In this study, a generalist VLM demonstrated limited vision capabilities for feature detection and management of macular disease, however, it showed low self-contradiction, suggesting strong language capabilities.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-05-02\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"41132090\",\\n            \"name\": \"F. Antaki\"\\n          },\\n          {\\n            \"authorId\": \"49334294\",\\n            \"name\": \"Reena Chopra\"\\n          },\\n          {\\n            \"authorId\": \"5638585\",\\n            \"name\": \"P. Keane\"\\n          }\\n        ],\\n        \"abstract\": \"Importance\\\\nVision-language models (VLMs) are a novel artificial intelligence technology capable of processing image and text inputs. While demonstrating strong generalist capabilities, their performance in ophthalmology has not been extensively studied.\\\\n\\\\n\\\\nObjective\\\\nTo assess the performance of the Gemini Pro VLM in expert-level tasks for macular diseases from optical coherence tomography (OCT) scans.\\\\n\\\\n\\\\nDesign, Setting, and Participants\\\\nThis was a cross-sectional diagnostic accuracy study evaluating a generalist VLM on ophthalmology-specific tasks using the open-source Optical Coherence Tomography Image Database. The dataset included OCT B-scans from 50 unique patients: healthy individuals and those with macular hole, diabetic macular edema, central serous chorioretinopathy, and age-related macular degeneration. Each OCT scan was labeled for 10 key pathological features, referral recommendations, and treatments. The images were captured using a Cirrus high definition OCT machine (Carl Zeiss Meditec) at Sankara Nethralaya Eye Hospital, Chennai, India, and the dataset was published in December 2018. Image acquisition dates were not specified.\\\\n\\\\n\\\\nExposures\\\\nGemini Pro, using a standard prompt to extract structured responses on December 15, 2023.\\\\n\\\\n\\\\nMain Outcomes and Measures\\\\nThe primary outcome was model responses compared against expert labels, calculating F1 scores for each pathological feature. Secondary outcomes included accuracy in diagnosis, referral urgency, and treatment recommendation. The model\\'s internal concordance was evaluated by measuring the alignment between referral and treatment recommendations, independent of diagnostic accuracy.\\\\n\\\\n\\\\nResults\\\\nThe mean F1 score was 10.7% (95% CI, 2.4-19.2). Measurable F1 scores were obtained for macular hole (36.4%; 95% CI, 0-71.4), pigment epithelial detachment (26.1%; 95% CI, 0-46.2), subretinal hyperreflective material (24.0%; 95% CI, 0-45.2), and subretinal fluid (20.0%; 95% CI, 0-45.5). A correct diagnosis was achieved in 17 of 50 cases (34%; 95% CI, 22-48). Referral recommendations varied: 28 of 50 were correct (56%; 95% CI, 42-70), 10 of 50 were overcautious (20%; 95% CI, 10-32), and 12 of 50 were undercautious (24%; 95% CI, 12-36). Referral and treatment concordance were very high, with 48 of 50 (96%; 95 % CI, 90-100) and 48 of 49 (98%; 95% CI, 94-100) correct answers, respectively.\\\\n\\\\n\\\\nConclusions and Relevance\\\\nIn this study, a generalist VLM demonstrated limited vision capabilities for feature detection and management of macular disease. However, it showed low self-contradiction, suggesting strong language capabilities. As VLMs continue to improve, validating their performance on large benchmarking datasets will help ascertain their potential in ophthalmology.\"\\n      },\\n      {\\n        \"paperId\": \"4bd41e401be5ca91d3302c2a888f565e8fc13749\",\\n        \"url\": \"https://www.semanticscholar.org/paper/4bd41e401be5ca91d3302c2a888f565e8fc13749\",\\n        \"title\": \"Large Language Models Can Enable Inductive Thematic Analysis of a Social Media Corpus in a Single Prompt: Human Validation Study\",\\n        \"citationCount\": 21,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://s3.ca-central-1.amazonaws.com/assets.jmir.org/assets/preprints/preprint-59641-accepted.pdf\",\\n          \"status\": \"GOLD\",\\n          \"license\": \"CCBY\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC11393503, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"LLMs can effectively and efficiently process large social media\\\\u2013based health-related data sets and extract themes from such data that human subject matter experts deem reasonable, but it is unable to show that the LLMs tested can replicate the depth of analysis from human subject matter experts by consistently extracting the same themes from the same data.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-04-24\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"6436816\",\\n            \"name\": \"Michael S. Deiner\"\\n          },\\n          {\\n            \"authorId\": \"2211301918\",\\n            \"name\": \"Vlad Honcharov\"\\n          },\\n          {\\n            \"authorId\": \"1492112680\",\\n            \"name\": \"Jiawei Li\"\\n          },\\n          {\\n            \"authorId\": \"2237755335\",\\n            \"name\": \"T. Mackey\"\\n          },\\n          {\\n            \"authorId\": \"2274513993\",\\n            \"name\": \"Travis C Porco\"\\n          },\\n          {\\n            \"authorId\": \"2261922685\",\\n            \"name\": \"Urmimala Sarkar\"\\n          }\\n        ],\\n        \"abstract\": \"Background Manually analyzing public health\\\\u2013related content from social media provides valuable insights into the beliefs, attitudes, and behaviors of individuals, shedding light on trends and patterns that can inform public understanding, policy decisions, targeted interventions, and communication strategies. Unfortunately, the time and effort needed from well-trained human subject matter experts makes extensive manual social media listening unfeasible. Generative large language models (LLMs) can potentially summarize and interpret large amounts of text, but it is unclear to what extent LLMs can glean subtle health-related meanings in large sets of social media posts and reasonably report health-related themes. Objective We aimed to assess the feasibility of using LLMs for topic model selection or inductive thematic analysis of large contents of social media posts by attempting to answer the following question: Can LLMs conduct topic model selection and inductive thematic analysis as effectively as humans did in a prior manual study, or at least reasonably, as judged by subject matter experts? Methods We asked the same research question and used the same set of social media content for both the LLM selection of relevant topics and the LLM analysis of themes as was conducted manually in a published study about vaccine rhetoric. We used the results from that study as background for this LLM experiment by comparing the results from the prior manual human analyses with the analyses from 3 LLMs: GPT4-32K, Claude-instant-100K, and Claude-2-100K. We also assessed if multiple LLMs had equivalent ability and assessed the consistency of repeated analysis from each LLM. Results The LLMs generally gave high rankings to the topics chosen previously by humans as most relevant. We reject a null hypothesis (P<.001, overall comparison) and conclude that these LLMs are more likely to include the human-rated top 5 content areas in their top rankings than would occur by chance. Regarding theme identification, LLMs identified several themes similar to those identified by humans, with very low hallucination rates. Variability occurred between LLMs and between test runs of an individual LLM. Despite not consistently matching the human-generated themes, subject matter experts found themes generated by the LLMs were still reasonable and relevant. Conclusions LLMs can effectively and efficiently process large social media\\\\u2013based health-related data sets. LLMs can extract themes from such data that human subject matter experts deem reasonable. However, we were unable to show that the LLMs we tested can replicate the depth of analysis from human subject matter experts by consistently extracting the same themes from the same data. There is vast potential, once better validated, for automated LLM-based real-time social listening for common and rare health conditions, informing public health understanding of the public\\\\u2019s interests and concerns and determining the public\\\\u2019s ideas to address them.\"\\n      },\\n      {\\n        \"paperId\": \"4913529041ac69912680d1f8c1b1e4229a68b61b\",\\n        \"url\": \"https://www.semanticscholar.org/paper/4913529041ac69912680d1f8c1b1e4229a68b61b\",\\n        \"title\": \"The Intellectual Evolution of Educational Leadership Research: A Combined Bibliometric and Thematic Analysis Using SciMAT\",\\n        \"citationCount\": 15,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://www.mdpi.com/2227-7102/14/4/429/pdf?version=1713520131\",\\n          \"status\": \"GOLD\",\\n          \"license\": \"CCBY\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.3390/educsci14040429?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.3390/educsci14040429, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": null,\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-04-19\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"66769167\",\\n            \"name\": \"Turgut Karakose\"\\n          },\\n          {\\n            \"authorId\": \"114837130\",\\n            \"name\": \"K. Leithwood\"\\n          },\\n          {\\n            \"authorId\": \"3224712\",\\n            \"name\": \"Tijen T\\\\u00fcl\\\\u00fcba\\\\u015f\"\\n          }\\n        ],\\n        \"abstract\": \"This study aims to describe the century-long trajectory of educational leadership research (ELR), including changes over time in its main and subsidiary themes, as well as its most influential authors, papers, and journals. The study combines the bibliometric performance and science mapping analysis of 7282 articles retrieved from the Scopus and WoS databases. SciMAT software (version 1.1.04) was used to analyze changes over four sequential time periods and to exhibit the thematic evolution of the field\\\\u2014Period 1 (1907 to 2004), Period 2 (2005 to 2012), Period 3 (2013 to 2019), and Period 4 (2020\\\\u20132023). Research during Period 1 focused on principals and included efforts to distinguish between their administrative functions and forms of \\\\u2018strong\\\\u2019 leadership contributing to school improvement. Period 2 included research aimed at understanding what strong principal leadership entailed, including the development and testing of more coherent models of such leadership. While instructional and transformational leadership models were prominent during Periods 1 and 2, Period 3 research invested heavily in conceptions of leadership distribution. Early research about \\\\u2018social justice leadership\\\\u2019 appeared during this period and eventually flourished during Period 4. While principals were an active focus through all Periods, the leadership of others gradually dominated ELR and accounted for the broader leadership theme found in all four periods. The results point to the evolutionary nature of ELR development, which eventually produced a relatively robust knowledge base. Experiences with the COVID-19 pandemic suggest that crises such as this might prompt more revolutionary orientations in the ELR field.\"\\n      },\\n      {\\n        \"paperId\": \"587b50d31e99f37489e1a2b338d8ddb07026da0b\",\\n        \"url\": \"https://www.semanticscholar.org/paper/587b50d31e99f37489e1a2b338d8ddb07026da0b\",\\n        \"title\": \"Exploring Students\\\\u2019 Perceptions of ChatGPT: Thematic Analysis and Follow-Up Survey\",\\n        \"citationCount\": 271,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://doi.org/10.1109/access.2023.3268224\",\\n          \"status\": \"GOLD\",\\n          \"license\": \"CCBYNCND\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ACCESS.2023.3268224?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ACCESS.2023.3268224, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"It is concluded that ChatGPT can and should be used for learning, however, students should be aware of its limitations.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Review\"\\n        ],\\n        \"publicationDate\": null,\\n        \"authors\": [\\n          {\\n            \"authorId\": \"1754800\",\\n            \"name\": \"Abdulhadi Shoufan\"\\n          }\\n        ],\\n        \"abstract\": \"ChatGPT has sparked both excitement and skepticism in education. To analyze its impact on teaching and learning it is crucial to understand how students perceive ChatGPT and assess its potential and challenges. Toward this, we conducted a two-stage study with senior students in a computer engineering program ( $n=56$ ). In the first stage, we asked the students to evaluate ChatGPT using their own words after they used it to complete one learning activity. The returned responses (3136 words) were analyzed by coding and theme building (36 codes and 15 themes). In the second stage, we used the derived codes and themes to create a 27-item questionnaire. The students responded to this questionnaire three weeks later after completing other activities with the help of ChatGPT. The results show that the students admire the capabilities of ChatGPT and find it interesting, motivating, and helpful for study and work. They find it easy to use and appreciate its human-like interface that provides well-structured responses and good explanations. However, many students feel that ChatGPT\\\\u2019s answers are not always accurate and most of them believe that it requires good background knowledge to work with since it does not replace human intelligence. So, most students think that ChatGPT needs to be improved but are optimistic that this will happen soon. When it comes to the negative impact of ChatGPT on learning, academic integrity, jobs, and life, the students are divided. We conclude that ChatGPT can and should be used for learning. However, students should be aware of its limitations. Educators should try using ChatGPT and guide students on effective prompting techniques and how to assess generated responses. The developers should improve their models to enhance the accuracy of given answers. The study provides insights into the capabilities and limitations of ChatGPT in education and informs future research and development.\"\\n      },\\n      {\\n        \"paperId\": \"a01a9c4a114fbf201540268f928ccf77bc3f9357\",\\n        \"url\": \"https://www.semanticscholar.org/paper/a01a9c4a114fbf201540268f928ccf77bc3f9357\",\\n        \"title\": \"Fine-Grained Visual Prompting\",\\n        \"citationCount\": 90,\\n        \"openAccessPdf\": {\\n          \"url\": \"http://arxiv.org/pdf/2306.04356\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2306.04356, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper introduces a new zero-shot framework that leverages pixel-level annotations acquired from a generalist segmentation model for fine-grained visual prompting and reveals that a straightforward application of blur outside the target mask, referred to as the Blur Reverse Mask, exhibits exceptional effectiveness.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2023-06-07\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2146416206\",\\n            \"name\": \"Lingfeng Yang\"\\n          },\\n          {\\n            \"authorId\": \"2217456303\",\\n            \"name\": \"Yueze Wang\"\\n          },\\n          {\\n            \"authorId\": \"2144439048\",\\n            \"name\": \"Xiang Li\"\\n          },\\n          {\\n            \"authorId\": \"51316629\",\\n            \"name\": \"Xinlong Wang\"\\n          },\\n          {\\n            \"authorId\": \"2146236917\",\\n            \"name\": \"Jian Yang\"\\n          }\\n        ],\\n        \"abstract\": \"Vision-Language Models (VLMs), such as CLIP, have demonstrated impressive zero-shot transfer capabilities in image-level visual perception. However, these models have shown limited performance in instance-level tasks that demand precise localization and recognition. Previous works have suggested that incorporating visual prompts, such as colorful boxes or circles, can improve the ability of models to recognize objects of interest. Nonetheless, compared to language prompting, visual prompting designs are rarely explored. Existing approaches, which employ coarse visual cues such as colorful boxes or circles, often result in sub-optimal performance due to the inclusion of irrelevant and noisy pixels. In this paper, we carefully study the visual prompting designs by exploring more fine-grained markings, such as segmentation masks and their variations. In addition, we introduce a new zero-shot framework that leverages pixel-level annotations acquired from a generalist segmentation model for fine-grained visual prompting. Consequently, our investigation reveals that a straightforward application of blur outside the target mask, referred to as the Blur Reverse Mask, exhibits exceptional effectiveness. This proposed prompting strategy leverages the precise mask annotations to reduce focus on weakly related regions while retaining spatial coherence between the target and the surrounding background. Our Fine-Grained Visual Prompting (FGVP) demonstrates superior performance in zero-shot comprehension of referring expressions on the RefCOCO, RefCOCO+, and RefCOCOg benchmarks. It outperforms prior methods by an average margin of 3.0% to 4.6%, with a maximum improvement of 12.5% on the RefCOCO+ testA subset. Code is available at https://github.com/ylingfeng/FGVP.\"\\n      },\\n      {\\n        \"paperId\": \"e1770838ec0667cad48729a81764ed9964d6a8e6\",\\n        \"url\": \"https://www.semanticscholar.org/paper/e1770838ec0667cad48729a81764ed9964d6a8e6\",\\n        \"title\": \"LLM-in-the-loop: Leveraging Large Language Model for Thematic Analysis\",\\n        \"citationCount\": 108,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2310.15100, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work proposes a human-LLM collaboration framework (i.e., LLM-in-the-loop) to conduct TA with in-context learning (ICL), which yields similar coding quality to that of human coders but reduces TA\\'s labor and time demands.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\",\\n          \"Review\"\\n        ],\\n        \"publicationDate\": \"2023-10-23\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"3400291\",\\n            \"name\": \"Shih-Chieh Dai\"\\n          },\\n          {\\n            \"authorId\": \"2261362789\",\\n            \"name\": \"Aiping Xiong\"\\n          },\\n          {\\n            \"authorId\": \"1746959\",\\n            \"name\": \"Lun-Wei Ku\"\\n          }\\n        ],\\n        \"abstract\": \"Thematic analysis (TA) has been widely used for analyzing qualitative data in many disciplines and fields. To ensure reliable analysis, the same piece of data is typically assigned to at least two human coders. Moreover, to produce meaningful and useful analysis, human coders develop and deepen their data interpretation and coding over multiple iterations, making TA labor-intensive and time-consuming. Recently the emerging field of large language models (LLMs) research has shown that LLMs have the potential replicate human-like behavior in various tasks: in particular, LLMs outperform crowd workers on text-annotation tasks, suggesting an opportunity to leverage LLMs on TA. We propose a human-LLM collaboration framework (i.e., LLM-in-the-loop) to conduct TA with in-context learning (ICL). This framework provides the prompt to frame discussions with a LLM (e.g., GPT-3.5) to generate the final codebook for TA. We demonstrate the utility of this framework using survey datasets on the aspects of the music listening experience and the usage of a password manager. Results of the two case studies show that the proposed framework yields similar coding quality to that of human coders but reduces TA\\'s labor and time demands.\"\\n      },\\n      {\\n        \"paperId\": \"b54398b8ccba47ecab43a5311e5b2a6c61461936\",\\n        \"url\": \"https://www.semanticscholar.org/paper/b54398b8ccba47ecab43a5311e5b2a6c61461936\",\\n        \"title\": \"Who Evaluates the Evaluations? Objectively Scoring Text-to-Image Prompt Coherence Metrics with T2IScoreScore (TS2)\",\\n        \"citationCount\": 12,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2404.04251, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"T2IScoreScore is introduced, a curated set of semantic error graphs containing a prompt and a set of increasingly erroneous images to rigorously judge whether a given prompt faithfulness metric can correctly order images with respect to their objective error count and significantly discriminate between different error nodes, using meta-metric scores derived from established statistical tests.\"\\n        },\\n        \"publicationTypes\": null,\\n        \"publicationDate\": \"2024-04-05\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"48227633\",\\n            \"name\": \"Michael Stephen Saxon\"\\n          },\\n          {\\n            \"authorId\": \"2069493939\",\\n            \"name\": \"Fatima Jahara\"\\n          },\\n          {\\n            \"authorId\": \"2302799897\",\\n            \"name\": \"Mahsa Khoshnoodi\"\\n          },\\n          {\\n            \"authorId\": \"2257339858\",\\n            \"name\": \"Yujie Lu\"\\n          },\\n          {\\n            \"authorId\": \"2309678102\",\\n            \"name\": \"Aditya Sharma\"\\n          },\\n          {\\n            \"authorId\": \"2257130316\",\\n            \"name\": \"W. Wang\"\\n          }\\n        ],\\n        \"abstract\": \"With advances in the quality of text-to-image (T2I) models has come interest in benchmarking their prompt faithfulness -- the semantic coherence of generated images to the prompts they were conditioned on. A variety of T2I faithfulness metrics have been proposed, leveraging advances in cross-modal embeddings and vision-language models (VLMs). However, these metrics are not rigorously compared and benchmarked, instead presented with correlation to human Likert scores over a set of easy-to-discriminate images against seemingly weak baselines. We introduce T2IScoreScore, a curated set of semantic error graphs containing a prompt and a set of increasingly erroneous images. These allow us to rigorously judge whether a given prompt faithfulness metric can correctly order images with respect to their objective error count and significantly discriminate between different error nodes, using meta-metric scores derived from established statistical tests. Surprisingly, we find that the state-of-the-art VLM-based metrics (e.g., TIFA, DSG, LLMScore, VIEScore) we tested fail to significantly outperform simple (and supposedly worse) feature-based metrics like CLIPScore, particularly on a hard subset of naturally-occurring T2I model errors. TS2 will enable the development of better T2I prompt faithfulness metrics through more rigorous comparison of their conformity to expected orderings and separations under objective criteria.\"\\n      },\\n      {\\n        \"paperId\": \"5121e40b45f33c2fe08087173cfb8ddfc2bf4fa2\",\\n        \"url\": \"https://www.semanticscholar.org/paper/5121e40b45f33c2fe08087173cfb8ddfc2bf4fa2\",\\n        \"title\": \"Super-Resolution Cloth Animation with Spatial and Temporal Coherence\",\\n        \"citationCount\": 10,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1145/3658143?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/3658143, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper decomposes garment meshes into overlapping patches for adaptability to various styles and geometric continuity and achieves an 8\\\\u00d7 improvement in resolution for cloth animations, leveraging two core modules.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-07-19\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2312152389\",\\n            \"name\": \"Jiawang Yu\"\\n          },\\n          {\\n            \"authorId\": \"2312169459\",\\n            \"name\": \"Zhendong Wang\"\\n          }\\n        ],\\n        \"abstract\": \"Creating super-resolution cloth animations, which refine coarse cloth meshes with fine wrinkle details, faces challenges in preserving spatial consistency and temporal coherence across frames. In this paper, we introduce a general framework to address these issues, leveraging two core modules. The first module interleaves a simulator and a corrector. The simulator handles cloth dynamics, while the corrector rectifies differences in low-frequency features across various resolutions. This interleaving ensures prompt correction of spatial errors from the coarse simulation, effectively preventing their temporal propagation. The second module performs mesh-based super-resolution for detailed wrinkle enhancements. We decompose garment meshes into overlapping patches for adaptability to various styles and geometric continuity. Our method achieves an 8\\\\u00d7 improvement in resolution for cloth animations. We showcase the effectiveness of our method through diverse animation examples, including simple cloth pieces and intricate garments.\"\\n      },\\n      {\\n        \"paperId\": \"65319931eb2af60a8b922bb5f9ce1194f300e6df\",\\n        \"url\": \"https://www.semanticscholar.org/paper/65319931eb2af60a8b922bb5f9ce1194f300e6df\",\\n        \"title\": \"\\\\\"We Are Visual Thinkers, Not Verbal Thinkers!\\\\\": A Thematic Analysis of How Professional Designers Use Generative AI Image Generation Tools\",\\n        \"citationCount\": 12,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1145/3679318.3685370?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/3679318.3685370, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A qualitative study involving 16 professional designers from the automotive industry identified their challenges with existing GenAI image generation tools in daily design practices, and revealed the need for visual input-centric multi-modal interfaces that extend beyond textual prompts.\"\\n        },\\n        \"publicationTypes\": [\\n          \"Book\",\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-10-13\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2264963307\",\\n            \"name\": \"Hyerim Park\"\\n          },\\n          {\\n            \"authorId\": \"29415466\",\\n            \"name\": \"Joscha Eirich\"\\n          },\\n          {\\n            \"authorId\": \"2292312388\",\\n            \"name\": \"Andr\\\\u00e9 Luckow\"\\n          },\\n          {\\n            \"authorId\": \"2243254992\",\\n            \"name\": \"Michael Sedlmair\"\\n          }\\n        ],\\n        \"abstract\": \"Generative artificial intelligence (GenAI) has become increasingly popular, influencing various creative domains. However, while broader societal perspectives have been analyzed, specific examinations of how practitioners utilize GenAI tools to enhance their current workflows remain limited. To address this gap, we conducted a qualitative study involving 16 professional designers from the automotive industry. We aimed to identify their challenges with existing GenAI image generation tools in daily design practices. Thematic analysis revealed four key themes: (1) the need for visual input-centric multi-modal interfaces that extend beyond textual prompts, (2) the lack of support for the iterative nature of design processes in GenAI tools, (3) difficulties in controlling prompts to achieve desired outputs, and (4) the significance of incorporating human experiences and emotions into design. Based on our findings, we propose and discuss potential design considerations for enhancing future GenAI image generation tool interfaces.\"\\n      },\\n      {\\n        \"paperId\": \"22438bcb9ef4fb5be5ea9ea8307d4691a3209f4d\",\\n        \"url\": \"https://www.semanticscholar.org/paper/22438bcb9ef4fb5be5ea9ea8307d4691a3209f4d\",\\n        \"title\": \"Quantitative and qualitative assessment of anterior segment optical coherence tomography capture of disease state in childhood anterior uveitis\",\\n        \"citationCount\": 18,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://discovery.ucl.ac.uk/10144499/3/Solebo_AS_OCT_clean_230122.pdf\",\\n          \"status\": \"GREEN\",\\n          \"license\": \"other-oa\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1136/bjophthalmol-2021-320448?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1136/bjophthalmol-2021-320448, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"The findings suggest that OCT-based imaging holds the promise of deeper understanding of disease, improved patient experience and more granular monitoring of activity with resultant improved outcomes, but further work is needed to refine acquisition and analysis protocols.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2022-02-23\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"1967704903\",\\n            \"name\": \"Katie Etherton\"\\n          },\\n          {\\n            \"authorId\": \"145884297\",\\n            \"name\": \"J. Rahi\"\\n          },\\n          {\\n            \"authorId\": \"3971827\",\\n            \"name\": \"H. Petrushkin\"\\n          },\\n          {\\n            \"authorId\": \"2090508795\",\\n            \"name\": \"A. Dick\"\\n          },\\n          {\\n            \"authorId\": \"1573631346\",\\n            \"name\": \"S. Akbarali\"\\n          },\\n          {\\n            \"authorId\": \"15498366\",\\n            \"name\": \"R. Pattani\"\\n          },\\n          {\\n            \"authorId\": \"4326096\",\\n            \"name\": \"S. Hau\"\\n          },\\n          {\\n            \"authorId\": \"46521197\",\\n            \"name\": \"S. Lacassagne\"\\n          },\\n          {\\n            \"authorId\": \"46521856\",\\n            \"name\": \"Xiaoxuan Liu\"\\n          },\\n          {\\n            \"authorId\": \"145661041\",\\n            \"name\": \"A. Denniston\"\\n          },\\n          {\\n            \"authorId\": \"8489719\",\\n            \"name\": \"A. Solebo\"\\n          }\\n        ],\\n        \"abstract\": \"Background/aims Anterior segment optical coherence tomography (AS-OCT) assessment of anterior chamber inflammation is an emerging tool. We describe the performance of AS-OCT in a paediatric population. Methods A mixed-methods prospective study, using routine clinical assessment as reference standard, and AS-OCT, with Tomey CASIA2 or Heidelberg Spectralis HS1, as index test, with data collected on patient perceptions of imaging. Repeatability, diagnostic indices, responsiveness to clinical change and clinical correlations of imaging-based metrics (image cell count, size, density and brightness) were assessed, with construction of receiver operated characteristic curves. Exploratory thematic analysis of responses from families was undertaken. Results A total of 90 children (180 eyes) underwent imaging. Bland Altman limits of agreement for CASIA2 repeatability ranged from +17 cells (95%\\\\u2009CI 13.6 to 21.1) to \\\\u221219 cells (95%\\\\u2009CI \\\\u221215.6 to \\\\u221223.2) and HS1 from +1 (95% CI 0.9 to 1.2) to \\\\u22121.0 (\\\\u22121.2 to \\\\u22120.8) cells. CASIA2 imaging had higher sensitivity of 0.92 (95% CI 0.78 to 0.97) vs HS1 imaging 0.17 (95% CI 0.07 to 0.34), with positive correlation between clinical grade and CASIA2 cell count (coefficient 12.8, p=0.02, 95%\\\\u2009CI 2.2 to 23.4). Change in clinical grade at follow-up examinations correlated with change in image based \\\\u2018cell\\\\u2019 count (r2=0.79, p<0.001). Patients reported a potential positive impact of seeing their disease activity. Conclusion Our findings suggest that OCT-based imaging holds the promise of deeper understanding of disease, improved patient experience and more granular monitoring of activity with resultant improved outcomes, but further work is needed to refine acquisition and analysis protocols.\"\\n      },\\n      {\\n        \"paperId\": \"a0968924129aee94ca2070004e657df3d34a41dd\",\\n        \"url\": \"https://www.semanticscholar.org/paper/a0968924129aee94ca2070004e657df3d34a41dd\",\\n        \"title\": \"Academic publisher guidelines on AI usage: A ChatGPT supported thematic analysis\",\\n        \"citationCount\": 67,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://f1000research.com/articles/12-1398/pdf\",\\n          \"status\": \"GOLD\",\\n          \"license\": \"CCBY\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC10844801, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A novel analysis supported by GenAI tools is used to identify themes emerging in the policy landscape, underscoring the need for an informed, flexible approach to policy formulation that can adapt to the rapidly evolving landscape of AI technologies.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Review\"\\n        ],\\n        \"publicationDate\": \"2023-10-23\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"48765116\",\\n            \"name\": \"Mike Perkins\"\\n          },\\n          {\\n            \"authorId\": \"114397354\",\\n            \"name\": \"Jasper Roe\"\\n          }\\n        ],\\n        \"abstract\": \"Background As Artificial Intelligence (AI) technologies such as Generative AI (GenAI) have become more common in academic settings, it is necessary to examine how these tools interact with issues of authorship, academic integrity, and research methodologies. The current landscape lacks cohesive policies and guidelines for regulating AI\\\\u2019s role in academic research which has prompted discussions among publishers, authors, and institutions. Methods This study employs inductive thematic analysis to explore publisher policies regarding AI-assisted authorship and academic work. Our methods involved a two-fold analysis using both AI-assisted and traditional unassisted techniques to examine the available policies from leading academic publishers and other publishing or academic entities. The framework was designed to offer multiple perspectives, harnessing the strengths of AI for pattern recognition while leveraging human expertise for nuanced interpretation. The results of these two analyses are combined to form the final themes. Results Our findings indicate six overall themes, three of which were independently identified in both the AI-assisted and unassisted, manual analysis using common software tools. A broad consensus appears among publishers that human authorship remains paramount and that the use of GenAI tools is permissible but must be disclosed. However, GenAI tools are increasingly acknowledged for their supportive roles, including text generation and data analysis. The study also discusses the inherent limitations and biases of AI-assisted analysis, necessitating rigorous scrutiny by authors, reviewers, and editors. Conclusions There is a growing recognition of AI\\\\u2019s role as a valuable auxiliary tool in academic research, but one that comes with caveats pertaining to integrity, accountability, and interpretive limitations. This study used a novel analysis supported by GenAI tools to identify themes emerging in the policy landscape, underscoring the need for an informed, flexible approach to policy formulation that can adapt to the rapidly evolving landscape of AI technologies.\"\\n      },\\n      {\\n        \"paperId\": \"5c8ad3edc40efc0da8937fdafae7796d487efdf1\",\\n        \"url\": \"https://www.semanticscholar.org/paper/5c8ad3edc40efc0da8937fdafae7796d487efdf1\",\\n        \"title\": \"Pain experiences during intrauterine device procedures: a thematic analysis of tweets\",\\n        \"citationCount\": 14,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://srh.bmj.com/content/familyplanning/early/2024/06/11/bmjsrh-2023-202011.full.pdf\",\\n          \"status\": \"HYBRID\",\\n          \"license\": \"CCBYNC\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC11503099, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"The findings attest the need for strategies to improve the patient experience for those opting for IUD as a clinical priority and further research should explore IUD users\\' experiences, expectations and wishes around pain management.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-06-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"6247626\",\\n            \"name\": \"Neda Taghinejadi\"\\n          },\\n          {\\n            \"authorId\": \"6056729\",\\n            \"name\": \"H. van der Westhuizen\"\\n          },\\n          {\\n            \"authorId\": \"1723406956\",\\n            \"name\": \"F. I. Ayomoh\"\\n          },\\n          {\\n            \"authorId\": \"47340073\",\\n            \"name\": \"Wasim Ahmed\"\\n          },\\n          {\\n            \"authorId\": \"2300379872\",\\n            \"name\": \"Trisha Greenhalgh\"\\n          },\\n          {\\n            \"authorId\": \"144511766\",\\n            \"name\": \"A. Boylan\"\\n          }\\n        ],\\n        \"abstract\": \"Introduction In June 2021, high-profile testimonials in the media about pain during intrauterine device (IUD) procedures in the UK prompted significant discussion across platforms including Twitter (subsequently renamed X). We examined a sample of Twitter postings (tweets) to gain insight into public perspectives and experiences. Methods We harvested tweets posted or retweeted on 21\\\\u201322 June 2021 which contained the search terms coil, intrauterine system, IUD or intrauterine. We analysed the dataset thematically and selected illustrative tweets with the authors\\\\u2019 consent for publication. Results Following deduplication and screening, we included 1431 tweets in our analysis. We identified testimonials with descriptions of varied pain experiences. Twitter users reported that clinicians had not warned them that pain could be severe or explained the options for pain relief. Some raised concerns about pain being minimised or dismissed and linked this to the management of women\\\\u2019s pain in medicine more broadly. Twitter users described connecting to an online community with shared experiences as validating and used this as a springboard for collective action. Conclusions While we acknowledge the limitations of our sample, this study highlights important perspectives and accounts relating to pain during IUD procedures. Our findings attest to the need for strategies to improve the patient experience for those opting for IUD as a clinical priority. Further research should explore IUD users\\' experiences, expectations and wishes around pain management.\"\\n      },\\n      {\\n        \"paperId\": \"b09ab1ca37173fbea6c24cda2bbda0c93cea6c91\",\\n        \"url\": \"https://www.semanticscholar.org/paper/b09ab1ca37173fbea6c24cda2bbda0c93cea6c91\",\\n        \"title\": \"Singing off the same hymn sheet? Examining coherence in a talent development pathway (part 2)\",\\n        \"citationCount\": 12,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1080/02640414.2021.2021702?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1080/02640414.2021.2021702, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"Results suggest that the talent development environment provides a long-term development experience supplemented with individual athlete attention at international level, but there is a need for improved direction from the National Governing Body if systematic coherence towards talent development is to be achieved.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2022-01-05\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"122738315\",\\n            \"name\": \"O. Curran\"\\n          },\\n          {\\n            \"authorId\": \"91859067\",\\n            \"name\": \"D. Passmore\"\\n          },\\n          {\\n            \"authorId\": \"6973495\",\\n            \"name\": \"\\\\u00c1. Macnamara\"\\n          }\\n        ],\\n        \"abstract\": \"ABSTRACT Existing literature highlights the common characteristics of successful talent development environments, notably the need for long-term development, individual athlete attention, communication, alignment, and psycho-behavioural development. Little is known however about the complex talent development environment of an international sport organisation where multiple contexts and various stakeholders exist. Considering the lack of research relating to females in talent development, we examined a female national hockey talent development environment and more specifically the level of coherence that existed within the talent development environment from different stakeholder perspectives. Twenty-seven international female hockey players and fourteen pathway staff members from across the talent development pathway participated in semi-structured focus groups. An inductive\\\\u2013deductive thematic analysis was conducted. Results suggest that the talent development environment provides a long-term development experience supplemented with individual athlete attention at international level. However, a general lack of coherence and systematic development was evident across the talent development environment contexts with varying levels of coherence found within the higher-order themes of appropriate development, not early success, individualised and ongoing development, and wide-ranging coherent messages and support. This highlights a need for improved direction from the National Governing Body if systematic coherence towards talent development is to be achieved.\"\\n      },\\n      {\\n        \"paperId\": \"c8e142c6ee5b8a044e406670e01d98f97a5d26bf\",\\n        \"url\": \"https://www.semanticscholar.org/paper/c8e142c6ee5b8a044e406670e01d98f97a5d26bf\",\\n        \"title\": \"Comparing the Efficacy and Efficiency of Human and Generative AI: Qualitative Thematic Analyses\",\\n        \"citationCount\": 32,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://doi.org/10.2196/54482\",\\n          \"status\": \"GOLD\",\\n          \"license\": \"CCBY\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC11329846, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"The promising consistency in the themes generated by human coders and GenAI suggests that these technologies hold promise in reducing the resource intensiveness of qualitative thematic analysis; however, the relatively lower reliability in coding between them suggests that hybrid approaches are necessary.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2023-11-11\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"34245122\",\\n            \"name\": \"Maximo R. Prescott\"\\n          },\\n          {\\n            \"authorId\": \"2305191451\",\\n            \"name\": \"S. Yeager\"\\n          },\\n          {\\n            \"authorId\": \"2275218415\",\\n            \"name\": \"Lillian Ham\"\\n          },\\n          {\\n            \"authorId\": \"2051442834\",\\n            \"name\": \"C. R. Rivera Saldana\"\\n          },\\n          {\\n            \"authorId\": \"2305195892\",\\n            \"name\": \"Vanessa Serrano\"\\n          },\\n          {\\n            \"authorId\": \"2305192478\",\\n            \"name\": \"J. Narez\"\\n          },\\n          {\\n            \"authorId\": \"84640204\",\\n            \"name\": \"D. Paltin\"\\n          },\\n          {\\n            \"authorId\": \"2262498185\",\\n            \"name\": \"Jorge Delgado\"\\n          },\\n          {\\n            \"authorId\": \"2300816756\",\\n            \"name\": \"David J Moore\"\\n          },\\n          {\\n            \"authorId\": \"2301912568\",\\n            \"name\": \"Jessica Montoya\"\\n          }\\n        ],\\n        \"abstract\": \"Background Qualitative methods are incredibly beneficial to the dissemination and implementation of new digital health interventions; however, these methods can be time intensive and slow down dissemination when timely knowledge from the data sources is needed in ever-changing health systems. Recent advancements in generative artificial intelligence (GenAI) and their underlying large language models (LLMs) may provide a promising opportunity to expedite the qualitative analysis of textual data, but their efficacy and reliability remain unknown. Objective The primary objectives of our study were to evaluate the consistency in themes, reliability of coding, and time needed for inductive and deductive thematic analyses between GenAI (ie, ChatGPT and Bard) and human coders. Methods The qualitative data for this study consisted of 40 brief SMS text message reminder prompts used in a digital health intervention for promoting antiretroviral medication adherence among people with HIV who use methamphetamine. Inductive and deductive thematic analyses of these SMS text messages were conducted by 2 independent teams of human coders. An independent human analyst conducted analyses following both approaches using ChatGPT and Bard. The consistency in themes (or the extent to which the themes were the same) and reliability (or agreement in coding of themes) between methods were compared. Results The themes generated by GenAI (both ChatGPT and Bard) were consistent with 71% (5/7) of the themes identified by human analysts following inductive thematic analysis. The consistency in themes was lower between humans and GenAI following a deductive thematic analysis procedure (ChatGPT: 6/12, 50%; Bard: 7/12, 58%). The percentage agreement (or intercoder reliability) for these congruent themes between human coders and GenAI ranged from fair to moderate (ChatGPT, inductive: 31/66, 47%; ChatGPT, deductive: 22/59, 37%; Bard, inductive: 20/54, 37%; Bard, deductive: 21/58, 36%). In general, ChatGPT and Bard performed similarly to each other across both types of qualitative analyses in terms of consistency of themes (inductive: 6/6, 100%; deductive: 5/6, 83%) and reliability of coding (inductive: 23/62, 37%; deductive: 22/47, 47%). On average, GenAI required significantly less overall time than human coders when conducting qualitative analysis (20, SD 3.5 min vs 567, SD 106.5 min). Conclusions The promising consistency in the themes generated by human coders and GenAI suggests that these technologies hold promise in reducing the resource intensiveness of qualitative thematic analysis; however, the relatively lower reliability in coding between them suggests that hybrid approaches are necessary. Human coders appeared to be better than GenAI at identifying nuanced and interpretative themes. Future studies should consider how these powerful technologies can be best used in collaboration with human coders to improve the efficiency of qualitative research in hybrid approaches while also mitigating potential ethical risks that they may pose.\"\\n      },\\n      {\\n        \"paperId\": \"0edcd6dd2e952523c35e77d0f9cce5927b97d63e\",\\n        \"url\": \"https://www.semanticscholar.org/paper/0edcd6dd2e952523c35e77d0f9cce5927b97d63e\",\\n        \"title\": \"CoAScore: Chain-of-Aspects Prompting for NLG Evaluation\",\\n        \"citationCount\": 15,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2312.10355, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"The experimental findings highlight that, in comparison to individual aspect evaluation, CoAScore exhibits a higher correlation with human judgments, which significantly outperforms existing unsupervised evaluation metrics, whether for assessing overall quality or other aspects.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2023-12-16\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2275055956\",\\n            \"name\": \"Peiyuan Gong\"\\n          },\\n          {\\n            \"authorId\": \"2275056887\",\\n            \"name\": \"Jiaxin Mao\"\\n          }\\n        ],\\n        \"abstract\": \"Recently, natural language generation (NLG) evaluation has shifted from a single-aspect to a multi-aspect paradigm, allowing for a more accurate assessment. Large language models (LLMs) achieve superior performance on various NLG evaluation tasks. However, current work often employs the LLM to independently evaluate different aspects, which largely ignores the rich correlation between various aspects. To fill this research gap, in this work, we propose an NLG evaluation metric called CoAScore. Powered by LLMs, the CoAScore utilizes multi-aspect knowledge through a CoA (\\\\\\\\textbf{C}hain-\\\\\\\\textbf{o}f-\\\\\\\\textbf{A}spects) prompting framework when assessing the quality of a certain aspect. Specifically, for a given aspect to evaluate, we first prompt the LLM to generate a chain of aspects that are relevant to the target aspect and could be useful for the evaluation. We then collect evaluation scores for each generated aspect, and finally, leverage the knowledge of these aspects to improve the evaluation of the target aspect. We evaluate CoAScore across five NLG evaluation tasks (e.g., summarization, dialog response generation, etc) and nine aspects (e.g., overall quality, relevance, coherence, etc). Our experimental findings highlight that, in comparison to individual aspect evaluation, CoAScore exhibits a higher correlation with human judgments. This improvement significantly outperforms existing unsupervised evaluation metrics, whether for assessing overall quality or other aspects. We also conducted extensive ablation studies to validate the effectiveness of the three stages within the CoAScore framework and conducted case studies to show how the LLM performs in these stages. Our code and scripts are available.\"\\n      },\\n      {\\n        \"paperId\": \"0cc88cef19caa4a22639e2393407258489aa61fb\",\\n        \"url\": \"https://www.semanticscholar.org/paper/0cc88cef19caa4a22639e2393407258489aa61fb\",\\n        \"title\": \"Thematic Analysis and Artificial Intelligence: A Step-by-Step Process for Using ChatGPT in Thematic Analysis\",\\n        \"citationCount\": 20,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1177/16094069251333886?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1177/16094069251333886, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": null\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2025-04-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2356693836\",\\n            \"name\": \"Muhammad Naeem\"\\n          },\\n          {\\n            \"authorId\": \"2357287042\",\\n            \"name\": \"Tracy Smith\"\\n          },\\n          {\\n            \"authorId\": \"2357851988\",\\n            \"name\": \"Lorna Thomas\"\\n          }\\n        ],\\n        \"abstract\": \"This study sets out how to use generative artificial intelligence (AI) in the six steps of systematic thematic analysis. It leverages AI to address the limitations of traditional thematic analysis. This paper developed prompts (inputs) for ChatGPT (a generative AI chatbot based on a large language model) that are based on many researchers\\\\u2019 discussions and criticisms of qualitative data analysis. The contributions of this paper are twofold. First, it addresses a critical research gap by showcasing ChatGPT prompts for each step of the six steps of systematic thematic analysis, which also addresses researcher training in thematic analysis. Second, it contributes to the development of input to train AI in thematic analysis, including a description of how to familiarize an AI system with the context of a research study and the researcher\\\\u2019s methodological and theoretical considerations; this approach helps to reduce human bias and improves accountability and transparency in thematic analysis.\"\\n      },\\n      {\\n        \"paperId\": \"87fd1c20910a10f4c98e5da9c5144d7231de7901\",\\n        \"url\": \"https://www.semanticscholar.org/paper/87fd1c20910a10f4c98e5da9c5144d7231de7901\",\\n        \"title\": \"Thematic analysis of interview data with ChatGPT: designing and testing a reliable research protocol for qualitative research\",\\n        \"citationCount\": 12,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {\\'abstract\\'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1007/s11135-025-02199-3?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1007/s11135-025-02199-3, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"The ChatGPT model in its current state is unable to substitute the contextual insights and subtle metaphorical nuances associated with human qualitative analysis, interpretation and reflexivity, and the protocol design is able to reliably identify different thematic patterns emerging from the text.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Review\"\\n        ],\\n        \"publicationDate\": \"2025-06-20\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2365893719\",\\n            \"name\": \"Manuel Goyanes\"\\n          },\\n          {\\n            \"authorId\": \"2279054737\",\\n            \"name\": \"Carlos Lopezosa\"\\n          },\\n          {\\n            \"authorId\": \"2142957088\",\\n            \"name\": \"Beatriz Jord\\\\u00e1\"\\n          }\\n        ],\\n        \"abstract\": null\\n      },\\n      {\\n        \"paperId\": \"973b86e015b48e1f4a77f8dacfe4364e3959acd5\",\\n        \"url\": \"https://www.semanticscholar.org/paper/973b86e015b48e1f4a77f8dacfe4364e3959acd5\",\\n        \"title\": \"From Overdiagnosis to Overtreatment of Low-Risk Thyroid Cancer: A Thematic Analysis of Attitudes and Beliefs of Endocrinologists, Surgeons, and Patients\",\\n        \"citationCount\": 69,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7232663\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1089/thy.2019.0587?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1089/thy.2019.0587, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"Surgeons\\' and endocrinologists\\' attitudes and beliefs about overtreatment focus on the automaticity of overdiagnosis, and both patients and providers are cognizant of the cascade of clinical events that propel patients from incidental discovery of a thyroid nodule to surgery.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2020-01-07\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"1481094858\",\\n            \"name\": \"Catherine B. Jensen\"\\n          },\\n          {\\n            \"authorId\": \"17311230\",\\n            \"name\": \"Megan C. Saucke\"\\n          },\\n          {\\n            \"authorId\": \"4717672\",\\n            \"name\": \"D. Francis\"\\n          },\\n          {\\n            \"authorId\": \"113479047\",\\n            \"name\": \"C. Voils\"\\n          },\\n          {\\n            \"authorId\": \"3861366\",\\n            \"name\": \"Susan C. Pitt\"\\n          }\\n        ],\\n        \"abstract\": \"Introduction: The optimal management for patients with small, low-risk thyroid cancer is often debated. We aimed to characterize the attitudes and beliefs of providers and patients about management of small, low-risk thyroid cancer and how they relate to overtreatment. Methods: We conducted 34 semi-structured interviews with surgeons (n\\\\u2009=\\\\u200912), endocrinologists (n\\\\u2009=\\\\u200912), and patients with <1.5\\\\u2009cm papillary thyroid cancer (n\\\\u2009=\\\\u200910). Interviews probed about diagnosis and treatment decision-making, including nonoperative options. We used thematic analysis to identify themes related to overtreatment and created concept diagrams to map observed relationships between themes. Results: When providers discussed management of small, low-risk thyroid cancer, most felt that overtreatment was a problem, and some brought it up without prompting. Providers often believed that overtreatment results from overdiagnosis and relayed how the process commonly starts with incidental discovery of a thyroid nodule on imaging. Providers viewed biopsy of the nodule as a reflexive or habitual action. They ascribed inappropriate biopsy to lack of adherence to or knowledge of guidelines, radiologist recommendations, and the desire of patients and physicians to minimize diagnostic uncertainty. Providers described subsequent cancer diagnosis as an event that \\\\u201copens Pandora\\'s box\\\\u201d and often provokes a strong instinctive, culturally rooted need to proceed with surgery\\\\u2014specifically total thyroidectomy. Consequently, most providers felt that it is easier to prevent overdiagnosis than overtreatment and recommended strategies such as improving guideline adherence, resetting patients\\' expectations, and engaging the media. In contrast, patients did not bring up or openly discuss overtreatment or overdiagnosis. Some patients described the seemingly automatic process from an incidental finding to surgery. Their statements confirmed that the \\\\u201cneed to know\\\\u201d was a major motivation for biopsying their nodule. Patients felt that once they had a cancer diagnosis, surgery was a foregone conclusion. Patients admitted their knowledge about thyroid nodules and cancer was low, leaving room for education about the need for biopsy and less extensive treatment options. Conclusions: Surgeons\\' and endocrinologists\\' attitudes and beliefs about overtreatment focus on the automaticity of overdiagnosis. Both patients and providers are cognizant of the cascade of clinical events that propel patients from incidental discovery of a thyroid nodule to surgery.\"\\n      },\\n      {\\n        \"paperId\": \"2f7899c5c8908c4696fa94d9601f44f019f1a68c\",\\n        \"url\": \"https://www.semanticscholar.org/paper/2f7899c5c8908c4696fa94d9601f44f019f1a68c\",\\n        \"title\": \"Optical coherence tomography confirms non\\\\u2010malignant pigmented lesions in phacomatosis pigmentokeratotica using a support vector machine learning algorithm\",\\n        \"citationCount\": 12,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://onlinelibrary.wiley.com/doi/pdfdirect/10.1111/srt.13377\",\\n          \"status\": \"HYBRID\",\\n          \"license\": \"CCBY\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC10228288, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": null\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2023-05-30\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2213561309\",\\n            \"name\": \"Jenna Lee\"\\n          },\\n          {\\n            \"authorId\": \"2086942038\",\\n            \"name\": \"M. Beirami\"\\n          },\\n          {\\n            \"authorId\": \"1707975\",\\n            \"name\": \"R. Ebrahimpour\"\\n          },\\n          {\\n            \"authorId\": \"51195617\",\\n            \"name\": \"Carolina Puyana\"\\n          },\\n          {\\n            \"authorId\": \"5516425\",\\n            \"name\": \"Maria Tsoukas\"\\n          },\\n          {\\n            \"authorId\": \"150169927\",\\n            \"name\": \"K. Avanaki\"\\n          }\\n        ],\\n        \"abstract\": \"Phacomatosis pigmentokeratotica (PPK), an epidermal nevus syndrome, is characterized by the coexistence of nevus spilus and nevus sebaceus. Within the nevus spilus, an extensive range of atypical nevi of different morphologies may manifest. Pigmented lesions may fulfill the ABCDE criteria for melanoma, which may prompt a physician to perform a full\\\\u2010thickness biopsy.\"\\n      },\\n      {\\n        \"paperId\": \"847ee2ae8b588e94fcc3ff33be0601cd354df75b\",\\n        \"url\": \"https://www.semanticscholar.org/paper/847ee2ae8b588e94fcc3ff33be0601cd354df75b\",\\n        \"title\": \"Professional noticing coherence: exploring relationships between component processes\",\\n        \"citationCount\": 15,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1080/10986065.2021.1977086?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1080/10986065.2021.1977086, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": null,\\n        \"publicationTypes\": null,\\n        \"publicationDate\": \"2021-09-22\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2118528180\",\\n            \"name\": \"Jonathan Thomas\"\\n          },\\n          {\\n            \"authorId\": \"66391853\",\\n            \"name\": \"David M. Dueber\"\\n          },\\n          {\\n            \"authorId\": \"50589798\",\\n            \"name\": \"Molly H. Fisher\"\\n          },\\n          {\\n            \"authorId\": \"71615154\",\\n            \"name\": \"C. Jong\"\\n          },\\n          {\\n            \"authorId\": \"73308684\",\\n            \"name\": \"E. Schack\"\\n          }\\n        ],\\n        \"abstract\": \"ABSTRACT Teacher noticing and related variants have ascended in prominence among the mathematics education research community. While the component processes of such noticing (e.g., attending, interpreting and deciding) have been cast as interrelated, capturing the relationships amongst the components has been more elusive. We focused on the component processes of teacher noticing with particular attention given to interrelatedness. Specifically, we were interested in how and the extent to which the component processes of professional noticing (attending, interpreting, deciding) are thematically connected when preservice elementary teachers are engaged in an assessment approximating professional noticing. We refer to this thematic linkage in this paper as coherence. Our findings suggest a complex interplay between the creation and continuation of themes when enacting professional noticing, and the quality of such noticing.\"\\n      },\\n      {\\n        \"paperId\": \"6312d430e9fd98c23af1198ea0f57e3f6e59d542\",\\n        \"url\": \"https://www.semanticscholar.org/paper/6312d430e9fd98c23af1198ea0f57e3f6e59d542\",\\n        \"title\": \"An Investigation of the Coherence of Oral Narratives: Associations With Mental Health, Social Support and the Coherence of Written Narratives\",\\n        \"citationCount\": 11,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://www.frontiersin.org/articles/10.3389/fpsyg.2020.602725/pdf\",\\n          \"status\": \"GOLD\",\\n          \"license\": \"CCBY\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC7838430, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"The ability to elaborate emotionally and make meaning out of important life events in oral narratives is, to a certain extent, related to better mental health and more social support and thematic coherence may be a relatively stable feature of individuals\\\\u2019 narrative styles that is reflected in narratives of different modalities.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2021-01-13\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"84155118\",\\n            \"name\": \"L. Vanaken\"\\n          },\\n          {\\n            \"authorId\": \"4062885\",\\n            \"name\": \"P. Bijttebier\"\\n          },\\n          {\\n            \"authorId\": \"145065355\",\\n            \"name\": \"D. Hermans\"\\n          }\\n        ],\\n        \"abstract\": \"Research Questions In a first research question, we examined whether the relations that are generally observed between the coherence of written autobiographical narratives and outcomes of mental health and social support, can be replicated for the coherence of oral narratives. Second, we studied whether the coherence of oral narratives is related to the coherence of written narratives. Methods Pearson correlations and t-tests were calculated on data of two separate studies to examine the research questions. Results First, only thematic coherence of oral narratives was significantly, although moderately, negatively associated to symptoms of depression, anxiety and negative social interactions. Second, the coherence of oral narratives was higher than the coherence of written narratives. Only the thematic coherence of oral narratives was positively associated with thematic and total coherence of written narratives. Furthermore, correlations between written and oral narratives were stronger for negative narratives as compared to positive narratives. Discussion The ability to elaborate emotionally and make meaning out of important life events in oral narratives is, to a certain extent, related to better mental health and more social support. Furthermore, thematic coherence may be a relatively stable feature of individuals\\\\u2019 narrative styles that is reflected in narratives of different modalities. Nonetheless, these topics need to be further researched to overcome present limitations.\"\\n      },\\n      {\\n        \"paperId\": \"5442ac92fbdbe515cbb71cd03ac5063ebb1d5e08\",\\n        \"url\": \"https://www.semanticscholar.org/paper/5442ac92fbdbe515cbb71cd03ac5063ebb1d5e08\",\\n        \"title\": \"Deep Learning Model Based on 3D Optical Coherence Tomography Images for the Automated Detection of Pathologic Myopia\",\\n        \"citationCount\": 30,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://www.mdpi.com/2075-4418/12/3/742/pdf?version=1647601223\",\\n          \"status\": \"GOLD\",\\n          \"license\": \"CCBY\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC8947335, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This study aimed to develop an algorithm that uses 3D optical coherence tomography volumetric images (C-scan) to automatically diagnose patients with pathologic myopia and found the model based on EfficientNetB4 showed the best performance.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2022-03-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2115235853\",\\n            \"name\": \"S. Park\"\\n          },\\n          {\\n            \"authorId\": \"2380782\",\\n            \"name\": \"T. Ko\"\\n          },\\n          {\\n            \"authorId\": \"8593047\",\\n            \"name\": \"Chan-Kee Park\"\\n          },\\n          {\\n            \"authorId\": \"2124919953\",\\n            \"name\": \"Yong-Chan Kim\"\\n          },\\n          {\\n            \"authorId\": \"2159622586\",\\n            \"name\": \"In-Young Choi\"\\n          }\\n        ],\\n        \"abstract\": \"Pathologic myopia causes vision impairment and blindness, and therefore, necessitates a prompt diagnosis. However, there is no standardized definition of pathologic myopia, and its interpretation by 3D optical coherence tomography images is subjective, requiring considerable time and money. Therefore, there is a need for a diagnostic tool that can automatically and quickly diagnose pathologic myopia in patients. This study aimed to develop an algorithm that uses 3D optical coherence tomography volumetric images (C-scan) to automatically diagnose patients with pathologic myopia. The study was conducted using 367 eyes of patients who underwent optical coherence tomography tests at the Ophthalmology Department of Incheon St. Mary\\\\u2019s Hospital and Seoul St. Mary\\\\u2019s Hospital from January 2012 to May 2020. To automatically diagnose pathologic myopia, a deep learning model was developed using 3D optical coherence tomography images. The model was developed using transfer learning based on four pre-trained convolutional neural networks (ResNet18, ResNext50, EfficientNetB0, EfficientNetB4). Grad-CAM was used to visualize features affecting the detection of pathologic myopia. The performance of each model was evaluated and compared based on accuracy, sensitivity, specificity, and area under the receiver operating characteristic curve (AUROC). The model based on EfficientNetB4 showed the best performance (95% accuracy, 93% sensitivity, 96% specificity, and 98% AUROC) in identifying pathologic myopia.\"\\n      },\\n      {\\n        \"paperId\": \"1157efb8ddfbfdbe3e23078ae823507e3b8971ea\",\\n        \"url\": \"https://www.semanticscholar.org/paper/1157efb8ddfbfdbe3e23078ae823507e3b8971ea\",\\n        \"title\": \"Discursive Fields and the Diversity-Coherence Paradox: An Ecological Perspective on the Blockchain Community Discourse\",\\n        \"citationCount\": 23,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.25300/misq/2022/15736?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.25300/misq/2022/15736, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": null,\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2022-09-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2088902\",\\n            \"name\": \"S. Miranda\"\\n          },\\n          {\\n            \"authorId\": \"2111270075\",\\n            \"name\": \"D. Wang\"\\n          },\\n          {\\n            \"authorId\": \"2159799633\",\\n            \"name\": \"Chuan (Annie) Tian\"\\n          }\\n        ],\\n        \"abstract\": \"Innovation breakthroughs prompt sensemaking discourses that promote community learning and socially construct the innovation. Through this discourse, interested actors advance diverse frames, appealing to consumers with disparate preferences but raising concerns for the coherence of that discourse. We unpack this diversity-coherence paradox by recasting coherence as the relatedness of innovation frames and spotlighting the role of discursive fields that circumscribe meaning. Our empirical context is the first six years of blockchain discourse across seven discursive fields. Our research offers three insights in furtherance of an ecological perspective on innovation discourse. First, framing diversity emanates from discursive fields rather than from actors. Second, fields play differentiated roles in the framing process. Enactment fields comprised of actors with direct experience with the technology limit diversity. They do so by erecting walls that circumscribe discourse through imprinting on their original frame and retracting from or abandoning frames learned from other fields. In contrast, mediated fields, in which actors lack direct experience with the technology, enhance diversity. They do so by imitating or learning from other fields and foreshadowing or anticipating the frames used by other fields, thereby building bridges. Third, rather than opposing each other, diversity and coherence coevolve as the diversity induced by mediated fields increases framing redundancies, synthesizing frames into a coherent community understanding of the innovation. Our research signals to the actors who serve as innovation ambassadors and gatekeepers that diverse views of an innovation are not only inevitable, given the many discourse fields in which those views are formulated, but can also be coherent and desirable.\"\\n      },\\n      {\\n        \"paperId\": \"5fe7d2d670adf17a2ae2a34d76129e43fb7c2d9f\",\\n        \"url\": \"https://www.semanticscholar.org/paper/5fe7d2d670adf17a2ae2a34d76129e43fb7c2d9f\",\\n        \"title\": \"The Irish Football Player Pathway: Examining Stakeholder Coherence Throughout and Across the Player Development System\",\\n        \"citationCount\": 20,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://www.frontiersin.org/articles/10.3389/fspor.2022.834633/pdf\",\\n          \"status\": \"GOLD\",\\n          \"license\": \"CCBY\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC8884116, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A lack of stakeholder coherence is highlighted across the Irish player pathway to maximize long-term player development in the future, with findings highlighting the need for organizational intervention and structural change across the pathway.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2022-02-14\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"119033786\",\\n            \"name\": \"Liam Sweeney\"\\n          },\\n          {\\n            \"authorId\": \"6973495\",\\n            \"name\": \"\\\\u00c1. Macnamara\"\\n          },\\n          {\\n            \"authorId\": \"103263632\",\\n            \"name\": \"Daniel M. Horan\"\\n          }\\n        ],\\n        \"abstract\": \"Maximizing the efficiency of the player development system is a strategic priority for any professional football club or association. However, the successful development of a young footballer is largely dependent upon the roles and relationships of the different stakeholders invested in the developmental process. This study examined the level of horizontal (i.e., extent to which stakeholders across a pathway stage work with players in an agreed fashion to optimize their experience) and vertical (i.e., extent to which multiple stages of the pathway are coordinated and build chronologically from previous involvement toward long-term needs) stakeholder coherence throughout the Irish football player pathway following a restructuring of development policies and the implementation of a nationwide academy system between 2016 and 2020 under the Football Association of Ireland\\'s (FAI) Player Development Plan. As a second aim, we explored each of the key stakeholders\\' alignment to academic talent development principles in order to provide practical recommendations for future player and coach development policies. Accordingly, a series of interviews were conducted with 31 key stakeholders currently engaged in the player pathway. These key stakeholders consisted of parents, coaches and members of the FAI as the National Governing Body for football in Ireland. Data were analyzed using Reflexive Thematic Analysis, with findings highlighting a lack of stakeholder coherence across the pathway, both vertically and horizontally. Stakeholders displayed inconsistency in their understanding of the purpose of the player pathway and its long-term strategic aims, as well as demonstrating poor and incohesive relationships with each of the different stakeholders. Moreover, talent development principles between the different stakeholders appeared well-understood overall, although the practical implementation of several of these principles in applied practice did not appear to exist. Results highlight the need for organizational intervention and structural change across the Irish player pathway to maximize long-term player development in the future. Practical implications for the FAI are discussed and recommendations are made to support optimal player development policies moving forward.\"\\n      },\\n      {\\n        \"paperId\": \"e55ed5e7e6f998874e4edeed7f257ec8d148fc11\",\\n        \"url\": \"https://www.semanticscholar.org/paper/e55ed5e7e6f998874e4edeed7f257ec8d148fc11\",\\n        \"title\": \"An Analysis of Cohesion and Coherence of Descriptive Texts Written by Junior High School Students\",\\n        \"citationCount\": 10,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://www.atlantis-press.com/article/125956016.pdf\",\\n          \"status\": \"GOLD\",\\n          \"license\": \"CCBYNC\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.2991/ASSEHR.K.210427.030?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.2991/ASSEHR.K.210427.030, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": null,\\n        \"publicationTypes\": null,\\n        \"publicationDate\": \"2021-04-28\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2121899244\",\\n            \"name\": \"Galis Muthia Zahra\"\\n          },\\n          {\\n            \"authorId\": \"2345026648\",\\n            \"name\": \"Emi Emilia\"\\n          },\\n          {\\n            \"authorId\": \"77566729\",\\n            \"name\": \"Iyen Nurlaelawati\"\\n          }\\n        ],\\n        \"abstract\": \"The study aims to investigate the cohesion and coherence of descriptive texts written by seventh grade junior high school students. This study was conducted in the even semester of the 2019/2020 academic year during the Covid-19 pandemic. This study used a qualitative case study design, and the data were obtained from six texts representing high, middle, and low achiever students. To identify the texts\\\\u2019 cohesion and coherence, the grammar of textual metafunction from systemic functional linguistics (SFL), that is the theme system was used. The findings show that all students had the ability to make descriptive texts in terms of cohesion and coherence. All texts successfully used different types of themes, including topical and textual themes, and thematic progression, including the zigzag and reiteration patterns to create coherence especially at the clause level. Various cohesive devices such as reference, conjunction, lexical, and ellipsis were also used to create a cohesive text. It was also found that the texts written by high achiever students were more coherent than the texts written by middle and low achiever students due to several aspects such as the more diverse pattern and the more frequent use of pattern. In addition, the high and middle achiever texts seemed more cohesive than the low achiever texts due to the high number of cohesive devices used in the middle achiever texts, and high number of conjunctions used in the middle and high achiever texts. Based on the findings, more support is needed from teachers when teaching descriptive text to middle and low achiever students, especially in a pandemic era.\"\\n      },\\n      {\\n        \"paperId\": \"46812cd7024c70dadaaecba7d854252f27b582ec\",\\n        \"url\": \"https://www.semanticscholar.org/paper/46812cd7024c70dadaaecba7d854252f27b582ec\",\\n        \"title\": \"Molecular Contrast Optical Coherence Tomography and Its Applications in Medicine\",\\n        \"citationCount\": 13,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://www.mdpi.com/1422-0067/23/6/3038/pdf?version=1647248458\",\\n          \"status\": \"GOLD\",\\n          \"license\": \"CCBY\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC8949853, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"The recent advances of molecular contrast OCT imaging techniques, the corresponding contrast agents, especially the nanoparticle-based ones, and their applications are reviewed and the properties, design criteria, merit, and demerit of those contrast agents are summarized.\"\\n        },\\n        \"publicationTypes\": [\\n          \"Review\",\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2022-03-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"35035162\",\\n            \"name\": \"Ancong Wang\"\\n          },\\n          {\\n            \"authorId\": \"2105875970\",\\n            \"name\": \"Wenliu Qi\"\\n          },\\n          {\\n            \"authorId\": \"4707531\",\\n            \"name\": \"Tianxin Gao\"\\n          },\\n          {\\n            \"authorId\": \"2118488394\",\\n            \"name\": \"Xiaoying Tang\"\\n          }\\n        ],\\n        \"abstract\": \"The growing need to understand the molecular mechanisms of diseases has prompted the revolution in molecular imaging techniques along with nanomedicine development. Conventional optical coherence tomography (OCT) is a low-cost in vivo imaging modality that provides unique high spatial and temporal resolution anatomic images but little molecular information. However, given the widespread adoption of OCT in research and clinical practice, its robust molecular imaging extensions are strongly desired to combine with anatomical images. A range of relevant approaches has been reported already. In this article, we review the recent advances of molecular contrast OCT imaging techniques, the corresponding contrast agents, especially the nanoparticle-based ones, and their applications. We also summarize the properties, design criteria, merit, and demerit of those contrast agents. In the end, the prospects and challenges for further research and development in this field are outlined.\"\\n      },\\n      {\\n        \"paperId\": \"5b906cd1a282df558db06fb94741c66294c77a43\",\\n        \"url\": \"https://www.semanticscholar.org/paper/5b906cd1a282df558db06fb94741c66294c77a43\",\\n        \"title\": \"The Textual-Visual Thematic Analysis: A Framework to Analyze the Conjunction and Interaction of Visual and Textual Data\",\\n        \"citationCount\": 25,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://nsuworks.nova.edu/cgi/viewcontent.cgi?article=5456&context=tqr\",\\n          \"status\": \"GOLD\",\\n          \"license\": \"CCBYNCSA\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.46743/2160-3715/2022.5456?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.46743/2160-3715/2022.5456, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": null,\\n        \"publicationTypes\": null,\\n        \"publicationDate\": null,\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2158241407\",\\n            \"name\": \"Gabriela Trombeta\"\\n          },\\n          {\\n            \"authorId\": \"120232113\",\\n            \"name\": \"S. Cox\"\\n          }\\n        ],\\n        \"abstract\": \"Visual methods offer an innovative approach to qualitative research through their potential to prompt dialogue, enrich verbal and textual data, and enable participants to communicate about difficult topics. However, the use of visual methods requires that researchers rethink methodological aspects of data generation and analysis, especially when working with participant-generated images. Although there are now many analytical frameworks and guidebooks providing instructions on the analysis of textual and visual materials, detailed descriptions of how these elements are brought together are often missing from research reports, precluding novice and other researchers from understanding how findings were attained. Our aim in this article is to describe and illustrate the Textual-Visual Thematic Analysis (TVTA), a framework we developed to collaboratively analyze the conjunction and interaction of textual and visual data in a photo-elicitation study. Given that the ethical and methodological aspects are deeply entwined, we begin the article by contextualizing the data obtained from the photo-elicitation study and then consider confidentiality and approaches to valuing participants\\' voices. Next, we share the TVTA framework, its procedural implementation, and insights derived from evolving our data analysis approach. We conclude by offering reflections on the limitations and possibilities for future research.\"\\n      },\\n      {\\n        \"paperId\": \"b2f8cbaa22e4a8aac716a6f035f324fd667d7e8f\",\\n        \"url\": \"https://www.semanticscholar.org/paper/b2f8cbaa22e4a8aac716a6f035f324fd667d7e8f\",\\n        \"title\": \"Assessment of COVID-19 pandemic responses in African countries: thematic synthesis of WHO intra-action review reports\",\\n        \"citationCount\": 27,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://bmjopen.bmj.com/content/bmjopen/12/5/e056896.full.pdf\",\\n          \"status\": \"GOLD\",\\n          \"license\": \"CCBY\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC9062458, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"It is suggested that African countries\\\\u2019 response to the COVID-19 pandemic was prompt and may have contributed to the lower cases and deaths in the region compared with countries in other regions.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Review\"\\n        ],\\n        \"publicationDate\": \"2022-05-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"6312621\",\\n            \"name\": \"A. Talisuna\"\\n          },\\n          {\\n            \"authorId\": \"8406874\",\\n            \"name\": \"C. Iwu\"\\n          },\\n          {\\n            \"authorId\": \"6612736\",\\n            \"name\": \"J. Okeibunor\"\\n          },\\n          {\\n            \"authorId\": \"31474564\",\\n            \"name\": \"Mary Stephen\"\\n          },\\n          {\\n            \"authorId\": \"144645159\",\\n            \"name\": \"E. Musa\"\\n          },\\n          {\\n            \"authorId\": \"32861033\",\\n            \"name\": \"B. Herring\"\\n          },\\n          {\\n            \"authorId\": \"6327315\",\\n            \"name\": \"Otim Patrick Cossy Ramadan\"\\n          },\\n          {\\n            \"authorId\": \"15071000\",\\n            \"name\": \"Daniel Yota\"\\n          },\\n          {\\n            \"authorId\": \"6230122\",\\n            \"name\": \"M. Nanyunja\"\\n          },\\n          {\\n            \"authorId\": \"6894832\",\\n            \"name\": \"A. Mpairwe\"\\n          },\\n          {\\n            \"authorId\": \"1442059469\",\\n            \"name\": \"F. Banza\"\\n          },\\n          {\\n            \"authorId\": \"153908087\",\\n            \"name\": \"A. Diallo\"\\n          },\\n          {\\n            \"authorId\": \"1442059350\",\\n            \"name\": \"Roland Kimbi Wango\"\\n          },\\n          {\\n            \"authorId\": \"2129120286\",\\n            \"name\": \"Christian Massidi\"\\n          },\\n          {\\n            \"authorId\": \"134071938\",\\n            \"name\": \"Hilary K. Njenge\"\\n          },\\n          {\\n            \"authorId\": \"2128400616\",\\n            \"name\": \"M. Traore\"\\n          },\\n          {\\n            \"authorId\": \"1442061510\",\\n            \"name\": \"Antonio Oke\"\\n          },\\n          {\\n            \"authorId\": \"1483613748\",\\n            \"name\": \"Boukare Bonkoungou\"\\n          },\\n          {\\n            \"authorId\": \"1381443808\",\\n            \"name\": \"Landry Ndriko Mayigane\"\\n          },\\n          {\\n            \"authorId\": \"6166357\",\\n            \"name\": \"I. Conteh\"\\n          },\\n          {\\n            \"authorId\": \"2129119453\",\\n            \"name\": \"Fekadu Senait\"\\n          },\\n          {\\n            \"authorId\": \"6150958\",\\n            \"name\": \"S. Chungong\"\\n          },\\n          {\\n            \"authorId\": \"10314719\",\\n            \"name\": \"B. Impouma\"\\n          },\\n          {\\n            \"authorId\": \"2121935091\",\\n            \"name\": \"Nsenga Ngoy\"\\n          },\\n          {\\n            \"authorId\": \"6979323\",\\n            \"name\": \"C. Wiysonge\"\\n          },\\n          {\\n            \"authorId\": \"4284508\",\\n            \"name\": \"Z. Yoti\"\\n          },\\n          {\\n            \"authorId\": \"2969537\",\\n            \"name\": \"A. Gueye\"\\n          }\\n        ],\\n        \"abstract\": \"Objectives We conducted a review of intra-action review (IAR) reports of the national response to the COVID-19 pandemic in Africa. We highlight best practices and challenges and offer perspectives for the future. Design A thematic analysis across 10 preparedness and response domains, namely, governance, leadership, and coordination; planning and monitoring; risk communication and community engagement; surveillance, rapid response, and case investigation; infection prevention and control; case management; screening and monitoring at points of entry; national laboratory system; logistics and supply chain management; and maintaining essential health services during the COVID-19 pandemic. Setting All countries in the WHO African Region were eligible for inclusion in the study. National IAR reports submitted by March 2021 were analysed. Results We retrieved IAR reports from 18 African countries. The COVID-19 pandemic response in African countries has relied on many existing response systems such as laboratory systems, surveillance systems for previous outbreaks of highly infectious diseases and a logistics management information system. These best practices were backed by strong political will. The key challenges included low public confidence in governments, inadequate adherence to infection prevention and control measures, shortages of personal protective equipment, inadequate laboratory capacity, inadequate contact tracing, poor supply chain and logistics management systems, and lack of training of key personnel at national and subnational levels. Conclusion These findings suggest that African countries\\\\u2019 response to the COVID-19 pandemic was prompt and may have contributed to the lower cases and deaths in the region compared with countries in other regions. The IARs demonstrate that many technical areas still require immediate improvement to guide decisions in subsequent waves or future outbreaks.\"\\n      },\\n      {\\n        \"paperId\": \"7b61104ddb7302f51dc30a77cfc766b483db2fdc\",\\n        \"url\": \"https://www.semanticscholar.org/paper/7b61104ddb7302f51dc30a77cfc766b483db2fdc\",\\n        \"title\": \"Multi-Temporal Sentinel-1 Backscatter and Coherence for Rainforest Mapping\",\\n        \"citationCount\": 33,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://www.mdpi.com/2072-4292/12/5/847/pdf?version=1583481683\",\\n          \"status\": \"GOLD\",\\n          \"license\": \"CCBY\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.3390/rs12050847?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.3390/rs12050847, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper presents how the interferometric capabilities of the Sentinel-1 satellites constellation can be exploited for the monthly mapping of the Amazon rainforest and an analysis on the benefits of the use of textural information, derived from Sentinel-2 backscatter, in order to enhance the classification accuracy.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2020-03-06\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2049260\",\\n            \"name\": \"A. Pulella\"\\n          },\\n          {\\n            \"authorId\": \"1617617187\",\\n            \"name\": \"Rodrigo Arag\\\\u00e3o Santos\"\\n          },\\n          {\\n            \"authorId\": \"51243828\",\\n            \"name\": \"F. Sica\"\\n          },\\n          {\\n            \"authorId\": \"70278054\",\\n            \"name\": \"Philipp Posovszky\"\\n          },\\n          {\\n            \"authorId\": \"2261116\",\\n            \"name\": \"P. Rizzoli\"\\n          }\\n        ],\\n        \"abstract\": \"This paper reports recent advancements in the field of Synthetic Aperture Radar (SAR) for forest mapping by using interferometric short-time-series. In particular, we first present how the interferometric capabilities of the Sentinel-1 satellites constellation can be exploited for the monthly mapping of the Amazon rainforest. Indeed, the evolution in time of the interferometric coherence can be properly modeled as an exponential decay and the retrieved interferometric parameters can be used, together with the backscatter, as input features to the machine learning Random Forests classifier. Furthermore, we present an analysis on the benefits of the use of textural information, derived from Sentinel-1 backscatter, in order to enhance the classification accuracy. These textures are computed through the Sum And Difference Histograms methodology and the final classification accuracy, resulting by adding them to the aforementioned features, is a thematic map that exceeds an overall agreement of 85%, when validated using the optical external reference Finer Resolution Observation and Monitoring of Global Land Cover (FROM-GLC) map. The experiments presented in the final part of the paper are enriched with a further analysis and discussion on the selected scenes using updated multispectral Sentinel-2 acquisitions.\"\\n      },\\n      {\\n        \"paperId\": \"a37bd8ad2c2356a39fa0271479c6b46e8b108d2b\",\\n        \"url\": \"https://www.semanticscholar.org/paper/a37bd8ad2c2356a39fa0271479c6b46e8b108d2b\",\\n        \"title\": \"\\\\u2018This bloody rona!\\\\u2019: using the digital story completion method and thematic analysis to explore the mental health impacts of COVID-19 in Australia\",\\n        \"citationCount\": 15,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://bmjopen.bmj.com/content/bmjopen/12/1/e057393.full.pdf\",\\n          \"status\": \"GOLD\",\\n          \"license\": \"CCBYNC\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC8764712, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"It is suggested that points of convergence across stories indicate a level of shared experience among participants relating to COVID-19 in Australia, due to intensive media coverage of the pandemic, persistent public health messaging, engagement with social media and instant messaging technologies, and extended lockdowns that impacted the mental health of vast numbers of Australians.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2022-01-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"153603815\",\\n            \"name\": \"Priya Vaughan\"\\n          },\\n          {\\n            \"authorId\": \"40481393\",\\n            \"name\": \"Caroline Lenette\"\\n          },\\n          {\\n            \"authorId\": \"5498463\",\\n            \"name\": \"K. Boydell\"\\n          }\\n        ],\\n        \"abstract\": \"Objective To use the digital story completion method to prompt participants to describe thoughts, fears and mental health experiences in response to a story stem about COVID-19, to capture a specific sociohistoric moment. Design We used digital story completion, a qualitative research method, to gather narratives from Australians coping with physical distancing and social restriction measures. Our reflexive thematic analysis of the data was underpinned by a constructionist approach to reflect the importance of social context in understanding health experiences. Setting Australia. Participants 52 people living in Australia (aged 18 years and over). Results Four meta-themes were prevalent across 52 stories submitted: (1) expressions of mental distress linked to COVID-19; (2) various coping strategies offered by characters in stories; (3) narratives outlining social support offered to alleviate distress; and (4) specialised COVID-19 vocabulary. Conclusion We cautiously propose that points of convergence across stories indicate a level of shared experience among participants relating to COVID-19 in Australia. We suggest this is due to intensive media coverage of the pandemic, persistent public health messaging, engagement with social media and instant messaging technologies, and extended lockdowns that impacted the mental health of vast numbers of Australians.\"\\n      },\\n      {\\n        \"paperId\": \"b3535815a1cd7ed648ea64c591802301822e576a\",\\n        \"url\": \"https://www.semanticscholar.org/paper/b3535815a1cd7ed648ea64c591802301822e576a\",\\n        \"title\": \"Achieving healthy ageing through the perspective of sense of coherence among senior-only households: a qualitative study\",\\n        \"citationCount\": 20,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1080/13607863.2020.1725805?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1080/13607863.2020.1725805, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"It is suggested that sense of coherence towards the pursuit of healthy ageing can be addressed by reducing the unpredictability of ageing-related processes and vulnerabilities (comprehensibility), supporting active adoption of actions which promotes physical, mental and social health (manageability) and individual reflection in making sense of old age to seek motivation in living each day purposefully (meaningfulness).\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2020-02-18\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"14137710\",\\n            \"name\": \"Betsy Seah\"\\n          },\\n          {\\n            \"authorId\": \"6102067\",\\n            \"name\": \"G. Espnes\"\\n          },\\n          {\\n            \"authorId\": \"1870959\",\\n            \"name\": \"E. Ang\"\\n          },\\n          {\\n            \"authorId\": \"71772268\",\\n            \"name\": \"Jian Yang Lim\"\\n          },\\n          {\\n            \"authorId\": \"6631742\",\\n            \"name\": \"Y. Kowitlawakul\"\\n          },\\n          {\\n            \"authorId\": \"2395324152\",\\n            \"name\": \"Wenru Wang\"\\n          }\\n        ],\\n        \"abstract\": \"Abstract Objectives: Explore perceptions towards healthy ageing through the perspective of sense of coherence among older adults residing in senior-only households. Methods: A qualitative study using focus group interviews was conducted and appreciative inquiry was adopted as a strengths-based interviewing approach. 27 older adults who either live alone or with their spouses only were involved in six focus group discussions at a community centre in Singapore. Data saturation was achieved and thematic analysis was performed to analyse the data. Results: The four emerging themes were (1) contending evolving vulnerabilities, (2) intrinsic value of health, (3) taking care of oneself is a personal responsibility, and (4) taking one day at a time: outlook towards later part of life. Older adults\\\\u2019 underlying pathogenic orientation towards health contributed to their perceived unpredictable confrontations with vicissitudes including illness and death. This played a part to their short outlook towards old age. Consequently, this could limit their will and abilities to seek meaningful pursuits or valued aspirations and movement towards the salutogenic health pole. Conclusion: By reframing the definition of health to pursuing and fulfilling valued accomplishments, optimal health can be achieved regardless of physical health state. This study suggested that sense of coherence towards the pursuit of healthy ageing can be addressed by reducing the unpredictability of ageing-related processes and vulnerabilities (comprehensibility), supporting active adoption of actions which promotes physical, mental and social health (manageability) and individual reflection in making sense of old age to seek motivation in living each day purposefully (meaningfulness).\"\\n      },\\n      {\\n        \"paperId\": \"dd80b198ab64a4c04cde06ed5cf31662270b86af\",\\n        \"url\": \"https://www.semanticscholar.org/paper/dd80b198ab64a4c04cde06ed5cf31662270b86af\",\\n        \"title\": \"The experience of using prompting technology from the perspective of people with Dementia and their primary carers\",\\n        \"citationCount\": 26,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1080/13607863.2020.1745145?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1080/13607863.2020.1745145, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"Carers and participants were influenced by their approaches to technology and determined the usefulness of the prompter according to whether it worked for them and fitted into their routines, consistent with the Technology Acceptance Model.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2020-03-30\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"145617572\",\\n            \"name\": \"N. Evans\"\\n          },\\n          {\\n            \"authorId\": \"48063215\",\\n            \"name\": \"H. Boyd\"\\n          },\\n          {\\n            \"authorId\": \"48688534\",\\n            \"name\": \"N. Harris\"\\n          },\\n          {\\n            \"authorId\": \"38349961\",\\n            \"name\": \"K. Noonan\"\\n          },\\n          {\\n            \"authorId\": \"40227345\",\\n            \"name\": \"T. Ingram\"\\n          },\\n          {\\n            \"authorId\": \"48756330\",\\n            \"name\": \"A. Jarvis\"\\n          },\\n          {\\n            \"authorId\": \"1613011708\",\\n            \"name\": \"J. Ridgers\"\\n          },\\n          {\\n            \"authorId\": \"144757940\",\\n            \"name\": \"R. Cheston\"\\n          }\\n        ],\\n        \"abstract\": \"Abstract Objectives People who are living with dementia typically experience difficulties in completing multi-step, everyday tasks. However, digital technology such as touchscreen tablets provide a means of delivering concise personalised prompts that combine audio, text and pictures. This study was one component of a broader, mixed methods study that tested how an application (app) \\\\u2013based prompter running on a touchscreen tablet computer could support everyday activities in individuals with mild to moderate dementia. In this study we set out to understand the experiences of people living with dementia and their primary carer in using the prompter over a four-week period. Method We collected qualitative data using semi-structured interviews from 26 dyads, composed of a person living with dementia and their carer. Dyads were interviewed at the start and end of this period. Transcripts were then analysed using thematic analysis. Results The study identified three overarching themes related to: participants\\\\u2019 attitudes towards the technology; their judgements about how useful the prompter would be; and the emotional impact of using it. Conclusion Consistent with the Technology Acceptance Model, carers and participants were influenced by their approaches to technology and determined the usefulness of the prompter according to whether it worked for them and fitted into their routines. In addition, participants\\\\u2019 decisions about using the prompter were also determined by the extent to which doing so would impact on their self-identity.\"\\n      },\\n      {\\n        \"paperId\": \"fb1d85fe28b5e92e22d084eca674d4a2b48cdc5a\",\\n        \"url\": \"https://www.semanticscholar.org/paper/fb1d85fe28b5e92e22d084eca674d4a2b48cdc5a\",\\n        \"title\": \"Prompting to Distill: Boosting Data-Free Knowledge Distillation via Reinforced Prompt\",\\n        \"citationCount\": 12,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://arxiv.org/pdf/2205.07523\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2205.07523, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"PromptDFD leverages a pre-trained generative model to provide language priors and introduces a reinforced topic prompter to control data synthesis, making the generated samples thematically relevant and semantically plausible, and thus friendly to downstream tasks.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2022-05-16\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"15532066\",\\n            \"name\": \"Xinyin Ma\"\\n          },\\n          {\\n            \"authorId\": \"48631088\",\\n            \"name\": \"Xinchao Wang\"\\n          },\\n          {\\n            \"authorId\": \"150110431\",\\n            \"name\": \"Gongfan Fang\"\\n          },\\n          {\\n            \"authorId\": \"1471660296\",\\n            \"name\": \"Yongliang Shen\"\\n          },\\n          {\\n            \"authorId\": \"1776903\",\\n            \"name\": \"Weiming Lu\"\\n          }\\n        ],\\n        \"abstract\": \"Data-free knowledge distillation (DFKD) conducts knowledge distillation via eliminating the dependence of original training data, and has recently achieved impressive results in accelerating pre-trained language models. At the heart of DFKD is to reconstruct a synthetic dataset by inverting the parameters of the uncompressed model. Prior DFKD approaches, however, have largely relied on hand-crafted priors of the target data distribution for the reconstruction, which can be inevitably biased and often incompetent to capture the intrinsic distributions. To address this problem, we propose a prompt-based method, termed as PromptDFD, that allows us to take advantage of learned language priors, which effectively harmonizes the synthetic sentences to be semantically and grammatically correct. Specifically, PromptDFD leverages a pre-trained generative model to provide language priors and introduces a reinforced topic prompter to control data synthesis, making the generated samples thematically relevant and semantically plausible, and thus friendly to downstream tasks. As shown in our experiments, the proposed method substantially improves the synthesis quality and achieves considerable improvements on distillation performance. In some cases, PromptDFD even gives rise to results on par with those from the data-driven knowledge distillation with access to the original training data.\"\\n      },\\n      {\\n        \"paperId\": \"89bbc885e69ff2098b478893cf2e17d8a13464ff\",\\n        \"url\": \"https://www.semanticscholar.org/paper/89bbc885e69ff2098b478893cf2e17d8a13464ff\",\\n        \"title\": \"Rethinking sense of coherence: Perceptions of comprehensibility, manageability, and meaningfulness in a group of Palestinian health care providers operating in the West Bank and Israel\",\\n        \"citationCount\": 12,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1177/1363461520941386?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1177/1363461520941386, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"It is found that context-specific features of SOC can mobilize generalized resistance resources for coping with traumatic and stressful experiences, even in an environment characterized by political instability, military violence, and social trauma.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2020-08-26\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"35149021\",\\n            \"name\": \"G. Veronese\"\\n          },\\n          {\\n            \"authorId\": \"1912267580\",\\n            \"name\": \"Yamina Dhaouadi\"\\n          },\\n          {\\n            \"authorId\": \"51039166\",\\n            \"name\": \"Abdelhamid Afana\"\\n          }\\n        ],\\n        \"abstract\": \"Drawing on a salutogenic perspective, we explored sense of coherence (SOC) in a group of Palestinian mental health care providers living and working in Israel and the occupied Palestinian territories (West Bank). Specifically, we conducted a qualitative exploration of the cultural characteristics of SOC and its components (comprehensibility, manageability, and meaningfulness) in two groups of Palestinian Muslim helpers. We found that context-specific features of SOC can mobilize generalized resistance resources for coping with traumatic and stressful experiences, even in an environment characterized by political instability, military violence, and social trauma. Ten main themes emerged from the thematic content analysis: acceptance, reacting to adversity, acknowledging human insecurity (comprehensibility), self-control, talking to family, education as a resource for survival, connecting to the severity of the event, responsibility as a source of control (manageability), religiosity, and sense of belonging (meaningfulness). The Islamic faith, as expressed through the concepts of Sumud and Taslim, seemed to permeate individuals\\\\u2019 ability to attribute meaning to historical and transgenerational trauma, as well as to their ongoing traumatic conditions, thus acting as their ultimate source of health and wellbeing. A holistic, spiritual, and collectivist outlook helped respondents to approach their lives with optimism. We discuss the implications for mental health care providers and future research directions.\"\\n      },\\n      {\\n        \"paperId\": \"9adfdd1b97b98638b23dc1e08842143136dea497\",\\n        \"url\": \"https://www.semanticscholar.org/paper/9adfdd1b97b98638b23dc1e08842143136dea497\",\\n        \"title\": \"Joint Segmentation and Quantification of Chorioretinal Biomarkers in Optical Coherence Tomography Scans: A Deep Learning Approach\",\\n        \"citationCount\": 44,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {\\'tldr\\'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/TIM.2021.3077988?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/TIM.2021.3077988, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": null,\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": null,\\n        \"authors\": [\\n          {\\n            \"authorId\": \"15356244\",\\n            \"name\": \"Bilal Hassan\"\\n          },\\n          {\\n            \"authorId\": \"121165014\",\\n            \"name\": \"S. Qin\"\\n          },\\n          {\\n            \"authorId\": \"38509414\",\\n            \"name\": \"Taimur Hassan\"\\n          },\\n          {\\n            \"authorId\": \"40988624\",\\n            \"name\": \"Ramsha Ahmed\"\\n          },\\n          {\\n            \"authorId\": \"1802072\",\\n            \"name\": \"N. Werghi\"\\n          }\\n        ],\\n        \"abstract\": \"In ophthalmology, chorioretinal biomarkers (CRBMs) play a significant role in detecting, quantifying, and ameliorating the treatment of chronic eye conditions. Optical coherence tomography (OCT) imaging is primarily used for investigating various CRBMs and prompt intervention of retinal conditions. However, with extensive clinical applications and increasing prevalence of ocular diseases, the number of OCT scans obtained globally exceeds ophthalmologists\\\\u2019 capacity to examine these in a meaningful manner. Instead, the emergence of deep learning provides a cost-effective and reliable alternative for automated analysis of scans, assisting ophthalmologists in clinical routines and research. This article presents a residual learning-based framework (RASP-Net) that integrates atrous spatial pyramid pooling, coherent preprocessing, and postprocessing mechanisms to achieve joint segmentation and quantification of 11 CRBMs. We used a total of 7000 annotated scans for training, validation, and testing purposes of RASP-Net. Moreover, a novel algorithm for 3-D macular profiles reconstruction is presented to give a more intuitive way for characterizing the CRBMs based on coarse contouring and quantification. The proposed framework is evaluated through several experiments using different performance metrics. The results presented in this study validate the optimal performance of RASP-Net in precise detection and segmentation of CRBMs, with mean balanced accuracy, intersection over union, and dice score values of 0.916, 0.634, and 0.776 respectively. The proposed RASP-Net model characterizes a wide range of CRBMs with fine-grained pixelwise segmentation, extraction, and quantification in the context of retinal pathologies. This proposed system can allow retina experts to monitor the improvement and deterioration of the underlying ocular conditions.\"\\n      }\\n    ]\\n  }\\n}', additional_kwargs={}, response_metadata={}, id='72bc78d3-e637-430e-b36d-02b9a73baa04')]}\n",
      "\u001b[1m[updates]\u001b[0m {'prior_work_analysis': {'messages': [AIMessage(content='{\\n  \"papers\": []\\n}', additional_kwargs={}, response_metadata={}, id='bc1324e1-1ae2-466e-9e21-dec6d67b8895')]}}\n",
      "\u001b[1m[values]\u001b[0m {'messages': [HumanMessage(content=\"Dynamic Prompt Adaptation:\\n                Problem: Large Language Models (LLMs) often struggle with maintaining coherence over extended interactions or creative tasks, leading to thematic inconsistencies and reduced reader engagement.\\n                Existing Methods: Current methods often use fixed prompts or few-shot examples, which may not adapt to the evolving context of a conversation or creative narrative. Techniques such as Chain-of-Thought prompting are utilized, but they do not inherently address continuity and adaptability across interactions.\\n                Motivation: Human creative writing often involves iterative dialogue and adaptation to the flow of discussion. The style and context of interactions can shift dynamically based on prior exchanges. Dynamic adaptation mirrors how authors and conversationalists adjust based on audience feedback and shifting themes.\\n                Proposed Method: We propose Dynamic Prompt Adaptation, involving three phases: (1) Contextual Analysis - Analyze previous outputs and user prompts to extract key themes and tonal shifts, applying a prompting structure like 'Reflect on the previous topic of [theme] and build on it.' (2) Adaptive Prompt Generation - Using insights from the analysis, generate updated prompts that introduce new elements or clarify past responses, e.g., 'Continuing from your last thought on [theme], can you expand on how this might be represented in [new context]?' (3) Iterative Context Update - As the dialogue progresses, generate a synthesis of all prior interactions to maintain thematic coherence, prompting with 'Summarize the key points discussed so far to keep track of our narrative.'\\n                Experiment Plan: Test against static prompting strategies by evaluating engagement scores, coherence assessments, and user satisfaction in storytelling scenarios using standard text generation metrics such as BLEU and ROUGE. Incorporate user feedback on naturalness and adaptability during the interaction. Datasets could include the 'Story Cloze Test' dataset and user-generated dialogue interactions scraped from platforms like Reddit to assess conversational engagement.\", additional_kwargs={}, response_metadata={}, id='63ebf44d-2223-4537-afd7-c4eb3eec973e'), AIMessage(content='{\\n  \"research_question\": \"How can dynamic prompt adaptation maintain thematic coherence and reader engagement across extended LLM conversations and creative narratives?\",\\n  \"problem_domain\": \"natural language processing and dialogue systems\",\\n  \"methodology_keywords\": [\\n    \"Dynamic Prompt Adaptation\",\\n    \"Contextual Analysis\",\\n    \"Adaptive Prompt Generation\",\\n    \"Iterative Context Update\",\\n    \"thematic coherence\",\\n    \"narrative continuity\",\\n    \"long-context dialogue\"\\n  ],\\n  \"key_concepts\": [\\n    \"Dynamic Prompt Adaptation\",\\n    \"Contextual Analysis of previous outputs and prompts\",\\n    \"Adaptive Prompt Generation\",\\n    \"Iterative Context Update\",\\n    \"thematic coherence\",\\n    \"audience feedback and shifting themes\",\\n    \"prompt templating for theme reflection\",\\n    \"synthesis of prior interactions\"\\n  ],\\n  \"existing_methods\": [\\n    \"fixed prompts\",\\n    \"few-shot examples\",\\n    \"Chain-of-Thought prompting\"\\n  ],\\n  \"claimed_novelty\": [\\n    \"Dynamic Prompt Adaptation to maintain coherence across extended interactions and creative tasks\",\\n    \"Three-phase framework: Contextual Analysis; Adaptive Prompt Generation; Iterative Context Update\",\\n    \"Iterative synthesis of all prior interactions to maintain thematic coherence\",\\n    \"Evaluation against static prompting strategies using BLEU and ROUGE on Story Cloze Test and Reddit dialogues\"\\n  ]\\n}', additional_kwargs={}, response_metadata={}, id='35ea71c3-1efb-4474-a8bb-b51930753e82'), AIMessage(content='{\\n  \"queries\": [\\n    {\\n      \"query_string\": \"dynamic prompt adaptation coherence\",\\n      \"rationale\": \"Targets work on dynamic prompt adaptation to sustain thematic coherence across extended LLM conversations and narratives\",\\n      \"priority_concept\": \"Dynamic Prompt Adaptation\"\\n    },\\n    {\\n      \"query_string\": \"long context dialogue\",\\n      \"rationale\": \"Searches for literature on maintaining coherence in long-context dialogues and continuous narratives\",\\n      \"priority_concept\": \"Long-context dialogue\"\\n    },\\n    {\\n      \"query_string\": \"adaptive prompt generation narrative\",\\n      \"rationale\": \"Looks for studies on generating prompts adaptively to guide narrative tasks and creative outputs\",\\n      \"priority_concept\": \"Adaptive Prompt Generation\"\\n    },\\n    {\\n      \"query_string\": \"iterative context update\",\\n      \"rationale\": \"Finds work on iterative updates to context across turns for consistency and coherence\",\\n      \"priority_concept\": \"Iterative Context Update\"\\n    },\\n    {\\n      \"query_string\": \"thematic coherence prompting\",\\n      \"rationale\": \"Identifies prompting strategies that preserve or reflect themes within extended interactions\",\\n      \"priority_concept\": \"Thematic coherence\"\\n    }\\n  ]\\n}', additional_kwargs={}, response_metadata={}, id='44970bb0-ab27-4ee3-8006-a6ceeaa39143'), AIMessage(content='{\\n  \"dynamic prompt adaptation coherence\": {\\n    \"total\": 99,\\n    \"offset\": 0,\\n    \"next\": 50,\\n    \"data\": [\\n      {\\n        \"paperId\": \"246482d9758e93d0b349e2926996d887417174d8\",\\n        \"url\": \"https://www.semanticscholar.org/paper/246482d9758e93d0b349e2926996d887417174d8\",\\n        \"title\": \"DPCore: Dynamic Prompt Coreset for Continual Test-Time Adaptation\",\\n        \"citationCount\": 13,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2406.10737, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"DPCore is proposed, a method designed for robust performance across diverse domain change patterns while ensuring computational efficiency, and it is demonstrated that DPCore consistently outperforms various CTTA methods.\"\\n        },\\n        \"publicationTypes\": null,\\n        \"publicationDate\": \"2024-06-15\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2221081346\",\\n            \"name\": \"Yunbei Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2090707280\",\\n            \"name\": \"Akshay Mehra\"\\n          },\\n          {\\n            \"authorId\": \"2261749247\",\\n            \"name\": \"Shuaicheng Niu\"\\n          },\\n          {\\n            \"authorId\": \"4832622\",\\n            \"name\": \"Jihun Hamm\"\\n          }\\n        ],\\n        \"abstract\": \"Continual Test-Time Adaptation (CTTA) seeks to adapt source pre-trained models to continually changing, unseen target domains. While existing CTTA methods assume structured domain changes with uniform durations, real-world environments often exhibit dynamic patterns where domains recur with varying frequencies and durations. Current approaches, which adapt the same parameters across different domains, struggle in such dynamic conditions-they face convergence issues with brief domain exposures, risk forgetting previously learned knowledge, or misapplying it to irrelevant domains. To remedy this, we propose DPCore, a method designed for robust performance across diverse domain change patterns while ensuring computational efficiency. DPCore integrates three key components: Visual Prompt Adaptation for efficient domain alignment, a Prompt Coreset for knowledge preservation, and a Dynamic Update mechanism that intelligently adjusts existing prompts for similar domains while creating new ones for substantially different domains. Extensive experiments on four benchmarks demonstrate that DPCore consistently outperforms various CTTA methods, achieving state-of-the-art performance in both structured and dynamic settings while reducing trainable parameters by 99% and computation time by 64% compared to previous approaches.\"\\n      },\\n      {\\n        \"paperId\": \"6b533de65b3d2cee190415f5a2a2e6afe2ac7c78\",\\n        \"url\": \"https://www.semanticscholar.org/paper/6b533de65b3d2cee190415f5a2a2e6afe2ac7c78\",\\n        \"title\": \"Dynamic Adapter Meets Prompt Tuning: Parameter-Efficient Transfer Learning for Point Cloud Analysis\",\\n        \"citationCount\": 41,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://arxiv.org/pdf/2403.01439\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2403.01439, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper freezes the parameters of the default pretrained models and proposes the Dynamic Adapter, which generates a dynamic scale for each token, considering the token significance to the downstream task, and seamlessly integrates Dynamic Adapter with Prompt Tuning (DAPT) by constructing Internal Prompts.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-03-03\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2238201928\",\\n            \"name\": \"Xin Zhou\"\\n          },\\n          {\\n            \"authorId\": \"94882716\",\\n            \"name\": \"Dingkang Liang\"\\n          },\\n          {\\n            \"authorId\": \"2284804294\",\\n            \"name\": \"Wei Xu\"\\n          },\\n          {\\n            \"authorId\": \"2284636478\",\\n            \"name\": \"Xingkui Zhu\"\\n          },\\n          {\\n            \"authorId\": \"2290029091\",\\n            \"name\": \"Yihan Xu\"\\n          },\\n          {\\n            \"authorId\": \"2287848763\",\\n            \"name\": \"Zhikang Zou\"\\n          },\\n          {\\n            \"authorId\": \"2238115894\",\\n            \"name\": \"Xiang Bai\"\\n          }\\n        ],\\n        \"abstract\": \"Point cloud analysis has achieved outstanding performance by transferring point cloud pretrained models. However, existing methods for model adaptation usually update all model parameters, i.e., full fine-tuning paradigm, which is inefficient as it relies on high computational costs (e.g., training GPU memory) and massive storage space. In this paper, we aim to study parameter-efficient transfer learning for point cloud analysis with an ideal tradeoff between task performance and parameter efficiency. To achieve this goal, we freeze the parameters of the default pretrained models and then propose the Dynamic Adapter, which generates a dynamic scale for each token, considering the token significance to the downstream task. We further seamlessly integrate Dynamic Adapter with Prompt Tuning (DAPT) by constructing Internal Prompts, capturing the instance-specific features for interaction. Extensive experiments conducted on five challenging datasets demonstrate that the proposed DAPT achieves superior performance compared to the full fine-tuning counterparts while significantly reducing the trainable parameters and training GPU memory by 95% and 35%, respectively. Code is available at https://github.com/LMD0311/DAPT.\"\\n      },\\n      {\\n        \"paperId\": \"eacb61136023a2f30c5a0313f222d50e5f63ac9b\",\\n        \"url\": \"https://www.semanticscholar.org/paper/eacb61136023a2f30c5a0313f222d50e5f63ac9b\",\\n        \"title\": \"Domain Prompt Tuning via Meta Relabeling for Unsupervised Adversarial Adaptation\",\\n        \"citationCount\": 14,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {\\'tldr\\'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/TMM.2023.3272742?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/TMM.2023.3272742, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": null,\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": null,\\n        \"authors\": [\\n          {\\n            \"authorId\": \"1485402830\",\\n            \"name\": \"Xin Jin\"\\n          },\\n          {\\n            \"authorId\": \"40093162\",\\n            \"name\": \"Cuiling Lan\"\\n          },\\n          {\\n            \"authorId\": \"1634494276\",\\n            \"name\": \"Wenjun Zeng\"\\n          },\\n          {\\n            \"authorId\": \"31482866\",\\n            \"name\": \"Zhibo Chen\"\\n          }\\n        ],\\n        \"abstract\": \"Unsupervised adversarial domain adaptation (ADA) aims to learn domain-invariant features by confusing a domain discriminator. As training goes on, the feature distributions of source and target samples are increasingly aligned/indistinguishable. The discrimination capability of the domain discriminator w.r.t. those aligned samples deteriorates due to the domain label of each sample is still fixed all through the learning process, which thus cannot effectively further drive the feature learning. A recently proposed method named Re-enforceable Adversarial Domain Adaptation (RADA) [1] tend to re-energize the domain discriminator during the training by using dynamic domain labels. Specifically, RADA sets up a heuristic criterion and uses it to relabel the well aligned target domain samples as source domain samples on the fly. In our study, we identify a critical problem of RADA: it is a kind of heuristic domain data re-partition solution without explicitly serving the adaptation task itself, suggesting that the criteria of RADA on which sample should be relabeled is hard to decide. To address the problem, we revisit domain relabeling process from a perspective of prompt tuning, and introduce a meta-optimized learnable prompts into RADA to replace some hand-craft designs in dynamic relabeling process, which scheme is named as RADA-prompt. Particularly, we employ a module of meta-prompter, which learns to adaptively relabel the samples based on the objective of serving UDA task. To train the meta-prompter, we leverage a domain alignment measurement and a classification measurement as the meta optimization objective. Extensive experiments on multiple unsupervised domain adaptation benchmarks demonstrate the effectiveness and superiority of RADA-prompt, this scheme also achieves state-of-the-art performance.\"\\n      },\\n      {\\n        \"paperId\": \"165503c48e553a5559190ce74cda823f4e166b54\",\\n        \"url\": \"https://www.semanticscholar.org/paper/165503c48e553a5559190ce74cda823f4e166b54\",\\n        \"title\": \"Decorate the Newcomers: Visual Domain Prompt for Continual Test Time Adaptation\",\\n        \"citationCount\": 128,\\n        \"openAccessPdf\": {\\n          \"url\": \"http://arxiv.org/pdf/2212.04145\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2212.04145, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper proposes to learn an image-layer visual domain prompt for target domains while having the source model parameters frozen, and designs a homeostasis-based adaptation strategy to suppress domain-sensitive parameters in domain-invariant prompts to learn domain-shared knowledge more effectively.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2022-12-08\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2142286278\",\\n            \"name\": \"Yulu Gan\"\\n          },\\n          {\\n            \"authorId\": \"1387903470\",\\n            \"name\": \"Xianzheng Ma\"\\n          },\\n          {\\n            \"authorId\": \"9920529\",\\n            \"name\": \"Yihang Lou\"\\n          },\\n          {\\n            \"authorId\": \"145079192\",\\n            \"name\": \"Yan Bai\"\\n          },\\n          {\\n            \"authorId\": \"2115713503\",\\n            \"name\": \"Renrui Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2194622137\",\\n            \"name\": \"Nian Shi\"\\n          },\\n          {\\n            \"authorId\": \"2194260623\",\\n            \"name\": \"Lin Luo\"\\n          }\\n        ],\\n        \"abstract\": \"Continual Test-Time Adaptation (CTTA) aims to adapt the source model to continually changing unlabeled target domains without access to the source data. Existing methods mainly focus on model-based adaptation in a self-training manner, such as predicting pseudo labels for new domain datasets. Since pseudo labels are noisy and unreliable, these methods suffer from catastrophic forgetting and error accumulation when dealing with dynamic data distributions. Motivated by the prompt learning in NLP, in this paper, we propose to learn an image-layer visual domain prompt for target domains while having the source model parameters frozen. During testing, the changing target datasets can be adapted to the source model by reformulating the input data with the learned visual prompts. Specifically, we devise two types of prompts, i.e., domains-specific prompts and domains-agnostic prompts, to extract current domain knowledge and maintain the domain-shared knowledge in the continual adaptation. Furthermore, we design a homeostasis-based adaptation strategy to suppress domain-sensitive parameters in domain-invariant prompts to learn domain-shared knowledge more effectively. This transition from the model-dependent paradigm to the model-free one enables us to bypass the catastrophic forgetting and error accumulation problems. Experiments show that our proposed method achieves significant performance gains over state-of-the-art methods on four widely-used benchmarks, including CIFAR-10C, CIFAR-100C, ImageNet-C, and VLCS datasets.\"\\n      },\\n      {\\n        \"paperId\": \"5db3cfc974c42bfa2d9518a8910762790b516037\",\\n        \"url\": \"https://www.semanticscholar.org/paper/5db3cfc974c42bfa2d9518a8910762790b516037\",\\n        \"title\": \"Robust Test-Time Adaptation for Zero-Shot Prompt Tuning\",\\n        \"citationCount\": 12,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://ojs.aaai.org/index.php/AAAI/article/download/29611/31034\",\\n          \"status\": \"GOLD\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1609/aaai.v38i15.29611?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1609/aaai.v38i15.29611, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This study proposes robust test-time Adaptation for zeroshot Prompt tuning (ADAPROMPT), which alleviates model bias, adapts to data bias and mostly outperforms the state-of-the-art methods at a small time cost and demonstrates that ADAPROMPT hardly encounters any performance degradation on these datasets.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-03-24\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2229108213\",\\n            \"name\": \"Ding-Chu Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2149135777\",\\n            \"name\": \"Zhi Zhou\"\\n          },\\n          {\\n            \"authorId\": \"2276808245\",\\n            \"name\": \"Yufeng Li\"\\n          }\\n        ],\\n        \"abstract\": \"CLIP has demonstrated remarkable generalization across diverse downstream tasks. By aligning images and texts in a shared feature space, they enable zero-shot classification via hand-crafted prompts. However, recent studies have shown that hand-crafted prompts may be unsuitable in practical applications. Specifically, choosing an appropriate prompt for a given task requires accurate data and knowledge, which may not be obtainable in practical situations. An inappropriate prompt can result in poor performance. Moreover, if there is no training data, tuning prompts arbitrarily through unlabeled test data may lead to serious performance degradation when giving hand-crafted prompts. Our study reveals that the aforementioned problems are mainly due to the biases in testing data (Data Bias) and pre-trained CLIP model (Model Bias). The Data Bias makes it challenging to choose an appropriate prompt, while Model Bias renders some predictions inaccurate and biased, which leads to error accumulation. To address these biases, we propose robust test-time Adaptation for zeroshot Prompt tuning (ADAPROMPT). Specifically, we ensemble multiple prompts to avoid the worst-case results and dynamically tune prompts to adapt to Data Bias during testing. Furthermore, we adopt a confidence-aware buffer to store balanced and confident unlabeled test data to tune prompts in order to overcome Model Bias. Our extensive experiments on several benchmarks demonstrate that ADAPROMPT alleviates model bias, adapts to data bias and mostly outperforms the state-of-the-art methods at a small time cost. Moreover, our experimental results reveal that ADAPROMPT hardly encounters any performance degradation on these datasets.\"\\n      },\\n      {\\n        \"paperId\": \"4362edfd3907204cf1b7ec8e3c16c56db5cd14cf\",\\n        \"url\": \"https://www.semanticscholar.org/paper/4362edfd3907204cf1b7ec8e3c16c56db5cd14cf\",\\n        \"title\": \"Large Language Models-guided Dynamic Adaptation for Temporal Knowledge Graph Reasoning\",\\n        \"citationCount\": 30,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2405.14170, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A Large Language Models-guided Dynamic Adaptation (LLM-DA) method for reasoning on TKGs that harnesses the capabilities of LLMs to analyze historical data and extract temporal logical rules that unveil temporal patterns and facilitate interpretable reasoning.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-05-23\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2163833353\",\\n            \"name\": \"Jiapu Wang\"\\n          },\\n          {\\n            \"authorId\": \"2220259324\",\\n            \"name\": \"Kai Sun\"\\n          },\\n          {\\n            \"authorId\": \"2238130759\",\\n            \"name\": \"Linhao Luo\"\\n          },\\n          {\\n            \"authorId\": \"2302892265\",\\n            \"name\": \"Wei Wei\"\\n          },\\n          {\\n            \"authorId\": \"2140879677\",\\n            \"name\": \"Yongli Hu\"\\n          },\\n          {\\n            \"authorId\": \"2243282363\",\\n            \"name\": \"Alan Wee-Chung Liew\"\\n          },\\n          {\\n            \"authorId\": \"2294378361\",\\n            \"name\": \"Shirui Pan\"\\n          },\\n          {\\n            \"authorId\": \"2239088112\",\\n            \"name\": \"Baocai Yin\"\\n          }\\n        ],\\n        \"abstract\": \"Temporal Knowledge Graph Reasoning (TKGR) is the process of utilizing temporal information to capture complex relations within a Temporal Knowledge Graph (TKG) to infer new knowledge. Conventional methods in TKGR typically depend on deep learning algorithms or temporal logical rules. However, deep learning-based TKGRs often lack interpretability, whereas rule-based TKGRs struggle to effectively learn temporal rules that capture temporal patterns. Recently, Large Language Models (LLMs) have demonstrated extensive knowledge and remarkable proficiency in temporal reasoning. Consequently, the employment of LLMs for Temporal Knowledge Graph Reasoning (TKGR) has sparked increasing interest among researchers. Nonetheless, LLMs are known to function as black boxes, making it challenging to comprehend their reasoning process. Additionally, due to the resource-intensive nature of fine-tuning, promptly updating LLMs to integrate evolving knowledge within TKGs for reasoning is impractical. To address these challenges, in this paper, we propose a Large Language Models-guided Dynamic Adaptation (LLM-DA) method for reasoning on TKGs. Specifically, LLM-DA harnesses the capabilities of LLMs to analyze historical data and extract temporal logical rules. These rules unveil temporal patterns and facilitate interpretable reasoning. To account for the evolving nature of TKGs, a dynamic adaptation strategy is proposed to update the LLM-generated rules with the latest events. This ensures that the extracted rules always incorporate the most recent knowledge and better generalize to the predictions on future events. Experimental results show that without the need of fine-tuning, LLM-DA significantly improves the accuracy of reasoning over several common datasets, providing a robust framework for TKGR tasks.\"\\n      },\\n      {\\n        \"paperId\": \"759b5f58e58a76f79a7d845acd3169dc899d0ac2\",\\n        \"url\": \"https://www.semanticscholar.org/paper/759b5f58e58a76f79a7d845acd3169dc899d0ac2\",\\n        \"title\": \"Domain Adaptation via Prompt Learning\",\\n        \"citationCount\": 210,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://arxiv.org/pdf/2202.06687\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2202.06687, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This article introduces a novel prompt learning paradigm for UDA, named domain adaptation via prompt learning (DAPrompt), which outperforms previous methods on several cross-domain benchmarks but also is very efficient to train and easy to implement.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2022-02-14\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2130368185\",\\n            \"name\": \"Chunjiang Ge\"\\n          },\\n          {\\n            \"authorId\": null,\\n            \"name\": \"Rui Huang\"\\n          },\\n          {\\n            \"authorId\": \"2112817811\",\\n            \"name\": \"Mixue Xie\"\\n          },\\n          {\\n            \"authorId\": \"51451501\",\\n            \"name\": \"Zihang Lai\"\\n          },\\n          {\\n            \"authorId\": \"1760750\",\\n            \"name\": \"Shiji Song\"\\n          },\\n          {\\n            \"authorId\": null,\\n            \"name\": \"Shuang Li\"\\n          },\\n          {\\n            \"authorId\": \"2115218570\",\\n            \"name\": \"Gao Huang\"\\n          }\\n        ],\\n        \"abstract\": \"Unsupervised domain adaptation (UDA) aims to adapt models learned from a well-annotated source domain to a target domain, where only unlabeled samples are given. Current UDA approaches learn domain-invariant features by aligning source and target feature spaces through statistical discrepancy minimization or adversarial training. However, these constraints could lead to the distortion of semantic feature structures and loss of class discriminability. In this article, we introduce a novel prompt learning paradigm for UDA, named domain adaptation via prompt learning (DAPrompt). In contrast to prior works, our approach learns the underlying label distribution for target domain rather than aligning domains. The main idea is to embed domain information into prompts, a form of representation generated from natural language, which is then used to perform classification. This domain information is shared only by images from the same domain, thereby dynamically adapting the classifier according to each domain. By adopting this paradigm, we show that our model not only outperforms previous methods on several cross-domain benchmarks but also is very efficient to train and easy to implement.\"\\n      },\\n      {\\n        \"paperId\": \"b32861f83f5426ddb0b797cfe9a5ea80b0adc3cf\",\\n        \"url\": \"https://www.semanticscholar.org/paper/b32861f83f5426ddb0b797cfe9a5ea80b0adc3cf\",\\n        \"title\": \"Dynamic 3D imaging of cerebral blood flow in awake mice using self-supervised-learning-enhanced optical coherence Doppler tomography\",\\n        \"citationCount\": 13,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://www.nature.com/articles/s42003-023-04656-x.pdf\",\\n          \"status\": \"GOLD\",\\n          \"license\": \"CCBY\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC10030663, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"The 3D imaging platform presented provides a powerful tool to study dynamic changes in vessel diameters and morphology alongside CBFv networks in the brain of awake animals that can advance the understanding of the effects of drugs and disease conditions.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2023-03-21\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"1826992\",\\n            \"name\": \"Yingtian Pan\"\\n          },\\n          {\\n            \"authorId\": \"2449537\",\\n            \"name\": \"Kicheon Park\"\\n          },\\n          {\\n            \"authorId\": \"48115953\",\\n            \"name\": \"Jiaxiang Ren\"\\n          },\\n          {\\n            \"authorId\": \"2184066\",\\n            \"name\": \"N. Volkow\"\\n          },\\n          {\\n            \"authorId\": \"81281627\",\\n            \"name\": \"H. Ling\"\\n          },\\n          {\\n            \"authorId\": \"1898658\",\\n            \"name\": \"A. Koretsky\"\\n          },\\n          {\\n            \"authorId\": \"2024857\",\\n            \"name\": \"C. Du\"\\n          }\\n        ],\\n        \"abstract\": \"Cerebral blood flow (CBF) is widely used to assess brain function. However, most preclinical CBF studies have been performed under anesthesia, which confounds findings. High spatiotemporal-resolution CBF imaging of awake animals is challenging due to motion artifacts and background noise, particularly for Doppler-based flow imaging. Here, we report ultrahigh-resolution optical coherence Doppler tomography (\\\\u00b5ODT) for 3D imaging of CBF velocity (CBFv) dynamics in awake mice by developing self-supervised deep-learning for effective image denoising and motion-artifact removal. We compare cortical CBFv in awake vs. anesthetized mice and their dynamic responses in arteriolar, venular and capillary networks to acute cocaine (1\\\\u2009mg/kg, i.v .), a highly addictive drug associated with neurovascular toxicity. Compared with awake, isoflurane (2-2.5%) induces vasodilation and increases CBFv within 2-4\\\\u2009min, whereas dexmedetomidine (0.025\\\\u2009mg/kg, i.p .) does not change vessel diameters nor flow. Acute cocaine decreases CBFv to the same extent in dexmedetomidine and awake states, whereas decreases are larger under isoflurane, suggesting that isoflurane-induced vasodilation might have facilitated detection of cocaine-induced vasoconstriction. Awake mice after chronic cocaine show severe vasoconstriction, CBFv decreases and vascular adaptations with extended diving arteriolar/venular vessels that prioritize blood supply to deeper cortical capillaries. The 3D imaging platform we present provides a powerful tool to study dynamic changes in vessel diameters and morphology alongside CBFv networks in the brain of awake animals that can advance our understanding of the effects of drugs and disease conditions (ischemia, tumors, wound healing). An imaging platform with self-supervised deep learning allows for the imaging of cerebral blood flows under the effect of cocaine in awake mice using 3D ultrahigh-resolution optical coherence Doppler tomography.\"\\n      },\\n      {\\n        \"paperId\": \"83ac79bb8e8695fb3c3c024be74790d862adea74\",\\n        \"url\": \"https://www.semanticscholar.org/paper/83ac79bb8e8695fb3c3c024be74790d862adea74\",\\n        \"title\": \"TEMPO: Prompt-based Generative Pre-trained Transformer for Time Series Forecasting\",\\n        \"citationCount\": 199,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://arxiv.org/pdf/2310.04948\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2310.04948, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"TEMPO expands the capability for dynamically modeling real-world temporal phenomena from data within diverse domains by utilizing two essential inductive biases of the time series task for pre-trained models, and introducing the design of prompts to facilitate distribution adaptation in different types of time series.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2023-10-08\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"120783624\",\\n            \"name\": \"Defu Cao\"\\n          },\\n          {\\n            \"authorId\": \"2256985863\",\\n            \"name\": \"Furong Jia\"\\n          },\\n          {\\n            \"authorId\": \"2676352\",\\n            \"name\": \"Sercan \\\\u00d6. Arik\"\\n          },\\n          {\\n            \"authorId\": \"1945962\",\\n            \"name\": \"Tomas Pfister\"\\n          },\\n          {\\n            \"authorId\": \"2257061490\",\\n            \"name\": \"Yixiang Zheng\"\\n          },\\n          {\\n            \"authorId\": \"2256992266\",\\n            \"name\": \"Wen Ye\"\\n          },\\n          {\\n            \"authorId\": \"2257088730\",\\n            \"name\": \"Yan Liu\"\\n          }\\n        ],\\n        \"abstract\": \"The past decade has witnessed significant advances in time series modeling with deep learning. While achieving state-of-the-art results, the best-performing architectures vary highly across applications and domains. Meanwhile, for natural language processing, the Generative Pre-trained Transformer (GPT) has demonstrated impressive performance via training one general-purpose model across various textual datasets. It is intriguing to explore whether GPT-type architectures can be effective for time series, capturing the intrinsic dynamic attributes and leading to significant accuracy improvements. In this paper, we propose a novel framework, TEMPO, that can effectively learn time series representations. We focus on utilizing two essential inductive biases of the time series task for pre-trained models: (i) decomposition of the complex interaction between trend, seasonal and residual components; and (ii) introducing the design of prompts to facilitate distribution adaptation in different types of time series. TEMPO expands the capability for dynamically modeling real-world temporal phenomena from data within diverse domains. Our experiments demonstrate the superior performance of TEMPO over state-of-the-art methods on zero shot setting for a number of time series benchmark datasets. This performance gain is observed not only in scenarios involving previously unseen datasets but also in scenarios with multi-modal inputs. This compelling finding highlights TEMPO\\'s potential to constitute a foundational model-building framework.\"\\n      },\\n      {\\n        \"paperId\": \"e3bb7012edfaac357311eca07515d193b4cf26bb\",\\n        \"url\": \"https://www.semanticscholar.org/paper/e3bb7012edfaac357311eca07515d193b4cf26bb\",\\n        \"title\": \"Towards Dynamic Spatial-Temporal Graph Learning: A Decoupled Perspective\",\\n        \"citationCount\": 29,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://ojs.aaai.org/index.php/AAAI/article/download/28759/29459\",\\n          \"status\": \"GOLD\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1609/aaai.v38i8.28759?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1609/aaai.v38i8.28759, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A decoupled learning framework (DLF) is proposed in this paper, which consists of a spatial-temporal graph learning network (DSTG) with a specialized decoupling training strategy to enable prompt adaptation to the evolving distribution of the dynamic graph.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-03-24\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"9071547\",\\n            \"name\": \"Binwu Wang\"\\n          },\\n          {\\n            \"authorId\": \"2108814780\",\\n            \"name\": \"Pengkun Wang\"\\n          },\\n          {\\n            \"authorId\": \"2292591250\",\\n            \"name\": \"Yudong Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2108599981\",\\n            \"name\": \"Xu Wang\"\\n          },\\n          {\\n            \"authorId\": \"6231985\",\\n            \"name\": \"Zhengyang Zhou\"\\n          },\\n          {\\n            \"authorId\": \"2187108259\",\\n            \"name\": \"Lei Bai\"\\n          },\\n          {\\n            \"authorId\": \"46396284\",\\n            \"name\": \"Yang Wang\"\\n          }\\n        ],\\n        \"abstract\": \"With the progress of urban transportation systems, a significant amount of high-quality traffic data is continuously collected through streaming manners, which has propelled the prosperity of the field of spatial-temporal graph prediction. In this paper, rather than solely focusing on designing powerful models for static graphs, we shift our focus to spatial-temporal graph prediction in the dynamic scenario, which involves a continuously expanding and evolving underlying graph. To address inherent challenges, a decoupled learning framework (DLF) is proposed in this paper, which consists of a spatial-temporal graph learning network (DSTG) with a specialized decoupling training strategy. Incorporating inductive biases of time-series structures, DSTG can interpret time dependencies into latent trend and seasonal terms. To enable prompt adaptation to the evolving distribution of the dynamic graph, our decoupling training strategy is devised to iteratively update these two types of patterns. Specifically, for learning seasonal patterns, we conduct thorough training for the model using a long time series (e.g., three months of data). To enhance the learning ability of the model, we also introduce the masked auto-encoding mechanism. During this period, we frequently update trend patterns to expand new information from dynamic graphs. Considering both effectiveness and efficiency, we develop a subnet sampling strategy to select a few representative nodes for fine-tuning the weights of the model. These sampled nodes cover unseen patterns and previously learned patterns. Experiments on dynamic spatial-temporal graph datasets further demonstrate the competitive performance, superior efficiency, and strong scalability of the proposed framework.\"\\n      },\\n      {\\n        \"paperId\": \"650a24da1702beca7eb70011a26f1f3238efad4b\",\\n        \"url\": \"https://www.semanticscholar.org/paper/650a24da1702beca7eb70011a26f1f3238efad4b\",\\n        \"title\": \"Time-LlaMA: Adapting Large Language Models for Time Series Modeling via Dynamic Low-rank Adaptation\",\\n        \"citationCount\": 12,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.18653/v1/2025.acl-srw.90?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.18653/v1/2025.acl-srw.90, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"The proposed Time-LlaMA framework achieves the state-of-the-art (SOTA) performance and have potentials for wide industrial usages.\"\\n        },\\n        \"publicationTypes\": null,\\n        \"publicationDate\": null,\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2346152667\",\\n            \"name\": \"Juyuan Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2334602368\",\\n            \"name\": \"Jiechao Gao\"\\n          },\\n          {\\n            \"authorId\": \"2374971695\",\\n            \"name\": \"Wenwen Ouyang\"\\n          },\\n          {\\n            \"authorId\": \"2334718086\",\\n            \"name\": \"Wei Zhu\"\\n          },\\n          {\\n            \"authorId\": \"2321408906\",\\n            \"name\": \"Hui Yi Leong\"\\n          }\\n        ],\\n        \"abstract\": \"Time series modeling holds significant importance in many industrial applications and has been extensively studied. A series of recent studies have demonstrated that large language models (LLMs) possess robust pattern recognition and semantic understanding capabilities over time series data. However, the current literature have yet striked a high-quality balance between (a) effectively aligning the time series and natural language modalities and (b) keeping the inference efficiency for industrial deployment. To address the above issues, we now propose the Time-LlaMA framework. Time-LlaMA first converts the time series input into token embeddings through a linear tokenization mechanism. Second, the time series token embeddings are aligned with the text prompts. Third, to further adapt the large languag model (LLM) backbone for time series modeling, we have developed a dynamic low-rank adaptation technique (DynaLoRA). DynaLoRA dynamically chooses the most suitable LoRA modules at each layer of the Transformer backbone for each time series input, enhancing the model\\\\u2019s predictive capabilities. Our experimental re-sults on an extensive collection of challenging open and proprietary time series tasks confirm that our proposed method achieves the state-of-the-art (SOTA) performance and have potentials for wide industrial usages. 1\"\\n      },\\n      {\\n        \"paperId\": \"0609513205d94646c740fc219e79b01043e80ba6\",\\n        \"url\": \"https://www.semanticscholar.org/paper/0609513205d94646c740fc219e79b01043e80ba6\",\\n        \"title\": \"Domain-Agnostic Mutual Prompting for Unsupervised Domain Adaptation\",\\n        \"citationCount\": 42,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://arxiv.org/pdf/2403.02899\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2403.02899, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"Domain-Agnostic Mutual Prompting (DAMP) is proposed to exploit domain-invariant semantics by mutually aligning visual and textual embeddings to exploit domain-invariant semantics.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-03-05\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"82980465\",\\n            \"name\": \"Zhekai Du\"\\n          },\\n          {\\n            \"authorId\": \"2108482672\",\\n            \"name\": \"Xinyao Li\"\\n          },\\n          {\\n            \"authorId\": \"2211944242\",\\n            \"name\": \"Fengling Li\"\\n          },\\n          {\\n            \"authorId\": \"1655484744\",\\n            \"name\": \"Ke Lu\"\\n          },\\n          {\\n            \"authorId\": \"2268796475\",\\n            \"name\": \"Lei Zhu\"\\n          },\\n          {\\n            \"authorId\": \"2109058078\",\\n            \"name\": \"Jingjing Li\"\\n          }\\n        ],\\n        \"abstract\": \"Conventional Unsupervised Domain Adaptation (UDA) strives to minimize distribution discrepancy between do-mains, which neglects to harness rich semantics from data and struggles to handle complex domain shifts. A promising technique is to leverage the knowledge of large-scale pretrained vision-language models for more guided adaptation. Despite some endeavors, current methods often learn textual prompts to embed domain semantics for source and target domains separately and perform classification within each domain, limiting cross-domain knowledge transfer. Moreover, prompting only the language branch lacks flex-ibility to adapt both modalities dynamically. To bridge this gap, we propose Domain-Agnostic Mutual Prompting (DAMP) to exploit domain-invariant semantics by mutually aligning visual and textual embeddings. Specifically, the image contextual information is utilized to prompt the language branch in a domain-agnostic and instance-conditioned way. Meanwhile, visual prompts are im-posed based on the domain-agnostic textual prompt to elicit domain-invariant visual embeddings. These two branches of prompts are learned mutually with a cross-attention module and regularized with a semantic-consistency loss and an instance-discrimination contrastive loss. Experiments on three UDA benchmarks demonstrate the superiority of DAMP over state-of-the-art approaches 1.\"\\n      },\\n      {\\n        \"paperId\": \"3f46e1b49c7289eb944365e1bef7bab5b2c891e4\",\\n        \"url\": \"https://www.semanticscholar.org/paper/3f46e1b49c7289eb944365e1bef7bab5b2c891e4\",\\n        \"title\": \"DLTTA: Dynamic Learning Rate for Test-Time Adaptation on Cross-Domain Medical Images\",\\n        \"citationCount\": 35,\\n        \"openAccessPdf\": {\\n          \"url\": \"http://arxiv.org/pdf/2205.13723\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2205.13723, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A novel dynamic learning rate adjustment method for test-time adaptation, called DLTTA, which dynamically modulates the amount of weights update for each test image to account for the differences in their distribution shift.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2022-05-27\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2162926363\",\\n            \"name\": \"Hongzheng Yang\"\\n          },\\n          {\\n            \"authorId\": \"1390805683\",\\n            \"name\": \"Cheng Chen\"\\n          },\\n          {\\n            \"authorId\": \"2050138741\",\\n            \"name\": \"Meirui Jiang\"\\n          },\\n          {\\n            \"authorId\": \"51306676\",\\n            \"name\": \"Quande Liu\"\\n          },\\n          {\\n            \"authorId\": \"48981374\",\\n            \"name\": \"Jianfeng Cao\"\\n          },\\n          {\\n            \"authorId\": \"1714602\",\\n            \"name\": \"P. Heng\"\\n          },\\n          {\\n            \"authorId\": \"35647880\",\\n            \"name\": \"Q. Dou\"\\n          }\\n        ],\\n        \"abstract\": \"Test-time adaptation (TTA) has increasingly been an important topic to efficiently tackle the cross-domain distribution shift at test time for medical images from different institutions. Previous TTA methods have a common limitation of using a fixed learning rate for all the test samples. Such a practice would be sub-optimal for TTA, because test data may arrive sequentially therefore the scale of distribution shift would change frequently. To address this problem, we propose a novel dynamic learning rate adjustment method for test-time adaptation, called DLTTA, which dynamically modulates the amount of weights update for each test image to account for the differences in their distribution shift. Specifically, our DLTTA is equipped with a memory bank based estimation scheme to effectively measure the discrepancy of a given test sample. Based on this estimated discrepancy, a dynamic learning rate adjustment strategy is then developed to achieve a suitable degree of adaptation for each test sample. The effectiveness and general applicability of our DLTTA is extensively demonstrated on three tasks including retinal optical coherence tomography (OCT) segmentation, histopathological image classification, and prostate 3D MRI segmentation. Our method achieves effective and fast test-time adaptation with consistent performance improvement over current state-of-the-art test-time adaptation methods. Code is available at https://github.com/med-air/DLTTA.\"\\n      },\\n      {\\n        \"paperId\": \"54150c69b5e273cdec053c6c95f5ccc707d0c9af\",\\n        \"url\": \"https://www.semanticscholar.org/paper/54150c69b5e273cdec053c6c95f5ccc707d0c9af\",\\n        \"title\": \"Prompt Learning on Temporal Interaction Graphs\",\\n        \"citationCount\": 13,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2402.06326, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work introduces Temporal Interaction Graph Prompting (TIGPrompt), a versatile framework that seamlessly integrates with TIG models, bridging both the temporal and semantic gaps, and proposes a temporal prompt generator to offer temporally-aware prompts for different tasks.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-02-09\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2283939419\",\\n            \"name\": \"Xi Chen\"\\n          },\\n          {\\n            \"authorId\": \"2237941783\",\\n            \"name\": \"Siwei Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2212411539\",\\n            \"name\": \"Yun Xiong\"\\n          },\\n          {\\n            \"authorId\": \"2181344415\",\\n            \"name\": \"Xixi Wu\"\\n          },\\n          {\\n            \"authorId\": \"2283820229\",\\n            \"name\": \"Jiawei Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2265792034\",\\n            \"name\": \"Xiangguo Sun\"\\n          },\\n          {\\n            \"authorId\": \"2283767676\",\\n            \"name\": \"Yao Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2334050630\",\\n            \"name\": \"Feng Zhao\"\\n          },\\n          {\\n            \"authorId\": \"2283820192\",\\n            \"name\": \"Yulin Kang\"\\n          }\\n        ],\\n        \"abstract\": \"Temporal Interaction Graphs (TIGs) are widely utilized to represent real-world systems. To facilitate representation learning on TIGs, researchers have proposed a series of TIG models. However, these models are still facing two tough gaps between the pre-training and downstream predictions in their ``pre-train, predict\\'\\' training paradigm. First, the temporal discrepancy between the pre-training and inference data severely undermines the models\\' applicability in distant future predictions on the dynamically evolving data. Second, the semantic divergence between pretext and downstream tasks hinders their practical applications, as they struggle to align with their learning and prediction capabilities across application scenarios. Recently, the ``pre-train, prompt\\'\\' paradigm has emerged as a lightweight mechanism for model generalization. Applying this paradigm is a potential solution to solve the aforementioned challenges. However, the adaptation of this paradigm to TIGs is not straightforward. The application of prompting in static graph contexts falls short in temporal settings due to a lack of consideration for time-sensitive dynamics and a deficiency in expressive power. To address this issue, we introduce Temporal Interaction Graph Prompting (TIGPrompt), a versatile framework that seamlessly integrates with TIG models, bridging both the temporal and semantic gaps. In detail, we propose a temporal prompt generator to offer temporally-aware prompts for different tasks. These prompts stand out for their minimalistic design, relying solely on the tuning of the prompt generator with very little supervision data. To cater to varying computational resource demands, we propose an extended ``pre-train, prompt-based fine-tune\\'\\' paradigm, offering greater flexibility. Through extensive experiments, the TIGPrompt demonstrates the SOTA performance and remarkable efficiency advantages.\"\\n      },\\n      {\\n        \"paperId\": \"a042571caf46d9053d81441da72ede243f8b421f\",\\n        \"url\": \"https://www.semanticscholar.org/paper/a042571caf46d9053d81441da72ede243f8b421f\",\\n        \"title\": \"PURE: Prompt Evolution with Graph ODE for Out-of-distribution Fluid Dynamics Modeling\",\\n        \"citationCount\": 17,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.52202/079017-3333?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.52202/079017-3333, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"The core of the PURE is to learn time-evolving prompts using a graph ODE to adapt spatio-temporal forecasting models to different scenarios and minimize the mutual information between prompt embeddings and observation embeddings to enhance the robustness of the model to different distributions.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": null,\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2288389213\",\\n            \"name\": \"Hao Wu\"\\n          },\\n          {\\n            \"authorId\": \"2146410855\",\\n            \"name\": \"C. Wang\"\\n          },\\n          {\\n            \"authorId\": \"2217950059\",\\n            \"name\": \"Fan Xu\"\\n          },\\n          {\\n            \"authorId\": \"2345442589\",\\n            \"name\": \"Jinbao Xue\"\\n          },\\n          {\\n            \"authorId\": \"2257378991\",\\n            \"name\": \"Chong Chen\"\\n          },\\n          {\\n            \"authorId\": \"2238119871\",\\n            \"name\": \"Xian-Sheng Hua\"\\n          },\\n          {\\n            \"authorId\": \"2345411659\",\\n            \"name\": \"Xiao Luo\"\\n          }\\n        ],\\n        \"abstract\": \"This work studies the problem of out-of-distribution fluid dynamics modeling. Previous works usually design effective neural operators to learn from mesh-based data structures. However, in real-world applications, they would suffer from distribution shifts from the variance of system parameters and temporal evolution of the dynamical system. In this paper, we propose a novel approach named Prompt Evolution with Graph ODE (PURE) for out-of-distribution fluid dynamics modeling. The core of our PURE is to learn time-evolving prompts using a graph ODE to adapt spatio-temporal forecasting models to different scenarios. In particular, our PURE first learns from historical observations and system parameters in the frequency domain to explore multi-view context information, which could effectively initialize prompt embeddings. More importantly, we incorporate the interpolation of observation sequences into a graph ODE, which can capture the temporal evolution of prompt embeddings for model adaptation. These time-evolving prompt embeddings are then incorporated into basic forecasting models to overcome temporal distribution shifts. We also minimize the mutual information between prompt embeddings and observation embeddings to enhance the robustness of our model to different distributions. Extensive experiments on various benchmark datasets validate the superiority of the proposed PURE in comparison to various baselines. Our codes are available at https://github.com/easylearningscores/\"\\n      },\\n      {\\n        \"paperId\": \"0d0dbfb1b315a43216020abaf74d289456198219\",\\n        \"url\": \"https://www.semanticscholar.org/paper/0d0dbfb1b315a43216020abaf74d289456198219\",\\n        \"title\": \"MaPLe: Multi-modal Prompt Learning\",\\n        \"citationCount\": 816,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://arxiv.org/pdf/2210.03117\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2210.03117, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"The design promotes strong coupling between the vision-language prompts to ensure mutual synergy and discourages learning independent uni-modal solutions, and the effectiveness of the approach is evaluated on three representative tasks of generalization to novel classes, new target datasets and unseen domain shifts.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2022-10-06\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2175250687\",\\n            \"name\": \"Muhammad Uzair Khattak\"\\n          },\\n          {\\n            \"authorId\": \"2097712964\",\\n            \"name\": \"H. Rasheed\"\\n          },\\n          {\\n            \"authorId\": \"32437679\",\\n            \"name\": \"Muhammad Maaz\"\\n          },\\n          {\\n            \"authorId\": \"2111181927\",\\n            \"name\": \"Salman H. Khan\"\\n          },\\n          {\\n            \"authorId\": \"2358803\",\\n            \"name\": \"F. Khan\"\\n          }\\n        ],\\n        \"abstract\": \"Pre-trained vision-language (V-L) models such as CLIP have shown excellent generalization ability to downstream tasks. However, they are sensitive to the choice of input text prompts and require careful selection of prompt templates to perform well. Inspired by the Natural Language Processing (NLP) literature, recent CLIP adaptation approaches learn prompts as the textual inputs to fine-tune CLIP for downstream tasks. We note that using prompting to adapt representations in a single branch of CLIP (language or vision) is sub-optimal since it does not allow the flexibility to dynamically adjust both representation spaces on a downstream task. In this work, we propose Multi-modal Prompt Learning (MaPLe) for both vision and language branches to improve alignment between the vision and language representations. Our design promotes strong coupling between the vision-language prompts to ensure mutual synergy and discourages learning independent uni-modal solutions. Further, we learn separate prompts across different early stages to progressively model the stage-wise feature relationships to allow rich context learning. We evaluate the effectiveness of our approach on three representative tasks of generalization to novel classes, new target datasets and unseen domain shifts. Compared with the state-of-the-art method Co-CoOp, MaPLe exhibits favorable performance and achieves an absolute gain of 3.45% on novel classes and 2.72% on overall harmonic-mean, averaged over 11 diverse image recognition datasets. Our code and pre-trained models are available at https://github.com/muzairkhattak/multimodal-prompt-learning.\"\\n      },\\n      {\\n        \"paperId\": \"ceac663aa9213bfdb457deacba9e5ed9ddd7ae92\",\\n        \"url\": \"https://www.semanticscholar.org/paper/ceac663aa9213bfdb457deacba9e5ed9ddd7ae92\",\\n        \"title\": \"CLIPArTT: Adaptation of CLIP to New Domains at Test Time\",\\n        \"citationCount\": 10,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2405.00754, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This study introduces CLIP Adaptation duRing Test-Time (CLIPArTT), a fully test-time adaptation (TTA) approach for CLIP, which involves automatic text prompts construction during inference for their use as text supervision and pioneer the standardization of TTA benchmarks in the realm of VLMs.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-05-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2037886454\",\\n            \"name\": \"G. Hakim\"\\n          },\\n          {\\n            \"authorId\": \"2188345071\",\\n            \"name\": \"David Osowiechi\"\\n          },\\n          {\\n            \"authorId\": \"1380287805\",\\n            \"name\": \"Mehrdad Noori\"\\n          },\\n          {\\n            \"authorId\": \"2188346816\",\\n            \"name\": \"Milad Cheraghalikhani\"\\n          },\\n          {\\n            \"authorId\": \"108062243\",\\n            \"name\": \"Ali Bahri\"\\n          },\\n          {\\n            \"authorId\": \"2168705225\",\\n            \"name\": \"Moslem Yazdanpanah\"\\n          },\\n          {\\n            \"authorId\": \"144019647\",\\n            \"name\": \"Ismail Ben Ayed\"\\n          },\\n          {\\n            \"authorId\": \"2260340228\",\\n            \"name\": \"Christian Desrosiers\"\\n          }\\n        ],\\n        \"abstract\": \"Pre-trained vision-language models (VLMs), exemplified by CLIP, demonstrate remarkable adaptability across zero-shot classification tasks without additional training. However, their performance diminishes in the presence of domain shifts. In this study, we introduce CLIP Adaptation duRing Test-Time (CLIPArTT), a fully test-time adaptation (TTA) approach for CLIP, which involves automatic text prompts construction during inference for their use as text supervision. Our method employs a unique, minimally invasive text prompt tuning process, wherein multiple predicted classes are aggregated into a single new text prompt, used as pseudo label to re-classify inputs in a transductive manner. Additionally, we pioneer the standardization of TTA benchmarks (e.g., TENT) in the realm of VLMs. Our findings demonstrate that, without requiring additional transformations nor new trainable modules, CLIPArTT enhances performance dynamically across non-corrupted datasets such as CIFAR-100, corrupted datasets like CIFAR-100-C and ImageNet-C, alongside synthetic datasets such as VisDA-C. This research underscores the potential for improving VLMs\\' adaptability through novel test-time strategies, offering insights for robust performance across varied datasets and environments. The code can be found at: https://github.com/dosowiechi/CLIPArTT.git\"\\n      },\\n      {\\n        \"paperId\": \"b7e42f7bc9c3ef6bf9315736fa149dff82fccf8f\",\\n        \"url\": \"https://www.semanticscholar.org/paper/b7e42f7bc9c3ef6bf9315736fa149dff82fccf8f\",\\n        \"title\": \"Learning Domain-Aware Detection Head with Prompt Tuning\",\\n        \"citationCount\": 21,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://arxiv.org/pdf/2306.05718\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2306.05718, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A novel DAOD framework named Domain-Aware detection head with Prompt tuning (DA-Pro), which applies the learnable domain-adaptive prompt to generate the dynamic detection head for each domain.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2023-06-09\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2156607591\",\\n            \"name\": \"Haochen Li\"\\n          },\\n          {\\n            \"authorId\": \"2118404461\",\\n            \"name\": \"Rui Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2222738\",\\n            \"name\": \"Hantao Yao\"\\n          },\\n          {\\n            \"authorId\": \"2109332957\",\\n            \"name\": \"Xinkai Song\"\\n          },\\n          {\\n            \"authorId\": \"2232700\",\\n            \"name\": \"Yifan Hao\"\\n          },\\n          {\\n            \"authorId\": \"46317288\",\\n            \"name\": \"Yongwei Zhao\"\\n          },\\n          {\\n            \"authorId\": \"3353457\",\\n            \"name\": \"Ling Li\"\\n          },\\n          {\\n            \"authorId\": \"7377735\",\\n            \"name\": \"Yunji Chen\"\\n          }\\n        ],\\n        \"abstract\": \"Domain adaptive object detection (DAOD) aims to generalize detectors trained on an annotated source domain to an unlabelled target domain. However, existing methods focus on reducing the domain bias of the detection backbone by inferring a discriminative visual encoder, while ignoring the domain bias in the detection head. Inspired by the high generalization of vision-language models (VLMs), applying a VLM as the robust detection backbone following a domain-aware detection head is a reasonable way to learn the discriminative detector for each domain, rather than reducing the domain bias in traditional methods. To achieve the above issue, we thus propose a novel DAOD framework named Domain-Aware detection head with Prompt tuning (DA-Pro), which applies the learnable domain-adaptive prompt to generate the dynamic detection head for each domain. Formally, the domain-adaptive prompt consists of the domain-invariant tokens, domain-specific tokens, and the domain-related textual description along with the class label. Furthermore, two constraints between the source and target domains are applied to ensure that the domain-adaptive prompt can capture the domains-shared and domain-specific knowledge. A prompt ensemble strategy is also proposed to reduce the effect of prompt disturbance. Comprehensive experiments over multiple cross-domain adaptation tasks demonstrate that using the domain-adaptive prompt can produce an effectively domain-related detection head for boosting domain-adaptive object detection. Our code is available at https://github.com/Therock90421/DA-Pro.\"\\n      },\\n      {\\n        \"paperId\": \"27cffd2e86a8af951679b00f091b89400376d711\",\\n        \"url\": \"https://www.semanticscholar.org/paper/27cffd2e86a8af951679b00f091b89400376d711\",\\n        \"title\": \"Hybrid Prompt-Driven Large Language Model for Robust State-of-Charge Estimation of Multitype Li-ion Batteries\",\\n        \"citationCount\": 12,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {\\'tldr\\'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/TTE.2024.3391938?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/TTE.2024.3391938, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": null,\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2025-02-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2034349220\",\\n            \"name\": \"Chong Bian\"\\n          },\\n          {\\n            \"authorId\": \"2186400636\",\\n            \"name\": \"Xue Han\"\\n          },\\n          {\\n            \"authorId\": \"2284894497\",\\n            \"name\": \"Zhiyu Duan\"\\n          },\\n          {\\n            \"authorId\": \"2237426629\",\\n            \"name\": \"Chao Deng\"\\n          },\\n          {\\n            \"authorId\": \"2269234236\",\\n            \"name\": \"Shunkun Yang\"\\n          },\\n          {\\n            \"authorId\": \"2144086553\",\\n            \"name\": \"Junlan Feng\"\\n          }\\n        ],\\n        \"abstract\": \"State-of-charge (SOC) estimation is critical for reliable operation of Li-ion batteries (LIBs). However, the distinct electrochemical characteristics coupled with harsh low-temperature environments make a single estimator struggle to robustly estimate the volatile SOC of multitype LIBs. To address these issues, this article proposes a hard-soft hybrid prompt learning method to unleash the potential of a pretrained large language model (LLM) for SOC estimation. A textual encoder is introduced to convert LIB measurements into hard text prompts for language modeling, naturally eliciting the pretrained LLM to capture the intrarelations of measured values over time and their interrelations with contextual semantics for accurate estimates. A side adapter network is constructed to reparameterize model adaptation towards different LIB tasks into optimizations within a low-dimensional subspace, strengthening the estimation generalization of the pretrained LLM in a parameter-efficient manner. A knowledge infusion mechanism is designed to encapsulate task-specific information as soft prompt vectors for model integration along forward propagation, dynamically conditioning the hidden states inside the pretrained LLM to enhance the estimation robustness against SOC volatilities. Extensive experiments verify that the hybrid prompt-driven LLM can simultaneously perform estimations for multitype LIBs under diverse operations and sub-zero temperatures with superior accuracy, generalization, and robustness.\"\\n      },\\n      {\\n        \"paperId\": \"564812800caef63f1c83dda51f71c23f4e14cd00\",\\n        \"url\": \"https://www.semanticscholar.org/paper/564812800caef63f1c83dda51f71c23f4e14cd00\",\\n        \"title\": \"Choriocapillaris Impairment Is Associated With Delayed Rod-Mediated Dark Adaptation in Age-Related Macular Degeneration\",\\n        \"citationCount\": 26,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://doi.org/10.1167/iovs.64.12.41\",\\n          \"status\": \"GOLD\",\\n          \"license\": \"CCBYNCND\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC10540875, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"Choriocapillaris FD% were associated with AMD severity and with impaired vision, especially RMDA, which is a causal factor for high-risk soft drusen formation.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2023-09-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"52465212\",\\n            \"name\": \"Deepayan Kar\"\\n          },\\n          {\\n            \"authorId\": \"6550345\",\\n            \"name\": \"G. Corradetti\"\\n          },\\n          {\\n            \"authorId\": \"40654031\",\\n            \"name\": \"Thomas A. Swain\"\\n          },\\n          {\\n            \"authorId\": \"6894914\",\\n            \"name\": \"Mark E. Clark\"\\n          },\\n          {\\n            \"authorId\": \"144347384\",\\n            \"name\": \"G. McGwin\"\\n          },\\n          {\\n            \"authorId\": \"144561682\",\\n            \"name\": \"C. Owsley\"\\n          },\\n          {\\n            \"authorId\": \"1420016526\",\\n            \"name\": \"S. Sadda\"\\n          },\\n          {\\n            \"authorId\": \"4477577\",\\n            \"name\": \"C. Curcio\"\\n          }\\n        ],\\n        \"abstract\": \"Purpose Progress toward treatment and prevention of age-related macular degeneration (AMD) requires imaging end points that relate to vision. We investigated choriocapillaris flow signal deficits (FD%) and visual function in eyes of individuals aged \\\\u226560 years, with and without AMD. Methods One eye of each participant in the baseline visit of the Alabama Study on Early Age-Related Macular Degeneration 2 (ALSTAR2; NCT04112667) was studied. AMD presence and severity was determined using the Age-Related Eye Disease Study (AREDS) grading system. FD% was quantified using macular spectral domain optical coherence tomography angiography (OCTA) scans. Vision tests included rod-mediated dark adaptation (RMDA), best-corrected visual acuity, and contrast sensitivity (photopic and mesopic), and microperimetric light sensitivity (scotopic, mesopic, and photopic). Presence of subretinal drusenoid deposits (SDD) was determined using multimodal imaging. Results In 410 study eyes of 410 participants (mean [SD] age = 71.7 years [5.9]), FD% was higher in early AMD (mean [SD] = 54.0% [5.5], N = 122) and intermediate AMD (59.8% [7.4], N = 92), compared to normal (52.1% [5.3], N = 196) eyes. Among visual functions evaluated, RMDA showed the strongest association with FD% (r = 0.35, P < 0.0001), followed by contrast sensitivity (r = \\\\u22120.22, P < 0.0001). Eyes with SDD had worse FD% (58.3% [7.4], N = 87), compared to eyes without SDD (53.4% [6.0], N = 323, P = < 0.0001). Conclusions Choriocapillaris FD% were associated with AMD severity and with impaired vision, especially RMDA. Reduced metabolic transport and exchange across the choriocapillaris-Bruch\\'s membrane retinal pigment epithelium (RPE) complex, a causal factor for high-risk soft drusen formation, also may impair photoreceptor sustenance from the circulation. This includes retinoid resupply, essential to dynamic rod function.\"\\n      },\\n      {\\n        \"paperId\": \"5f3adb1a1099169733a268c7597af130f200a5d7\",\\n        \"url\": \"https://www.semanticscholar.org/paper/5f3adb1a1099169733a268c7597af130f200a5d7\",\\n        \"title\": \"DA-LSTM: A Dynamic Drift-Adaptive Learning Framework for Interval Load Forecasting with LSTM Networks\",\\n        \"citationCount\": 53,\\n        \"openAccessPdf\": {\\n          \"url\": \"http://arxiv.org/pdf/2305.08767\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2305.08767, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A dynamic drift-adaptive Long Short-Term Memory (DA-LSTM) framework that can improve the performance of load forecasting models without requiring a drift threshold setting is proposed.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2023-05-15\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2047342626\",\\n            \"name\": \"Firas Bayram\"\\n          },\\n          {\\n            \"authorId\": \"2128949725\",\\n            \"name\": \"Phil Aupke\"\\n          },\\n          {\\n            \"authorId\": \"1991147\",\\n            \"name\": \"Bestoun S. Ahmed\"\\n          },\\n          {\\n            \"authorId\": \"10281628\",\\n            \"name\": \"A. Kassler\"\\n          },\\n          {\\n            \"authorId\": \"152623862\",\\n            \"name\": \"A. Theocharis\"\\n          },\\n          {\\n            \"authorId\": \"2086342190\",\\n            \"name\": \"Jonas Forsman\"\\n          }\\n        ],\\n        \"abstract\": \"Load forecasting is a crucial topic in energy management systems (EMS) due to its vital role in optimizing energy scheduling and enabling more flexible and intelligent power grid systems. As a result, these systems allow power utility companies to respond promptly to demands in the electricity market. Deep learning (DL) models have been commonly employed in load forecasting problems supported by adaptation mechanisms to cope with the changing pattern of consumption by customers, known as concept drift. A drift magnitude threshold should be defined to design change detection methods to identify drifts. While the drift magnitude in load forecasting problems can vary significantly over time, existing literature often assumes a fixed drift magnitude threshold, which should be dynamically adjusted rather than fixed during system evolution. To address this gap, in this paper, we propose a dynamic drift-adaptive Long Short-Term Memory (DA-LSTM) framework that can improve the performance of load forecasting models without requiring a drift threshold setting. We integrate several strategies into the framework based on active and passive adaptation approaches. To evaluate DA-LSTM in real-life settings, we thoroughly analyze the proposed framework and deploy it in a real-world problem through a cloud-based environment. Efficiency is evaluated in terms of the prediction performance of each approach and computational cost. The experiments show performance improvements on multiple evaluation metrics achieved by our framework compared to baseline methods from the literature. Finally, we present a trade-off analysis between prediction performance and computational costs.\"\\n      },\\n      {\\n        \"paperId\": \"8feb33300c04fffa050e0dca59c3fdcafc920a3b\",\\n        \"url\": \"https://www.semanticscholar.org/paper/8feb33300c04fffa050e0dca59c3fdcafc920a3b\",\\n        \"title\": \"FlowZero: Zero-Shot Text-to-Video Synthesis with LLM-Driven Dynamic Scene Syntax\",\\n        \"citationCount\": 20,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2311.15813, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"By using spatio-temporal syntaxes to guide the diffusion process, FlowZero achieves improvement in zero-shot video synthesis, generating coherent videos with vivid motion.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2023-11-27\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2247164321\",\\n            \"name\": \"Yu Lu\"\\n          },\\n          {\\n            \"authorId\": \"2948393\",\\n            \"name\": \"Linchao Zhu\"\\n          },\\n          {\\n            \"authorId\": \"3446334\",\\n            \"name\": \"Hehe Fan\"\\n          },\\n          {\\n            \"authorId\": \"2257587812\",\\n            \"name\": \"Yi Yang\"\\n          }\\n        ],\\n        \"abstract\": \"Text-to-video (T2V) generation is a rapidly growing research area that aims to translate the scenes, objects, and actions within complex video text into a sequence of coherent visual frames. We present FlowZero, a novel framework that combines Large Language Models (LLMs) with image diffusion models to generate temporally-coherent videos. FlowZero uses LLMs to understand complex spatio-temporal dynamics from text, where LLMs can generate a comprehensive dynamic scene syntax (DSS) containing scene descriptions, object layouts, and background motion patterns. These elements in DSS are then used to guide the image diffusion model for video generation with smooth object motions and frame-to-frame coherence. Moreover, FlowZero incorporates an iterative self-refinement process, enhancing the alignment between the spatio-temporal layouts and the textual prompts for the videos. To enhance global coherence, we propose enriching the initial noise of each frame with motion dynamics to control the background movement and camera motion adaptively. By using spatio-temporal syntaxes to guide the diffusion process, FlowZero achieves improvement in zero-shot video synthesis, generating coherent videos with vivid motion.\"\\n      },\\n      {\\n        \"paperId\": \"d1e202d981f03d17ebaa61f941d4366e4db39578\",\\n        \"url\": \"https://www.semanticscholar.org/paper/d1e202d981f03d17ebaa61f941d4366e4db39578\",\\n        \"title\": \"Accelerating Chain-of-Thought Reasoning: When Goal-Gradient Importance Meets Dynamic Skipping\",\\n        \"citationCount\": 10,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2505.08392, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work proposes Adaptive GoGI-Skip, a novel framework learning dynamic CoT compression via supervised fine-tuning that achieves substantial efficiency gains and significantly outperforms existing baselines by preserving accuracy even at high effective compression rates, advancing the state of the art in the CoT reasoning efficiency-accuracy trade-off.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2025-05-13\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2293320862\",\\n            \"name\": \"Zhuang Ren\"\\n          },\\n          {\\n            \"authorId\": \"2279253652\",\\n            \"name\": \"Ben Wang\"\\n          },\\n          {\\n            \"authorId\": \"2282384934\",\\n            \"name\": \"Shuifa Sun\"\\n          }\\n        ],\\n        \"abstract\": \"Large Language Models leverage Chain-of-Thought (CoT) prompting for complex tasks, but their reasoning traces are often excessively verbose and inefficient, leading to significant computational costs and latency. Current CoT compression techniques typically rely on generic importance metrics and static compression rates, which may inadvertently remove functionally critical tokens or fail to adapt to varying reasoning complexity. To overcome these limitations, we propose Adaptive GoGI-Skip, a novel framework learning dynamic CoT compression via supervised fine-tuning. This approach introduces two synergistic innovations: (1) Goal-Gradient Importance (GoGI), a novel metric accurately identifying functionally relevant tokens by measuring the gradient influence of their intermediate representations on the final answer loss, and (2) Adaptive Dynamic Skipping (ADS), a mechanism dynamically regulating the compression rate based on runtime model uncertainty while ensuring local coherence through an adaptive N-token constraint. To our knowledge, this is the first work unifying a goal-oriented, gradient-based importance metric with dynamic, uncertainty-aware skipping for CoT compression. Trained on compressed MATH data, Adaptive GoGI-Skip demonstrates strong cross-domain generalization across diverse reasoning benchmarks including AIME, GPQA, and GSM8K. It achieves substantial efficiency gains - reducing CoT token counts by over 45% on average and delivering 1.6-2.0 times inference speedups - while maintaining high reasoning accuracy. Notably, it significantly outperforms existing baselines by preserving accuracy even at high effective compression rates, advancing the state of the art in the CoT reasoning efficiency-accuracy trade-off.\"\\n      },\\n      {\\n        \"paperId\": \"1cf87d8c98636f88a076f7d245e69b819c6e556b\",\\n        \"url\": \"https://www.semanticscholar.org/paper/1cf87d8c98636f88a076f7d245e69b819c6e556b\",\\n        \"title\": \"FineCLIPER: Multi-modal Fine-grained CLIP for Dynamic Facial Expression Recognition with AdaptERs\",\\n        \"citationCount\": 34,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://arxiv.org/pdf/2407.02157\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2407.02157, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A novel framework, named Multi-modal Fine-grained CLIP for DFER with AdaptERs (FineCLIPER), incorporating the following novel designs, which achieves SOTA performance on the DFEW, FERV39k, and MAFW datasets in both supervised and zero-shot settings with few tunable parameters.\"\\n        },\\n        \"publicationTypes\": [\\n          \"Book\",\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-07-02\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2271664660\",\\n            \"name\": \"Haodong Chen\"\\n          },\\n          {\\n            \"authorId\": \"2296857672\",\\n            \"name\": \"Haojian Huang\"\\n          },\\n          {\\n            \"authorId\": \"2309753429\",\\n            \"name\": \"Junhao Dong\"\\n          },\\n          {\\n            \"authorId\": \"2333836576\",\\n            \"name\": \"Mingzhe Zheng\"\\n          },\\n          {\\n            \"authorId\": \"2301156673\",\\n            \"name\": \"Dian Shao\"\\n          }\\n        ],\\n        \"abstract\": \"Dynamic Facial Expression Recognition (DFER) is crucial for understanding human behavior. However, current methods exhibit limited performance mainly due to the insufficient utilization of facial dynamics, and the ambiguity of expression semantics, etc. To this end, we propose a novel framework, named Multi-modal Fine-grained CLIP for DFER with AdaptERs (FineCLIPER), incorporating the following novel designs: 1) To better distinguish between similar facial expressions, we extend the class labels to textual descriptions from both positive and negative aspects, and obtain supervision by calculating the cross-modal similarity based on the CLIP model; 2) Our FineCLIPER adopts a hierarchical manner to effectively mine useful cues from DFE videos. Specifically, besides directly embedding video frames as input (low semantic level), we propose to extract the face segmentation masks and landmarks based on each frame (middle semantic level) and utilize the Multi-modal Large Language Model (MLLM) to further generate detailed descriptions of facial changes across frames with designed prompts (high semantic level). Additionally, we also adopt Parameter-Efficient Fine-Tuning (PEFT) to enable efficient adaptation of large pre-trained models (i.e., CLIP) for this task. Our FineCLIPER achieves SOTA performance on the DFEW, FERV39k, and MAFW datasets in both supervised and zero-shot settings with few tunable parameters. Project page: https://haroldchen19.github.io/FineCLIPER-Page/\"\\n      },\\n      {\\n        \"paperId\": \"7af0f6f5113b69644c3a232a5bee61a1bd15df25\",\\n        \"url\": \"https://www.semanticscholar.org/paper/7af0f6f5113b69644c3a232a5bee61a1bd15df25\",\\n        \"title\": \"Machine learning based estimation of dynamic balance and gait adaptability in persons with neurological diseases using inertial sensors\",\\n        \"citationCount\": 17,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://www.nature.com/articles/s41598-023-35744-x.pdf\",\\n          \"status\": \"GOLD\",\\n          \"license\": \"CCBY\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC10224964, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"Results confirm that steady-state walking measurements provide information about dynamic balance and gait adaptability and can help clinicians identify important features to improve upon during rehabilitation.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2023-05-27\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2123025486\",\\n            \"name\": \"Piergiuseppe Liuzzi\"\\n          },\\n          {\\n            \"authorId\": \"4873550\",\\n            \"name\": \"I. Carpinella\"\\n          },\\n          {\\n            \"authorId\": \"51909015\",\\n            \"name\": \"D. Anastasi\"\\n          },\\n          {\\n            \"authorId\": \"5116603\",\\n            \"name\": \"E. Gervasoni\"\\n          },\\n          {\\n            \"authorId\": \"4840695\",\\n            \"name\": \"T. Lencioni\"\\n          },\\n          {\\n            \"authorId\": \"34764126\",\\n            \"name\": \"R. Bertoni\"\\n          },\\n          {\\n            \"authorId\": \"1798096\",\\n            \"name\": \"M. Carrozza\"\\n          },\\n          {\\n            \"authorId\": \"2687643\",\\n            \"name\": \"D. Cattaneo\"\\n          },\\n          {\\n            \"authorId\": \"1713849\",\\n            \"name\": \"M. Ferrarin\"\\n          },\\n          {\\n            \"authorId\": \"1758858\",\\n            \"name\": \"A. Mannini\"\\n          }\\n        ],\\n        \"abstract\": \"Poor dynamic balance and impaired gait adaptation to different contexts are hallmarks of people with neurological disorders (PwND), leading to difficulties in daily life and increased fall risk. Frequent assessment of dynamic balance and gait adaptability is therefore essential for monitoring the evolution of these impairments and/or the long-term effects of rehabilitation. The modified dynamic gait index (mDGI) is a validated clinical test specifically devoted to evaluating gait facets in clinical settings under a physiotherapist\\\\u2019s supervision. The need of a clinical environment, consequently, limits the number of assessments. Wearable sensors are increasingly used to measure balance and locomotion in real-world contexts and may permit an increase in monitoring frequency. This study aims to provide a preliminary test of this opportunity by using nested cross-validated machine learning regressors to predict the mDGI scores of 95 PwND via inertial signals collected from short steady-state walking bouts derived from the 6-minute walk test. Four different models were compared, one for each pathology (multiple sclerosis, Parkinson\\\\u2019s disease, and stroke) and one for the pooled multipathological cohort. Model explanations were computed on the best-performing solution; the model trained on the multipathological cohort yielded a median (interquartile range) absolute test error of 3.58 (5.38) points. In total, 76% of the predictions were within the mDGI\\\\u2019s minimal detectable change of 5 points. These results confirm that steady-state walking measurements provide information about dynamic balance and gait adaptability and can help clinicians identify important features to improve upon during rehabilitation. Future developments will include training of the method using short steady-state walking bouts in real-world settings, analysing the feasibility of this solution to intensify performance monitoring, providing prompt detection of worsening/improvements, and complementing clinical assessments.\"\\n      },\\n      {\\n        \"paperId\": \"088623b50102baf2cf84ea126d558b574e267967\",\\n        \"url\": \"https://www.semanticscholar.org/paper/088623b50102baf2cf84ea126d558b574e267967\",\\n        \"title\": \"Towards Robust and Efficient Cloud-Edge Elastic Model Adaptation via Selective Entropy Distillation\",\\n        \"citationCount\": 12,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2402.17316, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A Cloud-Edge Elastic Model Adaptation (CEMA) paradigm in which the edge models only need to perform forward propagation and the edge models can be adapted online and to reduce the communication burden is established.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-02-27\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"1816749481\",\\n            \"name\": \"Yaofo Chen\"\\n          },\\n          {\\n            \"authorId\": \"1411039233\",\\n            \"name\": \"Shuaicheng Niu\"\\n          },\\n          {\\n            \"authorId\": \"2157422974\",\\n            \"name\": \"Shoukai Xu\"\\n          },\\n          {\\n            \"authorId\": \"2287873264\",\\n            \"name\": \"Hengjie Song\"\\n          },\\n          {\\n            \"authorId\": \"2288039852\",\\n            \"name\": \"Yaowei Wang\"\\n          },\\n          {\\n            \"authorId\": \"2287854562\",\\n            \"name\": \"Mingkui Tan\"\\n          }\\n        ],\\n        \"abstract\": \"The conventional deep learning paradigm often involves training a deep model on a server and then deploying the model or its distilled ones to resource-limited edge devices. Usually, the models shall remain fixed once deployed (at least for some period) due to the potential high cost of model adaptation for both the server and edge sides. However, in many real-world scenarios, the test environments may change dynamically (known as distribution shifts), which often results in degraded performance. Thus, one has to adapt the edge models promptly to attain promising performance. Moreover, with the increasing data collected at the edge, this paradigm also fails to further adapt the cloud model for better performance. To address these, we encounter two primary challenges: 1) the edge model has limited computation power and may only support forward propagation; 2) the data transmission budget between cloud and edge devices is limited in latency-sensitive scenarios. In this paper, we establish a Cloud-Edge Elastic Model Adaptation (CEMA) paradigm in which the edge models only need to perform forward propagation and the edge models can be adapted online. In our CEMA, to reduce the communication burden, we devise two criteria to exclude unnecessary samples from uploading to the cloud, i.e., dynamic unreliable and low-informative sample exclusion. Based on the uploaded samples, we update and distribute the affine parameters of normalization layers by distilling from the stronger foundation model to the edge model with a sample replay strategy. Extensive experimental results on ImageNet-C and ImageNet-R verify the effectiveness of our CEMA.\"\\n      },\\n      {\\n        \"paperId\": \"5006e5be4c3cb1c4af84e4a2717ef886b3a22464\",\\n        \"url\": \"https://www.semanticscholar.org/paper/5006e5be4c3cb1c4af84e4a2717ef886b3a22464\",\\n        \"title\": \"DART: Dual-Modal Adaptive Online Prompting and Knowledge Retention for Test-Time Adaptation\",\\n        \"citationCount\": 12,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://ojs.aaai.org/index.php/AAAI/article/download/29320/30490\",\\n          \"status\": \"GOLD\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1609/aaai.v38i13.29320?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1609/aaai.v38i13.29320, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"To increase the learning capacity, DART captures knowledge from each test sample by learning class-specific text prompts and instance-level image prompts, and utilizes dual-modal knowledge retention prompts to adaptively retain the acquired knowledge, thereby enhancing the predictions on subsequent test samples.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-03-24\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2293575785\",\\n            \"name\": \"Zichen Liu\"\\n          },\\n          {\\n            \"authorId\": \"2261902040\",\\n            \"name\": \"Hongbo Sun\"\\n          },\\n          {\\n            \"authorId\": \"2267159976\",\\n            \"name\": \"Yuxin Peng\"\\n          },\\n          {\\n            \"authorId\": \"2261798088\",\\n            \"name\": \"Jiahuan Zhou\"\\n          }\\n        ],\\n        \"abstract\": \"As an up-and-coming area, CLIP-based pre-trained vision-language models can readily facilitate downstream tasks through the zero-shot or few-shot fine-tuning manners. However, they still face critical challenges in test-time generalization due to the shifts between the training and test data distributions, hindering the further improvement of the performance. To address this crucial problem, the latest works have introduced Test-Time Adaptation (TTA) techniques to CLIP which dynamically learn text prompts using only test samples. However, their limited learning capacity due to the overlook of visual modality information, and the underutilization of knowledge in previously seen test samples result in reduced performance. In this paper, we propose a novel Dual-modal Adaptive online prompting and knowledge ReTention method called DART to overcome these challenges. To increase the learning capacity, DART captures knowledge from each test sample by learning class-specific text prompts and instance-level image prompts. Additionally, to fully leverage the knowledge from previously seen test samples, DART utilizes dual-modal knowledge retention prompts to adaptively retain the acquired knowledge, thereby enhancing the predictions on subsequent test samples. Extensive experiments on various large-scale benchmarks demonstrate the effectiveness of our proposed DART against state-of-the-art methods.\"\\n      },\\n      {\\n        \"paperId\": \"5076bbbf831a92174c9cc1b347bd0584560435fc\",\\n        \"url\": \"https://www.semanticscholar.org/paper/5076bbbf831a92174c9cc1b347bd0584560435fc\",\\n        \"title\": \"Adaptive-Solver Framework for Dynamic Strategy Selection in Large Language Model Reasoning\",\\n        \"citationCount\": 10,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://arxiv.org/pdf/2310.01446\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2310.01446, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"An Adaptive-Solver (AS) framework that dynamically adapts solving strategies to suit various problems, enabling the flexible allocation of test-time computational resources and can help reduce computational consumption and improve performance.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2023-10-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2256299370\",\\n            \"name\": \"Jianpeng Zhou\"\\n          },\\n          {\\n            \"authorId\": \"81970097\",\\n            \"name\": \"Wanjun Zhong\"\\n          },\\n          {\\n            \"authorId\": \"2214155529\",\\n            \"name\": \"Yanlin Wang\"\\n          },\\n          {\\n            \"authorId\": \"2110182921\",\\n            \"name\": \"Jiahai Wang\"\\n          }\\n        ],\\n        \"abstract\": \"Large Language Models (LLMs) demonstrate impressive ability in handling reasoning tasks. However, unlike humans who can instinctively adapt their problem-solving strategies to the complexity of task, most LLM-based methods adopt a one-size-fits-all approach. These methods employ consistent models, sample sizes, prompting methods and levels of problem decomposition, regardless of the problem complexity. The inflexibility of these methods can bring unnecessary computational overhead or sub-optimal performance. To address this limitation, we introduce an Adaptive-Solver (AS) framework tha dynamically adapts solving strategies to suit various problems, enabling the flexible allocation of test-time computational resources. The framework functions with two primary modules. The initial evaluation module assesses the reliability of the current solution using answer consistency. If the solution is deemed unreliable, the subsequent adaptation module comes into play. Within this module, various types of adaptation strategies are employed collaboratively. Through such dynamic and multi-faceted adaptations, our framework can help reduce computational consumption and improve performance. Experimental results from complex reasoning benchmarks reveal that our method can significantly reduce API costs (up to 85%) while maintaining original performance. Alternatively, it achieves up to 4.5% higher accuracy compared to the baselines at the same cost. The code and dataset are available at https://github.com/john1226966735/Adaptive-Solver.\"\\n      },\\n      {\\n        \"paperId\": \"f4460f3e44e4d035830ab2c676c06e193e51d203\",\\n        \"url\": \"https://www.semanticscholar.org/paper/f4460f3e44e4d035830ab2c676c06e193e51d203\",\\n        \"title\": \"Intrinsic signal optoretinography of dark adaptation kinetics\",\\n        \"citationCount\": 13,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://www.nature.com/articles/s41598-022-06562-4.pdf\",\\n          \"status\": \"GOLD\",\\n          \"license\": \"CCBY\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2112.07838, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"Functional OCT of dark adaptation kinetics promises an objective method for rapid ORG assessment of physiological integrity of retinal photoreceptors.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2021-12-15\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"82743798\",\\n            \"name\": \"Tae-Hoon Kim\"\\n          },\\n          {\\n            \"authorId\": \"2111184482\",\\n            \"name\": \"Jie Ding\"\\n          },\\n          {\\n            \"authorId\": \"8073475\",\\n            \"name\": \"Xincheng Yao\"\\n          }\\n        ],\\n        \"abstract\": \"Delayed dark adaptation due to impaired rod photoreceptor homeostasis has been reported as the earliest symptom of eye diseases such as age-related macular degeneration, diabetic retinopathy, and retinitis pigmentosa. Objective measurement of dark adaptation can facilitate early diagnosis to enable prompt intervention to prevent vision loss. However, there is a lack of noninvasive methods capable of spatiotemporal monitoring of photoreceptor changes during dark adaptation. Here we demonstrate functional optical coherence tomography (OCT) for in vivo intrinsic signal optoretinography (ORG) of dark adaptation kinetics in the C57BL/6J mouse retina. Functional OCT revealed a shortening of the outer retina, a rearrangement of the cone and rod photoreceptor interdigitation zone, and a reduction in intrinsic signal amplitude at the photoreceptor inner segment ellipsoid (ISe). A strong positive correlation between the outer retinal shortening and ISe intensity reduction was also confirmed. Functional OCT of dark adaptation kinetics promises an objective method for rapid ORG assessment of physiological integrity of retinal photoreceptors.\"\\n      },\\n      {\\n        \"paperId\": \"9fe222cb8464e8157b3654ab96ef719331dd2357\",\\n        \"url\": \"https://www.semanticscholar.org/paper/9fe222cb8464e8157b3654ab96ef719331dd2357\",\\n        \"title\": \"Meta-Learning the Difference: Preparing Large Language Models for Efficient Adaptation\",\\n        \"citationCount\": 20,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl_a_00517/2059907/tacl_a_00517.pdf\",\\n          \"status\": \"GOLD\",\\n          \"license\": \"CCBY\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2207.03509, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"Experiments on few-shot dialogue completion, low-resource abstractive summarization, and multi-domain language modeling show improvements in adaptation time and performance over direct finetuning or preparation via domain-adaptive pretraining.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2022-07-07\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"26563401\",\\n            \"name\": \"Zejiang Hou\"\\n          },\\n          {\\n            \"authorId\": \"143733211\",\\n            \"name\": \"Julian Salazar\"\\n          },\\n          {\\n            \"authorId\": \"1402376936\",\\n            \"name\": \"George Polovets\"\\n          }\\n        ],\\n        \"abstract\": \"Abstract Large pretrained language models (PLMs) are often domain- or task-adapted via finetuning or prompting. Finetuning requires modifying all of the parameters and having enough data to avoid overfitting while prompting requires no training and few examples but limits performance. Instead, we prepare PLMs for data- and parameter-efficient adaptation by learning to learn the difference between general and adapted PLMs. This difference is expressed in terms of model weights and sublayer structure through our proposed dynamic low-rank reparameterization and learned architecture controller. Experiments on few-shot dialogue completion, low-resource abstractive summarization, and multi-domain language modeling show improvements in adaptation time and performance over direct finetuning or preparation via domain-adaptive pretraining. Ablations show our task-adaptive reparameterization (TARP) and model search (TAMS) components individually improve on other parameter-efficient transfer like adapters and structure-learning methods like learned sparsification.\"\\n      },\\n      {\\n        \"paperId\": \"792bdea700be87488c5fa135ecef27b94f0d5c31\",\\n        \"url\": \"https://www.semanticscholar.org/paper/792bdea700be87488c5fa135ecef27b94f0d5c31\",\\n        \"title\": \"Sparse-Based Domain Adaptation Network for OCTA Image Super-Resolution Reconstruction\",\\n        \"citationCount\": 11,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://arxiv.org/pdf/2207.11882\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2207.11882, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A multi-level super-resolution model is proposed for the fully-supervised reconstruction of the synthetic data, guiding the reconstructing of the realistic LR images through a generative-adversarial strategy that allows the synthetic and realism LR images to be unified in the feature domain.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2022-07-25\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"47412750\",\\n            \"name\": \"Huaying Hao\"\\n          },\\n          {\\n            \"authorId\": \"2216350318\",\\n            \"name\": \"C. Xu\"\\n          },\\n          {\\n            \"authorId\": \"2109982918\",\\n            \"name\": \"Dan Zhang\"\\n          },\\n          {\\n            \"authorId\": \"151482895\",\\n            \"name\": \"Qifeng Yan\"\\n          },\\n          {\\n            \"authorId\": \"2116222083\",\\n            \"name\": \"Jiong Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2119033864\",\\n            \"name\": \"Yue Liu\"\\n          },\\n          {\\n            \"authorId\": \"1956017\",\\n            \"name\": \"Yitian Zhao\"\\n          }\\n        ],\\n        \"abstract\": \"Retinal Optical Coherence Tomography Angiography (OCTA) with high-resolution is important for the quantification and analysis of retinal vasculature. However, the resolution of OCTA images is inversely proportional to the field of view at the same sampling frequency, which is not conducive to clinicians for analyzing larger vascular areas. In this paper, we propose a novel <bold>S</bold>parse-based domain <bold>A</bold>daptation <bold>S</bold>uper-<bold>R</bold>esolution network (SASR) for the reconstruction of realistic <inline-formula><tex-math notation=\\\\\"LaTeX\\\\\">$6\\\\\\\\times \\\\\\\\text{6}{\\\\\\\\rm{mm}}^{2}$</tex-math></inline-formula>/low-resolution (LR) OCTA images to high-resolution (HR) representations. To be more specific, we first perform a simple degradation of the <inline-formula><tex-math notation=\\\\\"LaTeX\\\\\">$3\\\\\\\\times \\\\\\\\text{3}\\\\\\\\,{\\\\\\\\rm{mm}}^{2}$</tex-math></inline-formula>/high-resolution (HR) image to obtain the synthetic LR image. An efficient registration method is then employed to register the synthetic LR with its corresponding <inline-formula><tex-math notation=\\\\\"LaTeX\\\\\">$3\\\\\\\\times \\\\\\\\text{3}\\\\\\\\,{\\\\\\\\rm{mm}}^{2}$</tex-math></inline-formula> image region within the <inline-formula><tex-math notation=\\\\\"LaTeX\\\\\">$6\\\\\\\\times \\\\\\\\text{6}\\\\\\\\,{\\\\\\\\rm{mm}}^{2}$</tex-math></inline-formula> image to obtain the cropped realistic LR image. We then propose a multi-level super-resolution model for the fully-supervised reconstruction of the synthetic data, guiding the reconstruction of the realistic LR images through a generative-adversarial strategy that allows the synthetic and realistic LR images to be unified in the feature domain. Finally, a novel sparse edge-aware loss is designed to dynamically optimize the vessel edge structure. Extensive experiments on two OCTA sets have shown that our method performs better than state-of-the-art super-resolution reconstruction methods. In addition, we have investigated the performance of the reconstruction results on retina structure segmentations, which further validate the effectiveness of our approach.\"\\n      },\\n      {\\n        \"paperId\": \"3836a5ca7b7884b2333299b65f3436d210587f07\",\\n        \"url\": \"https://www.semanticscholar.org/paper/3836a5ca7b7884b2333299b65f3436d210587f07\",\\n        \"title\": \"Dynamics of Smallholder Farmers\\\\u2019 Livelihood Adaptation Decision-Making in Central Ethiopia\",\\n        \"citationCount\": 22,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://www.mdpi.com/2071-1050/12/11/4526/pdf?version=1591863876\",\\n          \"status\": \"GOLD\",\\n          \"license\": \"CCBY\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.3390/su12114526?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.3390/su12114526, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": null,\\n        \"publicationTypes\": null,\\n        \"publicationDate\": \"2020-06-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"13422136\",\\n            \"name\": \"D. Etana\"\\n          },\\n          {\\n            \"authorId\": \"4266116\",\\n            \"name\": \"D. Snelder\"\\n          },\\n          {\\n            \"authorId\": \"2051704940\",\\n            \"name\": \"C. V. van Wesenbeeck\"\\n          },\\n          {\\n            \"authorId\": \"3606491\",\\n            \"name\": \"T. de Cock Buning\"\\n          }\\n        ],\\n        \"abstract\": \"In previous studies mainly focusing on determinants of adaptation, evidence of the dynamic process of adaptation decision-making is negligible. The objective of this study was to investigate the effects of socio-cultural factors, changes in household characteristics, and climate variables on the transition from non-use to use of adaptation strategies. The study integrated primary data collected from households with secondary rainfall and temperature data. The quantitative and qualitative data were analysed using a dynamic random-effects probit model and a thematic approach, respectively. The result shows strong evidence of path dependence in which use of a strategy during the previous year significantly increases its current use. Climate-related risk perception and factual knowledge may not necessarily prompt adaptation action, whereas access to financial resources and farming-related trainings were consistent positive predictors of farmers\\\\u2019 adaptation decisions. The findings entail that economic capacity and the associated intrinsic motivation help few farmers to utilise robust and contesting adaptation strategies. For most households, economic problems and the consequent fatalistic attitude and risk-avoidance behaviour induce either non-use or use of responsive and accommodating strategies aimed at ensuring survival. Path dependence in non-use of adaptation strategies and sub-optimal adaptation actions demand effective institutional supports to address the behavioural and economic barriers of these households in order to build overall community resilience.\"\\n      },\\n      {\\n        \"paperId\": \"fdb97e3f5fa700fa19c3274f4a35834adb214c23\",\\n        \"url\": \"https://www.semanticscholar.org/paper/fdb97e3f5fa700fa19c3274f4a35834adb214c23\",\\n        \"title\": \"\\\\u201cThe emotions were like a roller-coaster\\\\u201d: a qualitative analysis of e-diary data on healthcare worker resilience and adaptation during the COVID-19 outbreak in Singapore\",\\n        \"citationCount\": 12,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://human-resources-health.biomedcentral.com/counter/pdf/10.1186/s12960-022-00756-7\",\\n          \"status\": \"GOLD\",\\n          \"license\": \"CCBY\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC9285872, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"What characterises risk, vulnerability and resilient responses of HCWs during the early stages of the outbreak in Singapore is explored and the Loads\\\\u2013Levers\\\\u2013Lifts model of resilience is used to guide analysis to inform intervention designs.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Review\"\\n        ],\\n        \"publicationDate\": \"2022-07-15\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2014928933\",\\n            \"name\": \"Alyssa Yenyi Chan\"\\n          },\\n          {\\n            \"authorId\": \"1491848786\",\\n            \"name\": \"Celene Ting\"\\n          },\\n          {\\n            \"authorId\": \"6374024\",\\n            \"name\": \"L. G. Chan\"\\n          },\\n          {\\n            \"authorId\": \"6130448\",\\n            \"name\": \"Z. Hildon\"\\n          }\\n        ],\\n        \"abstract\": \"Background Uncertainties related to COVID-19 have strained the mental health of healthcare workers (HCWs) worldwide. Gaining the ability to adapt and thrive under pressure will be key to addressing this. We explore what characterises risk, vulnerability and resilient responses of HCWs during the early stages of the outbreak in Singapore. Methods We undertook qualitative theory-guided thematic analysis of e-diary entries from HCWs who navigated the outbreak from June\\\\u2013August 2020. Data were extracted from a subset of an online survey of n \\\\u2009=\\\\u20093616 participants collected across 9 institutions, including restructured hospitals, hospices and affiliated primary care partners. Results N \\\\u2009=\\\\u2009663 or 18% submitted qualitative journal entries included for analyses. All professional cadres, local as well as foreign HCWs participated. Themes are reported according to the Loads\\\\u2013Levers\\\\u2013Lifts model of resilience and highlighted in italics. The model assumes that resilience is a dynamic process. Key factors threatening mental health (loading) risk included a notable rise in anxiety, the effects of being separated from loved ones, and experiencing heightened emotions and emotional overload . Bad situations were made worse, prompting vulnerable outcomes when HCWs experienced stigma in the community and effects of \\\\u201cpublic paranoia\\\\u201d; or under conditions where HCWs ended up feeling like a prisoner with little control or choice when either confined to staff accommodation or placed on quarantine/Stay Home Notices. Those with strife in their place of residence also described already difficult situations at work being aggravated by home life. Protection (lifts) came from being able to muster a sense of optimism about the future or feeling grateful for the pace of life slowing down and having the space to reprioritise. In contrast, when risk factors were present , balancing these in the direction of resilient outcomes was achieved by choosing to re-direct stress into positive narratives, drawing on inner agency, uptake of therapeutic activities , social support as well as faith and prayer and drawing comfort from religious community among other factors. Conclusion The Loads\\\\u2013Levers\\\\u2013Lifts model is used to guide analysis to inform intervention designs. Levers promoting resilience through targeting therapies, workplace policies and awareness campaigns accounting for identified loads are proposed.\"\\n      },\\n      {\\n        \"paperId\": \"1a031fead8b57e4c24b04ba79b15b12d4eca583b\",\\n        \"url\": \"https://www.semanticscholar.org/paper/1a031fead8b57e4c24b04ba79b15b12d4eca583b\",\\n        \"title\": \"The Geometry of Prompting: Unveiling Distinct Mechanisms of Task Adaptation in Language Models\",\\n        \"citationCount\": 10,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2502.08009, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work investigates how different prompting methods affect the geometry of representations in decoder-only language models and reveals that various prompting techniques, while achieving similar performance, operate through distinct representational mechanisms for task adaptation.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2025-02-11\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2345003776\",\\n            \"name\": \"Artem Kirsanov\"\\n          },\\n          {\\n            \"authorId\": \"2276206718\",\\n            \"name\": \"Chi-Ning Chou\"\\n          },\\n          {\\n            \"authorId\": \"2345694335\",\\n            \"name\": \"Kyunghyun Cho\"\\n          },\\n          {\\n            \"authorId\": \"2267869180\",\\n            \"name\": \"SueYeon Chung\"\\n          }\\n        ],\\n        \"abstract\": \"Decoder-only language models have the ability to dynamically switch between various computational tasks based on input prompts. Despite many successful applications of prompting, there is very limited understanding of the internal mechanism behind such flexibility. In this work, we investigate how different prompting methods affect the geometry of representations in these models. Employing a framework grounded in statistical physics, we reveal that various prompting techniques, while achieving similar performance, operate through distinct representational mechanisms for task adaptation. Our analysis highlights the critical role of input distribution samples and label semantics in few-shot in-context learning. We also demonstrate evidence of synergistic and interfering interactions between different tasks on the representational level. Our work contributes to the theoretical understanding of large language models and lays the groundwork for developing more effective, representation-aware prompting strategies.\"\\n      },\\n      {\\n        \"paperId\": \"9e0d9c5d858aac35a51b88e0807ace1c3369f863\",\\n        \"url\": \"https://www.semanticscholar.org/paper/9e0d9c5d858aac35a51b88e0807ace1c3369f863\",\\n        \"title\": \"Quantification of intrinsic optical signals in the outer human retina using optical coherence tomography\",\\n        \"citationCount\": 10,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://onlinelibrary.wiley.com/doi/pdfdirect/10.1111/nyas.14721\",\\n          \"status\": \"HYBRID\",\\n          \"license\": \"CCBYNC\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC9299665, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"The presented approach allowed for assess to dynamic changes in the outer retina in response to light and the change in the subretinal space occurring in the context of light adaptation could be measured using a standard OCT platform and a dedicated signal model.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2021-12-10\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"47182275\",\\n            \"name\": \"A. Messner\"\\n          },\\n          {\\n            \"authorId\": \"9535158\",\\n            \"name\": \"V. Aranha dos Santos\"\\n          },\\n          {\\n            \"authorId\": \"51902564\",\\n            \"name\": \"Hannes Stegmann\"\\n          },\\n          {\\n            \"authorId\": \"2494261\",\\n            \"name\": \"S. Puchner\"\\n          },\\n          {\\n            \"authorId\": \"7533266\",\\n            \"name\": \"D. Schmidl\"\\n          },\\n          {\\n            \"authorId\": \"2084180\",\\n            \"name\": \"R. Leitgeb\"\\n          },\\n          {\\n            \"authorId\": \"144820338\",\\n            \"name\": \"L. Schmetterer\"\\n          },\\n          {\\n            \"authorId\": \"4052780\",\\n            \"name\": \"R. Werkmeister\"\\n          }\\n        ],\\n        \"abstract\": \"Intrinsic optical signals constitute a noninvasive biomarker promising the objective assessment of retinal photoreceptor function. We employed a commercial optical coherence tomography (OCT) system and an OCT signal model for evaluation of optical path length (OPL) changes in the temporal outer retina of five healthy subjects during light adaptation. Data were acquired at 30 time points, in ambient light and during long duration stimulation with white light, and analyzed, employing a signal model based on the sum of seven Gaussian curves corresponding to all relevant anatomical structures of the outer retina. During light stimulation, mean OPL between rod outer segment tips (ROST) and the retinal pigment epithelium (RPE) decreased by 21.4 \\\\u00b1 3.5%. Further, OPL between the external\\\\u2010limiting membrane (ELM) and the RPE decreased by 5.2 \\\\u00b1 0.9% versus baseline, while OPL between ELM and ROST showed an initial decrease by 2.1 \\\\u00b1 1.6% versus baseline and, thereafter, increased by 2.8 \\\\u00b1 2.1% versus baseline. Thus, the presented approach allowed for assess to dynamic changes in the outer retina in response to light. The change in the subretinal space occurring in the context of light adaptation could be measured using a standard OCT platform and a dedicated signal model.\"\\n      },\\n      {\\n        \"paperId\": \"0681da799c86c6a92bb0e31f4471656958e60d4f\",\\n        \"url\": \"https://www.semanticscholar.org/paper/0681da799c86c6a92bb0e31f4471656958e60d4f\",\\n        \"title\": \"Teacher adaptation to flexible learning environments\",\\n        \"citationCount\": 43,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {\\'abstract\\'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1007/s10984-019-09302-0?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1007/s10984-019-09302-0, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": null,\\n        \"publicationTypes\": null,\\n        \"publicationDate\": \"2020-07-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2647966\",\\n            \"name\": \"Craig Deed\"\\n          },\\n          {\\n            \"authorId\": \"1713923\",\\n            \"name\": \"D. Blake\"\\n          },\\n          {\\n            \"authorId\": \"50070183\",\\n            \"name\": \"Joanne Henriksen\"\\n          },\\n          {\\n            \"authorId\": \"6355499\",\\n            \"name\": \"A. Mooney\"\\n          },\\n          {\\n            \"authorId\": \"8245143\",\\n            \"name\": \"V. Prain\"\\n          },\\n          {\\n            \"authorId\": \"8348550\",\\n            \"name\": \"R. Tytler\"\\n          },\\n          {\\n            \"authorId\": \"115009297\",\\n            \"name\": \"Tina Zitzlaff\"\\n          },\\n          {\\n            \"authorId\": \"48003546\",\\n            \"name\": \"M. Edwards\"\\n          },\\n          {\\n            \"authorId\": \"117230810\",\\n            \"name\": \"S. Emery\"\\n          },\\n          {\\n            \"authorId\": \"47751037\",\\n            \"name\": \"T. Muir\"\\n          },\\n          {\\n            \"authorId\": \"71944724\",\\n            \"name\": \"K. Swabey\"\\n          },\\n          {\\n            \"authorId\": \"1586673162\",\\n            \"name\": \"Damon P. Thomas\"\\n          },\\n          {\\n            \"authorId\": \"51467279\",\\n            \"name\": \"Cathleen Farrelly\"\\n          },\\n          {\\n            \"authorId\": \"83362649\",\\n            \"name\": \"Valerie Lovejoy\"\\n          },\\n          {\\n            \"authorId\": \"87961076\",\\n            \"name\": \"N. Meyers\"\\n          },\\n          {\\n            \"authorId\": \"113948066\",\\n            \"name\": \"Doug Fingland\"\\n          }\\n        ],\\n        \"abstract\": null\\n      },\\n      {\\n        \"paperId\": \"f7175048d39ed1bceea572b872b72633167fc79d\",\\n        \"url\": \"https://www.semanticscholar.org/paper/f7175048d39ed1bceea572b872b72633167fc79d\",\\n        \"title\": \"Diff-Tracker: Text-to-Image Diffusion Models are Unsupervised Trackers\",\\n        \"citationCount\": 17,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2407.08394, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"Diff-Tracker, a novel approach for the challenging unsupervised visual tracking task leveraging the pre-trained text-to-image diffusion model, designs an initial prompt learner to enable the diffusion model to recognize the tracking target by learning a prompt representing the target.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-07-11\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2309491187\",\\n            \"name\": \"Zhengbo Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2288264614\",\\n            \"name\": \"Li Xu\"\\n          },\\n          {\\n            \"authorId\": \"2067913944\",\\n            \"name\": \"Duo Peng\"\\n          },\\n          {\\n            \"authorId\": \"2265553215\",\\n            \"name\": \"Hossein Rahmani\"\\n          },\\n          {\\n            \"authorId\": \"2309177751\",\\n            \"name\": \"Jun Liu\"\\n          }\\n        ],\\n        \"abstract\": \"We introduce Diff-Tracker, a novel approach for the challenging unsupervised visual tracking task leveraging the pre-trained text-to-image diffusion model. Our main idea is to leverage the rich knowledge encapsulated within the pre-trained diffusion model, such as the understanding of image semantics and structural information, to address unsupervised visual tracking. To this end, we design an initial prompt learner to enable the diffusion model to recognize the tracking target by learning a prompt representing the target. Furthermore, to facilitate dynamic adaptation of the prompt to the target\\'s movements, we propose an online prompt updater. Extensive experiments on five benchmark datasets demonstrate the effectiveness of our proposed method, which also achieves state-of-the-art performance.\"\\n      },\\n      {\\n        \"paperId\": \"cd652dfc58befc0cdf5d36b024aa89ec58e25e53\",\\n        \"url\": \"https://www.semanticscholar.org/paper/cd652dfc58befc0cdf5d36b024aa89ec58e25e53\",\\n        \"title\": \"Exploring a Structural Basis for Delayed Rod-Mediated Dark Adaptation in Age-Related Macular Degeneration Via Deep Learning\",\\n        \"citationCount\": 25,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://doi.org/10.1167/tvst.9.2.62\",\\n          \"status\": \"GOLD\",\\n          \"license\": \"CCBYNCND\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC7745629, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A novel framework for imaging biomarker discovery using deep learning is reported and its ability to identify and localize a previously undescribed biomarker in retinal imaging is demonstrated, strengthening the rationale for RMDA as an outcome measure in early AMD clinical trials.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2020-01-28\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"33556235\",\\n            \"name\": \"Aaron Y. Lee\"\\n          },\\n          {\\n            \"authorId\": \"1390581576\",\\n            \"name\": \"Cecilia S. Lee\"\\n          },\\n          {\\n            \"authorId\": \"1768174127\",\\n            \"name\": \"Marian Blazes\"\\n          },\\n          {\\n            \"authorId\": \"34629227\",\\n            \"name\": \"J. Owen\"\\n          },\\n          {\\n            \"authorId\": \"10732089\",\\n            \"name\": \"Y. Bagdasarova\"\\n          },\\n          {\\n            \"authorId\": \"2109036097\",\\n            \"name\": \"Yue Wu\"\\n          },\\n          {\\n            \"authorId\": \"1471465327\",\\n            \"name\": \"Ted Spaide\"\\n          },\\n          {\\n            \"authorId\": \"11022483\",\\n            \"name\": \"R. Yanagihara\"\\n          },\\n          {\\n            \"authorId\": \"47702381\",\\n            \"name\": \"Y. Kihara\"\\n          },\\n          {\\n            \"authorId\": \"6894914\",\\n            \"name\": \"Mark E. Clark\"\\n          },\\n          {\\n            \"authorId\": \"48882557\",\\n            \"name\": \"M. Kwon\"\\n          },\\n          {\\n            \"authorId\": \"144561682\",\\n            \"name\": \"C. Owsley\"\\n          },\\n          {\\n            \"authorId\": \"4477577\",\\n            \"name\": \"C. Curcio\"\\n          }\\n        ],\\n        \"abstract\": \"Purpose Delayed rod-mediated dark adaptation (RMDA) is a functional biomarker for incipient age-related macular degeneration (AMD). We used anatomically restricted spectral domain optical coherence tomography (SD-OCT) imaging data to localize de novo imaging features associated with and to test hypotheses about delayed RMDA. Methods Rod intercept time (RIT) was measured in participants with and without AMD at 5 degrees from the fovea, and macular SD-OCT images were obtained. A deep learning model was trained with anatomically restricted information using a single representative B-scan through the fovea of each eye. Mean-occlusion masking was utilized to isolate the relevant imaging features. Results The model identified hyporeflective outer retinal bands on macular SD-OCT associated with delayed RMDA. The validation mean standard error (MSE) registered to the foveal B-scan localized the lowest error to 0.5 mm temporal to the fovea center, within an overall low-error region across the rod-free zone and adjoining parafovea. Mean absolute error (MAE) on the test set was 4.71 minutes (8.8% of the dynamic range). Conclusions We report a novel framework for imaging biomarker discovery using deep learning and demonstrate its ability to identify and localize a previously undescribed biomarker in retinal imaging. The hyporeflective outer retinal bands in central macula on SD-OCT demonstrate a structural basis for dysfunctional rod vision that correlates to published histopathologic findings. Translational Relevance This agnostic approach to anatomic biomarker discovery strengthens the rationale for RMDA as an outcome measure in early AMD clinical trials, and also expands the utility of deep learning beyond automated diagnosis to fundamental discovery.\"\\n      },\\n      {\\n        \"paperId\": \"9752a2e33d76d3b2fd91c0285b21d8a6db5bcef4\",\\n        \"url\": \"https://www.semanticscholar.org/paper/9752a2e33d76d3b2fd91c0285b21d8a6db5bcef4\",\\n        \"title\": \"Effective Adaptation in Multi-Task Co-Training for Unified Autonomous Driving\",\\n        \"citationCount\": 46,\\n        \"openAccessPdf\": {\\n          \"url\": \"http://arxiv.org/pdf/2209.08953\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2209.08953, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A simple yet effective pretrain-adapt-finetune paradigm for general multi-task training, where the off-the-shelf pretrained models can be effectively adapted without increasing the training overhead is proposed.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2022-09-19\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"51291599\",\\n            \"name\": \"Xiwen Liang\"\\n          },\\n          {\\n            \"authorId\": \"51255576\",\\n            \"name\": \"Yangxin Wu\"\\n          },\\n          {\\n            \"authorId\": \"47180442\",\\n            \"name\": \"Jianhua Han\"\\n          },\\n          {\\n            \"authorId\": \"2143534132\",\\n            \"name\": \"Hang Xu\"\\n          },\\n          {\\n            \"authorId\": \"1691522\",\\n            \"name\": \"Chunjing Xu\"\\n          },\\n          {\\n            \"authorId\": \"2153397698\",\\n            \"name\": \"Xiaodan Liang\"\\n          }\\n        ],\\n        \"abstract\": \"Aiming towards a holistic understanding of multiple downstream tasks simultaneously, there is a need for extracting features with better transferability. Though many latest self-supervised pre-training methods have achieved impressive performance on various vision tasks under the prevailing pretrain-finetune paradigm, their generalization capacity to multi-task learning scenarios is yet to be explored. In this paper, we extensively investigate the transfer performance of various types of self-supervised methods, e.g., MoCo and SimCLR, on three downstream tasks, including semantic segmentation, drivable area segmentation, and traffic object detection, on the large-scale driving dataset BDD100K. We surprisingly find that their performances are sub-optimal or even lag far behind the single-task baseline, which may be due to the distinctions of training objectives and architectural design lied in the pretrain-finetune paradigm. To overcome this dilemma as well as avoid redesigning the resource-intensive pre-training stage, we propose a simple yet effective pretrain-adapt-finetune paradigm for general multi-task training, where the off-the-shelf pretrained models can be effectively adapted without increasing the training overhead. During the adapt stage, we utilize learnable multi-scale adapters to dynamically adjust the pretrained model weights supervised by multi-task objectives while leaving the pretrained knowledge untouched. Furthermore, we regard the vision-language pre-training model CLIP as a strong complement to the pretrain-adapt-finetune paradigm and propose a novel adapter named LV-Adapter, which incorporates language priors in the multi-task model via task-specific prompting and alignment between visual and textual features.\"\\n      },\\n      {\\n        \"paperId\": \"4001af48eb49bf367a2c0428fd66feec6db54180\",\\n        \"url\": \"https://www.semanticscholar.org/paper/4001af48eb49bf367a2c0428fd66feec6db54180\",\\n        \"title\": \"MEVG: Multi-event Video Generation with Text-to-Video Models\",\\n        \"citationCount\": 33,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2312.04086, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A last frame-aware diffusion process to preserve visual coherence between consecutive videos where each video consists of different events by initializing the latent and simultaneously adjusting noise in the latent to enhance the motion dynamic in a generated video.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2023-12-07\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2162960534\",\\n            \"name\": \"Gyeongrok Oh\"\\n          },\\n          {\\n            \"authorId\": \"2272756256\",\\n            \"name\": \"Jaehwan Jeong\"\\n          },\\n          {\\n            \"authorId\": \"2311572749\",\\n            \"name\": \"Sieun Kim\"\\n          },\\n          {\\n            \"authorId\": \"145965455\",\\n            \"name\": \"Wonmin Byeon\"\\n          },\\n          {\\n            \"authorId\": \"2239053726\",\\n            \"name\": \"Jinkyu Kim\"\\n          },\\n          {\\n            \"authorId\": \"2311578473\",\\n            \"name\": \"Sungwoong Kim\"\\n          },\\n          {\\n            \"authorId\": \"2142668751\",\\n            \"name\": \"Sangpil Kim\"\\n          }\\n        ],\\n        \"abstract\": \"We introduce a novel diffusion-based video generation method, generating a video showing multiple events given multiple individual sentences from the user. Our method does not require a large-scale video dataset since our method uses a pre-trained diffusion-based text-to-video generative model without a fine-tuning process. Specifically, we propose a last frame-aware diffusion process to preserve visual coherence between consecutive videos where each video consists of different events by initializing the latent and simultaneously adjusting noise in the latent to enhance the motion dynamic in a generated video. Furthermore, we find that the iterative update of latent vectors by referring to all the preceding frames maintains the global appearance across the frames in a video clip. To handle dynamic text input for video generation, we utilize a novel prompt generator that transfers course text messages from the user into the multiple optimal prompts for the text-to-video diffusion model. Extensive experiments and user studies show that our proposed method is superior to other video-generative models in terms of temporal coherency of content and semantics. Video examples are available on our project page: https://kuai-lab.github.io/eccv2024mevg.\"\\n      },\\n      {\\n        \"paperId\": \"a6f2f252c88983965f60dfa8325afcd2eed03eb1\",\\n        \"url\": \"https://www.semanticscholar.org/paper/a6f2f252c88983965f60dfa8325afcd2eed03eb1\",\\n        \"title\": \"SAM-OCTA: A Fine-Tuning Strategy for Applying Foundation Model OCTA Image Segmentation Tasks\",\\n        \"citationCount\": 22,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://arxiv.org/pdf/2309.11758\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2309.11758, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"The low-rank adaptation technique is adopted for foundation model fine-tuning and corresponding prompt point generation strategies to process various segmentation tasks on OCTA datasets to achieve state-of-the-art performance metrics.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2023-09-21\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2243437545\",\\n            \"name\": \"Chengliang Wang\"\\n          },\\n          {\\n            \"authorId\": \"2243298334\",\\n            \"name\": \"Xinrun Chen\"\\n          },\\n          {\\n            \"authorId\": \"2216447644\",\\n            \"name\": \"Haojian Ning\"\\n          },\\n          {\\n            \"authorId\": \"48831702\",\\n            \"name\": \"Shiying Li\"\\n          }\\n        ],\\n        \"abstract\": \"In the analysis of optical coherence tomography angiography (OCTA) images, the operation of segmenting specific targets is necessary. Existing methods typically train on supervised datasets with limited samples (approximately a few hundred), which can lead to overfitting. To address this, the low-rank adaptation technique is adopted for foundation model fine-tuning and proposed corresponding prompt point generation strategies to process various segmentation tasks on OCTA datasets. This method is named SAM-OCTA and has been experimented on the publicly available OCTA-500 dataset. While achieving state-of-the-art performance metrics, this method accomplishes local vessel segmentation as well as effective artery-vein segmentation, which was not well-solved in previous works. The code is available at: https://github.com/ShellRedia/SAM-OCTA.\"\\n      },\\n      {\\n        \"paperId\": \"7cf6085c39c60cbc45cd06aaa70242069828fda9\",\\n        \"url\": \"https://www.semanticscholar.org/paper/7cf6085c39c60cbc45cd06aaa70242069828fda9\",\\n        \"title\": \"Prompt-Based Multi-Modal Image Segmentation\",\\n        \"citationCount\": 17,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work proposes a system that can generate image segmentations based on arbitrary prompts at test time with a transformer-based decoder that enables dense prediction and allows for dynamic adaptation to generalized queries involving affordances or properties.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": null,\\n        \"authors\": [\\n          {\\n            \"authorId\": \"73235537\",\\n            \"name\": \"Timo L\\\\u00fcddecke\"\\n          },\\n          {\\n            \"authorId\": \"1746183\",\\n            \"name\": \"Alexander S. Ecker\"\\n          }\\n        ],\\n        \"abstract\": null\\n      },\\n      {\\n        \"paperId\": \"2d0b030d314a5aa8feaa03695e8471270130bdf9\",\\n        \"url\": \"https://www.semanticscholar.org/paper/2d0b030d314a5aa8feaa03695e8471270130bdf9\",\\n        \"title\": \"Imagine while Reasoning in Space: Multimodal Visualization-of-Thought\",\\n        \"citationCount\": 90,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2501.07542, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work proposes a new reasoning paradigm, Multimodal Visualization-of-Thought (MVoT), which enables visual thinking in MLLMs by generating image visualizations of their reasoning traces and introduces token discrepancy loss into autoregressive MLLMs to ensure high-quality visualization.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2025-01-13\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2155795167\",\\n            \"name\": \"Chengzu Li\"\\n          },\\n          {\\n            \"authorId\": \"51198241\",\\n            \"name\": \"Wenshan Wu\"\\n          },\\n          {\\n            \"authorId\": \"2339967968\",\\n            \"name\": \"Huanyu Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2258547658\",\\n            \"name\": \"Yan Xia\"\\n          },\\n          {\\n            \"authorId\": \"2273419590\",\\n            \"name\": \"Shaoguang Mao\"\\n          },\\n          {\\n            \"authorId\": \"2294850817\",\\n            \"name\": \"Li Dong\"\\n          },\\n          {\\n            \"authorId\": \"2339667880\",\\n            \"name\": \"Ivan Vuli\\'c\"\\n          },\\n          {\\n            \"authorId\": \"2249539478\",\\n            \"name\": \"Furu Wei\"\\n          }\\n        ],\\n        \"abstract\": \"Chain-of-Thought (CoT) prompting has proven highly effective for enhancing complex reasoning in Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs). Yet, it struggles in complex spatial reasoning tasks. Nonetheless, human cognition extends beyond language alone, enabling the remarkable capability to think in both words and images. Inspired by this mechanism, we propose a new reasoning paradigm, Multimodal Visualization-of-Thought (MVoT). It enables visual thinking in MLLMs by generating image visualizations of their reasoning traces. To ensure high-quality visualization, we introduce token discrepancy loss into autoregressive MLLMs. This innovation significantly improves both visual coherence and fidelity. We validate this approach through several dynamic spatial reasoning tasks. Experimental results reveal that MVoT demonstrates competitive performance across tasks. Moreover, it exhibits robust and reliable improvements in the most challenging scenarios where CoT fails. Ultimately, MVoT establishes new possibilities for complex reasoning tasks where visual thinking can effectively complement verbal reasoning.\"\\n      },\\n      {\\n        \"paperId\": \"06cb9838ae2e386360aaacbf50775e164a741bd9\",\\n        \"url\": \"https://www.semanticscholar.org/paper/06cb9838ae2e386360aaacbf50775e164a741bd9\",\\n        \"title\": \"AdaCLIP: Adapting CLIP with Hybrid Learnable Prompts for Zero-Shot Anomaly Detection\",\\n        \"citationCount\": 108,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2407.15795, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This study introduces AdaCLIP for the ZSAD task, leveraging a pre-trained vision-language model (VLM), CLIP, and incorporates learnable prompts into CLIP and optimizes them through training on auxiliary annotated anomaly detection data.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-07-22\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2165889312\",\\n            \"name\": \"Yunkang Cao\"\\n          },\\n          {\\n            \"authorId\": \"2281792059\",\\n            \"name\": \"Jiangning Zhang\"\\n          },\\n          {\\n            \"authorId\": \"1788584596\",\\n            \"name\": \"Luca Frittoli\"\\n          },\\n          {\\n            \"authorId\": \"2257750846\",\\n            \"name\": \"Yuqi Cheng\"\\n          },\\n          {\\n            \"authorId\": \"2257204312\",\\n            \"name\": \"Weiming Shen\"\\n          },\\n          {\\n            \"authorId\": \"2319130369\",\\n            \"name\": \"Giacomo Boracchi\"\\n          }\\n        ],\\n        \"abstract\": \"Zero-shot anomaly detection (ZSAD) targets the identification of anomalies within images from arbitrary novel categories. This study introduces AdaCLIP for the ZSAD task, leveraging a pre-trained vision-language model (VLM), CLIP. AdaCLIP incorporates learnable prompts into CLIP and optimizes them through training on auxiliary annotated anomaly detection data. Two types of learnable prompts are proposed: static and dynamic. Static prompts are shared across all images, serving to preliminarily adapt CLIP for ZSAD. In contrast, dynamic prompts are generated for each test image, providing CLIP with dynamic adaptation capabilities. The combination of static and dynamic prompts is referred to as hybrid prompts, and yields enhanced ZSAD performance. Extensive experiments conducted across 14 real-world anomaly detection datasets from industrial and medical domains indicate that AdaCLIP outperforms other ZSAD methods and can generalize better to different categories and even domains. Finally, our analysis highlights the importance of diverse auxiliary data and optimized prompts for enhanced generalization capacity. Code is available at https://github.com/caoyunkang/AdaCLIP.\"\\n      },\\n      {\\n        \"paperId\": \"8da668b01beb344624e8c47ee5b2110da0f339dd\",\\n        \"url\": \"https://www.semanticscholar.org/paper/8da668b01beb344624e8c47ee5b2110da0f339dd\",\\n        \"title\": \"VideoJAM: Joint Appearance-Motion Representations for Enhanced Motion Generation in Video Models\",\\n        \"citationCount\": 42,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2502.02492, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"VideoJAM is a novel framework that instills an effective motion prior to video generators, by encouraging the model to learn a joint appearance-motion representation, and introduces Inner-Guidance, a mechanism that steers the generation toward coherent motion by leveraging the model\\'s own evolving motion prediction as a dynamic guidance signal.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2025-02-04\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2038268012\",\\n            \"name\": \"Hila Chefer\"\\n          },\\n          {\\n            \"authorId\": \"88622696\",\\n            \"name\": \"Uriel Singer\"\\n          },\\n          {\\n            \"authorId\": \"2266842423\",\\n            \"name\": \"Amit Zohar\"\\n          },\\n          {\\n            \"authorId\": \"2044194129\",\\n            \"name\": \"Yuval Kirstain\"\\n          },\\n          {\\n            \"authorId\": \"33964593\",\\n            \"name\": \"Adam Polyak\"\\n          },\\n          {\\n            \"authorId\": \"2188620\",\\n            \"name\": \"Yaniv Taigman\"\\n          },\\n          {\\n            \"authorId\": \"2336960746\",\\n            \"name\": \"Lior Wolf\"\\n          },\\n          {\\n            \"authorId\": \"2086827528\",\\n            \"name\": \"Shelly Sheynin\"\\n          }\\n        ],\\n        \"abstract\": \"Despite tremendous recent progress, generative video models still struggle to capture real-world motion, dynamics, and physics. We show that this limitation arises from the conventional pixel reconstruction objective, which biases models toward appearance fidelity at the expense of motion coherence. To address this, we introduce VideoJAM, a novel framework that instills an effective motion prior to video generators, by encouraging the model to learn a joint appearance-motion representation. VideoJAM is composed of two complementary units. During training, we extend the objective to predict both the generated pixels and their corresponding motion from a single learned representation. During inference, we introduce Inner-Guidance, a mechanism that steers the generation toward coherent motion by leveraging the model\\'s own evolving motion prediction as a dynamic guidance signal. Notably, our framework can be applied to any video model with minimal adaptations, requiring no modifications to the training data or scaling of the model. VideoJAM achieves state-of-the-art performance in motion coherence, surpassing highly competitive proprietary models while also enhancing the perceived visual quality of the generations. These findings emphasize that appearance and motion can be complementary and, when effectively integrated, enhance both the visual quality and the coherence of video generation. Project website: https://hila-chefer.github.io/videojam-paper.github.io/\"\\n      },\\n      {\\n        \"paperId\": \"6851fb7196c27fdec6b787d4196f0a1c1c279e96\",\\n        \"url\": \"https://www.semanticscholar.org/paper/6851fb7196c27fdec6b787d4196f0a1c1c279e96\",\\n        \"title\": \"DialogGen: Multi-modal Interactive Dialogue System for Multi-turn Text-to-Image Generation\",\\n        \"citationCount\": 18,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2403.08857, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"The Multi-modal Dialogue Benchmark (DialogBen) is introduced, a comprehensive bilingual benchmark designed to assess the ability of MLLMs to generate accurate and coherent multi-modal content that supports image editing and contains two evaluation metrics to measure the model\\'s ability to switch modalities and the coherence of the output images.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-03-13\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2162225343\",\\n            \"name\": \"Minbin Huang\"\\n          },\\n          {\\n            \"authorId\": \"2072953799\",\\n            \"name\": \"Yanxin Long\"\\n          },\\n          {\\n            \"authorId\": \"2291203760\",\\n            \"name\": \"Xinchi Deng\"\\n          },\\n          {\\n            \"authorId\": \"2291135101\",\\n            \"name\": \"Ruihang Chu\"\\n          },\\n          {\\n            \"authorId\": \"2291439620\",\\n            \"name\": \"Jiangfeng Xiong\"\\n          },\\n          {\\n            \"authorId\": \"2291389227\",\\n            \"name\": \"Xiaodan Liang\"\\n          },\\n          {\\n            \"authorId\": \"2290858486\",\\n            \"name\": \"Hong Cheng\"\\n          },\\n          {\\n            \"authorId\": \"2268722917\",\\n            \"name\": \"Qinglin Lu\"\\n          },\\n          {\\n            \"authorId\": \"2269146248\",\\n            \"name\": \"Wei Liu\"\\n          }\\n        ],\\n        \"abstract\": \"Text-to-image (T2I) generation models have significantly advanced in recent years. However, effective interaction with these models is challenging for average users due to the need for specialized prompt engineering knowledge and the inability to perform multi-turn image generation, hindering a dynamic and iterative creation process. Recent attempts have tried to equip Multi-modal Large Language Models (MLLMs) with T2I models to bring the user\\'s natural language instructions into reality. Hence, the output modality of MLLMs is extended, and the multi-turn generation quality of T2I models is enhanced thanks to the strong multi-modal comprehension ability of MLLMs. However, many of these works face challenges in identifying correct output modalities and generating coherent images accordingly as the number of output modalities increases and the conversations go deeper. Therefore, we propose DialogGen, an effective pipeline to align off-the-shelf MLLMs and T2I models to build a Multi-modal Interactive Dialogue System (MIDS) for multi-turn Text-to-Image generation. It is composed of drawing prompt alignment, careful training data curation, and error correction. Moreover, as the field of MIDS flourishes, comprehensive benchmarks are urgently needed to evaluate MIDS fairly in terms of output modality correctness and multi-modal output coherence. To address this issue, we introduce the Multi-modal Dialogue Benchmark (DialogBen), a comprehensive bilingual benchmark designed to assess the ability of MLLMs to generate accurate and coherent multi-modal content that supports image editing. It contains two evaluation metrics to measure the model\\'s ability to switch modalities and the coherence of the output images. Our extensive experiments on DialogBen and user study demonstrate the effectiveness of DialogGen compared with other State-of-the-Art models.\"\\n      },\\n      {\\n        \"paperId\": \"fff1e9eb32f8591aea3f2db54035465093381b89\",\\n        \"url\": \"https://www.semanticscholar.org/paper/fff1e9eb32f8591aea3f2db54035465093381b89\",\\n        \"title\": \"Selective Prompting Tuning for Personalized Conversations with LLMs\",\\n        \"citationCount\": 13,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2406.18187, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work initializes a set of soft prompts and uses a trainable dense retriever to adaptively select suitable soft prompts for LLMs according to different input contexts, where the prompt retriever is dynamically updated through feedback from the LLMs.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-06-26\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2274015208\",\\n            \"name\": \"Qiushi Huang\"\\n          },\\n          {\\n            \"authorId\": \"2110814131\",\\n            \"name\": \"Xubo Liu\"\\n          },\\n          {\\n            \"authorId\": \"2273320275\",\\n            \"name\": \"Tom Ko\"\\n          },\\n          {\\n            \"authorId\": \"49814531\",\\n            \"name\": \"Boyong Wu\"\\n          },\\n          {\\n            \"authorId\": \"2239051433\",\\n            \"name\": \"Wenwu Wang\"\\n          },\\n          {\\n            \"authorId\": \"2273525536\",\\n            \"name\": \"Yu Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2189113746\",\\n            \"name\": \"Lilian Tang\"\\n          }\\n        ],\\n        \"abstract\": \"In conversational AI, personalizing dialogues with persona profiles and contextual understanding is essential. Despite large language models\\' (LLMs) improved response coherence, effective persona integration remains a challenge. In this work, we first study two common approaches for personalizing LLMs: textual prompting and direct fine-tuning. We observed that textual prompting often struggles to yield responses that are similar to the ground truths in datasets, while direct fine-tuning tends to produce repetitive or overly generic replies. To alleviate those issues, we propose \\\\\\\\textbf{S}elective \\\\\\\\textbf{P}rompt \\\\\\\\textbf{T}uning (SPT), which softly prompts LLMs for personalized conversations in a selective way. Concretely, SPT initializes a set of soft prompts and uses a trainable dense retriever to adaptively select suitable soft prompts for LLMs according to different input contexts, where the prompt retriever is dynamically updated through feedback from the LLMs. Additionally, we propose context-prompt contrastive learning and prompt fusion learning to encourage the SPT to enhance the diversity of personalized conversations. Experiments on the CONVAI2 dataset demonstrate that SPT significantly enhances response diversity by up to 90\\\\\\\\%, along with improvements in other critical performance indicators. Those results highlight the efficacy of SPT in fostering engaging and personalized dialogue generation. The SPT model code (https://github.com/hqsiswiliam/SPT) is publicly available for further exploration.\"\\n      },\\n      {\\n        \"paperId\": \"5139b7a2744749517163dcfdccf13588aab287bf\",\\n        \"url\": \"https://www.semanticscholar.org/paper/5139b7a2744749517163dcfdccf13588aab287bf\",\\n        \"title\": \"Transformer-Squared: Self-adaptive LLMs\",\\n        \"citationCount\": 10,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2501.06252, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"Transformer-Squared is introduced, a novel self-adaptation framework that adapts LLMs for unseen tasks in real-time by selectively adjusting only the singular components of their weight matrices, and demonstrates versatility across different LLM architectures and modalities, including vision-language tasks.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2025-01-09\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2326487319\",\\n            \"name\": \"Qi Sun\"\\n          },\\n          {\\n            \"authorId\": \"2326294214\",\\n            \"name\": \"Edoardo Cetin\"\\n          },\\n          {\\n            \"authorId\": \"2326422916\",\\n            \"name\": \"Yujin Tang\"\\n          }\\n        ],\\n        \"abstract\": \"Self-adaptive large language models (LLMs) aim to solve the challenges posed by traditional fine-tuning methods, which are often computationally intensive and static in their ability to handle diverse tasks. We introduce Transformer-Squared, a novel self-adaptation framework that adapts LLMs for unseen tasks in real-time by selectively adjusting only the singular components of their weight matrices. During inference, Transformer-Squared employs a two-pass mechanism: first, a dispatch system identifies the task properties, and then task-specific \\'expert\\' vectors, trained using reinforcement learning, are dynamically mixed to obtain targeted behavior for the incoming prompt. Our method consistently outperforms ubiquitous approaches such as LoRA, with fewer parameters and greater efficiency. Furthermore, Transformer-Squared demonstrates versatility across different LLM architectures and modalities, including vision-language tasks. Transformer-Squared represents a significant leap forward, offering a scalable, efficient solution for enhancing the adaptability and task-specific performance of LLMs, paving the way for truly dynamic, self-organizing AI systems.\"\\n      },\\n      {\\n        \"paperId\": \"77711615485303032fc878d495943139ab2558e1\",\\n        \"url\": \"https://www.semanticscholar.org/paper/77711615485303032fc878d495943139ab2558e1\",\\n        \"title\": \"Vivid-ZOO: Multi-View Video Generation with Diffusion Model\",\\n        \"citationCount\": 30,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2406.08659, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A novel diffusion-based pipeline is proposed that generates high-quality multi-view videos centered around a dynamic 3D object from text, exhibiting vivid motions, temporal coherence, and multi-view consistency, given a variety of text prompts.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-06-12\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2287925633\",\\n            \"name\": \"Bing Li\"\\n          },\\n          {\\n            \"authorId\": \"2306182343\",\\n            \"name\": \"Cheng Zheng\"\\n          },\\n          {\\n            \"authorId\": \"2306084700\",\\n            \"name\": \"Wenxuan Zhu\"\\n          },\\n          {\\n            \"authorId\": \"2005711751\",\\n            \"name\": \"Jinjie Mai\"\\n          },\\n          {\\n            \"authorId\": \"2129438190\",\\n            \"name\": \"Biao Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2262444458\",\\n            \"name\": \"Peter Wonka\"\\n          },\\n          {\\n            \"authorId\": \"2288557868\",\\n            \"name\": \"Bernard Ghanem\"\\n          }\\n        ],\\n        \"abstract\": \"While diffusion models have shown impressive performance in 2D image/video generation, diffusion-based Text-to-Multi-view-Video (T2MVid) generation remains underexplored. The new challenges posed by T2MVid generation lie in the lack of massive captioned multi-view videos and the complexity of modeling such multi-dimensional distribution. To this end, we propose a novel diffusion-based pipeline that generates high-quality multi-view videos centered around a dynamic 3D object from text. Specifically, we factor the T2MVid problem into viewpoint-space and time components. Such factorization allows us to combine and reuse layers of advanced pre-trained multi-view image and 2D video diffusion models to ensure multi-view consistency as well as temporal coherence for the generated multi-view videos, largely reducing the training cost. We further introduce alignment modules to align the latent spaces of layers from the pre-trained multi-view and the 2D video diffusion models, addressing the reused layers\\' incompatibility that arises from the domain gap between 2D and multi-view data. In support of this and future research, we further contribute a captioned multi-view video dataset. Experimental results demonstrate that our method generates high-quality multi-view videos, exhibiting vivid motions, temporal coherence, and multi-view consistency, given a variety of text prompts.\"\\n      },\\n      {\\n        \"paperId\": \"4be040c953d8580f0127105924a8bbf139c4804d\",\\n        \"url\": \"https://www.semanticscholar.org/paper/4be040c953d8580f0127105924a8bbf139c4804d\",\\n        \"title\": \"Unbounded: A Generative Infinite Game of Character Life Simulation\",\\n        \"citationCount\": 11,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2410.18975, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A specialized, distilled large language model (LLM) that dynamically generates game mechanics, narratives, and character interactions in real-time, and a new dynamic regional image prompt Adapter (IP-Adapter) for vision models that ensures consistent yet flexible visual generation of a character across multiple environments.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-10-24\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2108961694\",\\n            \"name\": \"Jialu Li\"\\n          },\\n          {\\n            \"authorId\": \"2167749913\",\\n            \"name\": \"Yuanzhen Li\"\\n          },\\n          {\\n            \"authorId\": \"2248172496\",\\n            \"name\": \"Neal Wadhwa\"\\n          },\\n          {\\n            \"authorId\": \"1782328\",\\n            \"name\": \"Y. Pritch\"\\n          },\\n          {\\n            \"authorId\": \"2248055731\",\\n            \"name\": \"David E. Jacobs\"\\n          },\\n          {\\n            \"authorId\": \"2248163830\",\\n            \"name\": \"Michael Rubinstein\"\\n          },\\n          {\\n            \"authorId\": \"2276608813\",\\n            \"name\": \"Mohit Bansal\"\\n          },\\n          {\\n            \"authorId\": \"2248173214\",\\n            \"name\": \"Nataniel Ruiz\"\\n          }\\n        ],\\n        \"abstract\": \"We introduce the concept of a generative infinite game, a video game that transcends the traditional boundaries of finite, hard-coded systems by using generative models. Inspired by James P. Carse\\'s distinction between finite and infinite games, we leverage recent advances in generative AI to create Unbounded: a game of character life simulation that is fully encapsulated in generative models. Specifically, Unbounded draws inspiration from sandbox life simulations and allows you to interact with your autonomous virtual character in a virtual world by feeding, playing with and guiding it - with open-ended mechanics generated by an LLM, some of which can be emergent. In order to develop Unbounded, we propose technical innovations in both the LLM and visual generation domains. Specifically, we present: (1) a specialized, distilled large language model (LLM) that dynamically generates game mechanics, narratives, and character interactions in real-time, and (2) a new dynamic regional image prompt Adapter (IP-Adapter) for vision models that ensures consistent yet flexible visual generation of a character across multiple environments. We evaluate our system through both qualitative and quantitative analysis, showing significant improvements in character life simulation, user instruction following, narrative coherence, and visual consistency for both characters and the environments compared to traditional related approaches.\"\\n      }\\n    ]\\n  },\\n  \"long context dialogue\": {\\n    \"message\": \"Too Many Requests. Please wait and try again or apply for a key for higher rate limits. https://www.semanticscholar.org/product/api#api-key-form\",\\n    \"code\": \"429\"\\n  },\\n  \"adaptive prompt generation narrative\": {\\n    \"total\": 410,\\n    \"offset\": 0,\\n    \"next\": 50,\\n    \"data\": [\\n      {\\n        \"paperId\": \"ef399ed62fcc00e73b02f286012080351652693c\",\\n        \"url\": \"https://www.semanticscholar.org/paper/ef399ed62fcc00e73b02f286012080351652693c\",\\n        \"title\": \"Prompt-prompted Adaptive Structured Pruning for Efficient LLM Generation\",\\n        \"citationCount\": 15,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2404.01365, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A novel training-free and calibration-free method that selects unique FF experts at the sequence level for efficient generation across a plethora of LLMs with different non-ReLU activation functions, which is possible due to a critical observation that many trained LLMs naturally produce highly structured FF activation patterns within a sequence, which the authors call flocking.\"\\n        },\\n        \"publicationTypes\": null,\\n        \"publicationDate\": \"2024-04-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2171964328\",\\n            \"name\": \"Harry Dong\"\\n          },\\n          {\\n            \"authorId\": \"2282555057\",\\n            \"name\": \"Beidi Chen\"\\n          },\\n          {\\n            \"authorId\": \"2284063779\",\\n            \"name\": \"Yuejie Chi\"\\n          }\\n        ],\\n        \"abstract\": \"With the development of transformer-based large language models (LLMs), they have been applied to many fields due to their remarkable utility, but this comes at a considerable computational cost at deployment. Fortunately, some methods such as pruning or constructing a mixture of experts (MoE) aim at exploiting sparsity in transformer feedforward (FF) blocks to gain boosts in speed and reduction in memory requirements. However, these techniques can be very costly and inflexible in practice, as they often require training or are restricted to specific types of architectures. To address this, we introduce GRIFFIN, a novel training-free and calibration-free method that selects unique FF experts at the sequence level for efficient generation across a plethora of LLMs with different non-ReLU activation functions. This is possible due to a critical observation that many trained LLMs naturally produce highly structured FF activation patterns within a sequence, which we call flocking. Despite our method\\'s simplicity, we show with 50% of the FF parameters, GRIFFIN maintains the original model\\'s performance with little to no degradation on a variety of classification and generation tasks, all while improving latency (e.g. 1.29$\\\\\\\\times$ and 1.25$\\\\\\\\times$ speed-ups in Gemma 7B and Llama 2 13B, respectively, on an NVIDIA L40). Code is available at https://github.com/hdong920/GRIFFIN.\"\\n      },\\n      {\\n        \"paperId\": \"bde9fc752842a2a91c70933169fb8cee2b81f8b2\",\\n        \"url\": \"https://www.semanticscholar.org/paper/bde9fc752842a2a91c70933169fb8cee2b81f8b2\",\\n        \"title\": \"Prompt Expansion for Adaptive Text-to-Image Generation\",\\n        \"citationCount\": 23,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2312.16720, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A human evaluation study shows that images generated through Prompt Expansion are more aesthetically pleasing and diverse than those generated by baseline methods.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2023-12-27\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2070966889\",\\n            \"name\": \"Siddhartha Datta\"\\n          },\\n          {\\n            \"authorId\": \"2276608298\",\\n            \"name\": \"Alexander Ku\"\\n          },\\n          {\\n            \"authorId\": \"2275054270\",\\n            \"name\": \"Deepak Ramachandran\"\\n          },\\n          {\\n            \"authorId\": \"2276610768\",\\n            \"name\": \"Peter Anderson\"\\n          }\\n        ],\\n        \"abstract\": \"Text-to-image generation models are powerful but difficult to use. Users craft specific prompts to get better images, though the images can be repetitive. This paper proposes a Prompt Expansion framework that helps users generate high-quality, diverse images with less effort. The Prompt Expansion model takes a text query as input and outputs a set of expanded text prompts that are optimized such that when passed to a text-to-image model, generates a wider variety of appealing images. We conduct a human evaluation study that shows that images generated through Prompt Expansion are more aesthetically pleasing and diverse than those generated by baseline methods. Overall, this paper presents a novel and effective approach to improving the text-to-image generation experience.\"\\n      },\\n      {\\n        \"paperId\": \"c36cb4a41369369d837ea170397f7818d02150dd\",\\n        \"url\": \"https://www.semanticscholar.org/paper/c36cb4a41369369d837ea170397f7818d02150dd\",\\n        \"title\": \"Soft Prompt Generation for Domain Generalization\",\\n        \"citationCount\": 23,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2404.19286, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A simple yet efficient method for the Domain Generalization (DG) task, namely Soft Prompt Generation (SPG), which consists of a two-stage training phase and an inference phase, aiming to incorporate the generative model domain knowledge.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-04-30\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2274938328\",\\n            \"name\": \"Shuanghao Bai\"\\n          },\\n          {\\n            \"authorId\": \"2290728057\",\\n            \"name\": \"Yuedi Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2275025483\",\\n            \"name\": \"Wanqi Zhou\"\\n          },\\n          {\\n            \"authorId\": \"7778036\",\\n            \"name\": \"Zhirong Luan\"\\n          },\\n          {\\n            \"authorId\": \"2275030348\",\\n            \"name\": \"Badong Chen\"\\n          }\\n        ],\\n        \"abstract\": \"Large pre-trained vision language models (VLMs) have shown impressive zero-shot ability on downstream tasks with manually designed prompt. To further adapt VLMs to downstream tasks, soft prompt is proposed to replace manually designed prompt, which undergoes fine-tuning based on specific domain data. Prior prompt learning methods primarily learn a fixed prompt or residuled prompt from training samples. However, the learned prompts lack diversity and ignore information about unseen domains. In this paper, we reframe the prompt learning framework from a generative perspective and propose a simple yet efficient method for the Domain Generalization (DG) task, namely Soft Prompt Generation (SPG). Specifically, SPG consists of a two-stage training phase and an inference phase. During the training phase, we introduce soft prompt label for each domain, aiming to incorporate the generative model domain knowledge. During the inference phase, the generator of the generative model is employed to obtain instance-specific soft prompts for the unseen target domain. Extensive experiments on five domain generalization benchmarks of three DG tasks demonstrate that SPG achieves state-of-the-art performance. The code is available at https://github.com/renytek13/Soft-Prompt-Generation-with-CGAN.\"\\n      },\\n      {\\n        \"paperId\": \"1d954600f17a7c3f16aa726a5eaa902d6851e808\",\\n        \"url\": \"https://www.semanticscholar.org/paper/1d954600f17a7c3f16aa726a5eaa902d6851e808\",\\n        \"title\": \"SAFREE: Training-Free and Adaptive Guard for Safe Text-to-Image And Video Generation\",\\n        \"citationCount\": 50,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2410.12761, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work proposes SAFREE, a novel, training-free approach for safe T2I and T2V, that does not alter the model\\'s weights and incorporates a novel self-validating filtering mechanism that dynamically adjusts the denoising steps when applying the filtered embeddings.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-10-16\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"13563486\",\\n            \"name\": \"Jaehong Yoon\"\\n          },\\n          {\\n            \"authorId\": \"2164249715\",\\n            \"name\": \"Shoubin Yu\"\\n          },\\n          {\\n            \"authorId\": \"2061083016\",\\n            \"name\": \"Vaidehi Patil\"\\n          },\\n          {\\n            \"authorId\": \"2267311471\",\\n            \"name\": \"Huaxiu Yao\"\\n          },\\n          {\\n            \"authorId\": \"2276608813\",\\n            \"name\": \"Mohit Bansal\"\\n          }\\n        ],\\n        \"abstract\": \"Recent advances in diffusion models have significantly enhanced their ability to generate high-quality images and videos, but they have also increased the risk of producing unsafe content. Existing unlearning/editing-based methods for safe generation remove harmful concepts from models but face several challenges: (1) They cannot instantly remove harmful concepts without training. (2) Their safe generation capabilities depend on collected training data. (3) They alter model weights, risking degradation in quality for content unrelated to toxic concepts. To address these, we propose SAFREE, a novel, training-free approach for safe T2I and T2V, that does not alter the model\\'s weights. Specifically, we detect a subspace corresponding to a set of toxic concepts in the text embedding space and steer prompt embeddings away from this subspace, thereby filtering out harmful content while preserving intended semantics. To balance the trade-off between filtering toxicity and preserving safe concepts, SAFREE incorporates a novel self-validating filtering mechanism that dynamically adjusts the denoising steps when applying the filtered embeddings. Additionally, we incorporate adaptive re-attention mechanisms within the diffusion latent space to selectively diminish the influence of features related to toxic concepts at the pixel level. In the end, SAFREE ensures coherent safety checking, preserving the fidelity, quality, and safety of the output. SAFREE achieves SOTA performance in suppressing unsafe content in T2I generation compared to training-free baselines and effectively filters targeted concepts while maintaining high-quality images. It also shows competitive results against training-based methods. We extend SAFREE to various T2I backbones and T2V tasks, showcasing its flexibility and generalization. SAFREE provides a robust and adaptable safeguard for ensuring safe visual generation.\"\\n      },\\n      {\\n        \"paperId\": \"37e1bc43d22da21d6be616522e3fe217bf7f3d8e\",\\n        \"url\": \"https://www.semanticscholar.org/paper/37e1bc43d22da21d6be616522e3fe217bf7f3d8e\",\\n        \"title\": \"Adapting to Distribution Shift by Visual Domain Prompt Generation\",\\n        \"citationCount\": 19,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2405.02797, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work proposes a domain-aware contrastive loss and employ meta-learning to facilitate domain knowledge extraction and outperforms previous work on 5 large-scale benchmarks including WILDS and DomainNet.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-05-05\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"35793956\",\\n            \"name\": \"Zhixiang Chi\"\\n          },\\n          {\\n            \"authorId\": \"2300096585\",\\n            \"name\": \"Li Gu\"\\n          },\\n          {\\n            \"authorId\": \"2300089295\",\\n            \"name\": \"Tao Zhong\"\\n          },\\n          {\\n            \"authorId\": \"2277793919\",\\n            \"name\": \"Huan Liu\"\\n          },\\n          {\\n            \"authorId\": \"1787848\",\\n            \"name\": \"Yuanhao Yu\"\\n          },\\n          {\\n            \"authorId\": \"2277598061\",\\n            \"name\": \"Konstantinos N. Plataniotis\"\\n          },\\n          {\\n            \"authorId\": \"2277695392\",\\n            \"name\": \"Yang Wang\"\\n          }\\n        ],\\n        \"abstract\": \"In this paper, we aim to adapt a model at test-time using a few unlabeled data to address distribution shifts. To tackle the challenges of extracting domain knowledge from a limited amount of data, it is crucial to utilize correlated information from pre-trained backbones and source domains. Previous studies fail to utilize recent foundation models with strong out-of-distribution generalization. Additionally, domain-centric designs are not flavored in their works. Furthermore, they employ the process of modelling source domains and the process of learning to adapt independently into disjoint training stages. In this work, we propose an approach on top of the pre-computed features of the foundation model. Specifically, we build a knowledge bank to learn the transferable knowledge from source domains. Conditioned on few-shot target data, we introduce a domain prompt generator to condense the knowledge bank into a domain-specific prompt. The domain prompt then directs the visual features towards a particular domain via a guidance module. Moreover, we propose a domain-aware contrastive loss and employ meta-learning to facilitate domain knowledge extraction. Extensive experiments are conducted to validate the domain knowledge extraction. The proposed method outperforms previous work on 5 large-scale benchmarks including WILDS and DomainNet.\"\\n      },\\n      {\\n        \"paperId\": \"ac4dc5ff917141f30b53314f765d30fb4b83c8b7\",\\n        \"url\": \"https://www.semanticscholar.org/paper/ac4dc5ff917141f30b53314f765d30fb4b83c8b7\",\\n        \"title\": \"EviPrompt: A Training-Free Evidential Prompt Generation Method for Adapting Segment Anything Model in Medical Images\",\\n        \"citationCount\": 14,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {\\'tldr\\'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/TIP.2024.3482175?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/TIP.2024.3482175, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": null,\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-10-22\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2145205189\",\\n            \"name\": \"Yinsong Xu\"\\n          },\\n          {\\n            \"authorId\": \"2266873357\",\\n            \"name\": \"Jiaqi Tang\"\\n          },\\n          {\\n            \"authorId\": \"2266387679\",\\n            \"name\": \"Aidong Men\"\\n          },\\n          {\\n            \"authorId\": \"2266583142\",\\n            \"name\": \"Qingchao Chen\"\\n          }\\n        ],\\n        \"abstract\": \"Medical image segmentation is a critical task in clinical applications. Recently, the Segment Anything Model (SAM) has demonstrated potential for natural image segmentation. However, the requirement for expert labour to provide prompts, and the domain gap between natural and medical images pose significant obstacles in adapting SAM to medical images. To overcome these challenges, this paper introduces a novel prompt generation method named EviPrompt. The proposed method requires only a single reference image-annotation pair, making it a training-free solution that significantly reduces the need for extensive labelling and computational resources. First, prompts are automatically generated based on the similarity between features of the reference and target images, and evidential learning is introduced to improve reliability. Then, to mitigate the impact of the domain gap, committee voting and inference-guided in-context learning are employed, generating prompts primarily based on human prior knowledge and reducing reliance on extracted semantic information. EviPrompt represents an efficient and robust approach to medical image segmentation. We evaluate it across a broad range of tasks and modalities, confirming its efficacy. The source code is available at https://github.com/SPIresearch/EviPrompt.\"\\n      },\\n      {\\n        \"paperId\": \"6ae06f2bf66f19637adc695bc47256eeb1635b10\",\\n        \"url\": \"https://www.semanticscholar.org/paper/6ae06f2bf66f19637adc695bc47256eeb1635b10\",\\n        \"title\": \"A Structured Narrative Prompt for Prompting Narratives from Large Language Models: Sentiment Assessment of ChatGPT-Generated Narratives and Real Tweets\",\\n        \"citationCount\": 22,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://www.mdpi.com/1999-5903/15/12/375/pdf?version=1700746546\",\\n          \"status\": \"GOLD\",\\n          \"license\": \"CCBY\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.3390/fi15120375?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.3390/fi15120375, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A structured narrative prompt for sending queries to LLMs, an experiment with the narrative generation process using OpenAI\\'s ChatGPT, and an assessment of statistical significance in the sentiment prevalence of the generated narratives compared to real tweets indicate significant promise in the utilization of LLMs for helping to connect a simulated agent\\\\u2019s experiences with real people.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2023-11-23\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2253939386\",\\n            \"name\": \"Christopher J. Lynch\"\\n          },\\n          {\\n            \"authorId\": \"2222695010\",\\n            \"name\": \"Erik J. Jensen\"\\n          },\\n          {\\n            \"authorId\": \"2268131942\",\\n            \"name\": \"Virginia Zamponi\"\\n          },\\n          {\\n            \"authorId\": \"2059262010\",\\n            \"name\": \"Kevin O\\'Brien\"\\n          },\\n          {\\n            \"authorId\": \"144563993\",\\n            \"name\": \"Erika F. Frydenlund\"\\n          },\\n          {\\n            \"authorId\": \"1442241538\",\\n            \"name\": \"Ross J. Gore\"\\n          }\\n        ],\\n        \"abstract\": \"Large language models (LLMs) excel in providing natural language responses that sound authoritative, reflect knowledge of the context area, and can present from a range of varied perspectives. Agent-based models and simulations consist of simulated agents that interact within a simulated environment to explore societal, social, and ethical, among other, problems. Simulated agents generate large volumes of data and discerning useful and relevant content is an onerous task. LLMs can help in communicating agents\\\\u2019 perspectives on key life events by providing natural language narratives. However, these narratives should be factual, transparent, and reproducible. Therefore, we present a structured narrative prompt for sending queries to LLMs, we experiment with the narrative generation process using OpenAI\\\\u2019s ChatGPT, and we assess statistically significant differences across 11 Positive and Negative Affect Schedule (PANAS) sentiment levels between the generated narratives and real tweets using chi-squared tests and Fisher\\\\u2019s exact tests. The narrative prompt structure effectively yields narratives with the desired components from ChatGPT. In four out of forty-four categories, ChatGPT generated narratives which have sentiment scores that were not discernibly different, in terms of statistical significance (alpha level \\\\u03b1=0.05), from the sentiment expressed in real tweets. Three outcomes are provided: (1) a list of benefits and challenges for LLMs in narrative generation; (2) a structured prompt for requesting narratives of an LLM chatbot based on simulated agents\\\\u2019 information; (3) an assessment of statistical significance in the sentiment prevalence of the generated narratives compared to real tweets. This indicates significant promise in the utilization of LLMs for helping to connect a simulated agent\\\\u2019s experiences with real people.\"\\n      },\\n      {\\n        \"paperId\": \"a05b33072a8330413947eab0833ed2fd21de4963\",\\n        \"url\": \"https://www.semanticscholar.org/paper/a05b33072a8330413947eab0833ed2fd21de4963\",\\n        \"title\": \"Compound Text-Guided Prompt Tuning via Image-Adaptive Cues\",\\n        \"citationCount\": 12,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2312.06401, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work introduces text supervision to the optimization of prompts, which enables two benefits: 1) releasing the model reliance on the pre-defined category names during inference, thereby enabling more flexible prompt generation; 2) reducing the number of inputs to the text encoder, which decreases GPU memory consumption significantly.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2023-12-11\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2273557458\",\\n            \"name\": \"Hao Tan\"\\n          },\\n          {\\n            \"authorId\": \"2273559489\",\\n            \"name\": \"Jun Li\"\\n          },\\n          {\\n            \"authorId\": \"2118764798\",\\n            \"name\": \"Yizhuang Zhou\"\\n          },\\n          {\\n            \"authorId\": \"2273589717\",\\n            \"name\": \"Jun Wan\"\\n          },\\n          {\\n            \"authorId\": \"2113457514\",\\n            \"name\": \"Zhen Lei\"\\n          },\\n          {\\n            \"authorId\": \"2274088311\",\\n            \"name\": \"Xiangyu Zhang\"\\n          }\\n        ],\\n        \"abstract\": \"Vision-Language Models (VLMs) such as CLIP have demonstrated remarkable generalization capabilities to downstream tasks. However, existing prompt tuning based frameworks need to parallelize learnable textual inputs for all categories, suffering from massive GPU memory consumption when there is a large number of categories in the target dataset. Moreover, previous works require to include category names within prompts, exhibiting subpar performance when dealing with ambiguous category names. To address these shortcomings, we propose Compound Text-Guided Prompt Tuning (TGP-T) that significantly reduces resource demand while achieving superior performance. We introduce text supervision to the optimization of prompts, which enables two benefits: 1) releasing the model reliance on the pre-defined category names during inference, thereby enabling more flexible prompt generation; 2) reducing the number of inputs to the text encoder, which decreases GPU memory consumption significantly. Specifically, we found that compound text supervisions, i.e., category-wise and content-wise, is highly effective, since they provide inter-class separability and capture intra-class variations, respectively. Moreover, we condition the prompt generation on visual features through a module called Bonder, which facilitates the alignment between prompts and visual features. Extensive experiments on few-shot recognition and domain generalization demonstrate that TGP-T achieves superior performance with consistently lower training costs. It reduces GPU memory usage by 93% and attains a 2.5% performance gain on 16-shot ImageNet. The code is available at https://github.com/EricTan7/TGP-T.\"\\n      },\\n      {\\n        \"paperId\": \"b53dba04b2518ebed943daa9ab58f19af81e2012\",\\n        \"url\": \"https://www.semanticscholar.org/paper/b53dba04b2518ebed943daa9ab58f19af81e2012\",\\n        \"title\": \"Efficient Model Personalization in Federated Learning via Client-Specific Prompt Generation\",\\n        \"citationCount\": 98,\\n        \"openAccessPdf\": {\\n          \"url\": \"http://arxiv.org/pdf/2308.15367\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2308.15367, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work proposes a novel personalized FL framework of client-specific Prompt Generation (pFedPG), which learns to deploy a personalized prompt generator at the server for producing client- specific visual prompts that efficiently adapts frozen backbones to local data distributions.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2023-08-29\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"41015732\",\\n            \"name\": \"Fu-En Yang\"\\n          },\\n          {\\n            \"authorId\": \"48586406\",\\n            \"name\": \"Chien-Yi Wang\"\\n          },\\n          {\\n            \"authorId\": \"2218628989\",\\n            \"name\": \"Yu-Chiang Frank Wang\"\\n          }\\n        ],\\n        \"abstract\": \"Federated learning (FL) emerges as a decentralized learning framework which trains models from multiple distributed clients without sharing their data to preserve privacy. Recently, large-scale pre-trained models (e.g., Vision Transformer) have shown a strong capability of deriving robust representations. However, the data heterogeneity among clients, the limited computation resources, and the communication bandwidth restrict the deployment of large-scale models in FL frameworks. To leverage robust representations from large-scale models while enabling efficient model personalization for heterogeneous clients, we propose a novel personalized FL framework of client-specific Prompt Generation (pFedPG), which learns to deploy a personalized prompt generator at the server for producing client-specific visual prompts that efficiently adapts frozen backbones to local data distributions. Our proposed framework jointly optimizes the stages of personalized prompt adaptation locally and personalized prompt generation globally. The former aims to train visual prompts that adapt foundation models to each client, while the latter observes local optimization directions to generate personalized prompts for all clients. Through extensive experiments on benchmark datasets, we show that our pFedPG is favorable against state-of-the-art personalized FL methods under various types of data heterogeneity, allowing computation and communication efficient model personalization.\"\\n      },\\n      {\\n        \"paperId\": \"d92a423e09804595c8a2e241f890f5a24d326bb5\",\\n        \"url\": \"https://www.semanticscholar.org/paper/d92a423e09804595c8a2e241f890f5a24d326bb5\",\\n        \"title\": \"Prompt-based Code Completion via Multi-Retrieval Augmented Generation\",\\n        \"citationCount\": 17,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://arxiv.org/pdf/2405.07530\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2405.07530, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"ProCC, a code completion framework leveraging prompt engineering and the contextual multi-armed bandits algorithm to flexibly incorporate and adapt to multiple perspectives of code, is proposed.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-05-13\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2176503078\",\\n            \"name\": \"Hanzhuo Tan\"\\n          },\\n          {\\n            \"authorId\": \"2290488730\",\\n            \"name\": \"Qi Luo\"\\n          },\\n          {\\n            \"authorId\": \"51181043\",\\n            \"name\": \"Lingixao Jiang\"\\n          },\\n          {\\n            \"authorId\": \"2301156520\",\\n            \"name\": \"Zizheng Zhan\"\\n          },\\n          {\\n            \"authorId\": \"2174027344\",\\n            \"name\": \"Jing Li\"\\n          },\\n          {\\n            \"authorId\": \"1557360433\",\\n            \"name\": \"Haotian Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2290557105\",\\n            \"name\": \"Yuqun Zhang\"\\n          }\\n        ],\\n        \"abstract\": \"Automated code completion, aiming at generating subsequent tokens from unfinished code, has significantly benefited from recent progress in pre-trained Large Language Models (LLMs). However, these models often suffer from coherence issues and hallucinations when dealing with complex code logic or extrapolating beyond their training data. Existing Retrieval Augmented Generation (RAG) techniques partially address these issues by retrieving relevant code with a separate encoding model where the retrieved snippet serves as contextual reference for code completion. However, their retrieval scope is subject to a singular perspective defined by the encoding model, which largely overlooks the complexity and diversity inherent in code semantics. To address this limitation, we propose ProCC, a code completion framework leveraging prompt engineering and the contextual multi-armed bandits algorithm to flexibly incorporate and adapt to multiple perspectives of code. ProCC first employs a prompt-based multi-retriever system which crafts prompt templates to elicit LLM knowledge to understand code semantics with multiple retrieval perspectives. Then, it adopts the adaptive retrieval selection algorithm to incorporate code similarity into the decision-making process to determine the most suitable retrieval perspective for the LLM to complete the code. Experimental results demonstrate that ProCC outperforms a widely-studied code completion technique RepoCoder by 7.92% on the public benchmark CCEval, 3.19% in HumanEval-Infilling, 2.80% on our collected open-source benchmark suite, and 4.48% on the private-domain benchmark suite collected from Kuaishou Technology in terms of Exact Match. ProCC also allows augmenting fine-tuned techniques in a plug-and-play manner, yielding an averaged 6.5% improvement over the fine-tuned model.\"\\n      },\\n      {\\n        \"paperId\": \"47b2e5c3e7a907d2b65b3f9b918f83532d12f41a\",\\n        \"url\": \"https://www.semanticscholar.org/paper/47b2e5c3e7a907d2b65b3f9b918f83532d12f41a\",\\n        \"title\": \"Automatic counter-narrative generation for hate speech in Spanish\",\\n        \"citationCount\": 16,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"The article shows that the use of GPT-3 outperforms other models in generating non-offensive and informative counter-narratives, which some-times present compelling arguments.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": null,\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2229887611\",\\n            \"name\": \"Mar\\\\u00eda Estrella Vallecillo Rodrguez\"\\n          },\\n          {\\n            \"authorId\": \"1799967\",\\n            \"name\": \"Arturo Montejo-R\\\\u00e1ez\"\\n          },\\n          {\\n            \"authorId\": \"51183850\",\\n            \"name\": \"M. T. M. Valdivia\"\\n          }\\n        ],\\n        \"abstract\": null\\n      },\\n      {\\n        \"paperId\": \"bc9f1025246efeed568650934b6e183415aca279\",\\n        \"url\": \"https://www.semanticscholar.org/paper/bc9f1025246efeed568650934b6e183415aca279\",\\n        \"title\": \"PREADD: Prefix-Adaptive Decoding for Controlled Text Generation\",\\n        \"citationCount\": 29,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://arxiv.org/pdf/2307.03214\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2307.03214, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"The proposed Prefix-Adaptive Decoding (PREADD), a flexible method for controlled text generation, outperforms not only prompting baselines, but also an auxiliary-expert control method, by 12% or more in relative gain on the authors\\' main metrics for each task.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2023-07-06\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2221313264\",\\n            \"name\": \"Jonathan Pei\"\\n          },\\n          {\\n            \"authorId\": \"1410652795\",\\n            \"name\": \"Kevin Yang\"\\n          },\\n          {\\n            \"authorId\": \"38666915\",\\n            \"name\": \"D. Klein\"\\n          }\\n        ],\\n        \"abstract\": \"We propose Prefix-Adaptive Decoding (PREADD), a flexible method for controlled text generation. Unlike existing methods that use auxiliary expert models to control for attributes, PREADD does not require an external model, instead relying on linearly combining output logits from multiple prompts. Specifically, PREADD contrasts the output logits generated using a raw prompt against those generated using a prefix-prepended prompt, enabling both positive and negative control with respect to any attribute encapsulated by the prefix. We evaluate PREADD on three tasks -- toxic output mitigation, gender bias reduction, and sentiment control -- and find that PREADD outperforms not only prompting baselines, but also an auxiliary-expert control method, by 12% or more in relative gain on our main metrics for each task.\"\\n      },\\n      {\\n        \"paperId\": \"5a4d1add108f2e6d8ccd9f6ac94ab3e0335db540\",\\n        \"url\": \"https://www.semanticscholar.org/paper/5a4d1add108f2e6d8ccd9f6ac94ab3e0335db540\",\\n        \"title\": \"Agents\\' Room: Narrative Generation through Multi-step Collaboration\",\\n        \"citationCount\": 31,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2410.02603, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"It is shown that Agents\\' Room generates stories that are preferred by expert evaluators over those produced by baseline systems by leveraging collaboration and specialization to decompose the complex story writing task into tractable components.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-10-03\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2174667321\",\\n            \"name\": \"Fantine Huot\"\\n          },\\n          {\\n            \"authorId\": \"23181472\",\\n            \"name\": \"Reinald Kim Amplayo\"\\n          },\\n          {\\n            \"authorId\": \"52578817\",\\n            \"name\": \"J. Palomaki\"\\n          },\\n          {\\n            \"authorId\": \"1411178896\",\\n            \"name\": \"Alice Shoshana Jakobovits\"\\n          },\\n          {\\n            \"authorId\": \"2324056615\",\\n            \"name\": \"Elizabeth Clark\"\\n          },\\n          {\\n            \"authorId\": \"1747893\",\\n            \"name\": \"Mirella Lapata\"\\n          }\\n        ],\\n        \"abstract\": \"Writing compelling fiction is a multifaceted process combining elements such as crafting a plot, developing interesting characters, and using evocative language. While large language models (LLMs) show promise for story writing, they currently rely heavily on intricate prompting, which limits their use. We propose Agents\\' Room, a generation framework inspired by narrative theory, that decomposes narrative writing into subtasks tackled by specialized agents. To illustrate our method, we introduce Tell Me A Story, a high-quality dataset of complex writing prompts and human-written stories, and a novel evaluation framework designed specifically for assessing long narratives. We show that Agents\\' Room generates stories that are preferred by expert evaluators over those produced by baseline systems by leveraging collaboration and specialization to decompose the complex story writing task into tractable components. We provide extensive analysis with automated and human-based metrics of the generated output.\"\\n      },\\n      {\\n        \"paperId\": \"ff3edac95dcc82eae72cd25519e5dff075fc58be\",\\n        \"url\": \"https://www.semanticscholar.org/paper/ff3edac95dcc82eae72cd25519e5dff075fc58be\",\\n        \"title\": \"Retrieval Augmented Generation in Prompt-based Text-to-Speech Synthesis with Context-Aware Contrastive Language-Audio Pretraining\",\\n        \"citationCount\": 14,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2406.03714, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper adapt retrieval augmented generation (RAG) from LLMs to prompt-based TTS, and presents a Context-Aware Contrastive Language-Audio Pre-training (CA-CLAP) model to extract context-aware, style-related features.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-06-06\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2159562819\",\\n            \"name\": \"Jinlong Xue\"\\n          },\\n          {\\n            \"authorId\": \"2111214299\",\\n            \"name\": \"Yayue Deng\"\\n          },\\n          {\\n            \"authorId\": \"2261912430\",\\n            \"name\": \"Yingming Gao\"\\n          },\\n          {\\n            \"authorId\": \"2161324824\",\\n            \"name\": \"Ya Li\"\\n          }\\n        ],\\n        \"abstract\": \"Recent prompt-based text-to-speech (TTS) models can clone an unseen speaker using only a short speech prompt. They leverage a strong in-context ability to mimic the speech prompts, including speaker style, prosody, and emotion. Therefore, the selection of a speech prompt greatly influences the generated speech, akin to the importance of a prompt in large language models (LLMs). However, current prompt-based TTS models choose the speech prompt manually or simply at random. Hence, in this paper, we adapt retrieval augmented generation (RAG) from LLMs to prompt-based TTS. Unlike traditional RAG methods, we additionally consider contextual information during the retrieval process and present a Context-Aware Contrastive Language-Audio Pre-training (CA-CLAP) model to extract context-aware, style-related features. The objective and subjective evaluations demonstrate that our proposed RAG method outperforms baselines, and our CA-CLAP achieves better results than text-only retrieval methods.\"\\n      },\\n      {\\n        \"paperId\": \"3a6d9e0d5896491dbdb192ea1a9032e9940abe54\",\\n        \"url\": \"https://www.semanticscholar.org/paper/3a6d9e0d5896491dbdb192ea1a9032e9940abe54\",\\n        \"title\": \"Text-driven Prompt Generation for Vision-Language Models in Federated Learning\",\\n        \"citationCount\": 17,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://arxiv.org/pdf/2310.06123\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2310.06123, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work proposes Federated Text-driven Prompt Generation (FedTPG), which learns a unified prompt generation network across multiple remote clients in a scalable manner and is superior to existing federated prompt learning methods, that achieve overall better generalization on both seen and unseen classes.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2023-10-09\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2256983385\",\\n            \"name\": \"Chen Qiu\"\\n          },\\n          {\\n            \"authorId\": \"2257324808\",\\n            \"name\": \"Xingyu Li\"\\n          },\\n          {\\n            \"authorId\": \"29359383\",\\n            \"name\": \"Chaithanya Kumar Mummadi\"\\n          },\\n          {\\n            \"authorId\": \"144487556\",\\n            \"name\": \"M. Ganesh\"\\n          },\\n          {\\n            \"authorId\": \"2257091754\",\\n            \"name\": \"Zhenzhen Li\"\\n          },\\n          {\\n            \"authorId\": \"2257130661\",\\n            \"name\": \"Lu Peng\"\\n          },\\n          {\\n            \"authorId\": \"2257132255\",\\n            \"name\": \"Wan-Yi Lin\"\\n          }\\n        ],\\n        \"abstract\": \"Prompt learning for vision-language models, e.g., CoOp, has shown great success in adapting CLIP to different downstream tasks, making it a promising solution for federated learning due to computational reasons. Existing prompt learning techniques replace hand-crafted text prompts with learned vectors that offer improvements on seen classes, but struggle to generalize to unseen classes. Our work addresses this challenge by proposing Federated Text-driven Prompt Generation (FedTPG), which learns a unified prompt generation network across multiple remote clients in a scalable manner. The prompt generation network is conditioned on task-related text input, thus is context-aware, making it suitable to generalize for both seen and unseen classes. Our comprehensive empirical evaluations on nine diverse image classification datasets show that our method is superior to existing federated prompt learning methods, that achieve overall better generalization on both seen and unseen classes and is also generalizable to unseen datasets.\"\\n      },\\n      {\\n        \"paperId\": \"d6d6b2c88bee4004428435c7e070733709df71a6\",\\n        \"url\": \"https://www.semanticscholar.org/paper/d6d6b2c88bee4004428435c7e070733709df71a6\",\\n        \"title\": \"Socrates: Data Story Generation via Adaptive Machine-Guided Elicitation of User Feedback\",\\n        \"citationCount\": 16,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {\\'tldr\\'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/TVCG.2023.3327363?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/TVCG.2023.3327363, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": null,\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2023-11-03\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2153300135\",\\n            \"name\": \"Guande Wu\"\\n          },\\n          {\\n            \"authorId\": \"30518075\",\\n            \"name\": \"Shunan Guo\"\\n          },\\n          {\\n            \"authorId\": \"1890683\",\\n            \"name\": \"J. Hoffswell\"\\n          },\\n          {\\n            \"authorId\": \"51192588\",\\n            \"name\": \"G. Chan\"\\n          },\\n          {\\n            \"authorId\": \"2176092943\",\\n            \"name\": \"R. Rossi\"\\n          },\\n          {\\n            \"authorId\": \"2176108906\",\\n            \"name\": \"E. Koh\"\\n          }\\n        ],\\n        \"abstract\": \"Visual data stories can effectively convey insights from data, yet their creation often necessitates intricate data exploration, insight discovery, narrative organization, and customization to meet the communication objectives of the storyteller. Existing automated data storytelling techniques, however, tend to overlook the importance of user customization during the data story authoring process, limiting the system\\'s ability to create tailored narratives that reflect the user\\'s intentions. We present a novel data story generation workflow that leverages adaptive machine-guided elicitation of user feedback to customize the story. Our approach employs an adaptive plug-in module for existing story generation systems, which incorporates user feedback through interactive questioning based on the conversation history and dataset. This adaptability refines the system\\'s understanding of the user\\'s intentions, ensuring the final narrative aligns with their goals. We demonstrate the feasibility of our approach through the implementation of an interactive prototype: Socrates. Through a quantitative user study with 18 participants that compares our method to a state-of-the-art data story generation algorithm, we show that Socrates produces more relevant stories with a larger overlap of insights compared to human-generated stories. We also demonstrate the usability of Socrates via interviews with three data analysts and highlight areas of future work.\"\\n      },\\n      {\\n        \"paperId\": \"57fca8aba9760ed8001be7ad074554ee4515d258\",\\n        \"url\": \"https://www.semanticscholar.org/paper/57fca8aba9760ed8001be7ad074554ee4515d258\",\\n        \"title\": \"Adaptive Radiotherapy: Next-Generation Radiotherapy\",\\n        \"citationCount\": 54,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://www.mdpi.com/2072-6694/16/6/1206/pdf?version=1710899842\",\\n          \"status\": \"GOLD\",\\n          \"license\": \"CCBY\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC10968833, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"Adaptive radiotherapy (ART), a notable innovation that addresses anatomy changes and optimizes the therapeutic ratio, is introduced, highlighting the ongoing development of imaging technologies and AI and emphasizing their contribution to enhancing the applicability and effectiveness of ART.\"\\n        },\\n        \"publicationTypes\": [\\n          \"Review\",\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-03-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2283275224\",\\n            \"name\": \"Olga Dona Lemus\"\\n          },\\n          {\\n            \"authorId\": \"2251160990\",\\n            \"name\": \"Minsong Cao\"\\n          },\\n          {\\n            \"authorId\": \"2054380388\",\\n            \"name\": \"Bin Cai\"\\n          },\\n          {\\n            \"authorId\": \"2292572199\",\\n            \"name\": \"Michael Cummings\"\\n          },\\n          {\\n            \"authorId\": \"2248857176\",\\n            \"name\": \"Dandan Zheng\"\\n          }\\n        ],\\n        \"abstract\": \"Simple Summary Radiotherapy, a crucial cancer treatment, has evolved significantly over the years. Traditionally, treatment plans were based on initial scans used throughout the treatment course, accounting for changes in the patient\\\\u2019s anatomy by additional margins to targets. However, the field has moved towards decreasing margins with the advancement of delivery and targeting accuracy in order to decrease toxicity, and the increasing use of image guidance has illuminated patient anatomical changes such as organ deformation, weight loss, tumor shrinkage, and even biological changes that are unaccounted for by the conventional approach. Adaptive radiotherapy (ART) addresses this by adjusting treatment plans according to these changes. ART can be conducted in two ways: online (adjustments made during treatment sessions) and offline (adjustments made between treatment sessions). Advances in technology, especially in medical imaging (CT, MRI, and PET scans) and artificial intelligence, have made ART more feasible and efficient. ART offers more precise cancer treatment by adapting to changes in the patient\\\\u2019s body, leading to better outcomes with fewer side effects. Abstract Radiotherapy, a crucial technique in cancer therapy, has traditionally relied on the premise of largely unchanging patient anatomy during the treatment course and encompassing uncertainties by target margins. This review introduces adaptive radiotherapy (ART), a notable innovation that addresses anatomy changes and optimizes the therapeutic ratio. ART utilizes advanced imaging techniques such as CT, MRI, and PET to modify the treatment plan based on observed anatomical changes and even biological changes during the course of treatment. The narrative review provides a comprehensive guide on ART for healthcare professionals and trainees in radiation oncology and anyone else interested in the topic. The incorporation of artificial intelligence in ART has played a crucial role in improving effectiveness, particularly in contour segmentation, treatment planning, and quality assurance. This has expedited the process to render online ART feasible, lowered the burden for radiation oncology practitioners, and enhanced the precision of dynamically personalized treatment. Current technical and clinical progress on ART is discussed in this review, highlighting the ongoing development of imaging technologies and AI and emphasizing their contribution to enhancing the applicability and effectiveness of ART.\"\\n      },\\n      {\\n        \"paperId\": \"62d4fcfb2f6717d4eb562f4e51f37004fc5d7109\",\\n        \"url\": \"https://www.semanticscholar.org/paper/62d4fcfb2f6717d4eb562f4e51f37004fc5d7109\",\\n        \"title\": \"Adaptive Inference-Time Compute: LLMs Can Predict if They Can Do Better, Even Mid-Generation\",\\n        \"citationCount\": 47,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2410.02725, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A new generative self-evaluation scheme designed to adaptively reduce the number of generated samples while maintaining or even improving performance and enabling more efficient and scalable compute utilization during inference for LLMs is introduced.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-10-03\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2161717022\",\\n            \"name\": \"Rohin Manvi\"\\n          },\\n          {\\n            \"authorId\": \"2324336057\",\\n            \"name\": \"Anikait Singh\"\\n          },\\n          {\\n            \"authorId\": \"2269095529\",\\n            \"name\": \"Stefano Ermon\"\\n          }\\n        ],\\n        \"abstract\": \"Inference-time computation is a powerful paradigm to enhance the performance of large language models (LLMs), with Best-of-N sampling being a widely used technique. However, this method is computationally expensive, requiring both (1) an external reward model and (2) the generation of multiple samples. In this work, we introduce a new generative self-evaluation scheme designed to adaptively reduce the number of generated samples while maintaining or even improving performance. We use a generative reward model formulation, allowing the LLM to predict mid-generation the probability that restarting the generation will yield a better response. These predictions are obtained without an external reward model and can be used to decide whether or not to generate more samples, prune unpromising samples early on, or to pick the best sample. This capability is very inexpensive as it involves generating a single predefined token. Trained using a dataset constructed with real unfiltered LMSYS user prompts, Llama 3.1 8B\\'s win rate against GPT-4 on AlpacaEval increases from 21% to 34% with 16 samples and math performance on GSM8K improves from 84% to 91%. By sampling only when the LLM determines that it is beneficial to do so and adaptively adjusting temperature annealing, we demonstrate that 74% of the improvement from using 16 samples can be achieved with only 1.2 samples on average. We further demonstrate that 50-75% of samples can be pruned early in generation with minimal degradation in performance. Overall, our methods enable more efficient and scalable compute utilization during inference for LLMs.\"\\n      },\\n      {\\n        \"paperId\": \"c0e24f98323c7114b9229ac17b8b63581e3e5914\",\\n        \"url\": \"https://www.semanticscholar.org/paper/c0e24f98323c7114b9229ac17b8b63581e3e5914\",\\n        \"title\": \"Dynamic and Adaptive Feature Generation with LLM\",\\n        \"citationCount\": 29,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2406.03505, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This research introduces a novel approach adopting large language models and feature-generating prompts to address challenges of explainability, applicability, and inflexible strategy and proposes a dynamic and adaptive feature generation method that enhances the interpretability of the feature generation process.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-06-04\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2130031226\",\\n            \"name\": \"XinHao Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2108045855\",\\n            \"name\": \"Jinghan Zhang\"\\n          },\\n          {\\n            \"authorId\": \"1966492\",\\n            \"name\": \"Banafsheh Rekabdar\"\\n          },\\n          {\\n            \"authorId\": \"2145108199\",\\n            \"name\": \"Yuanchun Zhou\"\\n          },\\n          {\\n            \"authorId\": \"2301248160\",\\n            \"name\": \"Pengfei Wang\"\\n          },\\n          {\\n            \"authorId\": \"2293571072\",\\n            \"name\": \"Kunpeng Liu\"\\n          }\\n        ],\\n        \"abstract\": \"The representation of feature space is a crucial environment where data points get vectorized and embedded for subsequent modeling. Thus, the efficacy of machine learning (ML) algorithms is closely related to the quality of feature engineering. As one of the most important techniques, feature generation transforms raw data into an optimized feature space conducive to model training and further refines the space. Despite the advancements in automated feature engineering and feature generation, current methodologies often suffer from three fundamental issues: lack of explainability, limited applicability, and inflexible strategy. These shortcomings frequently hinder and limit the deployment of ML models across varied scenarios. Our research introduces a novel approach adopting large language models (LLMs) and feature-generating prompts to address these challenges. We propose a dynamic and adaptive feature generation method that enhances the interpretability of the feature generation process. Our approach broadens the applicability across various data types and tasks and offers advantages in terms of strategic flexibility. A broad range of experiments showcases that our approach is significantly superior to existing methods.\"\\n      },\\n      {\\n        \"paperId\": \"a314b4c385e9732794a48d1d34f637b13245c71d\",\\n        \"url\": \"https://www.semanticscholar.org/paper/a314b4c385e9732794a48d1d34f637b13245c71d\",\\n        \"title\": \"RetrievalQA: Assessing Adaptive Retrieval-Augmented Generation for Short-form Open-Domain Question Answering\",\\n        \"citationCount\": 21,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2402.16457, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work presents a benchmark, RetrievalQA, comprising 1,271 short-form questions covering new world and long-tail knowledge, and proposes Time-Aware Adaptive Retrieval (TA-ARE), a simple yet effective method that helps LLMs assess the necessity of retrieval without calibration or additional training.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-02-26\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2144370264\",\\n            \"name\": \"Zihan Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2257039084\",\\n            \"name\": \"Meng Fang\"\\n          },\\n          {\\n            \"authorId\": \"2261455899\",\\n            \"name\": \"Ling Chen\"\\n          }\\n        ],\\n        \"abstract\": \"Adaptive retrieval-augmented generation (ARAG) aims to dynamically determine the necessity of retrieval for queries instead of retrieving indiscriminately to enhance the efficiency and relevance of the sourced information. However, previous works largely overlook the evaluation of ARAG approaches, leading to their effectiveness being understudied. This work presents a benchmark, RetrievalQA, comprising 1,271 short-form questions covering new world and long-tail knowledge. The knowledge necessary to answer the questions is absent from LLMs; therefore, external information must be retrieved to answer correctly. This makes RetrievalQA a suitable testbed to evaluate existing ARAG methods. We observe that calibration-based methods heavily rely on threshold tuning, while vanilla prompting is inadequate for guiding LLMs to make reliable retrieval decisions. Based on our findings, we propose Time-Aware Adaptive Retrieval (TA-ARE), a simple yet effective method that helps LLMs assess the necessity of retrieval without calibration or additional training. The dataset and code will be available at https://github.com/hyintell/RetrievalQA\"\\n      },\\n      {\\n        \"paperId\": \"c958736c08a86816c588fd8fe12f39cec8a64bf8\",\\n        \"url\": \"https://www.semanticscholar.org/paper/c958736c08a86816c588fd8fe12f39cec8a64bf8\",\\n        \"title\": \"SGDM: An Adaptive Style-Guided Diffusion Model for Personalized Text to Image Generation\",\\n        \"citationCount\": 21,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {\\'tldr\\'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/TMM.2024.3399075?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/TMM.2024.3399075, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": null,\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": null,\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2300989129\",\\n            \"name\": \"Yifei Xu\"\\n          },\\n          {\\n            \"authorId\": \"2238891797\",\\n            \"name\": \"Xiaolong Xu\"\\n          },\\n          {\\n            \"authorId\": \"2112666114\",\\n            \"name\": \"Honghao Gao\"\\n          },\\n          {\\n            \"authorId\": \"2166050115\",\\n            \"name\": \"Fu Xiao\"\\n          }\\n        ],\\n        \"abstract\": \"The existing personalized text-to-image generation models face issues such as repeated training and insufficient generalization capabilities. We present an adaptive Style-Guided Diffusion Model (SGDM). When provided with a set of stylistically consistent images and prompts as inputs, SGDM can generate images that align with the prompts while maintaining style consistency with the input images. SGDM first extracts features from the input style image and then combines style features from different depths. Last, style features are injected into the noise generation process of the original Stable Diffusion (SD) model by the style-guided module we propose. This strategy fully leverages the generative and generalization capabilities of the pre-trained text-to-image model to ensure the accuracy of the generated image\\'s content. We present a dataset construction method suitable for style personalized generation tasks of this kind, enabling the trained model to generate stylized images adaptively instead of re-training for each style. We also present an evaluation metric, StySim, to measure the style similarity between two images, and this metric shows that the style personalization capability of SGDM is the best. And metrics such as FID, KID, and CLIPSIM indicate that SGDM maintains good performance in text-to-image generation.\"\\n      },\\n      {\\n        \"paperId\": \"cf56a7b28fb27279b1c94fb920b5722cf50c8852\",\\n        \"url\": \"https://www.semanticscholar.org/paper/cf56a7b28fb27279b1c94fb920b5722cf50c8852\",\\n        \"title\": \"AdvPrompter: Fast Adaptive Adversarial Prompting for LLMs\",\\n        \"citationCount\": 113,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2404.16873, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A novel method that uses another LLM, called AdvPrompter, to generate human-readable adversarial prompts in seconds, and shows that training on adversarial suffixes generated by AdvPrompter is a promising strategy for improving the robustness of LLMs to jailbreaking attacks.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-04-21\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"1443432623\",\\n            \"name\": \"Anselm Paulus\"\\n          },\\n          {\\n            \"authorId\": \"3461866\",\\n            \"name\": \"Arman Zharmagambetov\"\\n          },\\n          {\\n            \"authorId\": \"2298951327\",\\n            \"name\": \"Chuan Guo\"\\n          },\\n          {\\n            \"authorId\": \"2298758184\",\\n            \"name\": \"Brandon Amos\"\\n          },\\n          {\\n            \"authorId\": \"2253746559\",\\n            \"name\": \"Yuandong Tian\"\\n          }\\n        ],\\n        \"abstract\": \"Large Language Models (LLMs) are vulnerable to jailbreaking attacks that lead to generation of inappropriate or harmful content. Manual red-teaming requires a time-consuming search for adversarial prompts, whereas automatic adversarial prompt generation often leads to semantically meaningless attacks that do not scale well. In this paper, we present a novel method that uses another LLM, called AdvPrompter, to generate human-readable adversarial prompts in seconds. AdvPrompter, which is trained using an alternating optimization algorithm, generates suffixes that veil the input instruction without changing its meaning, such that the TargetLLM is lured to give a harmful response. Experimental results on popular open source TargetLLMs show highly competitive results on the AdvBench and HarmBench datasets, that also transfer to closed-source black-box LLMs. We also show that training on adversarial suffixes generated by AdvPrompter is a promising strategy for improving the robustness of LLMs to jailbreaking attacks.\"\\n      },\\n      {\\n        \"paperId\": \"1a41d449f2ed4051ac4c763fb8b1b07357eaf5f4\",\\n        \"url\": \"https://www.semanticscholar.org/paper/1a41d449f2ed4051ac4c763fb8b1b07357eaf5f4\",\\n        \"title\": \"WavLLM: Towards Robust and Adaptive Speech Large Language Model\",\\n        \"citationCount\": 104,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2404.00656, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"WavLLM is introduced, a robust and adaptive speech large language model with dual encoders, and a prompt-aware LoRA weight adapter, optimized by a two-stage curriculum learning approach, exhibiting robust generalization capabilities in executing complex tasks using CoT approach.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-03-31\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2277450543\",\\n            \"name\": \"Shujie Hu\"\\n          },\\n          {\\n            \"authorId\": \"2135918679\",\\n            \"name\": \"Long Zhou\"\\n          },\\n          {\\n            \"authorId\": \"2107983441\",\\n            \"name\": \"Shujie Liu\"\\n          },\\n          {\\n            \"authorId\": \"2107970655\",\\n            \"name\": \"Sanyuan Chen\"\\n          },\\n          {\\n            \"authorId\": \"2294360053\",\\n            \"name\": \"Hongkun Hao\"\\n          },\\n          {\\n            \"authorId\": \"2258308585\",\\n            \"name\": \"Jing Pan\"\\n          },\\n          {\\n            \"authorId\": \"2274190703\",\\n            \"name\": \"Xunying Liu\"\\n          },\\n          {\\n            \"authorId\": \"2280887661\",\\n            \"name\": \"Jinyu Li\"\\n          },\\n          {\\n            \"authorId\": \"9075412\",\\n            \"name\": \"S. Sivasankaran\"\\n          },\\n          {\\n            \"authorId\": \"2294832157\",\\n            \"name\": \"Linquan Liu\"\\n          },\\n          {\\n            \"authorId\": \"2277299355\",\\n            \"name\": \"Furu Wei\"\\n          }\\n        ],\\n        \"abstract\": \"The recent advancements in large language models (LLMs) have revolutionized the field of natural language processing, progressively broadening their scope to multimodal perception and generation. However, effectively integrating listening capabilities into LLMs poses significant challenges, particularly with respect to generalizing across varied contexts and executing complex auditory tasks. In this work, we introduce WavLLM, a robust and adaptive speech large language model with dual encoders, and a prompt-aware LoRA weight adapter, optimized by a two-stage curriculum learning approach. Leveraging dual encoders, we decouple different types of speech information, utilizing a Whisper encoder to process the semantic content of speech, and a WavLM encoder to capture the unique characteristics of the speaker\\'s identity. Within the curriculum learning framework, WavLLM first builds its foundational capabilities by optimizing on mixed elementary single tasks, followed by advanced multi-task training on more complex tasks such as combinations of the elementary tasks. To enhance the flexibility and adherence to different tasks and instructions, a prompt-aware LoRA weight adapter is introduced in the second advanced multi-task training stage. We validate the proposed model on universal speech benchmarks including tasks such as ASR, ST, SV, ER, and also apply it to specialized datasets like Gaokao English listening comprehension set for SQA, and speech Chain-of-Thought (CoT) evaluation set. Experiments demonstrate that the proposed model achieves state-of-the-art performance across a range of speech tasks on the same model size, exhibiting robust generalization capabilities in executing complex tasks using CoT approach. Furthermore, our model successfully completes Gaokao tasks without specialized training. The codes, models, audio, and Gaokao evaluation set can be accessed at \\\\\\\\url{aka.ms/wavllm}.\"\\n      },\\n      {\\n        \"paperId\": \"5cacb35e7bd86e64e5aca126e5011b64630007d8\",\\n        \"url\": \"https://www.semanticscholar.org/paper/5cacb35e7bd86e64e5aca126e5011b64630007d8\",\\n        \"title\": \"OmniAvatar: Efficient Audio-Driven Avatar Video Generation with Adaptive Body Animation\",\\n        \"citationCount\": 12,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2506.18866, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"OmniAvatar is introduced, an innovative audio-driven full-body video generation model that enhances human animation with improved lip-sync accuracy and natural movements, and introduces a pixel-wise multi-hierarchical audio embedding strategy to better capture audio features in the latent space.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2025-06-23\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2293272991\",\\n            \"name\": \"Qijun Gan\"\\n          },\\n          {\\n            \"authorId\": \"2294510159\",\\n            \"name\": \"Ruizi Yang\"\\n          },\\n          {\\n            \"authorId\": \"2314648986\",\\n            \"name\": \"Jianke Zhu\"\\n          },\\n          {\\n            \"authorId\": \"2370937157\",\\n            \"name\": \"Shaofei Xue\"\\n          },\\n          {\\n            \"authorId\": \"2370937932\",\\n            \"name\": \"Steven Hoi\"\\n          }\\n        ],\\n        \"abstract\": \"Significant progress has been made in audio-driven human animation, while most existing methods focus mainly on facial movements, limiting their ability to create full-body animations with natural synchronization and fluidity. They also struggle with precise prompt control for fine-grained generation. To tackle these challenges, we introduce OmniAvatar, an innovative audio-driven full-body video generation model that enhances human animation with improved lip-sync accuracy and natural movements. OmniAvatar introduces a pixel-wise multi-hierarchical audio embedding strategy to better capture audio features in the latent space, enhancing lip-syncing across diverse scenes. To preserve the capability for prompt-driven control of foundation models while effectively incorporating audio features, we employ a LoRA-based training approach. Extensive experiments show that OmniAvatar surpasses existing models in both facial and semi-body video generation, offering precise text-based control for creating videos in various domains, such as podcasts, human interactions, dynamic scenes, and singing. Our project page is https://omni-avatar.github.io/.\"\\n      },\\n      {\\n        \"paperId\": \"e8a897a12ce666a898582c3454a3e49acae40422\",\\n        \"url\": \"https://www.semanticscholar.org/paper/e8a897a12ce666a898582c3454a3e49acae40422\",\\n        \"title\": \"SceneCraft: Automating Interactive Narrative Scene Generation in Digital Games with Large Language Models\",\\n        \"citationCount\": 56,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://ojs.aaai.org/index.php/AIIDE/article/download/27504/27277\",\\n          \"status\": \"BRONZE\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1609/aiide.v19i1.27504?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1609/aiide.v19i1.27504, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper proposes SCENECRAFT, a narrative scene generation framework that automates NPC interaction crucial to unfolding plot events and demonstrates its effectiveness in creating narrative experiences based on creativity, adaptability, and alignment with intended author instructions.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2023-10-06\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2071942775\",\\n            \"name\": \"Vikram Kumaran\"\\n          },\\n          {\\n            \"authorId\": \"2256208513\",\\n            \"name\": \"Jonathan Rowe\"\\n          },\\n          {\\n            \"authorId\": \"9808011\",\\n            \"name\": \"Bradford W. Mott\"\\n          },\\n          {\\n            \"authorId\": \"2238471264\",\\n            \"name\": \"James C. Lester\"\\n          }\\n        ],\\n        \"abstract\": \"Creating engaging interactive story-based experiences dynamically responding to individual player choices poses significant challenges for narrative-centered games. Recent advances in pre-trained large language models (LLMs) have the potential to revolutionize procedural content generation for narrative-centered games. Historically, interactive narrative generation has specified pivotal events in the storyline, often utilizing planning-based approaches toward achieving narrative coherence and maintaining the story arc. However, manual authorship is typically used to create detail and variety in non-player character (NPC) interaction to specify and instantiate plot events. This paper proposes SCENECRAFT, a narrative scene generation framework that automates NPC interaction crucial to unfolding plot events. SCENECRAFT interprets natural language instructions about scene objectives, NPC traits, location, and narrative variations. It then employs large language models to generate game scenes aligned with authorial intent. It generates branching conversation paths that adapt to player choices while adhering to the author\\\\u2019s interaction goals. LLMs generate interaction scripts, semantically extract character emotions and gestures to align with the script, and convert dialogues into a game scripting language. The generated script can then be played utilizing an existing narrative-centered game framework. Through empirical evaluation using automated and human assessments, we demonstrate SCENECRAFT\\\\u2019s effectiveness in creating narrative experiences based on creativity, adaptability, and alignment with intended author instructions.\"\\n      },\\n      {\\n        \"paperId\": \"d599dc40c9cb8d6d76554ee7d21d20c22cc7cdb5\",\\n        \"url\": \"https://www.semanticscholar.org/paper/d599dc40c9cb8d6d76554ee7d21d20c22cc7cdb5\",\\n        \"title\": \"ConsistI2V: Enhancing Visual Consistency for Image-to-Video Generation\",\\n        \"citationCount\": 99,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2402.04324, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work proposes ConsistI2V, a diffusion-based method to enhance visual consistency for I2V generation that introduces spatiotemporal attention over the first frame to maintain spatial and motion consistency and noise initialization from the low-frequency band of the first frame to enhance layout consistency.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-02-06\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2268493042\",\\n            \"name\": \"Weiming Ren\"\\n          },\\n          {\\n            \"authorId\": \"2283183497\",\\n            \"name\": \"Harry Yang\"\\n          },\\n          {\\n            \"authorId\": \"2143853895\",\\n            \"name\": \"Ge Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2268683835\",\\n            \"name\": \"Cong Wei\"\\n          },\\n          {\\n            \"authorId\": \"2279346001\",\\n            \"name\": \"Xinrun Du\"\\n          },\\n          {\\n            \"authorId\": \"2283188391\",\\n            \"name\": \"Stephen W. Huang\"\\n          },\\n          {\\n            \"authorId\": \"2253811180\",\\n            \"name\": \"Wenhu Chen\"\\n          }\\n        ],\\n        \"abstract\": \"Image-to-video (I2V) generation aims to use the initial frame (alongside a text prompt) to create a video sequence. A grand challenge in I2V generation is to maintain visual consistency throughout the video: existing methods often struggle to preserve the integrity of the subject, background, and style from the first frame, as well as ensure a fluid and logical progression within the video narrative. To mitigate these issues, we propose ConsistI2V, a diffusion-based method to enhance visual consistency for I2V generation. Specifically, we introduce (1) spatiotemporal attention over the first frame to maintain spatial and motion consistency, (2) noise initialization from the low-frequency band of the first frame to enhance layout consistency. These two approaches enable ConsistI2V to generate highly consistent videos. We also extend the proposed approaches to show their potential to improve consistency in auto-regressive long video generation and camera motion control. To verify the effectiveness of our method, we propose I2V-Bench, a comprehensive evaluation benchmark for I2V generation. Our automatic and human evaluation results demonstrate the superiority of ConsistI2V over existing methods.\"\\n      },\\n      {\\n        \"paperId\": \"f00a8162648403f32c9dfb937b9220b234c82531\",\\n        \"url\": \"https://www.semanticscholar.org/paper/f00a8162648403f32c9dfb937b9220b234c82531\",\\n        \"title\": \"viz2viz: Prompt-driven stylized visualization generation using a diffusion model\",\\n        \"citationCount\": 13,\\n        \"openAccessPdf\": {\\n          \"url\": \"http://arxiv.org/pdf/2304.01919\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2304.01919, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": null\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2023-04-04\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2305625074\",\\n            \"name\": \"Jiaqi Wu\"\\n          },\\n          {\\n            \"authorId\": \"152836325\",\\n            \"name\": \"John Joon Young Chung\"\\n          },\\n          {\\n            \"authorId\": \"2630700\",\\n            \"name\": \"Eytan Adar\"\\n          }\\n        ],\\n        \"abstract\": \"Creating stylized visualization requires going beyond the limited, abstract, geometric marks produced by most tools. Rather, the designer builds stylized idioms where the marks are both transformed (e.g., photographs of candles instead of bars) and also synthesized into a \\'scene\\' that pushes the boundaries of traditional visualizations. To support this, we introduce viz2viz, a system for transforming visualizations with a textual prompt to a stylized form. The system follows a high-level recipe that leverages various generative methods to produce new visualizations that retain the properties of the original dataset. While the base recipe is consistent across many visualization types, we demonstrate how it can be specifically adapted to the creation of different visualization types (bar charts, area charts, pie charts, and network visualizations). Our approach introduces techniques for using different prompts for different marks (i.e., each bar can be something completely different) while still retaining image\\\\\"coherence.\\\\\"We conclude with an evaluation of the approach and discussion on extensions and limitations.\"\\n      },\\n      {\\n        \"paperId\": \"80b8d03f910bb252cb251f7f74ceaaf8f4c4aabe\",\\n        \"url\": \"https://www.semanticscholar.org/paper/80b8d03f910bb252cb251f7f74ceaaf8f4c4aabe\",\\n        \"title\": \"Adaptive Test Generation Using a Large Language Model\",\\n        \"citationCount\": 69,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://arxiv.org/pdf/2302.06527\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {\\'abstract\\'}. Paper or abstract available at https://api.unpaywall.org/v2/10.48550/arXiv.2302.06527?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.48550/arXiv.2302.06527, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"T EST P ILOT uses Codex, an off-the-shelf LLM, to automatically generate unit tests for a given program without requiring additional training or few-shot learning on examples of existing tests, and does not generate memorized tests.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": null,\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2257193192\",\\n            \"name\": \"Max Sch\\\\u00e4fer\"\\n          },\\n          {\\n            \"authorId\": \"2317114000\",\\n            \"name\": \"Sarah Nadi\"\\n          },\\n          {\\n            \"authorId\": \"46235560\",\\n            \"name\": \"A. Eghbali\"\\n          },\\n          {\\n            \"authorId\": \"2257181137\",\\n            \"name\": \"Frank Tip\"\\n          }\\n        ],\\n        \"abstract\": null\\n      },\\n      {\\n        \"paperId\": \"9038f40c43e7d62d8f1dc4819093083090911f7a\",\\n        \"url\": \"https://www.semanticscholar.org/paper/9038f40c43e7d62d8f1dc4819093083090911f7a\",\\n        \"title\": \"Novelty Controlled Paraphrase Generation with Retrieval Augmented Conditional Prompt Tuning\",\\n        \"citationCount\": 39,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://ojs.aaai.org/index.php/AAAI/article/download/21297/21046\",\\n          \"status\": \"GOLD\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2202.00535, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"The effectiveness of the proposed approaches for retaining the semantic content of the original text while inducing lexical novelty in the generation of paraphrase generation is demonstrated.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2022-02-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"123467107\",\\n            \"name\": \"Jishnu Ray Chowdhury\"\\n          },\\n          {\\n            \"authorId\": \"2152482425\",\\n            \"name\": \"Yong Zhuang\"\\n          },\\n          {\\n            \"authorId\": \"2152375661\",\\n            \"name\": \"Shuyi Wang\"\\n          }\\n        ],\\n        \"abstract\": \"Paraphrase generation is a fundamental and long-standing task in natural language processing. In this paper, we concentrate on two contributions to the task: (1) we propose Retrieval Augmented Prompt Tuning (RAPT) as a parameter-efficient method to adapt large pre-trained language models for paraphrase generation; (2) we propose Novelty Conditioned RAPT (NC-RAPT) as a simple model-agnostic method of using specialized prompt tokens for controlled paraphrase generation with varying levels of lexical novelty. By conducting extensive experiments on four datasets, we demonstrate the effectiveness of the proposed approaches for retaining the semantic content of the original text while inducing lexical novelty in the generation.\"\\n      },\\n      {\\n        \"paperId\": \"df19bd0bb788474a578fac33d6cff9867af3eead\",\\n        \"url\": \"https://www.semanticscholar.org/paper/df19bd0bb788474a578fac33d6cff9867af3eead\",\\n        \"title\": \"FABULA: Intelligence Report Generation Using Retrieval-Augmented Narrative Construction\",\\n        \"citationCount\": 16,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://dl.acm.org/doi/pdf/10.1145/3625007.3627505\",\\n          \"status\": \"BRONZE\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2310.13848, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A Retrieval Augmented Generation (RAG) approach to augment prompting of an autoregressive decoder by retrieving structured information asserted in a knowledge graph to generate targeted information based on a narrative plot model is introduced.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Book\"\\n        ],\\n        \"publicationDate\": \"2023-10-20\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"51092885\",\\n            \"name\": \"P. Ranade\"\\n          },\\n          {\\n            \"authorId\": \"2269936199\",\\n            \"name\": \"Anupam Joshi\"\\n          }\\n        ],\\n        \"abstract\": \"Narrative construction is the process of representing disparate event information into a logical plot structure that models an end to end story. Intelligence analysis is an example of a domain that can benefit tremendously from narrative construction techniques, particularly in aiding analysts during the largely manual and costly process of synthesizing event information into comprehensive intelligence reports. Manual intelligence report generation is often prone to challenges such as integrating dynamic event information, writing fine-grained queries, and closing information gaps. This motivates the development of a system that retrieves and represents critical aspects of events in a form that aids in automatic generation of intelligence reports. We introduce a Retrieval Augmented Generation (RAG) approach to augment prompting of an autoregressive decoder by retrieving structured information asserted in a knowledge graph to generate targeted information based on a narrative plot model. We apply our approach to the problem of neural intelligence report generation and introduce FABULA, framework to augment intelligence analysis workflows using RAG. An analyst can use FABULA to query an Event Plot Graph (EPG) to retrieve relevant event plot points, which can be used to augment prompting of a Large Language Model (LLM) during intelligence report generation. Our evaluation studies show that the plot points included in the generated intelligence reports have high semantic relevance, high coherency, and low data redundancy.\"\\n      },\\n      {\\n        \"paperId\": \"bb2ef694e8b5a99e1f7ceb014968b4d1dc2e122a\",\\n        \"url\": \"https://www.semanticscholar.org/paper/bb2ef694e8b5a99e1f7ceb014968b4d1dc2e122a\",\\n        \"title\": \"EIPE-text: Evaluation-Guided Iterative Plan Extraction for Long-Form Narrative Text Generation\",\\n        \"citationCount\": 11,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://arxiv.org/pdf/2310.08185\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2310.08185, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A new framework called Evaluation-guided Iterative Plan Extraction for long-form narrative text generation (EIPE-text), which extracts plans from the corpus of narratives and utilizes the extracted plans to construct a better planner.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2023-10-12\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2214585980\",\\n            \"name\": \"Wang You\"\\n          },\\n          {\\n            \"authorId\": \"51198241\",\\n            \"name\": \"Wenshan Wu\"\\n          },\\n          {\\n            \"authorId\": \"3887469\",\\n            \"name\": \"Yaobo Liang\"\\n          },\\n          {\\n            \"authorId\": \"35374367\",\\n            \"name\": \"Shaoguang Mao\"\\n          },\\n          {\\n            \"authorId\": \"2151101534\",\\n            \"name\": \"Chenfei Wu\"\\n          },\\n          {\\n            \"authorId\": \"2257345975\",\\n            \"name\": \"Maosong Cao\"\\n          },\\n          {\\n            \"authorId\": \"2257125576\",\\n            \"name\": \"Yuzhe Cai\"\\n          },\\n          {\\n            \"authorId\": \"2214448244\",\\n            \"name\": \"Yiduo Guo\"\\n          },\\n          {\\n            \"authorId\": \"2258547658\",\\n            \"name\": \"Yan Xia\"\\n          },\\n          {\\n            \"authorId\": \"2249539478\",\\n            \"name\": \"Furu Wei\"\\n          },\\n          {\\n            \"authorId\": \"2072609829\",\\n            \"name\": \"Nan Duan\"\\n          }\\n        ],\\n        \"abstract\": \"Plan-and-Write is a common hierarchical approach in long-form narrative text generation, which first creates a plan to guide the narrative writing. Following this approach, several studies rely on simply prompting large language models for planning, which often yields suboptimal results. In this paper, we propose a new framework called Evaluation-guided Iterative Plan Extraction for long-form narrative text generation (EIPE-text), which extracts plans from the corpus of narratives and utilizes the extracted plans to construct a better planner. EIPE-text has three stages: plan extraction, learning, and inference. In the plan extraction stage, it iteratively extracts and improves plans from the narrative corpus and constructs a plan corpus. We propose a question answer (QA) based evaluation mechanism to automatically evaluate the plans and generate detailed plan refinement instructions to guide the iterative improvement. In the learning stage, we build a better planner by fine-tuning with the plan corpus or in-context learning with examples in the plan corpus. Finally, we leverage a hierarchical approach to generate long-form narratives. We evaluate the effectiveness of EIPE-text in the domains of novels and storytelling. Both GPT-4-based evaluations and human evaluations demonstrate that our method can generate more coherent and relevant long-form narratives. Our code will be released in the future.\"\\n      },\\n      {\\n        \"paperId\": \"02747bc59ddf832b0b7d04a1f491d9fa519066fe\",\\n        \"url\": \"https://www.semanticscholar.org/paper/02747bc59ddf832b0b7d04a1f491d9fa519066fe\",\\n        \"title\": \"PANGeA: Procedural Artificial Narrative Using Generative AI for Turn-Based, Role-Playing Video Games\",\\n        \"citationCount\": 15,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://ojs.aaai.org/index.php/AIIDE/article/download/31876/34043\",\\n          \"status\": \"BRONZE\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1609/aiide.v20i1.31876?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1609/aiide.v20i1.31876, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"PANGeA can help game designers generate narrative-consistent content while leveraging LLMs of different sizes, suitable for various devices, and is supported by a novel validation system for handling free-form text input during game development and gameplay.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-11-15\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2286339849\",\\n            \"name\": \"Stephanie Buongiorno\"\\n          },\\n          {\\n            \"authorId\": \"2132867162\",\\n            \"name\": \"Lawrence J. Klinkert\"\\n          },\\n          {\\n            \"authorId\": \"2298971047\",\\n            \"name\": \"Zixin Zhuang\"\\n          },\\n          {\\n            \"authorId\": \"2298969170\",\\n            \"name\": \"Tanishq Chawla\"\\n          },\\n          {\\n            \"authorId\": \"2299097774\",\\n            \"name\": \"Corey Clark\"\\n          }\\n        ],\\n        \"abstract\": \"Large language models (LLMs) offer unprecedented flexibility in procedural generation, enabling the creation of dynamic video game storylines that evolve with user input. A critical aspect of realizing this potential is allowing players and developers to provide dynamic or free-form text to drive generation. Ingesting free-form text for a video game poses challenges, however, as it can prompt the LLM to generate content beyond the intended narrative scope. In response to this challenge, this research introduces Procedural Artificial Narrative using Generative AI (PANGeA) for leveraging LLMs to create narrative content for turn-based, role-playing games (RPGs). PANGeA is an approach comprised of components including a memory system, validation system, a Unity game engine plug-in, and a server with a RESTful interface that enables connecting PANGeA components with any game engine as well as accessing local and private LLMs. PANGeA procedurally generates level data like setting, key items, non-playable characters (NPCs)), and dialogue based on a set of configuration and design rules provided by the game designer. This process is supported by a novel validation system for handling free-form text input during game development and gameplay, which aligns LLM generation with the narrative. It does this by evoking the LLM\\'s capabilities to dynamically evaluate the text input against game rules that reinforce the designer\\'s initial criteria. To enrich player-NPC interactions, PANGeA uses the Big Five Personality model to shape NPC responses. To explore its broad application, PANGeA is evaluated across two studies. First, this research presents a narrative test scenario of the prototype game, Dark Shadows, which was developed using PANGeA within the Unity game engine. This is followed by an ablation study that tests PANGeA\\'s performance across 10 different role-playing game scenarios\\\\u2013from western to science fiction\\\\u2013and across three model sizes: Llama-3 (8B), GPT-3.5, and GPT-4. These evaluations demonstrate that PANGeA\\'s NPCs can hold dynamic, narrative-consistent conversations that, without the memory system, would exceed the LLM\\'s context length. In addition, the results demonstrate PANGeA\\'s validation system not only aligns LLM responses with the game narrative but also improves the performance of Llama-3 (8B), enabling it to perform comparably to large-scale foundational models like GPT-4. With the validation system, Llama-3 (8B)\\'s performance improved from 28% accuracy to 98%, and GPT-4\\'s from 71% to 99%. These findings indicate PANGeA can help game designers generate narrative-consistent content while leveraging LLMs of different sizes, suitable for various devices.\"\\n      },\\n      {\\n        \"paperId\": \"22a0bfac8cc0cb9c01123d8a898e3235ddcab269\",\\n        \"url\": \"https://www.semanticscholar.org/paper/22a0bfac8cc0cb9c01123d8a898e3235ddcab269\",\\n        \"title\": \"Weigh Your Own Words: Improving Hate Speech Counter Narrative Generation via Attention Regularization\",\\n        \"citationCount\": 11,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://arxiv.org/pdf/2309.02311\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2309.02311, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"Novel attention regularization methodologies to improve the generalization capabilities of Pretrained Transformer-based Language Models for counter narratives generation and paves the way for better and more flexible counter-speech generation models.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2023-09-05\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2161343118\",\\n            \"name\": \"Helena Bonaldi\"\\n          },\\n          {\\n            \"authorId\": \"1481857041\",\\n            \"name\": \"Giuseppe Attanasio\"\\n          },\\n          {\\n            \"authorId\": \"2101317501\",\\n            \"name\": \"Debora Nozza\"\\n          },\\n          {\\n            \"authorId\": \"1912357\",\\n            \"name\": \"Marco Guerini\"\\n          }\\n        ],\\n        \"abstract\": \"Recent computational approaches for combating online hate speech involve the automatic generation of counter narratives by adapting Pretrained Transformer-based Language Models (PLMs) with human-curated data. This process, however, can produce in-domain overfitting, resulting in models generating acceptable narratives only for hatred similar to training data, with little portability to other targets or to real-world toxic language. This paper introduces novel attention regularization methodologies to improve the generalization capabilities of PLMs for counter narratives generation. Overfitting to training-specific terms is then discouraged, resulting in more diverse and richer narratives. We experiment with two attention-based regularization techniques on a benchmark English dataset. Regularized models produce better counter narratives than state-of-the-art approaches in most cases, both in terms of automatic metrics and human evaluation, especially when hateful targets are not present in the training data. This work paves the way for better and more flexible counter-speech generation models, a task for which datasets are highly challenging to produce.\"\\n      },\\n      {\\n        \"paperId\": \"919e7ce11dd6d6605259b7d176d8cebaec9e7d06\",\\n        \"url\": \"https://www.semanticscholar.org/paper/919e7ce11dd6d6605259b7d176d8cebaec9e7d06\",\\n        \"title\": \"AI Illustrator: Translating Raw Descriptions into Images by Prompt-based Cross-Modal Generation\",\\n        \"citationCount\": 10,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://arxiv.org/pdf/2209.03160\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2209.03160, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A Prompt-based Cross-Modal Generation Framework (PCM-Frame) to leverage two powerful pre-trained models, including CLIP and StyleGAN, and conducts a user study to demonstrate its superiority over the competing methods of text-to-image translation with complicated semantics.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Book\"\\n        ],\\n        \"publicationDate\": \"2022-09-07\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"144987142\",\\n            \"name\": \"Y. Ma\"\\n          },\\n          {\\n            \"authorId\": \"46402216\",\\n            \"name\": \"Huan Yang\"\\n          },\\n          {\\n            \"authorId\": \"2127734772\",\\n            \"name\": \"Bei Liu\"\\n          },\\n          {\\n            \"authorId\": \"3247966\",\\n            \"name\": \"Jianlong Fu\"\\n          },\\n          {\\n            \"authorId\": \"2168547810\",\\n            \"name\": \"Jiaying Liu\"\\n          }\\n        ],\\n        \"abstract\": \"AI illustrator aims to automatically design visually appealing images for books to provoke rich thoughts and emotions. To achieve this goal, we propose a framework for translating raw descriptions with complex semantics into semantically corresponding images. The main challenge lies in the complexity of the semantics of raw descriptions, which may be hard to be visualized e.g., \\\\\"gloomy\\\\\" or \\\\\"Asian\\\\\"). It usually poses challenges for existing methods to handle such descriptions. To address this issue, we propose a Prompt-based Cross-Modal Generation Framework (PCM-Frame) to leverage two powerful pre-trained models, including CLIP and StyleGAN. Our framework consists of two components: a projection module from Text Embeddings to Image Embeddings based on prompts, and an adapted image generation module built on StyleGAN which takes Image Embeddings as inputs and is trained by combined semantic consistency losses. To bridge the gap between realistic images and illustration designs, we further adopt a stylization model as post-processing in our framework for better visual effects. Benefiting from the pre-trained models, our method can handle complex descriptions and does not require external paired data for training. Furthermore, we have built a benchmark that consists of 200 descriptions from literature books or online resources. We conduct a user study to demonstrate our superiority over the competing methods of text-to-image translation with complicated semantics.\"\\n      },\\n      {\\n        \"paperId\": \"8e37dc1215681aa153a51c07078ba8befd6a6e01\",\\n        \"url\": \"https://www.semanticscholar.org/paper/8e37dc1215681aa153a51c07078ba8befd6a6e01\",\\n        \"title\": \"AdaPlanner: Adaptive Planning from Feedback with Language Models\",\\n        \"citationCount\": 174,\\n        \"openAccessPdf\": {\\n          \"url\": \"http://arxiv.org/pdf/2305.16653\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2305.16653, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A closed-loop approach, AdaPlanner, is proposed, which allows the LLM agent to refine its self-generated plan adaptively in response to environmental feedback, and develops a code-style LLM prompt structure that facilitates plan generation across a variety of tasks, environments, and agent capabilities.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2023-05-26\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2118180896\",\\n            \"name\": \"Haotian Sun\"\\n          },\\n          {\\n            \"authorId\": \"8103389\",\\n            \"name\": \"Yuchen Zhuang\"\\n          },\\n          {\\n            \"authorId\": \"2865034\",\\n            \"name\": \"Lingkai Kong\"\\n          },\\n          {\\n            \"authorId\": \"2218437288\",\\n            \"name\": \"Bo Dai\"\\n          },\\n          {\\n            \"authorId\": \"145657504\",\\n            \"name\": \"Chao Zhang\"\\n          }\\n        ],\\n        \"abstract\": \"Large language models (LLMs) have recently demonstrated the potential in acting as autonomous agents for sequential decision-making tasks. However, most existing methods either take actions greedily without planning or rely on static plans that are not adaptable to environmental feedback. Consequently, the sequential decision-making performance of LLM agents degenerates with problem complexity and plan horizons increase. We propose a closed-loop approach, AdaPlanner, which allows the LLM agent to refine its self-generated plan adaptively in response to environmental feedback. In AdaPlanner, the LLM agent adaptively refines its plan from feedback with both in-plan and out-of-plan refinement strategies. To mitigate hallucination, we develop a code-style LLM prompt structure that facilitates plan generation across a variety of tasks, environments, and agent capabilities. Furthermore, we propose a skill discovery mechanism that leverages successful plans as few-shot exemplars, enabling the agent to plan and refine with fewer task demonstrations. Our experiments in the ALFWorld and MiniWoB++ environments demonstrate that AdaPlanner outperforms state-of-the-art baselines by 3.73% and 4.11% while utilizing 2x and 600x fewer samples, respectively.\"\\n      },\\n      {\\n        \"paperId\": \"3c23a892605e55f260f647234eb6b5108c84ab84\",\\n        \"url\": \"https://www.semanticscholar.org/paper/3c23a892605e55f260f647234eb6b5108c84ab84\",\\n        \"title\": \"Decoding Methods for Neural Narrative Generation\",\\n        \"citationCount\": 27,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://aclanthology.org/2021.gem-1.16.pdf\",\\n          \"status\": \"HYBRID\",\\n          \"license\": \"CCBY\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2010.07375, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work employs GPT-2 and performs ablations across nucleus sampling thresholds and diverse decoding hyperparameters and analyses results over multiple criteria with automatic and human evaluation, finding that nucleus sampling is generally best with thresholds between 0.7 and 0.9 and a maximum mutual information objective can improve the quality of generated stories.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2020-10-14\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"34989844\",\\n            \"name\": \"Alexandra DeLucia\"\\n          },\\n          {\\n            \"authorId\": \"49355602\",\\n            \"name\": \"Aaron Mueller\"\\n          },\\n          {\\n            \"authorId\": \"32551341\",\\n            \"name\": \"Xiang Lisa Li\"\\n          },\\n          {\\n            \"authorId\": \"2319137716\",\\n            \"name\": \"Jo\\\\u00e3o Sedoc\"\\n          }\\n        ],\\n        \"abstract\": \"Narrative generation is an open-ended NLP task in which a model generates a story given a prompt. The task is similar to neural response generation for chatbots; however, innovations in response generation are often not applied to narrative generation, despite the similarity between these tasks. We aim to bridge this gap by applying and evaluating advances in decoding methods for neural response generation to neural narrative generation. In particular, we employ GPT-2 and perform ablations across nucleus sampling thresholds and diverse decoding hyperparameters\\\\u2014specifically, maximum mutual information\\\\u2014analyzing results over multiple criteria with automatic and human evaluation. We find that (1) nucleus sampling is generally best with thresholds between 0.7 and 0.9; (2) a maximum mutual information objective can improve the quality of generated stories; and (3) established automatic metrics do not correlate well with human judgments of narrative quality on any qualitative metric.\"\\n      },\\n      {\\n        \"paperId\": \"08b85bce712168998004ee80ce4e475390413c74\",\\n        \"url\": \"https://www.semanticscholar.org/paper/08b85bce712168998004ee80ce4e475390413c74\",\\n        \"title\": \"A Prompt Pattern Catalog to Enhance Prompt Engineering with ChatGPT\",\\n        \"citationCount\": 1442,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2302.11382, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A catalog of prompt engineering techniques presented in pattern form that have been applied to solve common problems when conversing with LLMs to improve the outputs of LLM conversations is described.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2023-02-21\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2111231491\",\\n            \"name\": \"Jules White\"\\n          },\\n          {\\n            \"authorId\": \"31669889\",\\n            \"name\": \"Quchen Fu\"\\n          },\\n          {\\n            \"authorId\": \"2159003474\",\\n            \"name\": \"Sam Hays\"\\n          },\\n          {\\n            \"authorId\": \"2087444219\",\\n            \"name\": \"Michael Sandborn\"\\n          },\\n          {\\n            \"authorId\": \"2088026203\",\\n            \"name\": \"Carlos Olea\"\\n          },\\n          {\\n            \"authorId\": \"2180248126\",\\n            \"name\": \"Henry Gilbert\"\\n          },\\n          {\\n            \"authorId\": \"2065484415\",\\n            \"name\": \"Ashraf Elnashar\"\\n          },\\n          {\\n            \"authorId\": \"1405594772\",\\n            \"name\": \"Jesse Spencer-Smith\"\\n          },\\n          {\\n            \"authorId\": \"2064746796\",\\n            \"name\": \"Douglas C. Schmidt\"\\n          }\\n        ],\\n        \"abstract\": \"Prompt engineering is an increasingly important skill set needed to converse effectively with large language models (LLMs), such as ChatGPT. Prompts are instructions given to an LLM to enforce rules, automate processes, and ensure specific qualities (and quantities) of generated output. Prompts are also a form of programming that can customize the outputs and interactions with an LLM. This paper describes a catalog of prompt engineering techniques presented in pattern form that have been applied to solve common problems when conversing with LLMs. Prompt patterns are a knowledge transfer method analogous to software patterns since they provide reusable solutions to common problems faced in a particular context, i.e., output generation and interaction when working with LLMs. This paper provides the following contributions to research on prompt engineering that apply LLMs to automate software development tasks. First, it provides a framework for documenting patterns for structuring prompts to solve a range of problems so that they can be adapted to different domains. Second, it presents a catalog of patterns that have been applied successfully to improve the outputs of LLM conversations. Third, it explains how prompts can be built from multiple patterns and illustrates prompt patterns that benefit from combination with other prompt patterns.\"\\n      },\\n      {\\n        \"paperId\": \"177d8c5d0b172cb1da3f2694949acfd840485a4b\",\\n        \"url\": \"https://www.semanticscholar.org/paper/177d8c5d0b172cb1da3f2694949acfd840485a4b\",\\n        \"title\": \"Manage Real Time Power Imbalance with Renewable Energy: Fast Generation Dispatch or Adaptive Frequency Regulation?\",\\n        \"citationCount\": 11,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {\\'tldr\\'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/pesgm52003.2023.10252632?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/pesgm52003.2023.10252632, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": null,\\n        \"publicationTypes\": null,\\n        \"publicationDate\": \"2023-07-16\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"30562087\",\\n            \"name\": \"Ningchao Gao\"\\n          },\\n          {\\n            \"authorId\": \"145870971\",\\n            \"name\": \"X. Fang\"\\n          }\\n        ],\\n        \"abstract\": \"The carbon neutrality objective requires a large amount of renewable energy integrated into power systems. The rapid deployment of variable renewable energy (VRE), such as solar photovoltaic (PV) generation, increases the system realtime power imbalance because of the random variation and uncertainty of VRE power generation. To manage this, system operators have two potential options: The first is a fast real-time generation dispatch to schedule generation resources with a small time interval to promptly follow the load and renewable power change. The second is to procure adaptive frequency regulation services, such as secondary frequency regulation (SFR), based on the system variation and imbalance conditions to reduce the power imbalance and maintain the stable intra-interval frequency. This paper proposes an integrated alternating current optimal power flow-based generation scheduling and time domain simulation framework to investigate the economic and reliability perspectives of these two options. The impacts of the SFR requirements on the generation cost and frequency response performance are analyzed. In addition, the SFR provided by the PV generation is investigated. The uncertainty of the PV power output is considered using chance constraints to guarantee the real-time delivery of its frequency regulation services. The framework is tested in the IEEE 39-bus network integrated with a large-scale PV power plant. The simulation results demonstrate that the small dispatch interval does not necessarily improve the system frequency response and cannot maintain stable frequency in real time. An adaptive frequency regulation with a 5-minute economic dispatch interval is more appropriate and efficient to reduce the generation cost and improve the frequency response with renewable energy.\"\\n      },\\n      {\\n        \"paperId\": \"6b7f54a8451977a5d24ed1cf93a5ad71b32a9828\",\\n        \"url\": \"https://www.semanticscholar.org/paper/6b7f54a8451977a5d24ed1cf93a5ad71b32a9828\",\\n        \"title\": \"Adaptive In-Context Learning with Large Language Models for Bundle Generation\",\\n        \"citationCount\": 11,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2312.16262, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"An adaptive in-context learning paradigm is proposed, which allows LLMs to draw tailored lessons from related sessions as demonstrations, enhancing the performance on target sessions, and introduces a self-correction strategy promoting mutual improvements of the two tasks without supervision signals.\"\\n        },\\n        \"publicationTypes\": [\\n          \"Book\",\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2023-12-26\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2274082438\",\\n            \"name\": \"Zhu Sun\"\\n          },\\n          {\\n            \"authorId\": \"2273938691\",\\n            \"name\": \"Kaidong Feng\"\\n          },\\n          {\\n            \"authorId\": \"2276986037\",\\n            \"name\": \"Jie Yang\"\\n          },\\n          {\\n            \"authorId\": \"40507824\",\\n            \"name\": \"Xinghua Qu\"\\n          },\\n          {\\n            \"authorId\": \"2276798784\",\\n            \"name\": \"Hui Fang\"\\n          },\\n          {\\n            \"authorId\": \"8748397\",\\n            \"name\": \"Y. Ong\"\\n          },\\n          {\\n            \"authorId\": \"2276742821\",\\n            \"name\": \"Wenyuan Liu\"\\n          }\\n        ],\\n        \"abstract\": \"Most existing bundle generation approaches fall short in generating fixed-size bundles. Furthermore, they often neglect the underlying user intents reflected by the bundles in the generation process, resulting in less intelligible bundles. This paper addresses these limitations through the exploration of two interrelated tasks, i.e., personalized bundle generation and the underlying intent inference, based on different user sessions. Inspired by the reasoning capabilities of large language models (LLMs), we propose an adaptive in-context learning paradigm, which allows LLMs to draw tailored lessons from related sessions as demonstrations, enhancing the performance on target sessions. Specifically, we first employ retrieval augmented generation to identify nearest neighbor sessions, and then carefully design prompts to guide LLMs in executing both tasks on these neighbor sessions. To tackle reliability and hallucination challenges, we further introduce (1) a self-correction strategy promoting mutual improvements of the two tasks without supervision signals and (2) an auto-feedback mechanism for adaptive supervision based on the distinct mistakes made by LLMs on different neighbor sessions. Thereby, the target session can gain customized lessons for improved performance by observing the demonstrations of its neighbor sessions. Experiments on three real-world datasets demonstrate the effectiveness of our proposed method.\"\\n      },\\n      {\\n        \"paperId\": \"5b33c0ab87c532261435c93e38819185f0132d99\",\\n        \"url\": \"https://www.semanticscholar.org/paper/5b33c0ab87c532261435c93e38819185f0132d99\",\\n        \"title\": \"Text Prompt with Normality Guidance for Weakly Supervised Video Anomaly Detection\",\\n        \"citationCount\": 61,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://arxiv.org/pdf/2404.08531\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2404.08531, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A novel pseudo-label generation and self-training framework based on Text Prompt with Normality Guidance (TPWNG) for WSVAD is proposed and achieves state-of-the-art performance on two benchmark datasets, UCF-Crime and XD-Violence.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-04-12\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2296889795\",\\n            \"name\": \"Zhiwei Yang\"\\n          },\\n          {\\n            \"authorId\": \"2163063860\",\\n            \"name\": \"Jing Liu\"\\n          },\\n          {\\n            \"authorId\": \"2678268\",\\n            \"name\": \"Peng Wu\"\\n          }\\n        ],\\n        \"abstract\": \"Weakly supervised video anomaly detection (WSVAD) is a challenging task. Generating fine-grained pseudo-labels based on weak-label and then self-training a classifier is currently a promising solution. However, since the existing methods use only RGB visual modality and the utilization of category text information is neglected, thus limiting the generation of more accurate pseudo-labels and affecting the performance of self-training. Inspired by the manual labeling process based on the event description, in this paper, we propose a novel pseudo-label generation and self-training framework based on Text Prompt with Normality Guidance (TPWNG) for WSVAD. Our idea is to transfer the rich language-visual knowledge of the contrastive language-image pre-training (CLIP) model for aligning the video event description text and corresponding video frames to generate pseudo-labels. Specifically, We first fine-tune the CLIP for domain adaptation by designing two ranking losses and a distributional inconsistency loss. Further, we propose a learnable text prompt mechanism with the assist of a normality visual prompt to further improve the matching accuracy of video event description text and video frames. Then, we design a pseudo-label generation module based on the normality guidance to infer reliable frame-level pseudo-labels. Finally, we introduce a temporal context self-adaptive learning module to learn the temporal dependencies of different video events more flexibly and accurately. Extensive experiments show that our method achieves state-of-the-art performance on two benchmark datasets, UCF-Crime and XD-Violence, demonstrating the effectiveness of our proposed method.\"\\n      },\\n      {\\n        \"paperId\": \"9689acb6cb760e8bc21c16f368368b37dee977f9\",\\n        \"url\": \"https://www.semanticscholar.org/paper/9689acb6cb760e8bc21c16f368368b37dee977f9\",\\n        \"title\": \"Adaptive Machine Translation with Large Language Models\",\\n        \"citationCount\": 105,\\n        \"openAccessPdf\": {\\n          \"url\": \"http://arxiv.org/pdf/2301.13294\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2301.13294, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"It is observed that the translation quality with few-shot in-context learning can surpass that of strong encoder-decoder MT systems, especially for high-resource languages.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2023-01-30\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"9400076\",\\n            \"name\": \"Yasmin Moslem\"\\n          },\\n          {\\n            \"authorId\": \"1748844\",\\n            \"name\": \"Rejwanul Haque\"\\n          },\\n          {\\n            \"authorId\": \"144315616\",\\n            \"name\": \"Andy Way\"\\n          }\\n        ],\\n        \"abstract\": \"Consistency is a key requirement of high-quality translation. It is especially important to adhere to pre-approved terminology and adapt to corrected translations in domain-specific projects. Machine translation (MT) has achieved significant progress in the area of domain adaptation. However, real-time adaptation remains challenging. Large-scale language models (LLMs) have recently shown interesting capabilities of in-context learning, where they learn to replicate certain input-output text generation patterns, without further fine-tuning. By feeding an LLM at inference time with a prompt that consists of a list of translation pairs, it can then simulate the domain and style characteristics. This work aims to investigate how we can utilize in-context learning to improve real-time adaptive MT. Our extensive experiments show promising results at translation time. For example, GPT-3.5 can adapt to a set of in-domain sentence pairs and/or terminology while translating a new sentence. We observe that the translation quality with few-shot in-context learning can surpass that of strong encoder-decoder MT systems, especially for high-resource languages. Moreover, we investigate whether we can combine MT from strong encoder-decoder models with fuzzy matches, which can further improve translation quality, especially for less supported languages. We conduct our experiments across five diverse language pairs, namely English-to-Arabic (EN-AR), English-to-Chinese (EN-ZH), English-to-French (EN-FR), English-to-Kinyarwanda (EN-RW), and English-to-Spanish (EN-ES).\"\\n      },\\n      {\\n        \"paperId\": \"7665642af9e682e012bec045102a4d009421067c\",\\n        \"url\": \"https://www.semanticscholar.org/paper/7665642af9e682e012bec045102a4d009421067c\",\\n        \"title\": \"HumanGaussian: Text-Driven 3D Human Generation with Gaussian Splatting\",\\n        \"citationCount\": 97,\\n        \"openAccessPdf\": {\\n          \"url\": \"http://arxiv.org/pdf/2311.17061\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2311.17061, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"An efficient yet effective framework that generates high-quality 3D humans with fine-grained geometry and realistic appearance, based on the insight that 3D Gaussian Splatting is an efficient renderer with peri-odic Gaussian shrinkage or growing, where such adaptive density control can be naturally guided by intrinsic human structures.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2023-11-28\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2257708193\",\\n            \"name\": \"Xian Liu\"\\n          },\\n          {\\n            \"authorId\": \"2260342453\",\\n            \"name\": \"Xiaohang Zhan\"\\n          },\\n          {\\n            \"authorId\": \"1397711601\",\\n            \"name\": \"Jiaxiang Tang\"\\n          },\\n          {\\n            \"authorId\": \"2260340529\",\\n            \"name\": \"Ying Shan\"\\n          },\\n          {\\n            \"authorId\": \"2247995148\",\\n            \"name\": \"Gang Zeng\"\\n          },\\n          {\\n            \"authorId\": \"2258618427\",\\n            \"name\": \"Dahua Lin\"\\n          },\\n          {\\n            \"authorId\": \"2257370021\",\\n            \"name\": \"Xihui Liu\"\\n          },\\n          {\\n            \"authorId\": \"2249080787\",\\n            \"name\": \"Ziwei Liu\"\\n          }\\n        ],\\n        \"abstract\": \"Realistic 3D human generationfrom text prompts is a de-sirable yet challenging task. Existing methods optimize 3D representations like mesh or neural fields via score distil-lation sampling (SDS), which suffers from inadequate fine details or excessive training time. In this paper, we pro-pose an efficient yet effective framework, HumanGaussian, that generates high-quality 3D humans with fine-grained geometry and realistic appearance. Our key insight is that 3D Gaussian Splatting is an efficient renderer with peri-odic Gaussian shrinkage or growing, where such adaptive density control can be naturally guided by intrinsic human structures. Specifically, 1) we first propose a Structure-Aware SDS that simultaneously optimizes human appear-ance and geometry. The multi-modal score function from both RGB and depth space is leveraged to distill the Gaus-sian densification and pruning process. 2) Moreover, we devise an Annealed Negative Prompt Guidance by decom-posing SDS into a noisier generative score and a cleaner classifier score, which well addresses the over-saturation issue. The floating artifacts are further eliminated based on Gaussian size in a prune-only phase to enhance generation smoothness. Extensive experiments demonstrate the supe-rior efficiency and competitive quality of our framework, rendering vivid 3D humans under diverse scenarios.\"\\n      },\\n      {\\n        \"paperId\": \"5888f7aba5c601a668c290bf57addf79cc1518f1\",\\n        \"url\": \"https://www.semanticscholar.org/paper/5888f7aba5c601a668c290bf57addf79cc1518f1\",\\n        \"title\": \"ChatUniTest: a ChatGPT-based automated unit test generation tool\",\\n        \"citationCount\": 84,\\n        \"openAccessPdf\": {\\n          \"url\": \"http://arxiv.org/pdf/2305.04764\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {\\'abstract\\'}. Paper or abstract available at https://api.unpaywall.org/v2/10.48550/arXiv.2305.04764?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.48550/arXiv.2305.04764, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper introduces ChatUniTest, a ChatGPT-based automated unit test generation tool developed under the Generation-Validation-Repair framework that outperforms EvoSuite in branch and line coverage, surpasses AthenaTest and A3Test in focal method coverage, and generates assertions while utilizing mock objects and re\\\\ufb02ection to achieve test objectives.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": null,\\n        \"authors\": [\\n          {\\n            \"authorId\": \"1387638000\",\\n            \"name\": \"Zhuo-Qi Xie\"\\n          },\\n          {\\n            \"authorId\": \"2300176046\",\\n            \"name\": \"Yinghao Chen\"\\n          },\\n          {\\n            \"authorId\": \"2064478633\",\\n            \"name\": \"Chen Zhi\"\\n          },\\n          {\\n            \"authorId\": \"145590434\",\\n            \"name\": \"Shuiguang Deng\"\\n          },\\n          {\\n            \"authorId\": \"2116398505\",\\n            \"name\": \"Jianwei Yin\"\\n          }\\n        ],\\n        \"abstract\": null\\n      },\\n      {\\n        \"paperId\": \"107dce4ffbdd8d83b75492216646269d8c037ab5\",\\n        \"url\": \"https://www.semanticscholar.org/paper/107dce4ffbdd8d83b75492216646269d8c037ab5\",\\n        \"title\": \"The Future of Learning in the Age of Generative AI: Automated Question Generation and Assessment with Large Language Models\",\\n        \"citationCount\": 10,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2410.09576, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"The discussion underscores the potential of LLMs to replace costly, time-consuming human assessments when appropriately guided, showcasing their advanced understanding and reasoning capabilities in streamlining educational processes.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-10-12\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"1847858\",\\n            \"name\": \"Subhankar Maity\"\\n          },\\n          {\\n            \"authorId\": \"2144085844\",\\n            \"name\": \"Aniket Deroy\"\\n          }\\n        ],\\n        \"abstract\": \"In recent years, large language models (LLMs) and generative AI have revolutionized natural language processing (NLP), offering unprecedented capabilities in education. This chapter explores the transformative potential of LLMs in automated question generation and answer assessment. It begins by examining the mechanisms behind LLMs, emphasizing their ability to comprehend and generate human-like text. The chapter then discusses methodologies for creating diverse, contextually relevant questions, enhancing learning through tailored, adaptive strategies. Key prompting techniques, such as zero-shot and chain-of-thought prompting, are evaluated for their effectiveness in generating high-quality questions, including open-ended and multiple-choice formats in various languages. Advanced NLP methods like fine-tuning and prompt-tuning are explored for their role in generating task-specific questions, despite associated costs. The chapter also covers the human evaluation of generated questions, highlighting quality variations across different methods and areas for improvement. Furthermore, it delves into automated answer assessment, demonstrating how LLMs can accurately evaluate responses, provide constructive feedback, and identify nuanced understanding or misconceptions. Examples illustrate both successful assessments and areas needing improvement. The discussion underscores the potential of LLMs to replace costly, time-consuming human assessments when appropriately guided, showcasing their advanced understanding and reasoning capabilities in streamlining educational processes.\"\\n      },\\n      {\\n        \"paperId\": \"c0a526a448a5d0d6b17c5b08fc5920c2d22fcb74\",\\n        \"url\": \"https://www.semanticscholar.org/paper/c0a526a448a5d0d6b17c5b08fc5920c2d22fcb74\",\\n        \"title\": \"UN-SAM: Universal Prompt-Free Segmentation for Generalized Nuclei Images\",\\n        \"citationCount\": 32,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2402.16663, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work proposes the Universal prompt-free SAM framework for Nuclei segmentation (UN-SAM), by providing a fully automated solution with remarkable generalization capabilities, and devise a multi-scale Self-Prompt Generation (SPGen) module to revolutionize clinical workflow.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-02-26\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2117102192\",\\n            \"name\": \"Zhen Chen\"\\n          },\\n          {\\n            \"authorId\": \"2287826940\",\\n            \"name\": \"Qing Xu\"\\n          },\\n          {\\n            \"authorId\": \"2275332982\",\\n            \"name\": \"Xinyu Liu\"\\n          },\\n          {\\n            \"authorId\": \"2275284236\",\\n            \"name\": \"Yixuan Yuan\"\\n          }\\n        ],\\n        \"abstract\": \"In digital pathology, precise nuclei segmentation is pivotal yet challenged by the diversity of tissue types, staining protocols, and imaging conditions. Recently, the segment anything model (SAM) revealed overwhelming performance in natural scenarios and impressive adaptation to medical imaging. Despite these advantages, the reliance of labor-intensive manual annotation as segmentation prompts severely hinders their clinical applicability, especially for nuclei image analysis containing massive cells where dense manual prompts are impractical. To overcome the limitations of current SAM methods while retaining the advantages, we propose the Universal prompt-free SAM framework for Nuclei segmentation (UN-SAM), by providing a fully automated solution with remarkable generalization capabilities. Specifically, to eliminate the labor-intensive requirement of per-nuclei annotations for prompt, we devise a multi-scale Self-Prompt Generation (SPGen) module to revolutionize clinical workflow by automatically generating high-quality mask hints to guide the segmentation tasks. Moreover, to unleash the generalization capability of SAM across a variety of nuclei images, we devise a Domain-adaptive Tuning Encoder (DT-Encoder) to seamlessly harmonize visual features with domain-common and domain-specific knowledge, and further devise a Domain Query-enhanced Decoder (DQ-Decoder) by leveraging learnable domain queries for segmentation decoding in different nuclei domains. Extensive experiments prove that UN-SAM with exceptional performance surpasses state-of-the-arts in nuclei instance and semantic segmentation, especially the generalization capability in zero-shot scenarios. The source code is available at https://github.com/CUHK-AIM-Group/UN-SAM.\"\\n      },\\n      {\\n        \"paperId\": \"3c38d98583bdfc0d02b618d94394161f65d4dc96\",\\n        \"url\": \"https://www.semanticscholar.org/paper/3c38d98583bdfc0d02b618d94394161f65d4dc96\",\\n        \"title\": \"Adapting LLMs for Efficient Context Processing through Soft Prompt Compression\",\\n        \"citationCount\": 35,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://arxiv.org/pdf/2404.04997\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2404.04997, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This investigation presents an innovative framework that strategically tailors LLMs for streamlined context processing by harnessing the synergies among natural language summarization, soft prompt com- pression, and augmented utility preservation mechanisms.\"\\n        },\\n        \"publicationTypes\": [\\n          \"Book\",\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-04-07\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2295677749\",\\n            \"name\": \"Cangqing Wang\"\\n          },\\n          {\\n            \"authorId\": \"2295682977\",\\n            \"name\": \"Yutian Yang\"\\n          },\\n          {\\n            \"authorId\": \"2303231934\",\\n            \"name\": \"Ruisi Li\"\\n          },\\n          {\\n            \"authorId\": \"2295684775\",\\n            \"name\": \"Dan Sun\"\\n          },\\n          {\\n            \"authorId\": \"2295667065\",\\n            \"name\": \"Ruicong Cai\"\\n          },\\n          {\\n            \"authorId\": \"2295678151\",\\n            \"name\": \"Yuzhu Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2295659472\",\\n            \"name\": \"Chengqian Fu\"\\n          }\\n        ],\\n        \"abstract\": \"The rapid advancement of Large Language Models (LLMs) has inaugurated a transformative epoch in natural language processing, fostering unprecedented proficiency in text generation, comprehension, and contextual scrutiny. Nevertheless, effectively handling extensive contexts, crucial for myriad applications, poses a formidable obstacle owing to the intrinsic constraints of the models\\\\u2019 context window sizes and the computational burdens entailed by their operations. This investigation presents an innovative framework that strategically tailors LLMs for streamlined context processing by harnessing the synergies among natural language summarization, soft prompt com- pression, and augmented utility preservation mechanisms. Our methodology, dubbed SoftPromptComp, amalgamates natural language prompts extracted from summarization methodologies with dynamically generated soft prompts to forge a concise yet semantically robust depiction of protracted contexts. This depiction undergoes further refinement via a weighting mechanism optimizing information retention and utility for subsequent tasks. We substantiate that our framework markedly diminishes computational overhead and enhances LLMs\\\\u2019 efficacy across various benchmarks, while upholding or even augmenting the caliber of the produced content. By amalgamating soft prompt compression with sophisticated summarization, SoftPromptComp confronts the dual challenges of managing lengthy contexts and ensuring model scalability. Our findings point towards a propitious trajectory for augmenting LLMs\\\\u2019 applicability and efficiency, rendering them more versatile and pragmatic for real- world applications. This research enriches the ongoing discourse on optimizing language models, providing insights into the potency of soft prompts and summarization techniques as pivotal instruments for the forthcoming generation of NLP solutions.\"\\n      },\\n      {\\n        \"paperId\": \"836e3069a83f455f916114e7265e00187e511838\",\\n        \"url\": \"https://www.semanticscholar.org/paper/836e3069a83f455f916114e7265e00187e511838\",\\n        \"title\": \"Locally Differentially Private Document Generation Using Zero Shot Prompting\",\\n        \"citationCount\": 55,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2310.16111, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work proposes a locally differentially private mechanism called DP-Prompt, which leverages the power of pretrained large language models and zero-shot prompting to counter author de-anonymization attacks while minimizing the impact on downstream utility.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2023-10-24\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"1512255229\",\\n            \"name\": \"Saiteja Utpala\"\\n          },\\n          {\\n            \"authorId\": \"2261493078\",\\n            \"name\": \"Sara Hooker\"\\n          },\\n          {\\n            \"authorId\": \"2261697074\",\\n            \"name\": \"Pin Yu Chen\"\\n          }\\n        ],\\n        \"abstract\": \"Numerous studies have highlighted the privacy risks associated with pretrained large language models. In contrast, our research offers a unique perspective by demonstrating that pretrained large language models can effectively contribute to privacy preservation. We propose a locally differentially private mechanism called DP-Prompt, which leverages the power of pretrained large language models and zero-shot prompting to counter author de-anonymization attacks while minimizing the impact on downstream utility. When DP-Prompt is used with a powerful language model like ChatGPT (gpt-3.5), we observe a notable reduction in the success rate of de-anonymization attacks, showing that it surpasses existing approaches by a considerable margin despite its simpler design. For instance, in the case of the IMDB dataset, DP-Prompt (with ChatGPT) perfectly recovers the clean sentiment F1 score while achieving a 46\\\\\\\\% reduction in author identification F1 score against static attackers and a 26\\\\\\\\% reduction against adaptive attackers. We conduct extensive experiments across six open-source large language models, ranging up to 7 billion parameters, to analyze various effects of the privacy-utility tradeoff.\"\\n      },\\n      {\\n        \"paperId\": \"f087d0175346f143e190fc39f4ec4ceb5b3cc093\",\\n        \"url\": \"https://www.semanticscholar.org/paper/f087d0175346f143e190fc39f4ec4ceb5b3cc093\",\\n        \"title\": \"Fine-Grained Visual Prompt Learning of Vision-Language Models for Image Recognition\",\\n        \"citationCount\": 17,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1145/3581783.3612403?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/3581783.3612403, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"Fine-grained visual prompt is introduced into the image encoder of the vision-language model for focusing on the target object and conducting information interaction within the object, which facilitates generating discriminative visual features for image recognition.\"\\n        },\\n        \"publicationTypes\": [\\n          \"Book\",\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2023-10-26\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2261902040\",\\n            \"name\": \"Hongbo Sun\"\\n          },\\n          {\\n            \"authorId\": \"153003087\",\\n            \"name\": \"Xiangteng He\"\\n          },\\n          {\\n            \"authorId\": \"2261798088\",\\n            \"name\": \"Jiahuan Zhou\"\\n          },\\n          {\\n            \"authorId\": \"143753918\",\\n            \"name\": \"Yuxin Peng\"\\n          }\\n        ],\\n        \"abstract\": \"Large-scale pre-trained vision-language (VL) models have shown powerful generic representation capabilities for adapting to downstream tasks with limited training data, which are data-efficient solutions to various applications such as image recognition. In order to enhance the adaption performance, most existing methods attempt to introduce learnable vectors into the text prompt to generate adaptive classification weights for the class in the downstream task. However, they generally focus on the text side while neglecting adaptive visual feature generation on the image side, which is insufficient to fit the downstream task data. In this paper, we propose fine-grained visual prompt learning (FG-VPL) of vision-language models for image recognition with few training samples, and the main contributions are: (1) Fine-grained visual prompt is introduced into the image encoder of the vision-language model for focusing on the target object and conducting information interaction within the object, which facilitates generating discriminative visual features for image recognition. (2) A two-pathway adaptive recognition module is proposed to narrow the domain gap and utilize both the cross-modal knowledge of the vision-language model and the visual information of the few-sample training set for classifying images with the help of feature adapters. We conduct extensive experiments on 11 image recognition benchmark datasets under the few training samples setting, which demonstrate that our proposed approach can achieve state-of-the-art performance. The code is available at https://github.com/PKU-ICST-MIPL/FG-VPL_ACMMM2023.\"\\n      },\\n      {\\n        \"paperId\": \"06d8562831c32844285a691c5250d04726df3c61\",\\n        \"url\": \"https://www.semanticscholar.org/paper/06d8562831c32844285a691c5250d04726df3c61\",\\n        \"title\": \"A Systematic Survey of Prompt Engineering on Vision-Language Foundation Models\",\\n        \"citationCount\": 206,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://arxiv.org/pdf/2307.12980\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {\\'abstract\\'}. Paper or abstract available at https://api.unpaywall.org/v2/10.48550/arXiv.2307.12980?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.48550/arXiv.2307.12980, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper aims to provide a comprehensive survey of cutting-edge research in prompt engineering on three types of vision-language models: multimodal-to-text generation models, image-text matching models, and text- to-image generation models.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Review\"\\n        ],\\n        \"publicationDate\": null,\\n        \"authors\": [\\n          {\\n            \"authorId\": \"52203056\",\\n            \"name\": \"Jindong Gu\"\\n          },\\n          {\\n            \"authorId\": \"2223193538\",\\n            \"name\": \"Zhen Han\"\\n          },\\n          {\\n            \"authorId\": \"2116572341\",\\n            \"name\": \"Shuo Chen\"\\n          },\\n          {\\n            \"authorId\": \"1791052\",\\n            \"name\": \"Ahmad Beirami\"\\n          },\\n          {\\n            \"authorId\": \"2147293727\",\\n            \"name\": \"Bailan He\"\\n          },\\n          {\\n            \"authorId\": \"2143853643\",\\n            \"name\": \"Gengyuan Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2072387342\",\\n            \"name\": \"Ruotong Liao\"\\n          },\\n          {\\n            \"authorId\": \"2219078907\",\\n            \"name\": \"Yao Qin\"\\n          },\\n          {\\n            \"authorId\": \"1742501819\",\\n            \"name\": \"Volker Tresp\"\\n          },\\n          {\\n            \"authorId\": \"143635540\",\\n            \"name\": \"Philip H. S. Torr\"\\n          }\\n        ],\\n        \"abstract\": null\\n      },\\n      {\\n        \"paperId\": \"b013c9eb1284554ae696fba02bd4d7fc599890b6\",\\n        \"url\": \"https://www.semanticscholar.org/paper/b013c9eb1284554ae696fba02bd4d7fc599890b6\",\\n        \"title\": \"StyleCrafter: Enhancing Stylized Text-to-Video Generation with Style Adapter\",\\n        \"citationCount\": 34,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {\\'abstract\\'}. Paper or abstract available at https://api.unpaywall.org/v2/10.48550/arXiv.2312.00330?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.48550/arXiv.2312.00330, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"StyleCrafter is introduced, a generic method that enhances pre-trained T2V models with a style control adapter, enabling video generation in any style by providing a reference image, and designs a scale-adaptive fusion module to balance the influences of text-based content features and image-based style features.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": null,\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2269171464\",\\n            \"name\": \"Gongye Liu\"\\n          },\\n          {\\n            \"authorId\": \"2257035878\",\\n            \"name\": \"Menghan Xia\"\\n          },\\n          {\\n            \"authorId\": \"2257199953\",\\n            \"name\": \"Yong Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2149052351\",\\n            \"name\": \"Haoxin Chen\"\\n          },\\n          {\\n            \"authorId\": \"2087273800\",\\n            \"name\": \"Jinbo Xing\"\\n          },\\n          {\\n            \"authorId\": \"2253795356\",\\n            \"name\": \"Xintao Wang\"\\n          },\\n          {\\n            \"authorId\": \"3001727\",\\n            \"name\": \"Yujiu Yang\"\\n          },\\n          {\\n            \"authorId\": \"2257019659\",\\n            \"name\": \"Ying Shan\"\\n          }\\n        ],\\n        \"abstract\": null\\n      }\\n    ]\\n  },\\n  \"iterative context update\": {\\n    \"total\": 9050,\\n    \"offset\": 0,\\n    \"next\": 50,\\n    \"data\": [\\n      {\\n        \"paperId\": \"a0d2ea210c9bd21676605682a76cec1a4004320a\",\\n        \"url\": \"https://www.semanticscholar.org/paper/a0d2ea210c9bd21676605682a76cec1a4004320a\",\\n        \"title\": \"Iterative Context-Aware Graph Inference for Visual Dialog\",\\n        \"citationCount\": 51,\\n        \"openAccessPdf\": {\\n          \"url\": \"http://arxiv.org/pdf/2004.02194\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2004.02194, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work proposes a novel Context-Aware Graph (CAG) neural network, where each node in the graph corresponds to a joint semantic feature, including both object-based (visual) and history-related (textual) context representations.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2020-04-05\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"144713153\",\\n            \"name\": \"Dan Guo\"\\n          },\\n          {\\n            \"authorId\": \"46507139\",\\n            \"name\": \"Haibo Wang\"\\n          },\\n          {\\n            \"authorId\": \"5462268\",\\n            \"name\": \"Hanwang Zhang\"\\n          },\\n          {\\n            \"authorId\": \"143962510\",\\n            \"name\": \"Zhengjun Zha\"\\n          },\\n          {\\n            \"authorId\": \"47446553\",\\n            \"name\": \"Meng Wang\"\\n          }\\n        ],\\n        \"abstract\": \"Visual dialog is a challenging task that requires the comprehension of the semantic dependencies among implicit visual and textual contexts. This task can refer to the relation inference in a graphical model with sparse contexts and unknown graph structure (relation descriptor), and how to model the underlying context-aware relation inference is critical. To this end, we propose a novel Context-Aware Graph (CAG) neural network. Each node in the graph corresponds to a joint semantic feature, including both object-based (visual) and history-related (textual) context representations. The graph structure (relations in dialog) is iteratively updated using an adaptive top-K message passing mechanism. Specifically, in every message passing step, each node selects the most K relevant nodes, and only receives messages from them. Then, after the update, we impose graph attention on all the nodes to get the final graph embedding and infer the answer. In CAG, each node has dynamic relations in the graph (different related K neighbor nodes), and only the most relevant nodes are attributive to the context-aware relational graph inference. Experimental results on VisDial v0.9 and v1.0 datasets show that CAG outperforms comparative methods. Visualization results further validate the interpretability of our method.\"\\n      },\\n      {\\n        \"paperId\": \"697e176d66a17c0b24613b8513ab951dc4112c34\",\\n        \"url\": \"https://www.semanticscholar.org/paper/697e176d66a17c0b24613b8513ab951dc4112c34\",\\n        \"title\": \"Iterative Geometry Encoding Volume for Stereo Matching\",\\n        \"citationCount\": 280,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://arxiv.org/pdf/2303.06615\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2303.06615, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper proposes Iterative Geometry Encoding Volume (IGEV-Stereo), a new deep network architecture for stereo matching that builds a combined geometry encoding volume that encodes geometry and context information as well as local matching details, and iteratively indexes it to update the disparity map.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2023-03-12\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2158317969\",\\n            \"name\": \"Gangwei Xu\"\\n          },\\n          {\\n            \"authorId\": \"2107956900\",\\n            \"name\": \"Xianqi Wang\"\\n          },\\n          {\\n            \"authorId\": \"2117436225\",\\n            \"name\": \"Xiao-Hua Ding\"\\n          },\\n          {\\n            \"authorId\": \"2150441002\",\\n            \"name\": \"Xin Yang\"\\n          }\\n        ],\\n        \"abstract\": \"Recurrent All-Pairs Field Transforms (RAFT) has shown great potentials in matching tasks. However, all-pairs correlations lack non-local geometry knowledge and have difficulties tackling local ambiguities in ill-posed regions. In this paper, we propose Iterative Geometry Encoding Volume (IGEV-Stereo), a new deep network architecture for stereo matching. The proposed IGEV-Stereo builds a combined geometry encoding volume that encodes geometry and context information as well as local matching details, and iteratively indexes it to update the disparity map. To speed up the convergence, we exploit GEV to regress an accurate starting point for ConvGRUs iterations. Our IGEV-Stereo ranks 1st on KITTI 2015 and 2012 (Reflective) among all published methods and is the fastest among the top 10 methods. In addition, IGEV-Stereo has strong cross-dataset generalization as well as high inference efficiency. We also extend our IGEV to multi-view stereo (MVS), i.e. IGEV-MVS, which achieves competitive accuracy on DTU benchmark. Code is available at https://github.com/gangweiX/IGEV.\"\\n      },\\n      {\\n        \"paperId\": \"8c3a6d134aa3374ed8e1de2c47bcc73e3f9ee15e\",\\n        \"url\": \"https://www.semanticscholar.org/paper/8c3a6d134aa3374ed8e1de2c47bcc73e3f9ee15e\",\\n        \"title\": \"Proximal Stabilized Interior Point Methods and Low-Frequency-Update Preconditioning Techniques\",\\n        \"citationCount\": 13,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://link.springer.com/content/pdf/10.1007/s10957-023-02194-4.pdf\",\\n          \"status\": \"HYBRID\",\\n          \"license\": \"CCBY\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1007/s10957-023-02194-4?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1007/s10957-023-02194-4, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work considers Primal Dual Regularized Interior Point Methods in the framework of the Proximal Point Method, and shows experimentally that low-frequency-update of the preconditioners are allowed, paving the path for an alternative class of second order methods characterized by reduced computational effort.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2023-04-05\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"15569181\",\\n            \"name\": \"S. Cipolla\"\\n          },\\n          {\\n            \"authorId\": \"51043392\",\\n            \"name\": \"J. Gondzio\"\\n          }\\n        ],\\n        \"abstract\": \"In this work, in the context of Linear and convex Quadratic Programming, we consider Primal Dual Regularized Interior Point Methods (PDR-IPMs) in the framework of the Proximal Point Method. The resulting Proximal Stabilized IPM (PS-IPM) is strongly supported by theoretical results concerning convergence and the rate of convergence, and can handle degenerate problems. Moreover, in the second part of this work, we analyse the interactions between the regularization parameters and the computational footprint of the linear algebra routines used to solve the Newton linear systems. In particular, when these systems are solved using an iterative Krylov method, we are able to show\\\\u2014using a new rearrangement of the Schur complement which exploits regularization\\\\u2014that general purposes preconditioners remain attractive for a series of subsequent IPM iterations. Indeed, if on the one hand a series of theoretical results underpin the fact that the approach here presented allows a better re-use of such computed preconditioners, on the other, we show experimentally that such (re)computations are needed only in a fraction of the total IPM iterations. The resulting regularized second order methods, for which low-frequency-update of the preconditioners are allowed, pave the path for an alternative class of second order methods characterized by reduced computational effort.\"\\n      },\\n      {\\n        \"paperId\": \"8fd4d5762de7c6861c841bc54208ebeec76b6213\",\\n        \"url\": \"https://www.semanticscholar.org/paper/8fd4d5762de7c6861c841bc54208ebeec76b6213\",\\n        \"title\": \"IMENet: Joint 3D Semantic Scene Completion and 2D Semantic Segmentation through Iterative Mutual Enhancement\",\\n        \"citationCount\": 13,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://www.ijcai.org/proceedings/2021/0110.pdf\",\\n          \"status\": \"BRONZE\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2106.15413, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work argues that this sequential scheme does not ensure these two tasks fully benefit each other, and presents an Iterative Mutual Enhancement Network (IMENet) to solve them jointly, which interactively refines the two tasks at the late prediction stage.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2021-06-29\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"1490486462\",\\n            \"name\": \"Jie Li\"\\n          },\\n          {\\n            \"authorId\": \"1518268141\",\\n            \"name\": \"Laiyan Ding\"\\n          },\\n          {\\n            \"authorId\": \"1516136186\",\\n            \"name\": \"Rui Huang\"\\n          }\\n        ],\\n        \"abstract\": \"3D semantic scene completion and 2D semantic segmentation are two tightly correlated tasks that are both essential for indoor scene understanding, because they predict the same semantic classes, using positively correlated high-level features. Current methods use 2D features extracted from early-fused RGB-D images for 2D segmentation to improve 3D scene completion. We argue that this sequential scheme does not ensure these two tasks fully benefit each other, and present an Iterative Mutual Enhancement Network (IMENet) to solve them jointly, which interactively refines the two tasks at the late prediction stage. Specifically, two refinement modules are developed under a unified framework for the two tasks. The first is a 2D Deformable Context Pyramid (DCP) module, which receives the projection from the current 3D predictions to refine the 2D predictions. In turn, a 3D Deformable Depth Attention (DDA) module is proposed to leverage the reprojected results from 2D predictions to update the coarse 3D predictions. This iterative fusion happens to the stable high-level features of both tasks at a late stage. Extensive experiments on NYU and NYUCAD datasets verify the effectiveness of the proposed iterative late fusion scheme, and our approach outperforms the state of the art on both 3D semantic scene completion and 2D semantic segmentation.\"\\n      },\\n      {\\n        \"paperId\": \"be089bc2e9983223068b4eb0d67b4b2f5fc30321\",\\n        \"url\": \"https://www.semanticscholar.org/paper/be089bc2e9983223068b4eb0d67b4b2f5fc30321\",\\n        \"title\": \"Iterative Privileged Learning\",\\n        \"citationCount\": 17,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {\\'tldr\\'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/TNNLS.2018.2889906?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/TNNLS.2018.2889906, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": null,\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2020-08-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2116383294\",\\n            \"name\": \"Xue Li\"\\n          },\\n          {\\n            \"authorId\": \"145728792\",\\n            \"name\": \"Bo Du\"\\n          },\\n          {\\n            \"authorId\": \"2760404\",\\n            \"name\": \"Yipeng Zhang\"\\n          },\\n          {\\n            \"authorId\": null,\\n            \"name\": \"Chang Xu\"\\n          },\\n          {\\n            \"authorId\": \"143719920\",\\n            \"name\": \"D. Tao\"\\n          }\\n        ],\\n        \"abstract\": \"While in the learning using privileged information paradigm, privileged information may not be as informative as example features in the context of making accurate label predictions, it may be able to provide some effective comments (e.g., the values of the auxiliary function) like a human teacher on the efficacy of the learned model. In a departure from conventional static manipulations of privileged information within the support vector machine framework, this paper investigates iterative privileged learning within the context of gradient boosted decision trees (GBDTs). As the learned model evolves, the comments learned from privileged information to assess the model should also be actively upgraded instead of remaining static and passive. During the learning phase of the GBDT method, new DTs are discovered to enhance the performance of the model, and iteratively update the comments generated from the privileged information to accurately assess and coach the up-to-date model. The resulting objective function can be efficiently solved within the gradient boosting framework. Experimental results on real-world data sets demonstrate the benefits of studying privileged information in an iterative manner, as well as the effectiveness of the proposed algorithm.\"\\n      },\\n      {\\n        \"paperId\": \"22ae025a4f0b644369d05a3e13c0021f868a8372\",\\n        \"url\": \"https://www.semanticscholar.org/paper/22ae025a4f0b644369d05a3e13c0021f868a8372\",\\n        \"title\": \"Hunyuan-DiT: A Powerful Multi-Resolution Diffusion Transformer with Fine-Grained Chinese Understanding\",\\n        \"citationCount\": 201,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2405.08748, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"Huyuan-DiT, a text-to-image diffusion transformer with fine-grained understanding of both English and Chinese, sets a new state-of-the-art in Chinese-to-image generation compared with other open-source models.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-05-14\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2286959603\",\\n            \"name\": \"Zhimin Li\"\\n          },\\n          {\\n            \"authorId\": \"2301232669\",\\n            \"name\": \"Jianwei Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2301456725\",\\n            \"name\": \"Qin Lin\"\\n          },\\n          {\\n            \"authorId\": \"2291439620\",\\n            \"name\": \"Jiangfeng Xiong\"\\n          },\\n          {\\n            \"authorId\": \"2301272172\",\\n            \"name\": \"Yanxin Long\"\\n          },\\n          {\\n            \"authorId\": \"2291203760\",\\n            \"name\": \"Xinchi Deng\"\\n          },\\n          {\\n            \"authorId\": \"2301231346\",\\n            \"name\": \"Yingfang Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2278003955\",\\n            \"name\": \"Xingchao Liu\"\\n          },\\n          {\\n            \"authorId\": \"2162225343\",\\n            \"name\": \"Minbin Huang\"\\n          },\\n          {\\n            \"authorId\": \"2301268000\",\\n            \"name\": \"Zedong Xiao\"\\n          },\\n          {\\n            \"authorId\": \"2301264655\",\\n            \"name\": \"Dayou Chen\"\\n          },\\n          {\\n            \"authorId\": \"2268375443\",\\n            \"name\": \"Jiajun He\"\\n          },\\n          {\\n            \"authorId\": \"2276668269\",\\n            \"name\": \"Jiahao Li\"\\n          },\\n          {\\n            \"authorId\": \"2301265720\",\\n            \"name\": \"Wenyue Li\"\\n          },\\n          {\\n            \"authorId\": \"2279868541\",\\n            \"name\": \"Chen Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2194482010\",\\n            \"name\": \"Rongwei Quan\"\\n          },\\n          {\\n            \"authorId\": \"2301417687\",\\n            \"name\": \"Jianxiang Lu\"\\n          },\\n          {\\n            \"authorId\": \"2301265144\",\\n            \"name\": \"Jiabin Huang\"\\n          },\\n          {\\n            \"authorId\": \"2302916549\",\\n            \"name\": \"Xiaoyan Yuan\"\\n          },\\n          {\\n            \"authorId\": \"2282541108\",\\n            \"name\": \"Xiao-Ting Zheng\"\\n          },\\n          {\\n            \"authorId\": \"2238393614\",\\n            \"name\": \"Yixuan Li\"\\n          },\\n          {\\n            \"authorId\": \"2290433340\",\\n            \"name\": \"Jihong Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2256776298\",\\n            \"name\": \"Chao Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2279065846\",\\n            \"name\": \"Mengxi Chen\"\\n          },\\n          {\\n            \"authorId\": \"2290446584\",\\n            \"name\": \"Jie Liu\"\\n          },\\n          {\\n            \"authorId\": \"2180527302\",\\n            \"name\": \"Zheng Fang\"\\n          },\\n          {\\n            \"authorId\": \"2280852403\",\\n            \"name\": \"Weiyan Wang\"\\n          },\\n          {\\n            \"authorId\": \"2067731583\",\\n            \"name\": \"J. Xue\"\\n          },\\n          {\\n            \"authorId\": \"2267016579\",\\n            \"name\": \"Yang-Dan Tao\"\\n          },\\n          {\\n            \"authorId\": \"2141509708\",\\n            \"name\": \"Jianchen Zhu\"\\n          },\\n          {\\n            \"authorId\": \"2278809013\",\\n            \"name\": \"Kai Liu\"\\n          },\\n          {\\n            \"authorId\": \"2107932778\",\\n            \"name\": \"Si-Da Lin\"\\n          },\\n          {\\n            \"authorId\": \"2373974424\",\\n            \"name\": \"Yifu Sun\"\\n          },\\n          {\\n            \"authorId\": \"2261466479\",\\n            \"name\": \"Yun Li\"\\n          },\\n          {\\n            \"authorId\": \"2195078271\",\\n            \"name\": \"Dongdong Wang\"\\n          },\\n          {\\n            \"authorId\": \"2393630397\",\\n            \"name\": \"Mingtao Chen\"\\n          },\\n          {\\n            \"authorId\": \"2297268600\",\\n            \"name\": \"Zhichao Hu\"\\n          },\\n          {\\n            \"authorId\": \"2290971075\",\\n            \"name\": \"Xiao Xiao\"\\n          },\\n          {\\n            \"authorId\": \"2266017750\",\\n            \"name\": \"Yan Chen\"\\n          },\\n          {\\n            \"authorId\": \"2265797902\",\\n            \"name\": \"Yuhong Liu\"\\n          },\\n          {\\n            \"authorId\": \"2283172530\",\\n            \"name\": \"Wei Liu\"\\n          },\\n          {\\n            \"authorId\": \"2256286591\",\\n            \"name\": \"Dingyong Wang\"\\n          },\\n          {\\n            \"authorId\": \"2284490220\",\\n            \"name\": \"Yong Yang\"\\n          },\\n          {\\n            \"authorId\": \"2280399696\",\\n            \"name\": \"Jie Jiang\"\\n          },\\n          {\\n            \"authorId\": \"2268722917\",\\n            \"name\": \"Qinglin Lu\"\\n          }\\n        ],\\n        \"abstract\": \"We present Hunyuan-DiT, a text-to-image diffusion transformer with fine-grained understanding of both English and Chinese. To construct Hunyuan-DiT, we carefully design the transformer structure, text encoder, and positional encoding. We also build from scratch a whole data pipeline to update and evaluate data for iterative model optimization. For fine-grained language understanding, we train a Multimodal Large Language Model to refine the captions of the images. Finally, Hunyuan-DiT can perform multi-turn multimodal dialogue with users, generating and refining images according to the context. Through our holistic human evaluation protocol with more than 50 professional human evaluators, Hunyuan-DiT sets a new state-of-the-art in Chinese-to-image generation compared with other open-source models. Code and pretrained models are publicly available at github.com/Tencent/HunyuanDiT\"\\n      },\\n      {\\n        \"paperId\": \"7d2f92bfcded0fe63cb3926155d769263b9580c9\",\\n        \"url\": \"https://www.semanticscholar.org/paper/7d2f92bfcded0fe63cb3926155d769263b9580c9\",\\n        \"title\": \"RLCoder: Reinforcement Learning for Repository-Level Code Completion\",\\n        \"citationCount\": 39,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2407.19487, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work proposes RLCoder, a novel reinforcement learning framework, which can enable the retriever to learn to retrieve useful content for code completion without the need for labeled data and introduces a stop signal mechanism.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-07-28\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2279774134\",\\n            \"name\": \"Yanlin Wang\"\\n          },\\n          {\\n            \"authorId\": \"2279774134\",\\n            \"name\": \"Yanlin Wang\"\\n          },\\n          {\\n            \"authorId\": \"2314374754\",\\n            \"name\": \"Daya Guo\"\\n          },\\n          {\\n            \"authorId\": \"2254800142\",\\n            \"name\": \"Jiachi Chen\"\\n          },\\n          {\\n            \"authorId\": \"2305869100\",\\n            \"name\": \"Ruikai Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2305694096\",\\n            \"name\": \"Yuchi Ma\"\\n          },\\n          {\\n            \"authorId\": \"2267902535\",\\n            \"name\": \"Zibin Zheng\"\\n          }\\n        ],\\n        \"abstract\": \"Repository-level code completion aims to generate code for unfinished code snippets within the context of a specified repository. Existing approaches mainly rely on retrievalaugmented generation strategies due to limitations in input sequence length. However, traditional lexical-based retrieval methods like BM25 struggle to capture code semantics, while model-based retrieval methods face challenges due to the lack of labeled data for training. Therefore, we propose RLCoder, a novel reinforcement learning framework, which can enable the retriever to learn to retrieve useful content for code completion without the need for labeled data. Specifically, we iteratively evaluate the usefulness of retrieved content based on the perplexity of the target code when provided with the retrieved content as additional context, and provide feedback to update the retriever parameters. This iterative process enables the retriever to learn from its successes and failures, gradually improving its ability to retrieve relevant and high-quality content. Considering that not all situations require information beyond code files and not all retrieved context is helpful for generation, we also introduce a stop signal mechanism, allowing the retriever to decide when to retrieve and which candidates to retain autonomously. Extensive experimental results demonstrate that RLCoder consistently outperforms state-of-the-art methods on CrossCodeEval and RepoEval, achieving 12.2% EM improvement over previous methods. Moreover, experiments show that our framework can generalize across different programming languages and further improve previous methods like RepoCoder. We provide the code and data at https://github.com/DeepSoftwareAnalytics/RLCoder.\"\\n      },\\n      {\\n        \"paperId\": \"69ec41a25a4ead52bab62ea220103fdde06f7126\",\\n        \"url\": \"https://www.semanticscholar.org/paper/69ec41a25a4ead52bab62ea220103fdde06f7126\",\\n        \"title\": \"Logic-in-Frames: Dynamic Keyframe Search via Visual Semantic-Logical Verification for Long Video Understanding\",\\n        \"citationCount\": 15,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2503.13139, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work introduces a semantics-driven search framework that reformulates keyframe selection under the paradigm of Visual Semantic-Logical Search, validating its effectiveness in bridging the logical gap between textual queries and visual-temporal reasoning.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2025-03-17\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2257320765\",\\n            \"name\": \"Weiyu Guo\"\\n          },\\n          {\\n            \"authorId\": \"2347655949\",\\n            \"name\": \"Ziyang Chen\"\\n          },\\n          {\\n            \"authorId\": \"2347552474\",\\n            \"name\": \"Shaoguang Wang\"\\n          },\\n          {\\n            \"authorId\": \"2347022035\",\\n            \"name\": \"Jianxiang He\"\\n          },\\n          {\\n            \"authorId\": \"2294806563\",\\n            \"name\": \"Yijie Xu\"\\n          },\\n          {\\n            \"authorId\": \"2348899671\",\\n            \"name\": \"Jinhui Ye\"\\n          },\\n          {\\n            \"authorId\": \"2257321422\",\\n            \"name\": \"Ying Sun\"\\n          },\\n          {\\n            \"authorId\": \"2346984163\",\\n            \"name\": \"Hui Xiong\"\\n          }\\n        ],\\n        \"abstract\": \"Understanding long video content is a complex endeavor that often relies on densely sampled frame captions or end-to-end feature selectors, yet these techniques commonly overlook the logical relationships between textual queries and visual elements. In practice, computational constraints necessitate coarse frame subsampling, a challenge analogous to\\\\\"finding a needle in a haystack.\\\\\"To address this issue, we introduce a semantics-driven search framework that reformulates keyframe selection under the paradigm of Visual Semantic-Logical Search. Specifically, we systematically define four fundamental logical dependencies: 1) spatial co-occurrence, 2) temporal proximity, 3) attribute dependency, and 4) causal order. These relations dynamically update frame sampling distributions through an iterative refinement process, enabling context-aware identification of semantically critical frames tailored to specific query requirements. Our method establishes new SOTA performance on the manually annotated benchmark in key-frame selection metrics. Furthermore, when applied to downstream video question-answering tasks, the proposed approach demonstrates the best performance gains over existing methods on LongVideoBench and Video-MME, validating its effectiveness in bridging the logical gap between textual queries and visual-temporal reasoning. The code will be publicly available.\"\\n      },\\n      {\\n        \"paperId\": \"c5dae02bb601106a51bec4497f0581affd6af8a7\",\\n        \"url\": \"https://www.semanticscholar.org/paper/c5dae02bb601106a51bec4497f0581affd6af8a7\",\\n        \"title\": \"Zero-Shot Class Unlearning in CLIP with Synthetic Samples\",\\n        \"citationCount\": 10,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2407.07485, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work focuses on unlearning within CLIP, a dual vision-language encoder model trained on a massive dataset of image-text pairs using contrastive loss, and expands the application of Lipschitz regularization to the multimodal context of CLIP to achieve forgetting.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-07-10\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2310610834\",\\n            \"name\": \"Alexey Kravets\"\\n          },\\n          {\\n            \"authorId\": \"2310607749\",\\n            \"name\": \"Vinay Namboodiri\"\\n          }\\n        ],\\n        \"abstract\": \"Machine unlearning is a crucial area of research. It is driven by the need to remove sensitive information from models to safeguard individuals\\' right to be forgotten under rigorous regulations such as GDPR. In this work, we focus on unlearning within CLIP, a dual vision-language encoder model trained on a massive dataset of image-text pairs using contrastive loss. To achieve forgetting we expand the application of Lipschitz regularization to the multimodal context of CLIP. Specifically, we smooth both visual and textual embeddings associated with the class intended to be forgotten relative to the perturbation introduced to the samples from that class. Additionally, importantly, we remove the necessity for real forgetting data by generating synthetic samples via gradient ascent maximizing the target class. Our forgetting procedure is iterative, where we track accuracy on a synthetic forget set and stop when accuracy falls below a chosen threshold. We employ a selective layers update strategy based on their average absolute gradient value to mitigate over-forgetting. We validate our approach on several standard datasets and provide thorough ablation analysis and comparisons with previous work.\"\\n      },\\n      {\\n        \"paperId\": \"096df636db478a4eb7ab9c74f8a6cae97efed149\",\\n        \"url\": \"https://www.semanticscholar.org/paper/096df636db478a4eb7ab9c74f8a6cae97efed149\",\\n        \"title\": \"Real-time parameter updating for nonlinear digital twins using inverse mapping models and transient-based features\",\\n        \"citationCount\": 20,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://link.springer.com/content/pdf/10.1007/s11071-023-08354-5.pdf\",\\n          \"status\": \"HYBRID\",\\n          \"license\": \"CCBY\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1007/s11071-023-08354-5?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1007/s11071-023-08354-5, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"Various types of transient response features are introduced to update parameter values of nonlinear dynamical systems with increased computational efficiency and accuracy and it is shown that a smart selection of features, based on the frequency content of the transient response, can improve the accuracy of the estimated parameter values, leading to more accurate updated models.\"\\n        },\\n        \"publicationTypes\": null,\\n        \"publicationDate\": \"2023-03-26\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2133908735\",\\n            \"name\": \"B. M. Kessels\"\\n          },\\n          {\\n            \"authorId\": \"102993518\",\\n            \"name\": \"R. Fey\"\\n          },\\n          {\\n            \"authorId\": \"2246982557\",\\n            \"name\": \"N. van de Wouw\"\\n          }\\n        ],\\n        \"abstract\": \"In the context of digital twins, it is essential that a model gives an accurate description of the (controlled) dynamic behavior of a physical system during the system\\\\u2019s entire operational life. Therefore, model updating techniques are required that enable real-time updating of physically interpretable parameter values and are applicable to a wide range of (nonlinear) dynamical systems. As traditional, iterative, parameter updating methods may be computationally too expensive for real-time updating, the inverse mapping parameter updating (IMPU) method is proposed as an alternative. For this method, first, an artificial neural network (ANN) is trained offline using novel features of simulated transient response data. Then, in the online phase, this ANN maps, with little computational cost, a set of measured output response features to parameter estimates enabling real-time model updating. In this paper, various types of transient response features are introduced to update parameter values of nonlinear dynamical systems with increased computational efficiency and accuracy. To analyze the efficacy of these features, the IMPU method is applied to a (simulated) nonlinear multibody system. It is shown that a smart selection of features, based on, e.g., the frequency content of the transient response, can improve the accuracy of the estimated parameter values, leading to more accurate updated models. Furthermore, the generalization capabilities of the ANNs are analyzed for these feature types, by varying the number of training samples and assessing the effect of incomplete training data. It is shown that the IMPU method can predict parameter values that are not part of the training data with acceptable accuracy as well.\"\\n      },\\n      {\\n        \"paperId\": \"82b7f2ab4faec66f16a62b9b5c610c7a5aa1e410\",\\n        \"url\": \"https://www.semanticscholar.org/paper/82b7f2ab4faec66f16a62b9b5c610c7a5aa1e410\",\\n        \"title\": \"Meta-AF: Meta-Learning for Adaptive Filters\",\\n        \"citationCount\": 31,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://ieeexplore.ieee.org/ielx7/6570655/9970249/09961879.pdf\",\\n          \"status\": \"HYBRID\",\\n          \"license\": \"CCBYNCND\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2204.11942, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work seeks to improve upon hand-derived adaptive filter algorithms and present a comprehensive framework for learning online, adaptive signal processing algorithms or update rules directly from data and frames the development of adaptive filters as a meta-learning problem in the context of deep learning and uses a form of self-supervision to learn online iterative update rules for adaptive filters.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2022-04-25\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"50823831\",\\n            \"name\": \"Jonah Casebeer\"\\n          },\\n          {\\n            \"authorId\": \"32125125\",\\n            \"name\": \"Nicholas J. Bryan\"\\n          },\\n          {\\n            \"authorId\": \"1718742\",\\n            \"name\": \"Paris Smaragdis\"\\n          }\\n        ],\\n        \"abstract\": \"Adaptive filtering algorithms are pervasive throughout signal processing and have had a material impact on a wide variety of domains including audio processing, telecommunications, biomedical sensing, astrophysics and cosmology, seismology, and many more. Adaptive filters typically operate via specialized online, iterative optimization methods such as least-mean squares or recursive least squares and aim to process signals in unknown or nonstationary environments. Such algorithms, however, can be slow and laborious to develop, require domain expertise to create, and necessitate mathematical insight for improvement. In this work, we seek to improve upon hand-derived adaptive filter algorithms and present a comprehensive framework for learning online, adaptive signal processing algorithms or update rules directly from data. To do so, we frame the development of adaptive filters as a meta-learning problem in the context of deep learning and use a form of self-supervision to learn online iterative update rules for adaptive filters. To demonstrate our approach, we focus on audio applications and systematically develop meta-learned adaptive filters for five canonical audio problems including system identification, acoustic echo cancellation, blind equalization, multi-channel dereverberation, and beamforming.We compare our approach against common baselines and/or recent state-of-the-art methods. We show we can learn high-performing adaptive filters that operate in real-time and, in most cases, significantly outperform each method we compare against \\\\u2013 all using a single general-purpose configuration of our approach.\"\\n      },\\n      {\\n        \"paperId\": \"f55c7f2156ecc389132acf5c3fbaca4c3f832abf\",\\n        \"url\": \"https://www.semanticscholar.org/paper/f55c7f2156ecc389132acf5c3fbaca4c3f832abf\",\\n        \"title\": \"Flexible Task Scheduling in Data Relay Satellite Networks\",\\n        \"citationCount\": 22,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {\\'tldr\\'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/TAES.2021.3115587?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/TAES.2021.3115587, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": null,\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2022-04-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"46532001\",\\n            \"name\": \"Guohua Wu\"\\n          },\\n          {\\n            \"authorId\": \"30176488\",\\n            \"name\": \"Qizhang Luo\"\\n          },\\n          {\\n            \"authorId\": \"2153095510\",\\n            \"name\": \"Yanqi Zhu\"\\n          },\\n          {\\n            \"authorId\": \"2049670966\",\\n            \"name\": \"Xinjiang Chen\"\\n          },\\n          {\\n            \"authorId\": \"2115387723\",\\n            \"name\": \"Yang Feng\"\\n          },\\n          {\\n            \"authorId\": \"1731634\",\\n            \"name\": \"W. Pedrycz\"\\n          }\\n        ],\\n        \"abstract\": \"The task-schedulingalgorithm is a key module to satisfy various complex user requirements, and improve the usage flexibility and efficiency of data relay satellites networks (DRSN). In this context, we first propose a novel application mode for DRSN, in which users are allowed to submit multiple optional service time windows and specify a preferred antenna as well as an expected execution duration for each task. Meanwhile, the start time of a service time window can be adjusted within a specified range. A mathematical programming model that maximizes the completion ratio of tasks and the expectation satisfaction of users is established. Moreover, a conflict resolution-assisted iterative task-scheduling algorithm (CRITS) is designed, composing of five closely dependent operators: resource matching, service durations generation, conflict evaluation, conflict resolution, and solution update. To verify the effectiveness of the proposed CRITS, extensive experiments are carried out. The experimental results demonstrate the competitive performance of CRITS in addressing the DRSN scheduling problem. In comparison with two heuristic algorithms (heuristic algorithm based on time-freedom degree and a heuristic algorithm based on task priority) and a meta-heuristic algorithm (adaptive variable neighborhood descent combined with a tabu list), the proposed CRITS increases the overall completion ratio of tasks by 6.65, 10.26, and 10.96%, respectively.\"\\n      },\\n      {\\n        \"paperId\": \"b12f40443c189e665520811f99f0ec2b47d90194\",\\n        \"url\": \"https://www.semanticscholar.org/paper/b12f40443c189e665520811f99f0ec2b47d90194\",\\n        \"title\": \"RSS-Based Byzantine Fault-Tolerant Localization Algorithm Under NLOS Environment\",\\n        \"citationCount\": 28,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {\\'tldr\\'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/LCOMM.2020.3027904?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/LCOMM.2020.3027904, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": null,\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2021-02-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"26411201\",\\n            \"name\": \"Xiaojun Mei\"\\n          },\\n          {\\n            \"authorId\": \"35250883\",\\n            \"name\": \"Huafeng Wu\"\\n          },\\n          {\\n            \"authorId\": \"144922818\",\\n            \"name\": \"J. Xian\"\\n          },\\n          {\\n            \"authorId\": \"150041356\",\\n            \"name\": \"Bowen Chen\"\\n          }\\n        ],\\n        \"abstract\": \"Localization is one of the most critical tasks in wireless sensor networks, but achieving a relatively accurate location estimation is challenging when there have Byzantine fault and non-line-of-sight (NLOS) bias simultaneously. In this context, a localization method, based on received signal strength (RSS), is proposed in this letter to mitigate the impact of Byzantine fault and NLOS bias on the localization accuracy of wireless sensor networks. The proposed method relies on a Byzantine fault-tolerant localization algorithm (BFLA), which converts the localization problem into a generalized trust-region subproblem (GTRS) by applying certain approximations. In order to obtain a feasible solution to the GTRS, a block-coordinate update (BCU) function with a regularization term is used to divide the localization problem into two subproblems. An iterative method, whose start-point is obtained by an unconstrained squared-range (USR) algorithm, is then used to obtain a solution. Numerical simulations are carried out to show the effectiveness of the proposed method, compared with the state-of-the-art approaches in different scenarios.\"\\n      },\\n      {\\n        \"paperId\": \"c04e9454a696086ea69f37f59d45347f5c6d438c\",\\n        \"url\": \"https://www.semanticscholar.org/paper/c04e9454a696086ea69f37f59d45347f5c6d438c\",\\n        \"title\": \"Enhancing Transferability of Adversarial Examples with Spatial Momentum\",\\n        \"citationCount\": 10,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2203.13479, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A novel method named Spatial Momentum Iterative FGSM attack (SMI-FGSM), which introduces the mechanism of momentum accumulation from temporal domain to spatial domain by considering the context information from different regions within the image to enhance adversarial transferability.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2022-03-25\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2445322\",\\n            \"name\": \"Guoqiu Wang\"\\n          },\\n          {\\n            \"authorId\": \"27066021\",\\n            \"name\": \"Huanqian Yan\"\\n          },\\n          {\\n            \"authorId\": \"2769710\",\\n            \"name\": \"Xingxing Wei\"\\n          }\\n        ],\\n        \"abstract\": \"Many adversarial attack methods achieve satisfactory attack success rates under the white-box setting, but they usually show poor transferability when attacking other DNN models. Momentum-based attack is one effective method to improve transferability. It integrates the momentum term into the iterative process, which can stabilize the update directions by adding the gradients\\' temporal correlation for each pixel. We argue that only this temporal momentum is not enough, the gradients from the spatial domain within an image, i.e. gradients from the context pixels centered on the target pixel are also important to the stabilization. For that, we propose a novel method named Spatial Momentum Iterative FGSM attack (SMI-FGSM), which introduces the mechanism of momentum accumulation from temporal domain to spatial domain by considering the context information from different regions within the image. SMI-FGSM is then integrated with temporal momentum to simultaneously stabilize the gradients\\' update direction from both the temporal and spatial domains. Extensive experiments show that our method indeed further enhances adversarial transferability. It achieves the best transferability success rate for multiple mainstream undefended and defended models, which outperforms the state-of-the-art attack methods by a large margin of 10\\\\\\\\% on average.\"\\n      },\\n      {\\n        \"paperId\": \"8a1f15ef8875b2f2a80a5ddd52ea41967203cd26\",\\n        \"url\": \"https://www.semanticscholar.org/paper/8a1f15ef8875b2f2a80a5ddd52ea41967203cd26\",\\n        \"title\": \"xERTE: Explainable Reasoning on Temporal Knowledge Graphs for Forecasting Future Links\",\\n        \"citationCount\": 35,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2012.15537, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A future link forecasting framework that reasons over query-relevant subgraphs of temporal KGs and jointly models the graph structures and the temporal context information and conducts a survey to show that the reasoning arguments extracted by the model for link forecasting are aligned with human understanding.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Review\"\\n        ],\\n        \"publicationDate\": \"2020-12-31\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2765914\",\\n            \"name\": \"Zhen Han\"\\n          },\\n          {\\n            \"authorId\": \"2158172225\",\\n            \"name\": \"Peng Chen\"\\n          },\\n          {\\n            \"authorId\": \"10684484\",\\n            \"name\": \"Yunpu Ma\"\\n          },\\n          {\\n            \"authorId\": \"1742501819\",\\n            \"name\": \"Volker Tresp\"\\n          }\\n        ],\\n        \"abstract\": \"Interest has been rising lately towards modeling time-evolving knowledge graphs (KGs). Recently, graph representation learning approaches have become the dominant paradigm for link prediction on temporal KGs. However, the embedding-based approaches largely operate in a black-box fashion, lacking the ability to judge the results\\' reliability. This paper provides a future link forecasting framework that reasons over query-relevant subgraphs of temporal KGs and jointly models the graph structures and the temporal context information. Especially, we propose a temporal relational attention mechanism and a novel reverse representation update scheme to guide the extraction of an enclosing subgraph around the query. The subgraph is expanded by an iterative sampling of temporal neighbors and attention propagation. As a result, our approach provides human-understandable arguments for the prediction. We evaluate our model on four benchmark temporal knowledge graphs for the link forecasting task. While being more explainable, our model also obtains a relative improvement of up to 17.7 $\\\\\\\\%$ on MRR compared to the previous best KG forecasting methods. We also conduct a survey with 53 respondents, and the results show that the reasoning arguments extracted by the model for link forecasting are aligned with human understanding.\"\\n      },\\n      {\\n        \"paperId\": \"51458bf945a446271cf857016582d3fc881f8f9a\",\\n        \"url\": \"https://www.semanticscholar.org/paper/51458bf945a446271cf857016582d3fc881f8f9a\",\\n        \"title\": \"Joint User Grouping, Scheduling, and Precoding for Multicast Energy Efficiency in Multigroup Multicast Systems\",\\n        \"citationCount\": 17,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://doi.org/10.1109/twc.2020.3020081\",\\n          \"status\": \"HYBRID\",\\n          \"license\": \"CCBY\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2005.06843, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper studies the joint design of user grouping, scheduling and precoding to optimize energy efficiency (EE) for multigroup multicast scenarios in single-cell multiuser MISO downlink channels and proposes a convex-concave procedure framework based iterative algorithm for each optimization criteria.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2020-05-14\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"40370059\",\\n            \"name\": \"Ashok Bandi\"\\n          },\\n          {\\n            \"authorId\": \"153604607\",\\n            \"name\": \"Bhavani Shankar\"\\n          },\\n          {\\n            \"authorId\": \"1760292\",\\n            \"name\": \"S. Chatzinotas\"\\n          },\\n          {\\n            \"authorId\": \"102896981\",\\n            \"name\": \"B. Ottersten\"\\n          }\\n        ],\\n        \"abstract\": \"This paper studies the joint design of user grouping, scheduling (or admission control) and precoding to optimize energy efficiency (EE) for multigroup multicast scenarios in single-cell multiuser MISO downlink channels. Noticing that the existing definition of EE fails to account for group sizes, a new metric called multicast energy efficiency (MEE) is proposed. In this context, the joint design is considered for the maximization of MEE, EE, and scheduled users. Firstly, with the help of binary variables (associated with grouping and scheduling) the joint design problem is formulated as a mixed-Boolean fractional programming problem such that it facilitates the joint update of grouping, scheduling and precoding variables. Further, several novel optimization formulations are proposed to reveal the hidden difference of convex/ concave structure in the objective and associated constraints. Thereafter, we propose a convex-concave procedure framework based iterative algorithm for each optimization criteria where grouping, scheduling, and precoding variables are updated jointly in each iteration. Finally, we compare the performance of the three design criteria concerning three performance metrics namely MEE, EE, and scheduled users through Monte-Carlo simulations. These simulations establish the need for MEE and the improvement from the system optimization.\"\\n      },\\n      {\\n        \"paperId\": \"8c8595a050845a8926aa52b22998b2ca609a5d96\",\\n        \"url\": \"https://www.semanticscholar.org/paper/8c8595a050845a8926aa52b22998b2ca609a5d96\",\\n        \"title\": \"Automatic Distortion Rectification of Wide-Angle Images Using Outlier Refinement for Streamlining Vision Tasks\",\\n        \"citationCount\": 16,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://www.mdpi.com/1424-8220/20/3/894/pdf?version=1581513374\",\\n          \"status\": \"GOLD\",\\n          \"license\": \"CCBY\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC7039389, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"The proposed algorithm was able to rectify the distortions of wide-angle and fish-eye cameras even in extreme conditions such as heavy illumination changes and severe lens distortions.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2020-02-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"97740251\",\\n            \"name\": \"Vijay Kakani\"\\n          },\\n          {\\n            \"authorId\": \"2380060\",\\n            \"name\": \"Hakil Kim\"\\n          },\\n          {\\n            \"authorId\": \"2108780171\",\\n            \"name\": \"Jongso Lee\"\\n          },\\n          {\\n            \"authorId\": \"34630628\",\\n            \"name\": \"Choonwoo Ryu\"\\n          },\\n          {\\n            \"authorId\": \"14653761\",\\n            \"name\": \"Mahendar Kumbham\"\\n          }\\n        ],\\n        \"abstract\": \"The study proposes an outlier refinement methodology for automatic distortion rectification of wide-angle and fish-eye lens camera models in the context of streamlining vision-based tasks. The line-members sets are estimated in a scene through accumulation of line candidates emerging from the same edge source. An iterative optimization with an outlier refinement scheme was applied to the loss value, to simultaneously remove the extremely curved outliers from the line-members set and update the robust line members as well as estimating the best-fit distortion parameters with lowest possible loss. The proposed algorithm was able to rectify the distortions of wide-angle and fish-eye cameras even in extreme conditions such as heavy illumination changes and severe lens distortions. Experiments were conducted using various evaluation metrics both at the pixel-level (image quality, edge stretching effects, pixel-point error) as well as higher-level use-cases (object detection, height estimation) with respect to real and synthetic data from publicly available, privately acquired sources. The performance evaluations of the proposed algorithm have been investigated using an ablation study on various datasets in correspondence to the significance analysis of the refinement scheme and loss function. Several quantitative and qualitative comparisons were carried out on the proposed approach against various self-calibration approaches.\"\\n      },\\n      {\\n        \"paperId\": \"07503b6fa32459203e176ce880c2a5ba23f0f8e5\",\\n        \"url\": \"https://www.semanticscholar.org/paper/07503b6fa32459203e176ce880c2a5ba23f0f8e5\",\\n        \"title\": \"Learning to Decode: Reinforcement Learning for Decoding of Sparse Graph-Based Channel Codes\",\\n        \"citationCount\": 11,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2010.05637, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work model the CN update process as a multi-armed bandit process with dependent arms and employ a Q-learning scheme for optimizing the CN scheduling policy to improve decoding performance and reduce the decoding complexity dramatically once the model is learned.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2020-10-12\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"49916135\",\\n            \"name\": \"Salman Habib\"\\n          },\\n          {\\n            \"authorId\": \"93037496\",\\n            \"name\": \"Allison Beemer\"\\n          },\\n          {\\n            \"authorId\": \"2064748249\",\\n            \"name\": \"J. Kliewer\"\\n          }\\n        ],\\n        \"abstract\": \"We show in this work that reinforcement learning can be successfully applied to decoding short to moderate length sparse graph-based channel codes. Specifically, we focus on low-density parity check (LDPC) codes, which for example have been standardized in the context of 5G cellular communication systems due to their excellent error correcting performance. These codes are typically decoded via belief propagation iterative decoding on the corresponding bipartite (Tanner) graph of the code via flooding, i.e., all check and variable nodes in the Tanner graph are updated at once. In contrast, in this paper we utilize a sequential update policy which selects the optimum check node (CN) scheduling in order to improve decoding performance. In particular, we model the CN update process as a multi-armed bandit process with dependent arms and employ a Q-learning scheme for optimizing the CN scheduling policy. In order to reduce the learning complexity, we propose a novel graph-induced CN clustering approach to partition the state space in such a way that dependencies between clusters are minimized. Our results show that compared to other decoding approaches from the literature, the proposed reinforcement learning scheme not only significantly improves the decoding performance, but also reduces the decoding complexity dramatically once the model is learned.\"\\n      },\\n      {\\n        \"paperId\": \"9aa6a885754a27fe42a87e4dfaed87d618fd8518\",\\n        \"url\": \"https://www.semanticscholar.org/paper/9aa6a885754a27fe42a87e4dfaed87d618fd8518\",\\n        \"title\": \"Iterative Refinement of Project-Level Code Context for Precise Code Generation with Compiler Feedback\",\\n        \"citationCount\": 37,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2403.16792, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"CoGen is presented, a new code generation approach that uses compiler feedback to improve the LLM-generated code and consistently outperforms the existing retrieval-based code generation baselines.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-03-25\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2170260327\",\\n            \"name\": \"Zhangqian Bi\"\\n          },\\n          {\\n            \"authorId\": \"2273871682\",\\n            \"name\": \"Yao Wan\"\\n          },\\n          {\\n            \"authorId\": \"2293395709\",\\n            \"name\": \"Zheng Wang\"\\n          },\\n          {\\n            \"authorId\": \"2273385018\",\\n            \"name\": \"Hongyu Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2293312291\",\\n            \"name\": \"Batu Guan\"\\n          },\\n          {\\n            \"authorId\": \"2293769071\",\\n            \"name\": \"Fangxin Lu\"\\n          },\\n          {\\n            \"authorId\": \"2293399901\",\\n            \"name\": \"Zili Zhang\"\\n          },\\n          {\\n            \"authorId\": \"34296085\",\\n            \"name\": \"Yulei Sui\"\\n          },\\n          {\\n            \"authorId\": \"1678835\",\\n            \"name\": \"Xuanhua Shi\"\\n          },\\n          {\\n            \"authorId\": \"2277587874\",\\n            \"name\": \"Hai Jin\"\\n          }\\n        ],\\n        \"abstract\": \"Large Language Models (LLMs) have shown remarkable progress in automated code generation. Yet, LLM-generated code may contain errors in API usage, class, data structure, or missing project-specific information. As much of this project-specific context cannot fit into the prompts of LLMs, we must find ways to allow the model to explore the project-level code context. We present CoCoGen, a new code generation approach that uses compiler feedback to improve the LLM-generated code. CoCoGen first leverages static analysis to identify mismatches between the generated code and the project\\'s context. It then iteratively aligns and fixes the identified errors using information extracted from the code repository. We integrate CoCoGen with two representative LLMs, i.e., GPT-3.5-Turbo and Code Llama (13B), and apply it to Python code generation. Experimental results show that CoCoGen significantly improves the vanilla LLMs by over 80% in generating code dependent on the project context and consistently outperforms the existing retrieval-based code generation baselines.\"\\n      },\\n      {\\n        \"paperId\": \"3e43429b24beb5b93513775a259c23ce3a133f67\",\\n        \"url\": \"https://www.semanticscholar.org/paper/3e43429b24beb5b93513775a259c23ce3a133f67\",\\n        \"title\": \"Addressing Background Context Bias in Few-Shot Segmentation Through Iterative Modulation\",\\n        \"citationCount\": 23,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {\\'tldr\\', \\'abstract\\'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/CVPR52733.2024.00324?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/CVPR52733.2024.00324, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": null,\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-06-16\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"151101398\",\\n            \"name\": \"Lanyun Zhu\"\\n          },\\n          {\\n            \"authorId\": \"2261909262\",\\n            \"name\": \"Tianrun Chen\"\\n          },\\n          {\\n            \"authorId\": \"2510538\",\\n            \"name\": \"Jianxiong Yin\"\\n          },\\n          {\\n            \"authorId\": \"2250849553\",\\n            \"name\": \"Simon See\"\\n          },\\n          {\\n            \"authorId\": \"2321778822\",\\n            \"name\": \"Jun Liu\"\\n          }\\n        ],\\n        \"abstract\": null\\n      },\\n      {\\n        \"paperId\": \"8745157f991013b23fbb79d300ba560f9005c8d4\",\\n        \"url\": \"https://www.semanticscholar.org/paper/8745157f991013b23fbb79d300ba560f9005c8d4\",\\n        \"title\": \"Energy-Based Cross Attention for Bayesian Context Update in Text-to-Image Diffusion Models\",\\n        \"citationCount\": 29,\\n        \"openAccessPdf\": {\\n          \"url\": \"http://arxiv.org/pdf/2306.09869\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2306.09869, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A novel energy-based model (EBM) framework is presented that first formulate EBMs of latent image representations and text embeddings in each cross-attention layer of the denoising autoencoder, thereby implicitly minimizing a nested hierarchy of energy functions.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2023-06-16\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"153118937\",\\n            \"name\": \"Geon Yeong Park\"\\n          },\\n          {\\n            \"authorId\": \"2109216792\",\\n            \"name\": \"Jeongsol Kim\"\\n          },\\n          {\\n            \"authorId\": \"3332270\",\\n            \"name\": \"Beomsu Kim\"\\n          },\\n          {\\n            \"authorId\": \"2152577026\",\\n            \"name\": \"Sang Wan Lee\"\\n          },\\n          {\\n            \"authorId\": \"30547794\",\\n            \"name\": \"Jong-Chul Ye\"\\n          }\\n        ],\\n        \"abstract\": \"Despite the remarkable performance of text-to-image diffusion models in image generation tasks, recent studies have raised the issue that generated images sometimes cannot capture the intended semantic contents of the text prompts, which phenomenon is often called semantic misalignment. To address this, here we present a novel energy-based model (EBM) framework. Specifically, we first formulate EBMs of latent image representations and text embeddings in each cross-attention layer of the denoising autoencoder. Then, we obtain the gradient of the log posterior of context vectors, which can be updated and transferred to the subsequent cross-attention layer, thereby implicitly minimizing a nested hierarchy of energy functions. Our latent EBMs further allow zero-shot compositional generation as a linear combination of cross-attention outputs from different contexts. Using extensive experiments, we demonstrate that the proposed method is highly effective in handling various image generation tasks, including multi-concept generation, text-guided image inpainting, and real and synthetic image editing.\"\\n      },\\n      {\\n        \"paperId\": \"b217b6bc340af9a10bebbf8acc36ea30871769bd\",\\n        \"url\": \"https://www.semanticscholar.org/paper/b217b6bc340af9a10bebbf8acc36ea30871769bd\",\\n        \"title\": \"In-Context Learning with Iterative Demonstration Selection\",\\n        \"citationCount\": 69,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2310.09881, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"Iterative Demonstration Selection (IDS) iteratively selects examples that are diverse but still strongly correlated with the test sample as ICL demonstrations, and can consistently outperform existing ICL demonstration selection methods.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2023-10-15\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2084609980\",\\n            \"name\": \"Chengwei Qin\"\\n          },\\n          {\\n            \"authorId\": \"2258754564\",\\n            \"name\": \"Aston Zhang\"\\n          },\\n          {\\n            \"authorId\": \"1627060158\",\\n            \"name\": \"Anirudh Dagar\"\\n          },\\n          {\\n            \"authorId\": \"2258719488\",\\n            \"name\": \"Wenming Ye\"\\n          }\\n        ],\\n        \"abstract\": \"Spurred by advancements in scale, large language models (LLMs) have demonstrated strong few-shot learning ability via in-context learning (ICL). However, the performance of ICL has been shown to be highly sensitive to the selection of few-shot demonstrations. Selecting the most suitable examples as context remains an ongoing challenge and an open problem. Existing literature has highlighted the importance of selecting examples that are diverse or semantically similar to the test sample while ignoring the fact that the optimal selection dimension, i.e., diversity or similarity, is task-specific. Based on how the test sample is answered, we propose Iterative Demonstration Selection (IDS) to leverage the merits of both dimensions. Using zero-shot chain-of-thought reasoning (Zero-shot-CoT), IDS iteratively selects examples that are diverse but still strongly correlated with the test sample as ICL demonstrations. Specifically, IDS applies Zero-shot-CoT to the test sample before demonstration selection. The output reasoning path is then used to choose demonstrations that are prepended to the test sample for inference. The generated answer is followed by its corresponding reasoning path for extracting a new set of demonstrations in the next iteration. After several iterations, IDS adopts majority voting to obtain the final result. Through extensive experiments on tasks including reasoning, question answering, and topic classification, we demonstrate that IDS can consistently outperform existing ICL demonstration selection methods.\"\\n      },\\n      {\\n        \"paperId\": \"43ffae1c1c6a7371dfe89ac84ae45f3456fdc5a0\",\\n        \"url\": \"https://www.semanticscholar.org/paper/43ffae1c1c6a7371dfe89ac84ae45f3456fdc5a0\",\\n        \"title\": \"Iterative Forward Tuning Boosts In-context Learning in Language Models\",\\n        \"citationCount\": 19,\\n        \"openAccessPdf\": {\\n          \"url\": \"http://arxiv.org/pdf/2305.13016\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2305.13016, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A novel two-stage framework to boost ICL in LLMs is introduced, which delineates the ICL process into two distinct stages: Deep-Thinking and test stages, and incorporates a unique attention mechanism, i.e., iterative enhanced attention, which enables multiple rounds of information accumulation.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2023-05-22\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2135964855\",\\n            \"name\": \"Jiaxi Yang\"\\n          },\\n          {\\n            \"authorId\": \"151471590\",\\n            \"name\": \"Binyuan Hui\"\\n          },\\n          {\\n            \"authorId\": \"2144399900\",\\n            \"name\": \"Min Yang\"\\n          },\\n          {\\n            \"authorId\": \"66200440\",\\n            \"name\": \"Binhua Li\"\\n          },\\n          {\\n            \"authorId\": \"2087380523\",\\n            \"name\": \"Fei Huang\"\\n          },\\n          {\\n            \"authorId\": \"1527090216\",\\n            \"name\": \"Yongbin Li\"\\n          }\\n        ],\\n        \"abstract\": \"Despite the advancements in in-context learning (ICL) for large language models (LLMs), current research centers on specific prompt engineering, such as demonstration selection, with the expectation that a single iteration of demonstrations processing can generalize effectively to a given test sample. However, this perspective overlooks the potential benefits derived from multiple iterations involving demonstrations, a practice aligning more closely with the iterative decision-making process exhibited by humans, who often learn through analogy. In this study, we introduce a novel two-stage framework to boost ICL in LLMs. Specifically, our framework delineates the ICL process into two distinct stages: Deep-Thinking and test stages. The Deep-Thinking stage incorporates a unique attention mechanism, i.e., iterative enhanced attention, which enables multiple rounds of information accumulation. This mechanism operates by manipulating the Key-Value matrices without training, fostering enhanced understanding capabilities in LLMs by thinking demonstrations multiple times. We evaluated Deep-Thinking across a range of benchmarks and LLMs, showing its superior performance over vanilla ICL methods and its effectiveness in challenging tasks where demonstration selection is infeasible.\"\\n      },\\n      {\\n        \"paperId\": \"fa4ea8b773ffd6706ba5cf427f9671f81b04dcdd\",\\n        \"url\": \"https://www.semanticscholar.org/paper/fa4ea8b773ffd6706ba5cf427f9671f81b04dcdd\",\\n        \"title\": \"ALLURE: Auditing and Improving LLM-based Evaluation of Text using Iterative In-Context-Learning\",\\n        \"citationCount\": 17,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://arxiv.org/pdf/2309.13701\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2309.13701, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"Allure, a systematic approach to Auditing Large Language Models Understanding and Reasoning Errors, involves comparing LLM-generated evaluations with annotated data, and iteratively incorporating instances of significant deviation into the evaluator, which leverages in-context learning (ICL) to enhance and improve robust evaluation of text by LLMs.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2023-09-24\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2245381886\",\\n            \"name\": \"Hosein Hasanbeig\"\\n          },\\n          {\\n            \"authorId\": \"20013278\",\\n            \"name\": \"Hiteshi Sharma\"\\n          },\\n          {\\n            \"authorId\": \"15449757\",\\n            \"name\": \"Leo Betthauser\"\\n          },\\n          {\\n            \"authorId\": \"1844283112\",\\n            \"name\": \"F. Frujeri\"\\n          },\\n          {\\n            \"authorId\": \"2248289111\",\\n            \"name\": \"Ida Momennejad\"\\n          }\\n        ],\\n        \"abstract\": \"From grading papers to summarizing medical documents, large language models (LLMs) are evermore used for evaluation of text generated by humans and AI alike. However, despite their extensive utility, LLMs exhibit distinct failure modes, necessitating a thorough audit and improvement of their text evaluation capabilities. Here we introduce ALLURE, a systematic approach to Auditing Large Language Models Understanding and Reasoning Errors. ALLURE involves comparing LLM-generated evaluations with annotated data, and iteratively incorporating instances of significant deviation into the evaluator, which leverages in-context learning (ICL) to enhance and improve robust evaluation of text by LLMs. Through this iterative process, we refine the performance of the evaluator LLM, ultimately reducing reliance on human annotators in the evaluation process. We anticipate ALLURE to serve diverse applications of LLMs in various domains related to evaluation of textual data, such as medical summarization, education, and and productivity.\"\\n      },\\n      {\\n        \"paperId\": \"860bafe8a3aa0e3a981951f0757996272276b54e\",\\n        \"url\": \"https://www.semanticscholar.org/paper/860bafe8a3aa0e3a981951f0757996272276b54e\",\\n        \"title\": \"ALLURE: A Systematic Protocol for Auditing and Improving LLM-based Evaluation of Text using Iterative In-Context-Learning\",\\n        \"citationCount\": 12,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"AllURE, a systematic approach to Auditing Large Language Models Understanding and Reasoning Errors, involves comparing LLM-generated evaluations with annotated data, and iteratively incorporating instances of significant deviation into the evaluator, which leverages in-context learning (ICL) to enhance and improve robust evaluation of text by LLMs.\"\\n        },\\n        \"publicationTypes\": null,\\n        \"publicationDate\": null,\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2245381886\",\\n            \"name\": \"Hosein Hasanbeig\"\\n          },\\n          {\\n            \"authorId\": \"2258705787\",\\n            \"name\": \"Microsoft Usa HITESHI SHARMA\"\\n          },\\n          {\\n            \"authorId\": \"2258705783\",\\n            \"name\": \"Microsoft Usa LEO BETTHAUSER\"\\n          },\\n          {\\n            \"authorId\": \"2258705381\",\\n            \"name\": \"Microsoft Usa FELIPE VIEIRA FRUJERI\"\\n          },\\n          {\\n            \"authorId\": \"2248289111\",\\n            \"name\": \"Ida Momennejad\"\\n          }\\n        ],\\n        \"abstract\": null\\n      },\\n      {\\n        \"paperId\": \"3f27df9f8b30a8c35fa3e7df7fdf7ec1a5cc28cd\",\\n        \"url\": \"https://www.semanticscholar.org/paper/3f27df9f8b30a8c35fa3e7df7fdf7ec1a5cc28cd\",\\n        \"title\": \"Enhancing Iterative Learning Control With Fractional Power Update Law\",\\n        \"citationCount\": 14,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {\\'tldr\\'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/JAS.2023.123525?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/JAS.2023.123525, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": null,\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2023-05-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2118272992\",\\n            \"name\": \"Zihan Li\"\\n          },\\n          {\\n            \"authorId\": \"144986261\",\\n            \"name\": \"D. Shen\"\\n          },\\n          {\\n            \"authorId\": \"2156506994\",\\n            \"name\": \"Xinghuo Yu\"\\n          }\\n        ],\\n        \"abstract\": \"The P-type update law has been the mainstream technique used in iterative learning control (ILC) systems, which resembles linear feedback control with asymptotical convergence. In recent years, finite-time control strategies such as terminal sliding mode control have been shown to be effective in ramping up convergence speed by introducing fractional power with feedback. In this paper, we show that such mechanism can equally ramp up the learning speed in ILC systems. We first propose a fractional power update rule for ILC of single-input-single-output linear systems. A nonlinear error dynamics is constructed along the iteration axis to illustrate the evolutionary converging process. Using the nonlinear mapping approach, fast convergence towards the limit cycles of tracking errors inherently existing in ILC systems is proven. The limit cycles are shown to be tunable to determine the steady states. Numerical simulations are provided to verify the theoretical results.\"\\n      },\\n      {\\n        \"paperId\": \"38333f6e8f0388968edc4b2ea7a683ce69677e69\",\\n        \"url\": \"https://www.semanticscholar.org/paper/38333f6e8f0388968edc4b2ea7a683ce69677e69\",\\n        \"title\": \"Monte Carlo Tree Search Boosts Reasoning via Iterative Preference Learning\",\\n        \"citationCount\": 182,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2405.00451, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work leverages Monte Carlo Tree Search (MCTS) to iteratively collect preference data, utilizing its look-ahead ability to break down instance-level rewards into more granular step-level signals, to enhance consistency in intermediate steps.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-05-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2299483768\",\\n            \"name\": \"Yuxi Xie\"\\n          },\\n          {\\n            \"authorId\": \"1996705\",\\n            \"name\": \"Anirudh Goyal\"\\n          },\\n          {\\n            \"authorId\": \"2289857220\",\\n            \"name\": \"Wenyue Zheng\"\\n          },\\n          {\\n            \"authorId\": \"2257033898\",\\n            \"name\": \"Min-Yen Kan\"\\n          },\\n          {\\n            \"authorId\": \"2542999\",\\n            \"name\": \"T. Lillicrap\"\\n          },\\n          {\\n            \"authorId\": \"2266466003\",\\n            \"name\": \"Kenji Kawaguchi\"\\n          },\\n          {\\n            \"authorId\": \"2289844602\",\\n            \"name\": \"Michael Shieh\"\\n          }\\n        ],\\n        \"abstract\": \"We introduce an approach aimed at enhancing the reasoning capabilities of Large Language Models (LLMs) through an iterative preference learning process inspired by the successful strategy employed by AlphaZero. Our work leverages Monte Carlo Tree Search (MCTS) to iteratively collect preference data, utilizing its look-ahead ability to break down instance-level rewards into more granular step-level signals. To enhance consistency in intermediate steps, we combine outcome validation and stepwise self-evaluation, continually updating the quality assessment of newly generated data. The proposed algorithm employs Direct Preference Optimization (DPO) to update the LLM policy using this newly generated step-level preference data. Theoretical analysis reveals the importance of using on-policy sampled data for successful self-improving. Extensive evaluations on various arithmetic and commonsense reasoning tasks demonstrate remarkable performance improvements over existing models. For instance, our approach outperforms the Mistral-7B Supervised Fine-Tuning (SFT) baseline on GSM8K, MATH, and ARC-C, with substantial increases in accuracy to $81.8\\\\\\\\%$ (+$5.9\\\\\\\\%$), $34.7\\\\\\\\%$ (+$5.8\\\\\\\\%$), and $76.4\\\\\\\\%$ (+$15.8\\\\\\\\%$), respectively. Additionally, our research delves into the training and inference compute tradeoff, providing insights into how our method effectively maximizes performance gains. Our code is publicly available at https://github.com/YuxiXie/MCTS-DPO.\"\\n      },\\n      {\\n        \"paperId\": \"a1675f47125aa409525c5f759b5e6bcc1c8831aa\",\\n        \"url\": \"https://www.semanticscholar.org/paper/a1675f47125aa409525c5f759b5e6bcc1c8831aa\",\\n        \"title\": \"Enhancing Retrieval-Augmented Large Language Models with Iterative Retrieval-Generation Synergy\",\\n        \"citationCount\": 360,\\n        \"openAccessPdf\": {\\n          \"url\": \"http://arxiv.org/pdf/2305.15294\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2305.15294, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper shows that strong performance can be achieved by a method, called Iter-RetGen, which synergizes retrieval and generation in an iterative manner, and shows that it can flexibly leverage parametric knowledge and non-parametric knowledge, and is superior to or competitive with state-of-the-art retrieval-augmented baselines while causing fewer overheads of retrieval andgeneration.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2023-05-24\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"144485528\",\\n            \"name\": \"Zhihong Shao\"\\n          },\\n          {\\n            \"authorId\": \"2171182\",\\n            \"name\": \"Yeyun Gong\"\\n          },\\n          {\\n            \"authorId\": \"1752875\",\\n            \"name\": \"Yelong Shen\"\\n          },\\n          {\\n            \"authorId\": \"1730108\",\\n            \"name\": \"Minlie Huang\"\\n          },\\n          {\\n            \"authorId\": \"46429989\",\\n            \"name\": \"Nan Duan\"\\n          },\\n          {\\n            \"authorId\": \"2109136147\",\\n            \"name\": \"Weizhu Chen\"\\n          }\\n        ],\\n        \"abstract\": \"Large language models are powerful text processors and reasoners, but are still subject to limitations including outdated knowledge and hallucinations, which necessitates connecting them to the world. Retrieval-augmented large language models have raised extensive attention for grounding model generation on external knowledge. However, retrievers struggle to capture relevance, especially for queries with complex information needs. Recent work has proposed to improve relevance modeling by having large language models actively involved in retrieval, i.e., to improve retrieval with generation. In this paper, we show that strong performance can be achieved by a method we call Iter-RetGen, which synergizes retrieval and generation in an iterative manner. A model output shows what might be needed to finish a task, and thus provides an informative context for retrieving more relevant knowledge which in turn helps generate a better output in the next iteration. Compared with recent work which interleaves retrieval with generation when producing an output, Iter-RetGen processes all retrieved knowledge as a whole and largely preserves the flexibility in generation without structural constraints. We evaluate Iter-RetGen on multi-hop question answering, fact verification, and commonsense reasoning, and show that it can flexibly leverage parametric knowledge and non-parametric knowledge, and is superior to or competitive with state-of-the-art retrieval-augmented baselines while causing fewer overheads of retrieval and generation. We can further improve performance via generation-augmented retrieval adaptation.\"\\n      },\\n      {\\n        \"paperId\": \"af5c7848417882012203ac21399977ebda695a2b\",\\n        \"url\": \"https://www.semanticscholar.org/paper/af5c7848417882012203ac21399977ebda695a2b\",\\n        \"title\": \"RepoCoder: Repository-Level Code Completion Through Iterative Retrieval and Generation\",\\n        \"citationCount\": 325,\\n        \"openAccessPdf\": {\\n          \"url\": \"http://arxiv.org/pdf/2303.12570\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2303.12570, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"RepoCoder streamlines the repository-level code completion process by incorporating a similarity-based retriever and a pre-trained code language model in an iterative retrieval-generation pipeline and has the ability to generate code at various levels of granularity.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2023-03-22\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2158120018\",\\n            \"name\": \"Fengji Zhang\"\\n          },\\n          {\\n            \"authorId\": \"143876723\",\\n            \"name\": \"B. Chen\"\\n          },\\n          {\\n            \"authorId\": \"2211964951\",\\n            \"name\": \"Yue Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2155352529\",\\n            \"name\": \"Jin Liu\"\\n          },\\n          {\\n            \"authorId\": \"2134434187\",\\n            \"name\": \"Daoguang Zan\"\\n          },\\n          {\\n            \"authorId\": \"145469202\",\\n            \"name\": \"Yi Mao\"\\n          },\\n          {\\n            \"authorId\": \"153249455\",\\n            \"name\": \"Jian-Guang Lou\"\\n          },\\n          {\\n            \"authorId\": \"2109136147\",\\n            \"name\": \"Weizhu Chen\"\\n          }\\n        ],\\n        \"abstract\": \"The task of repository-level code completion is to continue writing the unfinished code based on a broader context of the repository. While for automated code completion tools, it is difficult to utilize the useful information scattered in different files. We propose RepoCoder, a simple, generic, and effective framework to address the challenge. It streamlines the repository-level code completion process by incorporating a similarity-based retriever and a pre-trained code language model in an iterative retrieval-generation pipeline. RepoCoder makes effective utilization of repository-level information for code completion and has the ability to generate code at various levels of granularity. Moreover, we propose a new benchmark RepoEval, which consists of the latest and high-quality real-world repositories covering line, API invocation, and function body completion scenarios. Experimental results indicate that RepoCoder significantly improves the In-File completion baseline by over 10% in all settings and consistently outperforms the vanilla retrieval-augmented code completion approach. Furthermore, we validate the effectiveness of RepoCoder through comprehensive analysis, providing valuable insights for future research. Our source code and benchmark are publicly available: https://github.com/microsoft/CodeT/tree/main/RepoCoder\"\\n      },\\n      {\\n        \"paperId\": \"cc26cab4d4e0c6e3dddbae96abd65ee2d1f87f69\",\\n        \"url\": \"https://www.semanticscholar.org/paper/cc26cab4d4e0c6e3dddbae96abd65ee2d1f87f69\",\\n        \"title\": \"An iterative method to compute conformal mappings and their inverses in the context of water waves over topographies\",\\n        \"citationCount\": 13,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://arxiv.org/pdf/2103.02022\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2103.02022, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"An iterative numerical method to compute the conformal mapping in the context of propagating water waves over uneven topographies using the MATLAB Schwarz-Christoffel toolbox mapping as a benchmark is investigated.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2021-03-02\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"104491233\",\\n            \"name\": \"M. Flamarion\"\\n          },\\n          {\\n            \"authorId\": \"1429030643\",\\n            \"name\": \"R. Ribeiro-Jr\"\\n          }\\n        ],\\n        \"abstract\": \"An iterative numerical method to compute the conformal mapping in the context of propagating water waves over uneven topographies is investigated. The map flattens the fluid domain onto a canonical strip in which computations are performed. The accuracy of the method is tested by using the MATLAB Schwarz\\\\u2013Christoffel toolbox mapping as a benchmark. Besides, we give a numerical alternative to compute the inverse of the conformal map.\"\\n      },\\n      {\\n        \"paperId\": \"edf23ce5689d0acf18cc7ff8556c696f9231644d\",\\n        \"url\": \"https://www.semanticscholar.org/paper/edf23ce5689d0acf18cc7ff8556c696f9231644d\",\\n        \"title\": \"Five-Precision GMRES-Based Iterative Refinement\",\\n        \"citationCount\": 36,\\n        \"openAccessPdf\": {\\n          \"url\": \"http://eprints.maths.manchester.ac.uk/2807/1/paper.pdf\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1137/23m1549079?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1137/23m1549079, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"GMRES-IR5 has the potential to solve relatively badly conditioned problems in less time and memory than GMRES-IR3, thanks to the use of lower precision arithmetic in the GMRES iterations.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-02-08\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"38318287\",\\n            \"name\": \"P. Amestoy\"\\n          },\\n          {\\n            \"authorId\": \"1760511\",\\n            \"name\": \"A. Buttari\"\\n          },\\n          {\\n            \"authorId\": \"1699285\",\\n            \"name\": \"N. Higham\"\\n          },\\n          {\\n            \"authorId\": \"1398545502\",\\n            \"name\": \"J. L\\\\u2019Excellent\"\\n          },\\n          {\\n            \"authorId\": \"144615299\",\\n            \"name\": \"Th\\\\u00e9o Mary\"\\n          },\\n          {\\n            \"authorId\": \"1720746305\",\\n            \"name\": \"Bastien Vieubl\\\\u00e9\"\\n          }\\n        ],\\n        \"abstract\": \". GMRES-based iterative re\\\\ufb01nement in three precisions (GMRES-IR3) uses a low pre- cision LU factorization to accelerate the solution of a linear system without compromising numerical stability or robustness. GMRES-IR3 solves the update equation using GMRES preconditioned by the LU factors, where all operations within GMRES are carried out in the working precision u , except for the matrix-vector products and the application of the preconditioner, which require the use of extra precision u 2 . The use of extra precision can be expensive, and is especially unattractive if it is not available in hardware; for this reason, existing implementations have not used extra precision, despite the absence of an error analysis for this approach. We relax the requirements on the precisions used within GMRES, allowing the use of arbitrary precisions u p (for applying the preconditioner) and u g (for the rest of the operations). We obtain the \\\\ufb01ve-precision GMRES-based iterative re\\\\ufb01nement (GMRES-IR5) algorithm. We carry out a rounding error analysis that generalizes that of GMRES-IR3, obtaining conditions under which the forward and backward errors converge to their limiting values. Our analysis makes use of a new result on the backward stability of MGS-GMRES in two precisions. On hardware where up to \\\\ufb01ve arithmetics are available, the number of possible combinations of precisions in GMRES-IR5 is extremely large, but our analysis identi\\\\ufb01es a small subset of relevant combinations. By choosing from within this subset one can achieve di\\\\ufb00erent levels of tradeo\\\\ufb00 between cost and robustness, which allows for a \\\\ufb01ner choice of precisions depending on the problem di\\\\ufb03culty and the available hardware. Our numerical experiments on both random dense matrices and real-life sparse matrices from a wide range of applications show that the practical behavior of GMRES-IR5 is in good agreement with our theoretical analysis. GMRES-IR5 therefore has the potential to solve relatively badly conditioned problems in less time and memory than GMRES-IR3, thanks to the use of lower precision arithmetic in the GMRES iterations.\"\\n      },\\n      {\\n        \"paperId\": \"dc256e179d4e8eff48879a40ddc414b15b0b2300\",\\n        \"url\": \"https://www.semanticscholar.org/paper/dc256e179d4e8eff48879a40ddc414b15b0b2300\",\\n        \"title\": \"RAT: Retrieval Augmented Thoughts Elicit Context-Aware Reasoning in Long-Horizon Generation\",\\n        \"citationCount\": 69,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2403.05313, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work explores how iterative revising a chain of thoughts with the help of information retrieval significantly improves large language models\\' reasoning and generation ability in long-horizon generation tasks, while hugely mitigating hallucination.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-03-08\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"47196237\",\\n            \"name\": \"Zihao Wang\"\\n          },\\n          {\\n            \"authorId\": \"70097297\",\\n            \"name\": \"Anji Liu\"\\n          },\\n          {\\n            \"authorId\": \"2257447835\",\\n            \"name\": \"Haowei Lin\"\\n          },\\n          {\\n            \"authorId\": \"2290567624\",\\n            \"name\": \"Jiaqi Li\"\\n          },\\n          {\\n            \"authorId\": \"2257636629\",\\n            \"name\": \"Xiaojian Ma\"\\n          },\\n          {\\n            \"authorId\": \"2257367774\",\\n            \"name\": \"Yitao Liang\"\\n          }\\n        ],\\n        \"abstract\": \"We explore how iterative revising a chain of thoughts with the help of information retrieval significantly improves large language models\\' reasoning and generation ability in long-horizon generation tasks, while hugely mitigating hallucination. In particular, the proposed method -- *retrieval-augmented thoughts* (RAT) -- revises each thought step one by one with retrieved information relevant to the task query, the current and the past thought steps, after the initial zero-shot CoT is generated. Applying RAT to GPT-3.5, GPT-4, and CodeLLaMA-7b substantially improves their performances on various long-horizon generation tasks; on average of relatively increasing rating scores by 13.63% on code generation, 16.96% on mathematical reasoning, 19.2% on creative writing, and 42.78% on embodied task planning. The demo page can be found at https://craftjarvis.github.io/RAT\"\\n      },\\n      {\\n        \"paperId\": \"ca5cfff2fcc87e1fb88c74b6eca3f7ee2c638bde\",\\n        \"url\": \"https://www.semanticscholar.org/paper/ca5cfff2fcc87e1fb88c74b6eca3f7ee2c638bde\",\\n        \"title\": \"Learned Two-Step Iterative Shrinkage Thresholding Algorithm for Deep Compressive Sensing\",\\n        \"citationCount\": 31,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {\\'tldr\\'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/TCSVT.2023.3325340?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/TCSVT.2023.3325340, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": null,\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-05-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"35361812\",\\n            \"name\": \"Hongping Gan\"\\n          },\\n          {\\n            \"authorId\": \"2260282205\",\\n            \"name\": \"Xiaoyang Wang\"\\n          },\\n          {\\n            \"authorId\": \"2260279661\",\\n            \"name\": \"Lijun He\"\\n          },\\n          {\\n            \"authorId\": \"2260264613\",\\n            \"name\": \"Jie Liu\"\\n          }\\n        ],\\n        \"abstract\": \"Deep unrolling architectures have revitalized compressive sensing (CS) by seamlessly blending deep neural networks with traditional optimization-based reconstruction algorithms. In pursuit of an efficient and deep interpretable approach, we propose LTwIST for CS problem, a novel deep unrolling framework that draws inspiration from the well-known two-step iterative shrinkage thresholding (TwIST) algorithm. LTwIST uses a trainable sensing matrix to adaptively learn structural information in images, and introduces a customized U-block architecture to solve the proximal mapping of nonlinear transformations connected with the sparsity-inducing regularizer. Specifically, each iteration recovery step of LTwIST corresponds to an iterative update step of the traditional TwIST algorithm. Moreover, the proposed method is designed to learn all the parameters end-to-end without manual tuning such as shrinkable thresholds, step sizes, etc. As a result, LTwIST obviates the need for manual parameter optimization, allows for high-quality image recovery and provides unambiguous interpretability. Moreover, our proposed LTwIST is also applicable to CS-based magnetic resonance imaging and exhibits a strong reconstruction performance. Extensive experiments on several public benchmark datasets demonstrate that the proposed LTwIST outperforms existing state-of-the-art deep CS methods by considerable margins in terms of quality evaluation metrics and visual performance. Our code is available on LTwIST.\"\\n      },\\n      {\\n        \"paperId\": \"53dcf71b6be11d543f1bc7753e8ff8e9945eff77\",\\n        \"url\": \"https://www.semanticscholar.org/paper/53dcf71b6be11d543f1bc7753e8ff8e9945eff77\",\\n        \"title\": \"Iterative and Scenario-based Requirements Specification in a System of Systems Context\",\\n        \"citationCount\": 10,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2102.05400, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper describes an extension for the scenario-modeling language for Kotlin (SMLK) to continuously and formally model requirements on SoS and CS level and combines SMLK with agile development techniques to support the iterative requirements specification and modeling.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2021-02-10\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"1423730807\",\\n            \"name\": \"Carsten Wiecher\"\\n          },\\n          {\\n            \"authorId\": \"2909345\",\\n            \"name\": \"Joel Greenyer\"\\n          },\\n          {\\n            \"authorId\": \"40500399\",\\n            \"name\": \"Carsten Wolff\"\\n          },\\n          {\\n            \"authorId\": \"152617022\",\\n            \"name\": \"H. Anacker\"\\n          },\\n          {\\n            \"authorId\": \"3261846\",\\n            \"name\": \"R. Dumitrescu\"\\n          }\\n        ],\\n        \"abstract\": \"[Context&Motivation]Due to the managerial ,operational and evolutionary independence of constituent systems (CSs) in a System of Systems (SoS) context, top-down and linear requirements engineering (RE) approaches are insufficient. RE techniques for SoS must support iterating, changing, synchronizing, and communicating requirements across different abstraction and hierarchy levels as well as scopes of responsibility. [Question/Problem] We address the challenge of SoS requirements specification, where requirements can describe the SoS behavior, but also the behavior of CSs that are developed independently. [Principal Ideas] To support the requirements specification in an SoS environment, we propose a scenario-based and iterative specification technique. This allows requirements engineers to continuously model and jointly execute and test the system behavior for the SoS and the CS in order to detect contradictions in the requirement specifications at an early stage. [Contribution] In this paper, we describe an extension for the scenario-modeling language for Kotlin (SMLK) to continuously and formally model requirements on SoS and CS level. To support the iterative requirements specification and modeling we combine SMLK with agile development techniques. We demonstrate the applicability of our approach with the help of an example from the field of e-mobility.\"\\n      },\\n      {\\n        \"paperId\": \"f6ca551535a92b669ebd97b246ac582e73af30ee\",\\n        \"url\": \"https://www.semanticscholar.org/paper/f6ca551535a92b669ebd97b246ac582e73af30ee\",\\n        \"title\": \"An attempt to analyse Iterative Data Snooping and L1-norm based on Monte Carlo simulation in the context of leveling networks\",\\n        \"citationCount\": 11,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://riunet.upv.es/bitstream/10251/193468/3/KleinSuracide%20-%20An%20attempt%20to%20analyse%20Iterative%20Data%20Snooping%20and%20L1-norm%20based%20on%20Monte%20Carlo%20si....pdf\",\\n          \"status\": \"GREEN\",\\n          \"license\": \"CCBYNC\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1080/00396265.2021.1878338?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1080/00396265.2021.1878338, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"Evaluating the outlier identification performance of iterative Data Snooping (IDS) and L1-norm in levelling networks by considering the redundancy of the network, number and size of the outliers shows that L 1-norm performs better than IDS for the case of networks with low redundancy.\"\\n        },\\n        \"publicationTypes\": null,\\n        \"publicationDate\": \"2021-02-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"31183250\",\\n            \"name\": \"I. Klein\"\\n          },\\n          {\\n            \"authorId\": \"1834987\",\\n            \"name\": \"S. Suraci\"\\n          },\\n          {\\n            \"authorId\": \"2125043152\",\\n            \"name\": \"Leonardo Castro de Oliveira\"\\n          },\\n          {\\n            \"authorId\": \"72414897\",\\n            \"name\": \"V. F. Rofatto\"\\n          },\\n          {\\n            \"authorId\": \"2059636056\",\\n            \"name\": \"M. T. Matsuoka\"\\n          },\\n          {\\n            \"authorId\": \"1691074\",\\n            \"name\": \"S. Baselga\"\\n          }\\n        ],\\n        \"abstract\": \"The goal of this paper is to evaluate the outlier identification performance of iterative Data Snooping (IDS) and L1-norm in levelling networks by considering the redundancy of the network, number and size of the outliers. For this purpose, several Monte-Carlo experiments were conducted into three different levelling networks configurations. In addition, a new way to compare the results of IDS based on Least Squares (LS) residuals and robust estimators such as the L1-norm has also been developed and presented. From the perspective of analysis only according to the success rate, it is shown that L1-norm performs better than IDS for the case of networks with low redundancy , especially for cases where more than one outlier is present in the dataset. In the relationship between false positive rate and outlier identification success rate, however, IDS performs better than L1-norm, independently of the levelling network configuration, number and size of outliers.\"\\n      },\\n      {\\n        \"paperId\": \"2a5e4419c4bf16178a441c975b970fa33a9f5361\",\\n        \"url\": \"https://www.semanticscholar.org/paper/2a5e4419c4bf16178a441c975b970fa33a9f5361\",\\n        \"title\": \"Nash Equilibria for Linear Quadratic Discrete-Time Dynamic Games via Iterative and Data-Driven Algorithms\",\\n        \"citationCount\": 15,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://ieeexplore.ieee.org/ielx7/9/4601496/10463100.pdf\",\\n          \"status\": \"HYBRID\",\\n          \"license\": \"CCBY\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/TAC.2024.3375249?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/TAC.2024.3375249, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This article proposes four different iterative algorithms to find Nash equilibrium strategies for discrete-time linear quadratic games based on the solution of either Lyapunov or Riccati equations for each player.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-10-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2046956775\",\\n            \"name\": \"Benita Nortmann\"\\n          },\\n          {\\n            \"authorId\": \"2267944139\",\\n            \"name\": \"Andrea Monti\"\\n          },\\n          {\\n            \"authorId\": \"1764871\",\\n            \"name\": \"M. Sassano\"\\n          },\\n          {\\n            \"authorId\": \"2695624\",\\n            \"name\": \"T. Mylvaganam\"\\n          }\\n        ],\\n        \"abstract\": \"Determining feedback Nash equilibrium solutions of nonzero-sum dynamic games is generally challenging. In this article, we propose four different iterative algorithms to find Nash equilibrium strategies for discrete-time linear quadratic games. The strategy update laws are based on the solution of either Lyapunov or Riccati equations for each player. Local convergence criteria are discussed. Motivated by the fact that in many practical scenarios each player in the game may have access to different (incomplete) information, we also introduce purely data-driven implementations of the algorithms. This allows the players to reach a Nash equilibrium solution of the game via scheduled experiments and without knowledge of each other\\'s performance criteria or of the system dynamics. The efficacy of the presented algorithms is illustrated via numerical examples and a practical example involving human\\\\u2013robot interaction.\"\\n      },\\n      {\\n        \"paperId\": \"d47ecf36e7e67d9349ecac29dfb981feaf229229\",\\n        \"url\": \"https://www.semanticscholar.org/paper/d47ecf36e7e67d9349ecac29dfb981feaf229229\",\\n        \"title\": \"iMSCGnet: Iterative Multi-Scale Context-Guided Segmentation of Skin Lesion in Dermoscopic Images\",\\n        \"citationCount\": 23,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://ieeexplore.ieee.org/ielx7/6287639/8948470/09007375.pdf\",\\n          \"status\": \"GOLD\",\\n          \"license\": \"CCBY\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ACCESS.2020.2974512?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ACCESS.2020.2974512, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A multi-scale context-guided network named as MSCGnet is proposed to segment the skin lesions accurately and generally outperforms the state-of-the-art methods.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": null,\\n        \"authors\": [\\n          {\\n            \"authorId\": \"5691344\",\\n            \"name\": \"Yujiao Tang\"\\n          },\\n          {\\n            \"authorId\": \"39271955\",\\n            \"name\": \"Zhiwen Fang\"\\n          },\\n          {\\n            \"authorId\": \"153431402\",\\n            \"name\": \"Shaofeng Yuan\"\\n          },\\n          {\\n            \"authorId\": \"30380386\",\\n            \"name\": \"C. Zhan\"\\n          },\\n          {\\n            \"authorId\": \"2072728505\",\\n            \"name\": \"Yanyan Xing\"\\n          },\\n          {\\n            \"authorId\": \"10638646\",\\n            \"name\": \"Joey Tianyi Zhou\"\\n          },\\n          {\\n            \"authorId\": \"2115412895\",\\n            \"name\": \"Feng Yang\"\\n          }\\n        ],\\n        \"abstract\": \"Despite much effort has been devoted to skin lesion segmentation, the performance of existing methods is still not satisfactory enough for practical applications. The challenges may include fuzzy lesion boundary, uneven and low contrast, and variation of colors across space, which often lead to fragmentary segmentation and inaccurate boundary. To alleviate this problem, we propose a multi-scale context-guided network named as MSCGnet to segment the skin lesions accurately. In MSCGnet, the context information is utilized to guide the feature encoding procedure. Moreover, because of the information loss in spatial down-sampling, a context-based attention structure (CAs) is designed to select effective context features in the decoding path. Furthermore, we boost the performance of MSCGnet with iterations and term this upgraded version as iterative MSCGnet, denoted as iMSCGnet. To supervise the training of iMSCGnet in an end-to-end fashion, a novel objective function of deep supervision, which consists of the terms of each encoding layers and the terms from each MSCGnet output of iMSCGnet, is employed. Our method is evaluated extensively on the four publicly available datasets, including ISBI2016 <xref ref-type=\\\\\"bibr\\\\\" rid=\\\\\"ref1\\\\\">[1]</xref>, ISBI2017 <xref ref-type=\\\\\"bibr\\\\\" rid=\\\\\"ref2\\\\\">[2]</xref>, ISIC2018 <xref ref-type=\\\\\"bibr\\\\\" rid=\\\\\"ref3\\\\\">[3]</xref> and PH2 <xref ref-type=\\\\\"bibr\\\\\" rid=\\\\\"ref4\\\\\">[4]</xref> datasets. The experimental results prove the effectiveness of proposed components and show that our method generally outperforms the state-of-the-art methods.\"\\n      },\\n      {\\n        \"paperId\": \"29528d8cb030a65f62a35b1237f1f5483077ad0a\",\\n        \"url\": \"https://www.semanticscholar.org/paper/29528d8cb030a65f62a35b1237f1f5483077ad0a\",\\n        \"title\": \"Inference Scaling for Long-Context Retrieval Augmented Generation\",\\n        \"citationCount\": 47,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2410.04343, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work investigates inference scaling for retrieval augmented generation (RAG), exploring the combination of multiple strategies beyond simply increasing the quantity of knowledge, including in-context learning and iterative prompting, and reveals that increasing inference computation leads to nearly linear gains in RAG performance when optimally allocated.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-10-06\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2028213158\",\\n            \"name\": \"Zhenrui Yue\"\\n          },\\n          {\\n            \"authorId\": \"39371343\",\\n            \"name\": \"Honglei Zhuang\"\\n          },\\n          {\\n            \"authorId\": \"2324782053\",\\n            \"name\": \"Aijun Bai\"\\n          },\\n          {\\n            \"authorId\": \"2261281337\",\\n            \"name\": \"Kai Hui\"\\n          },\\n          {\\n            \"authorId\": \"1886219\",\\n            \"name\": \"R. Jagerman\"\\n          },\\n          {\\n            \"authorId\": \"2324910979\",\\n            \"name\": \"Hansi Zeng\"\\n          },\\n          {\\n            \"authorId\": \"2099586642\",\\n            \"name\": \"Zhen Qin\"\\n          },\\n          {\\n            \"authorId\": \"2325158655\",\\n            \"name\": \"Dong Wang\"\\n          },\\n          {\\n            \"authorId\": \"2261356664\",\\n            \"name\": \"Xuanhui Wang\"\\n          },\\n          {\\n            \"authorId\": \"1815447\",\\n            \"name\": \"Michael Bendersky\"\\n          }\\n        ],\\n        \"abstract\": \"The scaling of inference computation has unlocked the potential of long-context large language models (LLMs) across diverse settings. For knowledge-intensive tasks, the increased compute is often allocated to incorporate more external knowledge. However, without effectively utilizing such knowledge, solely expanding context does not always enhance performance. In this work, we investigate inference scaling for retrieval augmented generation (RAG), exploring the combination of multiple strategies beyond simply increasing the quantity of knowledge, including in-context learning and iterative prompting. These strategies provide additional flexibility to scale test-time computation (e.g., by increasing retrieved documents or generation steps), thereby enhancing LLMs\\' ability to effectively acquire and utilize contextual information. We address two key questions: (1) How does RAG performance benefit from the scaling of inference computation when optimally configured? (2) Can we predict the optimal test-time compute allocation for a given budget by modeling the relationship between RAG performance and inference parameters? Our observations reveal that increasing inference computation leads to nearly linear gains in RAG performance when optimally allocated, a relationship we describe as the inference scaling laws for RAG. Building on this, we further develop the computation allocation model to estimate RAG performance across different inference configurations. The model predicts optimal inference parameters under various computation constraints, which align closely with the experimental results. By applying these optimal configurations, we demonstrate that scaling inference compute on long-context LLMs achieves up to 58.9% gains on benchmark datasets compared to standard RAG.\"\\n      },\\n      {\\n        \"paperId\": \"ca47c63a97848e60389037c93a4feb2daf849c3e\",\\n        \"url\": \"https://www.semanticscholar.org/paper/ca47c63a97848e60389037c93a4feb2daf849c3e\",\\n        \"title\": \"Iterative Data Smoothing: Mitigating Reward Overfitting and Overoptimization in RLHF\",\\n        \"citationCount\": 41,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2401.16335, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"The core idea is that during each training epoch, the model is updated with the data, but also update the date using the model, replacing hard labels with soft labels, and the empirical findings highlight the superior performance of this approach over the traditional methods.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-01-29\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"23111704\",\\n            \"name\": \"Banghua Zhu\"\\n          },\\n          {\\n            \"authorId\": \"2273930444\",\\n            \"name\": \"Michael I. Jordan\"\\n          },\\n          {\\n            \"authorId\": \"2258657022\",\\n            \"name\": \"Jiantao Jiao\"\\n          }\\n        ],\\n        \"abstract\": \"Reinforcement Learning from Human Feedback (RLHF) is a pivotal technique that aligns language models closely with human-centric values. The initial phase of RLHF involves learning human values using a reward model from ranking data. It is observed that the performance of the reward model degrades after one epoch of training, and optimizing too much against the learned reward model eventually hinders the true objective. This paper delves into these issues, leveraging the theoretical insights to design improved reward learning algorithm termed \\'Iterative Data Smoothing\\' (IDS). The core idea is that during each training epoch, we not only update the model with the data, but also update the date using the model, replacing hard labels with soft labels. Our empirical findings highlight the superior performance of this approach over the traditional methods.\"\\n      },\\n      {\\n        \"paperId\": \"8051818817a9a3815be6623a679d4a7f5a7b7964\",\\n        \"url\": \"https://www.semanticscholar.org/paper/8051818817a9a3815be6623a679d4a7f5a7b7964\",\\n        \"title\": \"Retrieve, Summarize, Plan: Advancing Multi-hop Question Answering with an Iterative Approach\",\\n        \"citationCount\": 36,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2407.13101, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A novel iterative RAG method called ReSP is proposed, equipped with a dual-function summarizer that compresses information from retrieved documents, targeting both the overarching question and the current sub-question concurrently.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Book\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-07-18\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2305310515\",\\n            \"name\": \"Zhouyu Jiang\"\\n          },\\n          {\\n            \"authorId\": \"2273904059\",\\n            \"name\": \"Mengshu Sun\"\\n          },\\n          {\\n            \"authorId\": \"2303947668\",\\n            \"name\": \"Lei Liang\"\\n          },\\n          {\\n            \"authorId\": \"2290024603\",\\n            \"name\": \"Zhiqiang Zhang\"\\n          }\\n        ],\\n        \"abstract\": \"Multi-hop question answering is a challenging task with distinct industrial relevance, and Retrieval-Augmented Generation (RAG) methods based on large language models (LLMs) have become a popular approach to tackle this task. Owing to the potential inability to retrieve all necessary information in a single iteration, a series of iterative RAG methods has been recently developed, showing significant performance improvements. However, existing methods still face two critical challenges: context overload resulting from multiple rounds of retrieval, and over-planning and repetitive planning due to the lack of a recorded retrieval trajectory. In this paper, we propose a novel iterative RAG method called ReSP, equipped with a dual-function summarizer. This summarizer compresses information from retrieved documents, targeting both the overarching question and the current sub-question concurrently. Experimental results on the multi-hop question-answering datasets HotpotQA and 2WikiMultihopQA demonstrate that our method significantly outperforms the state-of-the-art, and exhibits excellent robustness concerning context length.\"\\n      },\\n      {\\n        \"paperId\": \"9a1b5eb2a3e17621feab02c0f7f8c4df65ebf71c\",\\n        \"url\": \"https://www.semanticscholar.org/paper/9a1b5eb2a3e17621feab02c0f7f8c4df65ebf71c\",\\n        \"title\": \"CTIF-Net: A CNN-Transformer Iterative Fusion Network for Salient Object Detection\",\\n        \"citationCount\": 36,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {\\'tldr\\'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/TCSVT.2023.3321190?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/TCSVT.2023.3321190, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": null,\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-05-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2116395954\",\\n            \"name\": \"Junbin Yuan\"\\n          },\\n          {\\n            \"authorId\": \"2254058034\",\\n            \"name\": \"Aiqing Zhu\"\\n          },\\n          {\\n            \"authorId\": \"2575087\",\\n            \"name\": \"Qingzhen Xu\"\\n          },\\n          {\\n            \"authorId\": \"2757120\",\\n            \"name\": \"Kanoksak Wattanachote\"\\n          },\\n          {\\n            \"authorId\": \"2240697074\",\\n            \"name\": \"Yongyi Gong\"\\n          }\\n        ],\\n        \"abstract\": \"Capturing sufficient global context and rich spatial structure information is critical for dense prediction tasks. Convolutional Neural Network (CNN) is particularly adept at modeling fine-grained local features, while Transformer excels at modeling global context information. It is evident that CNN and Transformer exhibit complementary characteristics. Exploring the design of a network, that efficiently fuses these two models to leverage their strengths fully and achieve more accurate detection, represents a promising and worthwhile research topic. In this paper, we introduce a novel CNN-Transformer Iterative Fusion Network (CTIF-Net) for salient object detection. It efficiently combines CNN and Transformer to achieve superior performance by using a parallel dual encoder structure and a feature iterative fusion module. Firstly, CTIF-Net extracts features from the image using the CNN and the Transformer, respectively. Secondly, two feature convertors and a feature iterative fusion module are employed to combine and iteratively refine the two sets of features. The experimental results on multiple SOD datasets show that CTIF-Net outperforms 17 state-of-the-art methods, achieving higher performance in various mainstream evaluation metrics such as F-measure, S-measure, and MAE value. Code can be found at https://github.com/danielfaster/CTIF-Net/.\"\\n      },\\n      {\\n        \"paperId\": \"c811bedbe8f4c21d0cba9f9175f7c2eb203284a7\",\\n        \"url\": \"https://www.semanticscholar.org/paper/c811bedbe8f4c21d0cba9f9175f7c2eb203284a7\",\\n        \"title\": \"Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context\",\\n        \"citationCount\": 2813,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2403.05530, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"Gemini 1.5 models achieve near-perfect recall on long-context retrieval tasks across modalities, improve the state-of-the-art in long-document QA, long-video QA and long-context ASR, and match or surpass Gemini 1.0 Ultra\\'s state-of-the-art performance across a broad set of benchmarks.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-03-08\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"1557386977\",\\n            \"name\": \"Machel Reid\"\\n          },\\n          {\\n            \"authorId\": \"2417003\",\\n            \"name\": \"Nikolay Savinov\"\\n          },\\n          {\\n            \"authorId\": \"3035073\",\\n            \"name\": \"Denis Teplyashin\"\\n          },\\n          {\\n            \"authorId\": \"150077954\",\\n            \"name\": \"Dmitry Lepikhin\"\\n          },\\n          {\\n            \"authorId\": \"2542999\",\\n            \"name\": \"T. Lillicrap\"\\n          },\\n          {\\n            \"authorId\": \"2285263\",\\n            \"name\": \"Jean-Baptiste Alayrac\"\\n          },\\n          {\\n            \"authorId\": \"1737285\",\\n            \"name\": \"Radu Soricut\"\\n          },\\n          {\\n            \"authorId\": \"2672644\",\\n            \"name\": \"Angeliki Lazaridou\"\\n          },\\n          {\\n            \"authorId\": \"2273534960\",\\n            \"name\": \"Orhan Firat\"\\n          },\\n          {\\n            \"authorId\": \"4337102\",\\n            \"name\": \"Julian Schrittwieser\"\\n          },\\n          {\\n            \"authorId\": \"2460849\",\\n            \"name\": \"Ioannis Antonoglou\"\\n          },\\n          {\\n            \"authorId\": \"1508890387\",\\n            \"name\": \"Rohan Anil\"\\n          },\\n          {\\n            \"authorId\": \"148016269\",\\n            \"name\": \"Sebastian Borgeaud\"\\n          },\\n          {\\n            \"authorId\": \"2273563615\",\\n            \"name\": \"Andrew M. Dai\"\\n          },\\n          {\\n            \"authorId\": \"2143434227\",\\n            \"name\": \"Katie Millican\"\\n          },\\n          {\\n            \"authorId\": \"2275180676\",\\n            \"name\": \"Ethan Dyer\"\\n          },\\n          {\\n            \"authorId\": \"2143471164\",\\n            \"name\": \"Mia Glaese\"\\n          },\\n          {\\n            \"authorId\": \"2070364520\",\\n            \"name\": \"Thibault Sottiaux\"\\n          },\\n          {\\n            \"authorId\": \"2275292292\",\\n            \"name\": \"Ben-jamin Lee\"\\n          },\\n          {\\n            \"authorId\": \"2290484786\",\\n            \"name\": \"Fabio Viola\"\\n          },\\n          {\\n            \"authorId\": \"47447264\",\\n            \"name\": \"Malcolm Reynolds\"\\n          },\\n          {\\n            \"authorId\": \"2145139570\",\\n            \"name\": \"Yuanzhong Xu\"\\n          },\\n          {\\n            \"authorId\": \"2065370007\",\\n            \"name\": \"James Molloy\"\\n          },\\n          {\\n            \"authorId\": \"2249566095\",\\n            \"name\": \"Jilin Chen\"\\n          },\\n          {\\n            \"authorId\": \"2090818\",\\n            \"name\": \"M. Isard\"\\n          },\\n          {\\n            \"authorId\": \"152399055\",\\n            \"name\": \"P. Barham\"\\n          },\\n          {\\n            \"authorId\": \"2146532222\",\\n            \"name\": \"Tom Hennigan\"\\n          },\\n          {\\n            \"authorId\": \"2275176102\",\\n            \"name\": \"Ross Mcilroy\"\\n          },\\n          {\\n            \"authorId\": \"2275525680\",\\n            \"name\": \"Melvin Johnson\"\\n          },\\n          {\\n            \"authorId\": \"1698491\",\\n            \"name\": \"J. Schalkwyk\"\\n          },\\n          {\\n            \"authorId\": \"2275181648\",\\n            \"name\": \"Eli Collins\"\\n          },\\n          {\\n            \"authorId\": \"2143538252\",\\n            \"name\": \"Eliza Rutherford\"\\n          },\\n          {\\n            \"authorId\": \"2275185558\",\\n            \"name\": \"Erica Moreira\"\\n          },\\n          {\\n            \"authorId\": \"34122449\",\\n            \"name\": \"Kareem W. Ayoub\"\\n          },\\n          {\\n            \"authorId\": \"2275186741\",\\n            \"name\": \"Megha Goel\"\\n          },\\n          {\\n            \"authorId\": \"1406288863\",\\n            \"name\": \"Clemens Meyer\"\\n          },\\n          {\\n            \"authorId\": \"2005813\",\\n            \"name\": \"Gregory Thornton\"\\n          },\\n          {\\n            \"authorId\": \"2275729730\",\\n            \"name\": \"Zhen Yang\"\\n          },\\n          {\\n            \"authorId\": \"47407464\",\\n            \"name\": \"H. Michalewski\"\\n          },\\n          {\\n            \"authorId\": \"2275185143\",\\n            \"name\": \"Zaheer Abbas\"\\n          },\\n          {\\n            \"authorId\": \"74530494\",\\n            \"name\": \"Nathan Schucher\"\\n          },\\n          {\\n            \"authorId\": \"12679121\",\\n            \"name\": \"Ankesh Anand\"\\n          },\\n          {\\n            \"authorId\": \"2275185509\",\\n            \"name\": \"Richard Ives\"\\n          },\\n          {\\n            \"authorId\": \"2058168486\",\\n            \"name\": \"James Keeling\"\\n          },\\n          {\\n            \"authorId\": \"3257286\",\\n            \"name\": \"Karel Lenc\"\\n          },\\n          {\\n            \"authorId\": \"40269586\",\\n            \"name\": \"S. Haykal\"\\n          },\\n          {\\n            \"authorId\": \"2944868\",\\n            \"name\": \"Siamak Shakeri\"\\n          },\\n          {\\n            \"authorId\": \"67311962\",\\n            \"name\": \"Pranav Shyam\"\\n          },\\n          {\\n            \"authorId\": \"2841893\",\\n            \"name\": \"A. Chowdhery\"\\n          },\\n          {\\n            \"authorId\": \"81387328\",\\n            \"name\": \"Roman Ring\"\\n          },\\n          {\\n            \"authorId\": \"2135383313\",\\n            \"name\": \"Stephen Spencer\"\\n          },\\n          {\\n            \"authorId\": \"1413718981\",\\n            \"name\": \"Eren Sezener\"\\n          },\\n          {\\n            \"authorId\": \"2289035179\",\\n            \"name\": \"Luke Vilnis\"\\n          },\\n          {\\n            \"authorId\": \"2275186577\",\\n            \"name\": \"Os-car Chang\"\\n          },\\n          {\\n            \"authorId\": \"2288269273\",\\n            \"name\": \"Nobuyuki Morioka\"\\n          },\\n          {\\n            \"authorId\": \"2275183383\",\\n            \"name\": \"George Tucker\"\\n          },\\n          {\\n            \"authorId\": \"2276211400\",\\n            \"name\": \"Ce Zheng\"\\n          },\\n          {\\n            \"authorId\": \"2275186839\",\\n            \"name\": \"Oliver Woodman\"\\n          },\\n          {\\n            \"authorId\": \"80930649\",\\n            \"name\": \"Nithya Attaluri\"\\n          },\\n          {\\n            \"authorId\": \"2367821\",\\n            \"name\": \"Tom\\\\u00e1s Kocisk\\\\u00fd\"\\n          },\\n          {\\n            \"authorId\": \"2275187189\",\\n            \"name\": \"Evgenii Eltyshev\"\\n          },\\n          {\\n            \"authorId\": \"2275535939\",\\n            \"name\": \"Xi Chen\"\\n          },\\n          {\\n            \"authorId\": \"2275188933\",\\n            \"name\": \"Timothy Chung\"\\n          },\\n          {\\n            \"authorId\": \"1394635460\",\\n            \"name\": \"Vittorio Selo\"\\n          },\\n          {\\n            \"authorId\": \"1791585\",\\n            \"name\": \"Siddhartha Brahma\"\\n          },\\n          {\\n            \"authorId\": \"1737522\",\\n            \"name\": \"Petko Georgiev\"\\n          },\\n          {\\n            \"authorId\": \"133666998\",\\n            \"name\": \"Ambrose Slone\"\\n          },\\n          {\\n            \"authorId\": \"2275539055\",\\n            \"name\": \"Zhenkai Zhu\"\\n          },\\n          {\\n            \"authorId\": \"2266398107\",\\n            \"name\": \"James Lottes\"\\n          },\\n          {\\n            \"authorId\": \"2275178766\",\\n            \"name\": \"Siyuan Qiao\"\\n          },\\n          {\\n            \"authorId\": \"2290484287\",\\n            \"name\": \"Ben Caine\"\\n          },\\n          {\\n            \"authorId\": \"2287841795\",\\n            \"name\": \"Sebastian Riedel\"\\n          },\\n          {\\n            \"authorId\": \"2275176047\",\\n            \"name\": \"Alex Tomala\"\\n          },\\n          {\\n            \"authorId\": \"2159545857\",\\n            \"name\": \"Martin Chadwick\"\\n          },\\n          {\\n            \"authorId\": \"2253158807\",\\n            \"name\": \"J Christopher Love\"\\n          },\\n          {\\n            \"authorId\": \"2070068655\",\\n            \"name\": \"Peter Choy\"\\n          },\\n          {\\n            \"authorId\": \"2073395505\",\\n            \"name\": \"Sid Mittal\"\\n          },\\n          {\\n            \"authorId\": \"2815290\",\\n            \"name\": \"N. Houlsby\"\\n          },\\n          {\\n            \"authorId\": \"2269752766\",\\n            \"name\": \"Yunhao Tang\"\\n          },\\n          {\\n            \"authorId\": \"2289446231\",\\n            \"name\": \"Matthew Lamm\"\\n          },\\n          {\\n            \"authorId\": \"2275159462\",\\n            \"name\": \"Libin Bai\"\\n          },\\n          {\\n            \"authorId\": \"2197671266\",\\n            \"name\": \"Qiao Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2253917827\",\\n            \"name\": \"Luheng He\"\\n          },\\n          {\\n            \"authorId\": \"2275287219\",\\n            \"name\": \"Yong Cheng\"\\n          },\\n          {\\n            \"authorId\": \"2275186692\",\\n            \"name\": \"Peter Humphreys\"\\n          },\\n          {\\n            \"authorId\": \"2275290025\",\\n            \"name\": \"Yujia Li\"\\n          },\\n          {\\n            \"authorId\": \"1786259\",\\n            \"name\": \"Sergey Brin\"\\n          },\\n          {\\n            \"authorId\": \"51042571\",\\n            \"name\": \"Albin Cassirer\"\\n          },\\n          {\\n            \"authorId\": \"2283231534\",\\n            \"name\": \"Ying-Qi Miao\"\\n          },\\n          {\\n            \"authorId\": \"1780245\",\\n            \"name\": \"Luk\\\\u00e1s Zilka\"\\n          },\\n          {\\n            \"authorId\": \"2275189014\",\\n            \"name\": \"Taylor Tobin\"\\n          },\\n          {\\n            \"authorId\": \"2266735761\",\\n            \"name\": \"Kelvin Xu\"\\n          },\\n          {\\n            \"authorId\": \"2161966573\",\\n            \"name\": \"Lev Proleev\"\\n          },\\n          {\\n            \"authorId\": \"2275175792\",\\n            \"name\": \"Daniel Sohn\"\\n          },\\n          {\\n            \"authorId\": \"2275182383\",\\n            \"name\": \"Al-berto Magni\"\\n          },\\n          {\\n            \"authorId\": \"2258347245\",\\n            \"name\": \"L. Hendricks\"\\n          },\\n          {\\n            \"authorId\": \"2290513267\",\\n            \"name\": \"Isabel Gao\"\\n          },\\n          {\\n            \"authorId\": \"2217756237\",\\n            \"name\": \"Santiago Ontan\\'on\"\\n          },\\n          {\\n            \"authorId\": \"2275177720\",\\n            \"name\": \"Oskar Bunyan\"\\n          },\\n          {\\n            \"authorId\": \"2275185667\",\\n            \"name\": \"Nathan Byrd\"\\n          },\\n          {\\n            \"authorId\": \"2275537981\",\\n            \"name\": \"Abhanshu Sharma\"\\n          },\\n          {\\n            \"authorId\": \"48335426\",\\n            \"name\": \"Biao Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2290662953\",\\n            \"name\": \"Mario Pinto\"\\n          },\\n          {\\n            \"authorId\": \"2275170606\",\\n            \"name\": \"Rishika Sinha\"\\n          },\\n          {\\n            \"authorId\": \"18138802\",\\n            \"name\": \"Harsh Mehta\"\\n          },\\n          {\\n            \"authorId\": \"2275186531\",\\n            \"name\": \"Dawei Jia\"\\n          },\\n          {\\n            \"authorId\": \"1413064976\",\\n            \"name\": \"Sergi Caelles\"\\n          },\\n          {\\n            \"authorId\": \"1991019030\",\\n            \"name\": \"Albert Webson\"\\n          },\\n          {\\n            \"authorId\": \"2275162692\",\\n            \"name\": \"Alex Morris\"\\n          },\\n          {\\n            \"authorId\": \"2080504963\",\\n            \"name\": \"Becca Roelofs\"\\n          },\\n          {\\n            \"authorId\": \"2290634593\",\\n            \"name\": \"Yifan Ding\"\\n          },\\n          {\\n            \"authorId\": \"86898863\",\\n            \"name\": \"Robin Strudel\"\\n          },\\n          {\\n            \"authorId\": \"2275193471\",\\n            \"name\": \"Xuehan Xiong\"\\n          },\\n          {\\n            \"authorId\": \"39687627\",\\n            \"name\": \"Marvin Ritter\"\\n          },\\n          {\\n            \"authorId\": \"2256989598\",\\n            \"name\": \"Mostafa Dehghani\"\\n          },\\n          {\\n            \"authorId\": \"1706980\",\\n            \"name\": \"R. Chaabouni\"\\n          },\\n          {\\n            \"authorId\": \"2078909017\",\\n            \"name\": \"Abhijit Karmarkar\"\\n          },\\n          {\\n            \"authorId\": \"2290484705\",\\n            \"name\": \"Guangda Lai\"\\n          },\\n          {\\n            \"authorId\": \"3468078\",\\n            \"name\": \"Fabian Mentzer\"\\n          },\\n          {\\n            \"authorId\": \"2290664586\",\\n            \"name\": \"Bibo Xu\"\\n          },\\n          {\\n            \"authorId\": \"2261797906\",\\n            \"name\": \"YaGuang Li\"\\n          },\\n          {\\n            \"authorId\": \"2275534739\",\\n            \"name\": \"Yujing Zhang\"\\n          },\\n          {\\n            \"authorId\": \"40470211\",\\n            \"name\": \"T. Paine\"\\n          },\\n          {\\n            \"authorId\": \"40034895\",\\n            \"name\": \"Alex Goldin\"\\n          },\\n          {\\n            \"authorId\": \"3007442\",\\n            \"name\": \"Behnam Neyshabur\"\\n          },\\n          {\\n            \"authorId\": \"1734809439\",\\n            \"name\": \"Kate Baumli\"\\n          },\\n          {\\n            \"authorId\": \"6639036\",\\n            \"name\": \"Anselm Levskaya\"\\n          },\\n          {\\n            \"authorId\": \"2274104519\",\\n            \"name\": \"Michael Laskin\"\\n          },\\n          {\\n            \"authorId\": \"2275193644\",\\n            \"name\": \"Wenhao Jia\"\\n          },\\n          {\\n            \"authorId\": \"34269227\",\\n            \"name\": \"Jack W. Rae\"\\n          },\\n          {\\n            \"authorId\": \"2268673324\",\\n            \"name\": \"Kefan Xiao\"\\n          },\\n          {\\n            \"authorId\": \"2290485493\",\\n            \"name\": \"Antoine He\"\\n          },\\n          {\\n            \"authorId\": \"2290487747\",\\n            \"name\": \"Skye Giordano\"\\n          },\\n          {\\n            \"authorId\": \"2307454258\",\\n            \"name\": \"Lakshman Yagati\"\\n          },\\n          {\\n            \"authorId\": \"143783339\",\\n            \"name\": \"Jean-Baptiste Lespiau\"\\n          },\\n          {\\n            \"authorId\": \"122704930\",\\n            \"name\": \"Paul Natsev\"\\n          },\\n          {\\n            \"authorId\": \"2275185831\",\\n            \"name\": \"Sanjay Ganapathy\"\\n          },\\n          {\\n            \"authorId\": \"144097210\",\\n            \"name\": \"Fangyu Liu\"\\n          },\\n          {\\n            \"authorId\": \"2290487616\",\\n            \"name\": \"Danilo Martins\"\\n          },\\n          {\\n            \"authorId\": \"2249840944\",\\n            \"name\": \"Nanxin Chen\"\\n          },\\n          {\\n            \"authorId\": \"2275191526\",\\n            \"name\": \"Yunhan Xu\"\\n          },\\n          {\\n            \"authorId\": \"2275180117\",\\n            \"name\": \"Megan Barnes\"\\n          },\\n          {\\n            \"authorId\": \"2268760156\",\\n            \"name\": \"Rhys May\"\\n          },\\n          {\\n            \"authorId\": \"2275188533\",\\n            \"name\": \"Arpi Vezer\"\\n          },\\n          {\\n            \"authorId\": \"2275114643\",\\n            \"name\": \"Junhyuk Oh\"\\n          },\\n          {\\n            \"authorId\": \"2118834006\",\\n            \"name\": \"Ken Franko\"\\n          },\\n          {\\n            \"authorId\": \"2273670422\",\\n            \"name\": \"Sophie Bridgers\"\\n          },\\n          {\\n            \"authorId\": \"2275832693\",\\n            \"name\": \"Ruizhe Zhao\"\\n          },\\n          {\\n            \"authorId\": \"2275291909\",\\n            \"name\": \"Boxi Wu\"\\n          },\\n          {\\n            \"authorId\": \"40608942\",\\n            \"name\": \"Basil Mustafa\"\\n          },\\n          {\\n            \"authorId\": \"2290487447\",\\n            \"name\": \"Sean Sechrist\"\\n          },\\n          {\\n            \"authorId\": \"3166516\",\\n            \"name\": \"Emilio Parisotto\"\\n          },\\n          {\\n            \"authorId\": \"2598683\",\\n            \"name\": \"Thanumalayan Sankaranarayana Pillai\"\\n          },\\n          {\\n            \"authorId\": \"2290487293\",\\n            \"name\": \"Chris Larkin\"\\n          },\\n          {\\n            \"authorId\": \"2275149073\",\\n            \"name\": \"Chenjie Gu\"\\n          },\\n          {\\n            \"authorId\": \"2275186804\",\\n            \"name\": \"Christina Sorokin\"\\n          },\\n          {\\n            \"authorId\": \"2048712\",\\n            \"name\": \"M. Krikun\"\\n          },\\n          {\\n            \"authorId\": \"2275182203\",\\n            \"name\": \"Alexey Guseynov\"\\n          },\\n          {\\n            \"authorId\": \"2065404873\",\\n            \"name\": \"Jessica Landon\"\\n          },\\n          {\\n            \"authorId\": \"2275187147\",\\n            \"name\": \"Romina Datta\"\\n          },\\n          {\\n            \"authorId\": \"1863250\",\\n            \"name\": \"A. Pritzel\"\\n          },\\n          {\\n            \"authorId\": \"2151245633\",\\n            \"name\": \"Phoebe Thacker\"\\n          },\\n          {\\n            \"authorId\": \"2275801132\",\\n            \"name\": \"Fan Yang\"\\n          },\\n          {\\n            \"authorId\": \"2290487874\",\\n            \"name\": \"Kevin Hui\"\\n          },\\n          {\\n            \"authorId\": \"119556335\",\\n            \"name\": \"A.E. Hauth\"\\n          },\\n          {\\n            \"authorId\": \"2273556813\",\\n            \"name\": \"C. Yeh\"\\n          },\\n          {\\n            \"authorId\": \"2290481818\",\\n            \"name\": \"David Barker\"\\n          },\\n          {\\n            \"authorId\": \"1423275766\",\\n            \"name\": \"J. Mao-Jones\"\\n          },\\n          {\\n            \"authorId\": \"2166051497\",\\n            \"name\": \"Sophia Austin\"\\n          },\\n          {\\n            \"authorId\": \"2307453241\",\\n            \"name\": \"Hannah Sheahan\"\\n          },\\n          {\\n            \"authorId\": \"2620528\",\\n            \"name\": \"Parker Schuh\"\\n          },\\n          {\\n            \"authorId\": \"2275188153\",\\n            \"name\": \"James Svensson\"\\n          },\\n          {\\n            \"authorId\": \"2275193365\",\\n            \"name\": \"Rohan Jain\"\\n          },\\n          {\\n            \"authorId\": \"96641652\",\\n            \"name\": \"V. Ramasesh\"\\n          },\\n          {\\n            \"authorId\": \"2275185833\",\\n            \"name\": \"Anton Briukhov\"\\n          },\\n          {\\n            \"authorId\": \"2275180366\",\\n            \"name\": \"D. Chung\"\\n          },\\n          {\\n            \"authorId\": \"51029932\",\\n            \"name\": \"Tamara von Glehn\"\\n          },\\n          {\\n            \"authorId\": \"2275166845\",\\n            \"name\": \"Christina Butterfield\"\\n          },\\n          {\\n            \"authorId\": \"2275184551\",\\n            \"name\": \"Priya Jhakra\"\\n          },\\n          {\\n            \"authorId\": \"2275252154\",\\n            \"name\": \"Matt Wiethoff\"\\n          },\\n          {\\n            \"authorId\": \"2275193725\",\\n            \"name\": \"Justin Frye\"\\n          },\\n          {\\n            \"authorId\": \"2275175432\",\\n            \"name\": \"Jordan Grimstad\"\\n          },\\n          {\\n            \"authorId\": \"2158369306\",\\n            \"name\": \"Beer Changpinyo\"\\n          },\\n          {\\n            \"authorId\": \"153892869\",\\n            \"name\": \"Charline Le Lan\"\\n          },\\n          {\\n            \"authorId\": \"2275181572\",\\n            \"name\": \"Anna Bortsova\"\\n          },\\n          {\\n            \"authorId\": \"2275892922\",\\n            \"name\": \"Yonghui Wu\"\\n          },\\n          {\\n            \"authorId\": \"2767859\",\\n            \"name\": \"P. Voigtlaender\"\\n          },\\n          {\\n            \"authorId\": \"2279918122\",\\n            \"name\": \"Tara N. Sainath\"\\n          },\\n          {\\n            \"authorId\": \"2275575378\",\\n            \"name\": \"Charlotte Smith\"\\n          },\\n          {\\n            \"authorId\": \"2191689971\",\\n            \"name\": \"Will Hawkins\"\\n          },\\n          {\\n            \"authorId\": \"2275191626\",\\n            \"name\": \"Kris Cao\"\\n          },\\n          {\\n            \"authorId\": \"2275186515\",\\n            \"name\": \"James Besley\"\\n          },\\n          {\\n            \"authorId\": \"2059763226\",\\n            \"name\": \"S. Srinivasan\"\\n          },\\n          {\\n            \"authorId\": \"3175815\",\\n            \"name\": \"Mark Omernick\"\\n          },\\n          {\\n            \"authorId\": \"2160887964\",\\n            \"name\": \"Colin Gaffney\"\\n          },\\n          {\\n            \"authorId\": \"1956049835\",\\n            \"name\": \"G. Surita\"\\n          },\\n          {\\n            \"authorId\": \"2290484991\",\\n            \"name\": \"Ryan Burnell\"\\n          },\\n          {\\n            \"authorId\": \"2143374656\",\\n            \"name\": \"Bogdan Damoc\"\\n          },\\n          {\\n            \"authorId\": \"2275220028\",\\n            \"name\": \"Junwhan Ahn\"\\n          },\\n          {\\n            \"authorId\": \"2285740851\",\\n            \"name\": \"Andrew Brock\"\\n          },\\n          {\\n            \"authorId\": \"2146532125\",\\n            \"name\": \"Mantas Pajarskas\"\\n          },\\n          {\\n            \"authorId\": \"2275187155\",\\n            \"name\": \"Anastasia Petrushkina\"\\n          },\\n          {\\n            \"authorId\": \"30155667\",\\n            \"name\": \"Seb Noury\"\\n          },\\n          {\\n            \"authorId\": \"2275186192\",\\n            \"name\": \"Lorenzo Blanco\"\\n          },\\n          {\\n            \"authorId\": \"1754860\",\\n            \"name\": \"Kevin Swersky\"\\n          },\\n          {\\n            \"authorId\": \"2275185727\",\\n            \"name\": \"Arun Ahuja\"\\n          },\\n          {\\n            \"authorId\": \"2261737895\",\\n            \"name\": \"Thi Avrahami\"\\n          },\\n          {\\n            \"authorId\": \"40055795\",\\n            \"name\": \"Vedant Misra\"\\n          },\\n          {\\n            \"authorId\": \"2275184736\",\\n            \"name\": \"Raoul de Liedekerke\"\\n          },\\n          {\\n            \"authorId\": \"2275181534\",\\n            \"name\": \"Mariko Iinuma\"\\n          },\\n          {\\n            \"authorId\": \"144703404\",\\n            \"name\": \"A. Polozov\"\\n          },\\n          {\\n            \"authorId\": \"143981350\",\\n            \"name\": \"Sarah York\"\\n          },\\n          {\\n            \"authorId\": \"47568983\",\\n            \"name\": \"George van den Driessche\"\\n          },\\n          {\\n            \"authorId\": \"2275185732\",\\n            \"name\": \"Paul Michel\"\\n          },\\n          {\\n            \"authorId\": \"2273650801\",\\n            \"name\": \"Justin Chiu\"\\n          },\\n          {\\n            \"authorId\": \"46901218\",\\n            \"name\": \"Rory Blevins\"\\n          },\\n          {\\n            \"authorId\": \"2275185661\",\\n            \"name\": \"Zach Gleicher\"\\n          },\\n          {\\n            \"authorId\": \"39257069\",\\n            \"name\": \"Adri\\\\u00e0 Recasens\"\\n          },\\n          {\\n            \"authorId\": \"2275186093\",\\n            \"name\": \"Alban Rrustemi\"\\n          },\\n          {\\n            \"authorId\": \"1980809\",\\n            \"name\": \"E. Gribovskaya\"\\n          },\\n          {\\n            \"authorId\": \"2275277736\",\\n            \"name\": \"Au-rko Roy\"\\n          },\\n          {\\n            \"authorId\": \"2290487054\",\\n            \"name\": \"Wiktor Gworek\"\\n          },\\n          {\\n            \"authorId\": \"2275186656\",\\n            \"name\": \"S\\\\u00e9bastien M. R. Arnold\"\\n          },\\n          {\\n            \"authorId\": \"2275291886\",\\n            \"name\": \"Lisa Lee\"\\n          },\\n          {\\n            \"authorId\": \"2267341862\",\\n            \"name\": \"James Lee-Thorp\"\\n          },\\n          {\\n            \"authorId\": \"2090812426\",\\n            \"name\": \"M. Maggioni\"\\n          },\\n          {\\n            \"authorId\": \"2275183119\",\\n            \"name\": \"Enrique Piqueras\"\\n          },\\n          {\\n            \"authorId\": \"2051018967\",\\n            \"name\": \"Kartikeya Badola\"\\n          },\\n          {\\n            \"authorId\": \"2425230\",\\n            \"name\": \"S. Vikram\"\\n          },\\n          {\\n            \"authorId\": \"2275585027\",\\n            \"name\": \"Lucas Gonzalez\"\\n          },\\n          {\\n            \"authorId\": \"2275186584\",\\n            \"name\": \"Anirudh Baddepudi\"\\n          },\\n          {\\n            \"authorId\": \"2268665228\",\\n            \"name\": \"Evan Senter\"\\n          },\\n          {\\n            \"authorId\": \"2261961752\",\\n            \"name\": \"J. Devlin\"\\n          },\\n          {\\n            \"authorId\": \"47901308\",\\n            \"name\": \"James Qin\"\\n          },\\n          {\\n            \"authorId\": \"2275148073\",\\n            \"name\": \"Michael Azzam\"\\n          },\\n          {\\n            \"authorId\": \"1994939814\",\\n            \"name\": \"Maja Trebacz\"\\n          },\\n          {\\n            \"authorId\": \"35930544\",\\n            \"name\": \"M. Polacek\"\\n          },\\n          {\\n            \"authorId\": \"2290485355\",\\n            \"name\": \"Kashyap Krishnakumar\"\\n          },\\n          {\\n            \"authorId\": \"2275193337\",\\n            \"name\": \"Shuo-Yiin Chang\"\\n          },\\n          {\\n            \"authorId\": \"2275176212\",\\n            \"name\": \"Matthew Tung\"\\n          },\\n          {\\n            \"authorId\": \"2275187196\",\\n            \"name\": \"Ivo Penchev\"\\n          },\\n          {\\n            \"authorId\": \"2258551072\",\\n            \"name\": \"Rishabh Joshi\"\\n          },\\n          {\\n            \"authorId\": \"2275180557\",\\n            \"name\": \"Kate Olszewska\"\\n          },\\n          {\\n            \"authorId\": \"2275184985\",\\n            \"name\": \"Carrie Muir\"\\n          },\\n          {\\n            \"authorId\": \"2275185968\",\\n            \"name\": \"Mateo Wirth\"\\n          },\\n          {\\n            \"authorId\": \"2275184113\",\\n            \"name\": \"A. Hartman\"\\n          },\\n          {\\n            \"authorId\": \"2160888100\",\\n            \"name\": \"Joshua Newlan\"\\n          },\\n          {\\n            \"authorId\": \"2252586080\",\\n            \"name\": \"S. Kashem\"\\n          },\\n          {\\n            \"authorId\": \"2218882489\",\\n            \"name\": \"Vijay Bolina\"\\n          },\\n          {\\n            \"authorId\": \"2290484418\",\\n            \"name\": \"Elahe Dabir\"\\n          },\\n          {\\n            \"authorId\": \"3038326\",\\n            \"name\": \"Joost R. van Amersfoort\"\\n          },\\n          {\\n            \"authorId\": \"2275130349\",\\n            \"name\": \"Zafarali Ahmed\"\\n          },\\n          {\\n            \"authorId\": \"2275185511\",\\n            \"name\": \"James Cobon-Kerr\"\\n          },\\n          {\\n            \"authorId\": \"2269391198\",\\n            \"name\": \"Aishwarya B Kamath\"\\n          },\\n          {\\n            \"authorId\": \"2259962018\",\\n            \"name\": \"A. M. Hrafnkelsson\"\\n          },\\n          {\\n            \"authorId\": \"2274787555\",\\n            \"name\": \"Le Hou\"\\n          },\\n          {\\n            \"authorId\": \"2290485798\",\\n            \"name\": \"Ian Mackinnon\"\\n          },\\n          {\\n            \"authorId\": \"2156930381\",\\n            \"name\": \"Alexandre Frechette\"\\n          },\\n          {\\n            \"authorId\": \"51210148\",\\n            \"name\": \"Eric Noland\"\\n          },\\n          {\\n            \"authorId\": \"2275182246\",\\n            \"name\": \"Xi-ance Si\"\\n          },\\n          {\\n            \"authorId\": \"2779842\",\\n            \"name\": \"Emanuel Taropa\"\\n          },\\n          {\\n            \"authorId\": \"2350430090\",\\n            \"name\": \"Dong Li\"\\n          },\\n          {\\n            \"authorId\": \"2275183277\",\\n            \"name\": \"Phil Crone\"\\n          },\\n          {\\n            \"authorId\": \"4478284\",\\n            \"name\": \"Anmol Gulati\"\\n          },\\n          {\\n            \"authorId\": \"2275180682\",\\n            \"name\": \"S\\'ebastien Cevey\"\\n          },\\n          {\\n            \"authorId\": \"2275173231\",\\n            \"name\": \"Jonas Adler\"\\n          },\\n          {\\n            \"authorId\": \"2275786213\",\\n            \"name\": \"Ada Ma\"\\n          },\\n          {\\n            \"authorId\": \"2275185813\",\\n            \"name\": \"David Silver\"\\n          },\\n          {\\n            \"authorId\": \"148152480\",\\n            \"name\": \"Simon Tokumine\"\\n          },\\n          {\\n            \"authorId\": \"2067745837\",\\n            \"name\": \"Richard Powell\"\\n          },\\n          {\\n            \"authorId\": \"2275280377\",\\n            \"name\": \"Stephan Lee\"\\n          },\\n          {\\n            \"authorId\": \"2275571997\",\\n            \"name\": \"Michael B. Chang\"\\n          },\\n          {\\n            \"authorId\": \"2275396476\",\\n            \"name\": \"Samer Hassan\"\\n          },\\n          {\\n            \"authorId\": \"2007712128\",\\n            \"name\": \"Diana Mincu\"\\n          },\\n          {\\n            \"authorId\": \"2064599701\",\\n            \"name\": \"Antoine Yang\"\\n          },\\n          {\\n            \"authorId\": \"153898744\",\\n            \"name\": \"Nir Levine\"\\n          },\\n          {\\n            \"authorId\": \"2275186701\",\\n            \"name\": \"Jenny Brennan\"\\n          },\\n          {\\n            \"authorId\": \"2249764807\",\\n            \"name\": \"Mingqiu Wang\"\\n          },\\n          {\\n            \"authorId\": \"2265053608\",\\n            \"name\": \"Sarah Hodkinson\"\\n          },\\n          {\\n            \"authorId\": \"2144551262\",\\n            \"name\": \"Jeffrey Zhao\"\\n          },\\n          {\\n            \"authorId\": \"2290487597\",\\n            \"name\": \"Josh Lipschultz\"\\n          },\\n          {\\n            \"authorId\": \"20702300\",\\n            \"name\": \"Aedan Pope\"\\n          },\\n          {\\n            \"authorId\": \"2275571997\",\\n            \"name\": \"Michael B. Chang\"\\n          },\\n          {\\n            \"authorId\": \"2275767067\",\\n            \"name\": \"Cheng Li\"\\n          },\\n          {\\n            \"authorId\": \"2121764\",\\n            \"name\": \"Laurent El Shafey\"\\n          },\\n          {\\n            \"authorId\": \"2264591527\",\\n            \"name\": \"M. Paganini\"\\n          },\\n          {\\n            \"authorId\": \"2269733876\",\\n            \"name\": \"Sholto Douglas\"\\n          },\\n          {\\n            \"authorId\": \"2266464503\",\\n            \"name\": \"Bernd Bohnet\"\\n          },\\n          {\\n            \"authorId\": \"2274107421\",\\n            \"name\": \"Fabio Pardo\"\\n          },\\n          {\\n            \"authorId\": \"2275182230\",\\n            \"name\": \"Seth Odoom\"\\n          },\\n          {\\n            \"authorId\": \"2269541835\",\\n            \"name\": \"Mihaela Ro\\\\u0219ca\"\\n          },\\n          {\\n            \"authorId\": \"2267546965\",\\n            \"name\": \"Cicero Nogueira dos Santos\"\\n          },\\n          {\\n            \"authorId\": \"2275185640\",\\n            \"name\": \"Kedar Soparkar\"\\n          },\\n          {\\n            \"authorId\": \"35099444\",\\n            \"name\": \"A. Guez\"\\n          },\\n          {\\n            \"authorId\": \"2275187110\",\\n            \"name\": \"Tom Hudson\"\\n          },\\n          {\\n            \"authorId\": \"2275188563\",\\n            \"name\": \"Steven Hansen\"\\n          },\\n          {\\n            \"authorId\": \"50844587\",\\n            \"name\": \"Chulayuth Asawaroengchai\"\\n          },\\n          {\\n            \"authorId\": \"104000494\",\\n            \"name\": \"Ravichandra Addanki\"\\n          },\\n          {\\n            \"authorId\": \"2290486855\",\\n            \"name\": \"Tianhe Yu\"\\n          },\\n          {\\n            \"authorId\": \"3448463\",\\n            \"name\": \"Wojciech Stokowiec\"\\n          },\\n          {\\n            \"authorId\": \"2258793616\",\\n            \"name\": \"Mina Khan\"\\n          },\\n          {\\n            \"authorId\": \"2243002880\",\\n            \"name\": \"Justin Gilmer\"\\n          },\\n          {\\n            \"authorId\": \"2253808003\",\\n            \"name\": \"Jaehoon Lee\"\\n          },\\n          {\\n            \"authorId\": \"2290485108\",\\n            \"name\": \"Carrie Grimes Bostock\"\\n          },\\n          {\\n            \"authorId\": \"1996199677\",\\n            \"name\": \"Keran Rong\"\\n          },\\n          {\\n            \"authorId\": \"2263289033\",\\n            \"name\": \"Jonathan Caton\"\\n          },\\n          {\\n            \"authorId\": \"2275184275\",\\n            \"name\": \"Pedram Pejman\"\\n          },\\n          {\\n            \"authorId\": \"1696719\",\\n            \"name\": \"Filip Pavetic\"\\n          },\\n          {\\n            \"authorId\": \"2259937157\",\\n            \"name\": \"Geoff Brown\"\\n          },\\n          {\\n            \"authorId\": \"2290595918\",\\n            \"name\": \"Vivek Sharma\"\\n          },\\n          {\\n            \"authorId\": \"2170162986\",\\n            \"name\": \"Mario Luvci\\'c\"\\n          },\\n          {\\n            \"authorId\": \"2275176205\",\\n            \"name\": \"Rajku-mar Samuel\"\\n          },\\n          {\\n            \"authorId\": \"2941141\",\\n            \"name\": \"J. Djolonga\"\\n          },\\n          {\\n            \"authorId\": \"2063800905\",\\n            \"name\": \"Amol Mandhane\"\\n          },\\n          {\\n            \"authorId\": \"2275187845\",\\n            \"name\": \"Lars Lowe Sjosund\"\\n          },\\n          {\\n            \"authorId\": \"118801223\",\\n            \"name\": \"Elena Buchatskaya\"\\n          },\\n          {\\n            \"authorId\": \"2275158927\",\\n            \"name\": \"Elspeth White\"\\n          },\\n          {\\n            \"authorId\": \"2201776471\",\\n            \"name\": \"Natalie Clay\"\\n          },\\n          {\\n            \"authorId\": \"2260169185\",\\n            \"name\": \"Jiepu Jiang\"\\n          },\\n          {\\n            \"authorId\": \"2275798209\",\\n            \"name\": \"Hyeontaek Lim\"\\n          },\\n          {\\n            \"authorId\": \"38637384\",\\n            \"name\": \"Ross Hemsley\"\\n          },\\n          {\\n            \"authorId\": \"2275184618\",\\n            \"name\": \"Jane Labanowski\"\\n          },\\n          {\\n            \"authorId\": \"41019080\",\\n            \"name\": \"Nicola De Cao\"\\n          },\\n          {\\n            \"authorId\": \"2275188258\",\\n            \"name\": \"David Steiner\"\\n          },\\n          {\\n            \"authorId\": \"3362306\",\\n            \"name\": \"Sayed Hadi Hashemi\"\\n          },\\n          {\\n            \"authorId\": \"2288056644\",\\n            \"name\": \"Jacob Austin\"\\n          },\\n          {\\n            \"authorId\": \"2105841261\",\\n            \"name\": \"Anita Gergely\"\\n          },\\n          {\\n            \"authorId\": \"2221119859\",\\n            \"name\": \"Tim Blyth\"\\n          },\\n          {\\n            \"authorId\": \"2275190309\",\\n            \"name\": \"Joe Stanton\"\\n          },\\n          {\\n            \"authorId\": \"2272718153\",\\n            \"name\": \"K. Shivakumar\"\\n          },\\n          {\\n            \"authorId\": \"9356387\",\\n            \"name\": \"Aditya Siddhant\"\\n          },\\n          {\\n            \"authorId\": \"39552848\",\\n            \"name\": \"Anders Andreassen\"\\n          },\\n          {\\n            \"authorId\": \"2279996944\",\\n            \"name\": \"Carlos L. Araya\"\\n          },\\n          {\\n            \"authorId\": \"2275187415\",\\n            \"name\": \"Nikhil Sethi\"\\n          },\\n          {\\n            \"authorId\": \"2934334\",\\n            \"name\": \"Rakesh Shivanna\"\\n          },\\n          {\\n            \"authorId\": \"2275161833\",\\n            \"name\": \"Steven Hand\"\\n          },\\n          {\\n            \"authorId\": \"12295226\",\\n            \"name\": \"Ankur Bapna\"\\n          },\\n          {\\n            \"authorId\": \"2402489\",\\n            \"name\": \"A. Khodaei\"\\n          },\\n          {\\n            \"authorId\": \"19200186\",\\n            \"name\": \"Antoine Miech\"\\n          },\\n          {\\n            \"authorId\": \"2287809580\",\\n            \"name\": \"Garrett Tanzer\"\\n          },\\n          {\\n            \"authorId\": \"1394189636\",\\n            \"name\": \"Andy Swing\"\\n          },\\n          {\\n            \"authorId\": \"41037204\",\\n            \"name\": \"S. Thakoor\"\\n          },\\n          {\\n            \"authorId\": \"2291169360\",\\n            \"name\": \"Zhufeng Pan\"\\n          },\\n          {\\n            \"authorId\": \"81408931\",\\n            \"name\": \"Zachary Nado\"\\n          },\\n          {\\n            \"authorId\": \"2218062983\",\\n            \"name\": \"Stephanie Winkler\"\\n          },\\n          {\\n            \"authorId\": \"2256337021\",\\n            \"name\": \"Dian Yu\"\\n          },\\n          {\\n            \"authorId\": \"144413479\",\\n            \"name\": \"Mohammad Saleh\"\\n          },\\n          {\\n            \"authorId\": \"108173905\",\\n            \"name\": \"Lorenzo Maggiore\"\\n          },\\n          {\\n            \"authorId\": \"2159207795\",\\n            \"name\": \"Iain Barr\"\\n          },\\n          {\\n            \"authorId\": \"2275187490\",\\n            \"name\": \"Minh Giang\"\\n          },\\n          {\\n            \"authorId\": \"2275186582\",\\n            \"name\": \"Thais Kagohara\"\\n          },\\n          {\\n            \"authorId\": \"1841008\",\\n            \"name\": \"Ivo Danihelka\"\\n          },\\n          {\\n            \"authorId\": \"2275176043\",\\n            \"name\": \"Amit Marathe\"\\n          },\\n          {\\n            \"authorId\": \"2275181199\",\\n            \"name\": \"Vladimir Feinberg\"\\n          },\\n          {\\n            \"authorId\": \"2275176049\",\\n            \"name\": \"Mohamed Elhawaty\"\\n          },\\n          {\\n            \"authorId\": \"3404697\",\\n            \"name\": \"Nimesh Ghelani\"\\n          },\\n          {\\n            \"authorId\": \"48257711\",\\n            \"name\": \"Dan Horgan\"\\n          },\\n          {\\n            \"authorId\": \"2275121046\",\\n            \"name\": \"Helen Miller\"\\n          },\\n          {\\n            \"authorId\": \"2275184334\",\\n            \"name\": \"Lexi Walker\"\\n          },\\n          {\\n            \"authorId\": \"1825728\",\\n            \"name\": \"Richard Tanburn\"\\n          },\\n          {\\n            \"authorId\": \"2275180099\",\\n            \"name\": \"Mukarram Tariq\"\\n          },\\n          {\\n            \"authorId\": \"2275113487\",\\n            \"name\": \"Disha Shrivastava\"\\n          },\\n          {\\n            \"authorId\": \"2290487337\",\\n            \"name\": \"Fei Xia\"\\n          },\\n          {\\n            \"authorId\": \"2284761701\",\\n            \"name\": \"Chung-Cheng Chiu\"\\n          },\\n          {\\n            \"authorId\": \"2333511945\",\\n            \"name\": \"Zoe Ashwood\"\\n          },\\n          {\\n            \"authorId\": \"2290486431\",\\n            \"name\": \"Khuslen Baatarsukh\"\\n          },\\n          {\\n            \"authorId\": \"2412073\",\\n            \"name\": \"Sina Samangooei\"\\n          },\\n          {\\n            \"authorId\": \"2275177971\",\\n            \"name\": \"Fred Alcober\"\\n          },\\n          {\\n            \"authorId\": \"2163521750\",\\n            \"name\": \"Axel Stjerngren\"\\n          },\\n          {\\n            \"authorId\": \"2258235140\",\\n            \"name\": \"P. Komarek\"\\n          },\\n          {\\n            \"authorId\": \"2275185589\",\\n            \"name\": \"Katerina Tsihlas\"\\n          },\\n          {\\n            \"authorId\": \"11167300\",\\n            \"name\": \"Anudhyan Boral\"\\n          },\\n          {\\n            \"authorId\": \"89066101\",\\n            \"name\": \"R. Comanescu\"\\n          },\\n          {\\n            \"authorId\": \"2275275439\",\\n            \"name\": \"Jeremy Chen\"\\n          },\\n          {\\n            \"authorId\": \"7247867\",\\n            \"name\": \"Ruibo Liu\"\\n          },\\n          {\\n            \"authorId\": \"2275185808\",\\n            \"name\": \"Dawn Bloxwich\"\\n          },\\n          {\\n            \"authorId\": \"2182971260\",\\n            \"name\": \"Charlie Chen\"\\n          },\\n          {\\n            \"authorId\": \"2265240845\",\\n            \"name\": \"Yanhua Sun\"\\n          },\\n          {\\n            \"authorId\": \"2275173841\",\\n            \"name\": \"Fangxi-aoyu Feng\"\\n          },\\n          {\\n            \"authorId\": \"2251517316\",\\n            \"name\": \"M. Mauger\"\\n          },\\n          {\\n            \"authorId\": \"1404332584\",\\n            \"name\": \"Xerxes Dotiwalla\"\\n          },\\n          {\\n            \"authorId\": \"2297847306\",\\n            \"name\": \"V. Hellendoorn\"\\n          },\\n          {\\n            \"authorId\": \"2275184531\",\\n            \"name\": \"Michael Sharman\"\\n          },\\n          {\\n            \"authorId\": \"2275187038\",\\n            \"name\": \"Ivy Zheng\"\\n          },\\n          {\\n            \"authorId\": \"2256873459\",\\n            \"name\": \"Krishna Haridasan\"\\n          },\\n          {\\n            \"authorId\": \"1403998955\",\\n            \"name\": \"Gabriel Barth-Maron\"\\n          },\\n          {\\n            \"authorId\": \"2275181554\",\\n            \"name\": \"Craig Swanson\"\\n          },\\n          {\\n            \"authorId\": \"2275184739\",\\n            \"name\": \"Dominika Rogozi\\'nska\"\\n          },\\n          {\\n            \"authorId\": \"2290741315\",\\n            \"name\": \"Alek Andreev\"\\n          },\\n          {\\n            \"authorId\": \"2249760524\",\\n            \"name\": \"P. Rubenstein\"\\n          },\\n          {\\n            \"authorId\": \"2275189194\",\\n            \"name\": \"Ruoxin Sang\"\\n          },\\n          {\\n            \"authorId\": \"2265528853\",\\n            \"name\": \"Dan Hurt\"\\n          },\\n          {\\n            \"authorId\": \"2275189864\",\\n            \"name\": \"Gamaleldin Elsayed\"\\n          },\\n          {\\n            \"authorId\": \"2290529512\",\\n            \"name\": \"Ren-shen Wang\"\\n          },\\n          {\\n            \"authorId\": \"2290485332\",\\n            \"name\": \"Dave Lacey\"\\n          },\\n          {\\n            \"authorId\": \"2279830514\",\\n            \"name\": \"Anastasija Ili\\'c\"\\n          },\\n          {\\n            \"authorId\": \"2275112414\",\\n            \"name\": \"Yao Zhao\"\\n          },\\n          {\\n            \"authorId\": \"2215449616\",\\n            \"name\": \"Woohyun Han\"\\n          },\\n          {\\n            \"authorId\": \"2257256357\",\\n            \"name\": \"Lora Aroyo\"\\n          },\\n          {\\n            \"authorId\": \"2275177173\",\\n            \"name\": \"Chimezie Iwuanyanwu\"\\n          },\\n          {\\n            \"authorId\": \"48942032\",\\n            \"name\": \"Vitaly Nikolaev\"\\n          },\\n          {\\n            \"authorId\": \"40627523\",\\n            \"name\": \"Balaji Lakshminarayanan\"\\n          },\\n          {\\n            \"authorId\": \"2290484919\",\\n            \"name\": \"Sadegh Jazayeri\"\\n          },\\n          {\\n            \"authorId\": \"31713635\",\\n            \"name\": \"Raphael Lopez Kaufman\"\\n          },\\n          {\\n            \"authorId\": \"2150348369\",\\n            \"name\": \"Mani Varadarajan\"\\n          },\\n          {\\n            \"authorId\": \"118505443\",\\n            \"name\": \"Chetan Tekur\"\\n          },\\n          {\\n            \"authorId\": \"2275187305\",\\n            \"name\": \"Doug Fritz\"\\n          },\\n          {\\n            \"authorId\": \"2140488873\",\\n            \"name\": \"Misha Khalman\"\\n          },\\n          {\\n            \"authorId\": \"2257286979\",\\n            \"name\": \"David Reitter\"\\n          },\\n          {\\n            \"authorId\": \"2290487762\",\\n            \"name\": \"Kingshuk Dasgupta\"\\n          },\\n          {\\n            \"authorId\": \"1658856741\",\\n            \"name\": \"Shourya Sarcar\"\\n          },\\n          {\\n            \"authorId\": \"103861813\",\\n            \"name\": \"T. Ornduff\"\\n          },\\n          {\\n            \"authorId\": \"2265527968\",\\n            \"name\": \"Javier Snaider\"\\n          },\\n          {\\n            \"authorId\": \"2174667321\",\\n            \"name\": \"Fantine Huot\"\\n          },\\n          {\\n            \"authorId\": \"2275694953\",\\n            \"name\": \"Johnson Jia\"\\n          },\\n          {\\n            \"authorId\": \"2275186180\",\\n            \"name\": \"Rupert Kemp\"\\n          },\\n          {\\n            \"authorId\": \"1702423\",\\n            \"name\": \"Nejc Trdin\"\\n          },\\n          {\\n            \"authorId\": \"2275186554\",\\n            \"name\": \"Anitha Vijayakumar\"\\n          },\\n          {\\n            \"authorId\": \"2290490486\",\\n            \"name\": \"Lucy Kim\"\\n          },\\n          {\\n            \"authorId\": \"2269460640\",\\n            \"name\": \"Christof Angermueller\"\\n          },\\n          {\\n            \"authorId\": \"2290485653\",\\n            \"name\": \"Li Lao\"\\n          },\\n          {\\n            \"authorId\": \"2275249023\",\\n            \"name\": \"Tianqi Liu\"\\n          },\\n          {\\n            \"authorId\": \"2290556567\",\\n            \"name\": \"Haibin Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2290485958\",\\n            \"name\": \"David Engel\"\\n          },\\n          {\\n            \"authorId\": \"2275190069\",\\n            \"name\": \"Somer Greene\"\\n          },\\n          {\\n            \"authorId\": \"2275169305\",\\n            \"name\": \"Anais White\"\\n          },\\n          {\\n            \"authorId\": \"2290488307\",\\n            \"name\": \"Jessica Austin\"\\n          },\\n          {\\n            \"authorId\": \"2290666013\",\\n            \"name\": \"Lilly Taylor\"\\n          },\\n          {\\n            \"authorId\": \"2275181309\",\\n            \"name\": \"Shereen Ashraf\"\\n          },\\n          {\\n            \"authorId\": \"2290539499\",\\n            \"name\": \"Dangyi Liu\"\\n          },\\n          {\\n            \"authorId\": \"2280669236\",\\n            \"name\": \"Maria Georgaki\"\\n          },\\n          {\\n            \"authorId\": \"2290485789\",\\n            \"name\": \"Irene Cai\"\\n          },\\n          {\\n            \"authorId\": \"2275177999\",\\n            \"name\": \"Yana Kulizhskaya\"\\n          },\\n          {\\n            \"authorId\": \"2096063076\",\\n            \"name\": \"Sonam Goenka\"\\n          },\\n          {\\n            \"authorId\": \"4125424\",\\n            \"name\": \"Brennan Saeta\"\\n          },\\n          {\\n            \"authorId\": \"4529644\",\\n            \"name\": \"Kiran Vodrahalli\"\\n          },\\n          {\\n            \"authorId\": \"2290488254\",\\n            \"name\": \"Christian Frank\"\\n          },\\n          {\\n            \"authorId\": \"47182967\",\\n            \"name\": \"D. Cesare\"\\n          },\\n          {\\n            \"authorId\": \"2275186837\",\\n            \"name\": \"Brona Robenek\"\\n          },\\n          {\\n            \"authorId\": \"2290487825\",\\n            \"name\": \"Harry Richardson\"\\n          },\\n          {\\n            \"authorId\": \"2275186342\",\\n            \"name\": \"Mah-moud Alnahlawi\"\\n          },\\n          {\\n            \"authorId\": \"2275187959\",\\n            \"name\": \"Christo-pher Yew\"\\n          },\\n          {\\n            \"authorId\": \"2275181434\",\\n            \"name\": \"Priya Ponnapalli\"\\n          },\\n          {\\n            \"authorId\": \"1749128\",\\n            \"name\": \"M. Tagliasacchi\"\\n          },\\n          {\\n            \"authorId\": \"2275188906\",\\n            \"name\": \"Alex Korchemniy\"\\n          },\\n          {\\n            \"authorId\": \"2275999038\",\\n            \"name\": \"Yelin Kim\"\\n          },\\n          {\\n            \"authorId\": \"2275195333\",\\n            \"name\": \"Dinghua Li\"\\n          },\\n          {\\n            \"authorId\": \"2080520726\",\\n            \"name\": \"B. Rosgen\"\\n          },\\n          {\\n            \"authorId\": \"2275186260\",\\n            \"name\": \"Kyle Levin\"\\n          },\\n          {\\n            \"authorId\": \"2275187022\",\\n            \"name\": \"Jeremy Wiesner\"\\n          },\\n          {\\n            \"authorId\": \"2275187506\",\\n            \"name\": \"Praseem Banzal\"\\n          },\\n          {\\n            \"authorId\": \"2275182960\",\\n            \"name\": \"Praveen Srinivasan\"\\n          },\\n          {\\n            \"authorId\": \"2254035020\",\\n            \"name\": \"Hongkun Yu\"\\n          },\\n          {\\n            \"authorId\": \"2275186671\",\\n            \"name\": \"cCauglar Unlu\"\\n          },\\n          {\\n            \"authorId\": \"2275179889\",\\n            \"name\": \"David Reid\"\\n          },\\n          {\\n            \"authorId\": \"9941702\",\\n            \"name\": \"Zora Tung\"\\n          },\\n          {\\n            \"authorId\": \"2591720\",\\n            \"name\": \"D. Finchelstein\"\\n          },\\n          {\\n            \"authorId\": \"2290629265\",\\n            \"name\": \"Ravin Kumar\"\\n          },\\n          {\\n            \"authorId\": \"2288791213\",\\n            \"name\": \"A. Elisseeff\"\\n          },\\n          {\\n            \"authorId\": \"2290557316\",\\n            \"name\": \"Jin Huang\"\\n          },\\n          {\\n            \"authorId\": \"2290594698\",\\n            \"name\": \"Ming Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2070271342\",\\n            \"name\": \"Rui Zhu\"\\n          },\\n          {\\n            \"authorId\": \"2275185644\",\\n            \"name\": \"Ricardo Aguilar\"\\n          },\\n          {\\n            \"authorId\": \"2275181171\",\\n            \"name\": \"Mai Gim\\'enez\"\\n          },\\n          {\\n            \"authorId\": \"2275552322\",\\n            \"name\": \"Jiawei Xia\"\\n          },\\n          {\\n            \"authorId\": \"2770149\",\\n            \"name\": \"Olivier Dousse\"\\n          },\\n          {\\n            \"authorId\": \"145556052\",\\n            \"name\": \"W. Gierke\"\\n          },\\n          {\\n            \"authorId\": \"1735318\",\\n            \"name\": \"S. Yeganeh\"\\n          },\\n          {\\n            \"authorId\": \"2290486731\",\\n            \"name\": \"Damion Yates\"\\n          },\\n          {\\n            \"authorId\": \"153776147\",\\n            \"name\": \"Komal Jalan\"\\n          },\\n          {\\n            \"authorId\": \"2275716550\",\\n            \"name\": \"Lu Li\"\\n          },\\n          {\\n            \"authorId\": \"2275189350\",\\n            \"name\": \"Eri Latorre-Chimoto\"\\n          },\\n          {\\n            \"authorId\": \"2112293680\",\\n            \"name\": \"D. D. Nguyen\"\\n          },\\n          {\\n            \"authorId\": \"2275187515\",\\n            \"name\": \"Ken Durden\"\\n          },\\n          {\\n            \"authorId\": \"2561675\",\\n            \"name\": \"Praveen Kallakuri\"\\n          },\\n          {\\n            \"authorId\": \"2290524803\",\\n            \"name\": \"Yaxin Liu\"\\n          },\\n          {\\n            \"authorId\": \"2275221227\",\\n            \"name\": \"Matthew Johnson\"\\n          },\\n          {\\n            \"authorId\": \"2275175372\",\\n            \"name\": \"Tomy Tsai\"\\n          },\\n          {\\n            \"authorId\": \"2275188417\",\\n            \"name\": \"Alice Talbert\"\\n          },\\n          {\\n            \"authorId\": \"2275539011\",\\n            \"name\": \"Jasmine Liu\"\\n          },\\n          {\\n            \"authorId\": \"40390373\",\\n            \"name\": \"Alexander Neitz\"\\n          },\\n          {\\n            \"authorId\": \"2217508229\",\\n            \"name\": \"C. Elkind\"\\n          },\\n          {\\n            \"authorId\": \"2269473701\",\\n            \"name\": \"Marco Selvi\"\\n          },\\n          {\\n            \"authorId\": \"2275188903\",\\n            \"name\": \"Mimi Jasarevic\"\\n          },\\n          {\\n            \"authorId\": \"2258550407\",\\n            \"name\": \"Livio Baldini Soares\"\\n          },\\n          {\\n            \"authorId\": \"7353832\",\\n            \"name\": \"Livio Baldini Soares\"\\n          },\\n          {\\n            \"authorId\": \"2164862499\",\\n            \"name\": \"Pidong Wang\"\\n          },\\n          {\\n            \"authorId\": \"2290580195\",\\n            \"name\": \"A. Wang\"\\n          },\\n          {\\n            \"authorId\": \"2181807096\",\\n            \"name\": \"Xinyu Ye\"\\n          },\\n          {\\n            \"authorId\": \"2214770531\",\\n            \"name\": \"Krystal Kallarackal\"\\n          },\\n          {\\n            \"authorId\": \"2275188993\",\\n            \"name\": \"Lucia Loher\"\\n          },\\n          {\\n            \"authorId\": \"2290486901\",\\n            \"name\": \"Hoi Lam\"\\n          },\\n          {\\n            \"authorId\": \"2290485721\",\\n            \"name\": \"Josef Broder\"\\n          },\\n          {\\n            \"authorId\": \"1404655176\",\\n            \"name\": \"D. Holtmann-Rice\"\\n          },\\n          {\\n            \"authorId\": \"2275150753\",\\n            \"name\": \"Nina Martin\"\\n          },\\n          {\\n            \"authorId\": \"2257926827\",\\n            \"name\": \"Bramandia Ramadhana\"\\n          },\\n          {\\n            \"authorId\": \"1393948967\",\\n            \"name\": \"Daniel Toyama\"\\n          },\\n          {\\n            \"authorId\": \"2290488378\",\\n            \"name\": \"Mrinal Shukla\"\\n          },\\n          {\\n            \"authorId\": \"2266467648\",\\n            \"name\": \"Sujoy Basu\"\\n          },\\n          {\\n            \"authorId\": \"2290784246\",\\n            \"name\": \"Abhi Mohan\"\\n          }\\n        ],\\n        \"abstract\": \"In this report, we introduce the Gemini 1.5 family of models, representing the next generation of highly compute-efficient multimodal models capable of recalling and reasoning over fine-grained information from millions of tokens of context, including multiple long documents and hours of video and audio. The family includes two new models: (1) an updated Gemini 1.5 Pro, which exceeds the February version on the great majority of capabilities and benchmarks; (2) Gemini 1.5 Flash, a more lightweight variant designed for efficiency with minimal regression in quality. Gemini 1.5 models achieve near-perfect recall on long-context retrieval tasks across modalities, improve the state-of-the-art in long-document QA, long-video QA and long-context ASR, and match or surpass Gemini 1.0 Ultra\\'s state-of-the-art performance across a broad set of benchmarks. Studying the limits of Gemini 1.5\\'s long-context ability, we find continued improvement in next-token prediction and near-perfect retrieval (>99%) up to at least 10M tokens, a generational leap over existing models such as Claude 3.0 (200k) and GPT-4 Turbo (128k). Finally, we highlight real-world use cases, such as Gemini 1.5 collaborating with professionals on completing their tasks achieving 26 to 75% time savings across 10 different job categories, as well as surprising new capabilities of large language models at the frontier; when given a grammar manual for Kalamang, a language with fewer than 200 speakers worldwide, the model learns to translate English to Kalamang at a similar level to a person who learned from the same content.\"\\n      },\\n      {\\n        \"paperId\": \"ac4c1c56a196523f42643f0722fe2fde776fd1fa\",\\n        \"url\": \"https://www.semanticscholar.org/paper/ac4c1c56a196523f42643f0722fe2fde776fd1fa\",\\n        \"title\": \"Context Dependant Iterative Parameter Optimisation for Robust Robot Navigation\",\\n        \"citationCount\": 17,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {\\'tldr\\'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ICRA40945.2020.9196550?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ICRA40945.2020.9196550, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": null,\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2020-05-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"144621045\",\\n            \"name\": \"Adam Binch\"\\n          },\\n          {\\n            \"authorId\": \"145785386\",\\n            \"name\": \"Gautham P. Das\"\\n          },\\n          {\\n            \"authorId\": \"3291831\",\\n            \"name\": \"J. P. Fentanes\"\\n          },\\n          {\\n            \"authorId\": \"1728609\",\\n            \"name\": \"Marc Hanheide\"\\n          }\\n        ],\\n        \"abstract\": \"Progress in autonomous mobile robotics has seen significant advances in the development of many algorithms for motion control and path planning. However, robust performance from these algorithms can often only be expected if the parameters controlling them are tuned specifically for the respective robot model, and optimised for specific scenarios in the environment the robot is working in. Such parameter tuning can, depending on the underlying algorithm, amount to a substantial combinatorial challenge, often rendering extensive manual tuning of these parameters intractable. In this paper, we present a framework that permits the use of different navigation actions and/or parameters depending on the spatial context of the navigation task. We consider the respective navigation algorithms themselves mostly as a \\\\\"black box\\\\\", and find suitable parameters by means of an iterative optimisation, improving for performance metrics in simulated environments. We present a genetic algorithm incorporated into the framework, and empirically show that the resulting parameter sets lead to substantial performance improvements in both simulated and real-world environments in the domain of agricultural robots.\"\\n      },\\n      {\\n        \"paperId\": \"1d82f61ee52331a94141a50b59b186ccf105f6a9\",\\n        \"url\": \"https://www.semanticscholar.org/paper/1d82f61ee52331a94141a50b59b186ccf105f6a9\",\\n        \"title\": \"Building Math Agents with Multi-Turn Iterative Preference Learning\",\\n        \"citationCount\": 58,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2409.02392, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work introduces a multi-turn direct preference learning framework, tailored for this context, that leverages feedback from code interpreters and optimizes trajectory-level preferences and includes multi-turn DPO and multi-turn KTO as specific implementations.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-09-04\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2275119437\",\\n            \"name\": \"Wei Xiong\"\\n          },\\n          {\\n            \"authorId\": \"1382573171\",\\n            \"name\": \"Chengshuai Shi\"\\n          },\\n          {\\n            \"authorId\": \"2266463492\",\\n            \"name\": \"Jiaming Shen\"\\n          },\\n          {\\n            \"authorId\": \"2302798001\",\\n            \"name\": \"Aviv Rosenberg\"\\n          },\\n          {\\n            \"authorId\": \"2266819166\",\\n            \"name\": \"Zhen Qin\"\\n          },\\n          {\\n            \"authorId\": \"2439765\",\\n            \"name\": \"Daniele Calandriello\"\\n          },\\n          {\\n            \"authorId\": \"2140488873\",\\n            \"name\": \"Misha Khalman\"\\n          },\\n          {\\n            \"authorId\": \"2258551072\",\\n            \"name\": \"Rishabh Joshi\"\\n          },\\n          {\\n            \"authorId\": \"1808897\",\\n            \"name\": \"Bilal Piot\"\\n          },\\n          {\\n            \"authorId\": \"2316576401\",\\n            \"name\": \"Mohammad Saleh\"\\n          },\\n          {\\n            \"authorId\": \"2319960151\",\\n            \"name\": \"Chi Jin\"\\n          },\\n          {\\n            \"authorId\": \"2301173100\",\\n            \"name\": \"Tong Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2239381730\",\\n            \"name\": \"Tianqi Liu\"\\n          }\\n        ],\\n        \"abstract\": \"Recent studies have shown that large language models\\' (LLMs) mathematical problem-solving capabilities can be enhanced by integrating external tools, such as code interpreters, and employing multi-turn Chain-of-Thought (CoT) reasoning. While current methods focus on synthetic data generation and Supervised Fine-Tuning (SFT), this paper studies the complementary direct preference learning approach to further improve model performance. However, existing direct preference learning algorithms are originally designed for the single-turn chat task, and do not fully address the complexities of multi-turn reasoning and external tool integration required for tool-integrated mathematical reasoning tasks. To fill in this gap, we introduce a multi-turn direct preference learning framework, tailored for this context, that leverages feedback from code interpreters and optimizes trajectory-level preferences. This framework includes multi-turn DPO and multi-turn KTO as specific implementations. The effectiveness of our framework is validated through training of various language models using an augmented prompt set from the GSM8K and MATH datasets. Our results demonstrate substantial improvements: a supervised fine-tuned Gemma-1.1-it-7B model\\'s performance increased from 77.5% to 83.9% on GSM8K and from 46.1% to 51.2% on MATH. Similarly, a Gemma-2-it-9B model improved from 84.1% to 86.3% on GSM8K and from 51.0% to 54.5% on MATH.\"\\n      },\\n      {\\n        \"paperId\": \"a8176838a636651324b9bac1b3443c803b44e1b3\",\\n        \"url\": \"https://www.semanticscholar.org/paper/a8176838a636651324b9bac1b3443c803b44e1b3\",\\n        \"title\": \"Online Iterative Reinforcement Learning from Human Feedback with General Preference Model\",\\n        \"citationCount\": 29,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2402.07314, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work considers a standard mathematical formulation, the reverse-KL regularized minimax game between two LLMs for RLHF under general preference oracle, and proposes sample-efficient algorithms for both the offline learning from a pre-collected preference dataset and online learning where the preference oracle is queried along the way of training.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-02-11\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2105550407\",\\n            \"name\": \"Chen Ye\"\\n          },\\n          {\\n            \"authorId\": \"2275119437\",\\n            \"name\": \"Wei Xiong\"\\n          },\\n          {\\n            \"authorId\": \"2283877837\",\\n            \"name\": \"Yuheng Zhang\"\\n          },\\n          {\\n            \"authorId\": \"35279146\",\\n            \"name\": \"Hanze Dong\"\\n          },\\n          {\\n            \"authorId\": \"2275167899\",\\n            \"name\": \"Nan Jiang\"\\n          },\\n          {\\n            \"authorId\": \"2260473361\",\\n            \"name\": \"Tong Zhang\"\\n          }\\n        ],\\n        \"abstract\": \"We investigate Reinforcement Learning from Human Feedback (RLHF) in the context of a general preference oracle. In particular, we do not assume the existence of a reward function and an oracle preference signal drawn from the Bradley-Terry model as most of the prior works do. We consider a standard mathematical formulation, the reverse-KL regularized minimax game between two LLMs for RLHF under general preference oracle. The learning objective of this formulation is to find a policy so that it is consistently preferred by the KL-regularized preference oracle over any competing LLMs. We show that this framework is strictly more general than the reward-based one, and propose sample-efficient algorithms for both the offline learning from a pre-collected preference dataset and online learning where we can query the preference oracle along the way of training. Empirical studies verify the effectiveness of the proposed framework.\"\\n      },\\n      {\\n        \"paperId\": \"0145bde24bb4e8482c2e69dd28efe5ead1fe7d8c\",\\n        \"url\": \"https://www.semanticscholar.org/paper/0145bde24bb4e8482c2e69dd28efe5ead1fe7d8c\",\\n        \"title\": \"Generalized Iterative Bayesian Update and Applications to Mechanisms for Privacy Protection\",\\n        \"citationCount\": 13,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {\\'tldr\\'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/EuroSP48549.2020.00038?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/EuroSP48549.2020.00038, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": null,\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2020-09-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2292169\",\\n            \"name\": \"Ehab ElSalamouny\"\\n          },\\n          {\\n            \"authorId\": \"1722055\",\\n            \"name\": \"C. Palamidessi\"\\n          }\\n        ],\\n        \"abstract\": \"The iterative Bayesian update (IBU) and the matrix inversion (INV) are the main methods to retrieve the original distribution from noisy data resulting from the application of privacy protection mechanisms. We show that the foundations of IBU established in the literature are flawed, as they rely on an assumption that in general is not satisfied in typical datasets. We then propose an extension of the method, covering a more general privacy model, where different users are allowed to apply different privacy mechanisms. We call our algorithm GIBU, for Generalized IBU, and we prove its convergence to the maximum likelihood estimate, constructing a proof that does not rely on the problematic assumption, thus fixing also the theory of IBU. Finally we evaluate the precision of GIBU on data sanitized with k-RR, Rappor, geo-indistinguishability and exponential mechanisms. We show that, while GIBU and INV are comparable in the first two cases, the performance of GIBU is definitely superior in the latter cases.\"\\n      },\\n      {\\n        \"paperId\": \"7d49f330d114ec3d65ffb67eb9dc79aea7b8befc\",\\n        \"url\": \"https://www.semanticscholar.org/paper/7d49f330d114ec3d65ffb67eb9dc79aea7b8befc\",\\n        \"title\": \"Spontaneous Reward Hacking in Iterative Self-Refinement\",\\n        \"citationCount\": 15,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2407.04549, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"Using an essay editing task, it is shown that iterative self-refinement leads to deviation between the language model evaluator and human judgment, demonstrating that reward hacking can occur spontaneously in-context with the use of iterative self-refinement.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-07-05\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2311112294\",\\n            \"name\": \"Jane Pan\"\\n          },\\n          {\\n            \"authorId\": \"2321875898\",\\n            \"name\": \"He He\"\\n          },\\n          {\\n            \"authorId\": \"2297768298\",\\n            \"name\": \"Samuel R. Bowman\"\\n          },\\n          {\\n            \"authorId\": \"2297816489\",\\n            \"name\": \"Shi Feng\"\\n          }\\n        ],\\n        \"abstract\": \"Language models are capable of iteratively improving their outputs based on natural language feedback, thus enabling in-context optimization of user preference. In place of human users, a second language model can be used as an evaluator, providing feedback along with numerical ratings which the generator attempts to optimize. However, because the evaluator is an imperfect proxy of user preference, this optimization can lead to reward hacking, where the evaluator\\'s ratings improve while the generation quality remains stagnant or even decreases as judged by actual user preference. The concern of reward hacking is heightened in iterative self-refinement where the generator and the evaluator use the same underlying language model, in which case the optimization pressure can drive them to exploit shared vulnerabilities. Using an essay editing task, we show that iterative self-refinement leads to deviation between the language model evaluator and human judgment, demonstrating that reward hacking can occur spontaneously in-context with the use of iterative self-refinement. In addition, we study conditions under which reward hacking occurs and observe two factors that affect reward hacking severity: model size and context sharing between the generator and the evaluator.\"\\n      },\\n      {\\n        \"paperId\": \"21ed545a3b02bbcba6a62f5858bca948c2ed4641\",\\n        \"url\": \"https://www.semanticscholar.org/paper/21ed545a3b02bbcba6a62f5858bca948c2ed4641\",\\n        \"title\": \"Explainable Benchmarking for Iterative Optimization Heuristics\",\\n        \"citationCount\": 19,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://arxiv.org/pdf/2401.17842\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2401.17842, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"The IOHxplainer software library is introduced, for systematic analysing the performance of various optimization algorithms and the impact of their different components and hyperparameters, aiming to improve future benchmarking and algorithm design practices.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-01-31\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2218156728\",\\n            \"name\": \"N. V. Stein\"\\n          },\\n          {\\n            \"authorId\": \"108119673\",\\n            \"name\": \"Diederick Vermetten\"\\n          },\\n          {\\n            \"authorId\": \"3160375\",\\n            \"name\": \"Anna V. Kononova\"\\n          },\\n          {\\n            \"authorId\": \"2237990304\",\\n            \"name\": \"T. Back\"\\n          }\\n        ],\\n        \"abstract\": \"Benchmarking heuristic algorithms is vital to understand under which conditions and on what kind of problems certain algorithms perform well. In most current research into heuristic optimization algorithms, only a very limited number of scenarios, algorithm configurations and hyper-parameter settings are explored, leading to incomplete and often biased insights and results. This article presents a novel approach that we call explainable benchmarking. We introduce the IOHxplainer software library, for systematic analysing the performance of various optimization algorithms and the impact of their different components and hyperparameters. We showcase the methodology in the context of two modular optimization implementations. Through this library, we examine the impact of different algorithmic components and configurations, offering insights into their performance across diverse scenarios. We provide a systematic method for evaluating and interpreting the behaviour and efficiency of iterative optimization heuristics in a more transparent and comprehensible manner, aiming to improve future benchmarking and algorithm design practices.\"\\n      },\\n      {\\n        \"paperId\": \"0019cb24ec04498836b8215e9495b968f2c01666\",\\n        \"url\": \"https://www.semanticscholar.org/paper/0019cb24ec04498836b8215e9495b968f2c01666\",\\n        \"title\": \"IGEV++: Iterative Multi-Range Geometry Encoding Volumes for Stereo Matching\",\\n        \"citationCount\": 41,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2409.00638, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper introduces an adaptive patch matching module that efficiently and effectively computes matching costs for large disparity ranges and/or ill-posed regions, and proposes a selective geometry feature fusion module to adaptively fuse multi-range and multi-granularity geometry features in MGEV.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-09-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2158317969\",\\n            \"name\": \"Gangwei Xu\"\\n          },\\n          {\\n            \"authorId\": \"2289775621\",\\n            \"name\": \"Xianqi Wang\"\\n          },\\n          {\\n            \"authorId\": \"2319408912\",\\n            \"name\": \"Zhaoxing Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2049033628\",\\n            \"name\": \"Junda Cheng\"\\n          },\\n          {\\n            \"authorId\": \"2319622516\",\\n            \"name\": \"Chunyuan Liao\"\\n          },\\n          {\\n            \"authorId\": \"2265575132\",\\n            \"name\": \"Xin Yang\"\\n          }\\n        ],\\n        \"abstract\": \"Stereo matching is a core component in many computer vision and robotics systems. Despite significant advances over the last decade, handling matching ambiguities in ill-posed regions and large disparities remains an open challenge. In this paper, we propose a new deep network architecture, called IGEV++, for stereo matching. The proposed IGEV++ constructs Multi-range Geometry Encoding Volumes (MGEV), which encode coarse-grained geometry information for ill-posed regions and large disparities, while preserving fine-grained geometry information for details and small disparities. To construct MGEV, we introduce an adaptive patch matching module that efficiently and effectively computes matching costs for large disparity ranges and/or ill-posed regions. We further propose a selective geometry feature fusion module to adaptively fuse multi-range and multi-granularity geometry features in MGEV. Then, we input the fused geometry features into ConvGRUs to iteratively update the disparity map. MGEV allows to efficiently handle large disparities and ill-posed regions, such as occlusions and textureless regions, and enjoys rapid convergence during iterations. Our IGEV++ achieves the best performance on the Scene Flow test set across all disparity ranges, up to 768px. Our IGEV++ also achieves state-of-the-art accuracy on the Middlebury, ETH3D, KITTI 2012, and 2015 benchmarks. Specifically, IGEV++ achieves a 3.23% 2-pixel outlier rate (Bad 2.0) on the large disparity benchmark, Middlebury, representing error reductions of 31.9% and 54.8% compared to RAFT-Stereo and GMStereo, respectively. We also present a real-time version of IGEV++ that achieves the best performance among all published real-time methods on the KITTI benchmarks.\"\\n      },\\n      {\\n        \"paperId\": \"f305fddaf0aa92cd19b8714db6ef67ade6ece1ee\",\\n        \"url\": \"https://www.semanticscholar.org/paper/f305fddaf0aa92cd19b8714db6ef67ade6ece1ee\",\\n        \"title\": \"De-Hallucinator: Iterative Grounding for LLM-Based Code Completion\",\\n        \"citationCount\": 29,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {\\'abstract\\'}. Paper or abstract available at https://api.unpaywall.org/v2/10.48550/arXiv.2401.01701?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.48550/arXiv.2401.01701, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"De-Hallucinator is presented, an LLM-based code completion technique that grounds the predictions of a model through a novel combination of retrieving suitable API references and iteratively querying the model with increasingly suitable context information in the prompt.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": null,\\n        \"authors\": [\\n          {\\n            \"authorId\": \"46235560\",\\n            \"name\": \"A. Eghbali\"\\n          },\\n          {\\n            \"authorId\": \"2282536979\",\\n            \"name\": \"Michael Pradel\"\\n          }\\n        ],\\n        \"abstract\": null\\n      }\\n    ]\\n  },\\n  \"thematic coherence prompting\": {\\n    \"total\": 692,\\n    \"offset\": 0,\\n    \"next\": 50,\\n    \"data\": [\\n      {\\n        \"paperId\": \"f50be995ae7bc8b5889df68be77bf89d1d253a50\",\\n        \"url\": \"https://www.semanticscholar.org/paper/f50be995ae7bc8b5889df68be77bf89d1d253a50\",\\n        \"title\": \"Supporting best practice in reflexive thematic analysis reporting in Palliative Medicine: A review of published research and introduction to the Reflexive Thematic Analysis Reporting Guidelines (RTARG)\",\\n        \"citationCount\": 350,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://journals.sagepub.com/doi/pdf/10.1177/02692163241234800\",\\n          \"status\": \"HYBRID\",\\n          \"license\": \"CCBY\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC11157981, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"The Reflexive Thematic Analysis Reporting Guidelines (the RTARG) are developed, informed by this review, other reviews the authors have done and their values and experience as qualitative researchers, to support qualitative researchers in producing coherent and reflexively open reports of reflexive thematic analysis.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Review\"\\n        ],\\n        \"publicationDate\": \"2024-03-12\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2290884456\",\\n            \"name\": \"Virginia Braun\"\\n          },\\n          {\\n            \"authorId\": \"2287216787\",\\n            \"name\": \"Victoria Clarke\"\\n          }\\n        ],\\n        \"abstract\": \"Background: Reflexive thematic analysis is widely used in qualitative research published in Palliative Medicine, and in the broader field of health research. However, this approach is often not used well. Common problems in published reflexive thematic analysis in general include assuming thematic analysis is a singular approach, rather than a family of methods, confusing themes and topics, and treating and reporting reflexive thematic analysis as if it is atheoretical. Purpose: We reviewed 20 papers published in Palliative Medicine between 2014 and 2022 that cited Braun and Clarke, identified using the search term \\\\u2018thematic analysis\\\\u2019 and the default \\\\u2018relevance\\\\u2019 setting on the journal webpage. The aim of the review was to identify common problems and instances of good practice. Problems centred around a lack of methodological coherence, and a lack of reflexive openness, clarity and detail in reporting. We considered contributors to these common problems, including the use of reporting checklists that are not coherent with the values of reflexive thematic analysis. To support qualitative researchers in producing coherent and reflexively open reports of reflexive thematic analysis we have developed the Reflexive Thematic Analysis Reporting Guidelines (the RTARG; in Supplemental Materials) informed by this review, other reviews we have done and our values and experience as qualitative researchers. The RTARG is also intended for use by peer reviewers to encourage methodologically coherent reviewing. Key learning points: Methodological incoherence and a lack of transparency are common problems in reflexive thematic analysis research published in Palliative Medicine. Coherence can be facilitated by researchers and reviewers striving to be knowing \\\\u2013 thoughtful, deliberative, reflexive and theoretically aware \\\\u2013 practitioners and appraisers of reflexive thematic analysis and developing an understanding of the diversity within the thematic analysis family of methods.\"\\n      },\\n      {\\n        \"paperId\": \"8b12b59aef23566a7a3461f69230c83ab632a918\",\\n        \"url\": \"https://www.semanticscholar.org/paper/8b12b59aef23566a7a3461f69230c83ab632a918\",\\n        \"title\": \"Weak central coherence in adults with ASD: Evidence from eye-tracking and thematic content analysis of social scenes\",\\n        \"citationCount\": 11,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://figshare.com/articles/journal_contribution/Weak_central_coherence_in_adults_with_ASD_Evidence_from_eye-tracking_and_thematic_content_analysis_of_social_scenes/19633669/1/files/34870696.pdf\",\\n          \"status\": \"GREEN\",\\n          \"license\": \"CCBY\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1080/23279095.2022.2060105?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1080/23279095.2022.2060105, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": null,\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2021-08-24\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"1677019029\",\\n            \"name\": \"S. Tassini\"\\n          },\\n          {\\n            \"authorId\": \"39763547\",\\n            \"name\": \"M. C. Melo\"\\n          },\\n          {\\n            \"authorId\": \"2922008\",\\n            \"name\": \"O. Bueno\"\\n          },\\n          {\\n            \"authorId\": \"2222542539\",\\n            \"name\": \"Claudia Berlim de Mello\"\\n          }\\n        ],\\n        \"abstract\": \"Abstract Central Coherence Weakness has been defined as a tendency for local rather than global processing that may underlie core deficits in Autism Spectrum Disorder (ASD). In social contexts it may be expressed in difficulties to integrate social cues arising from the recognition of emotions in faces or from the environment in order to understand people\\'s interactions. A sample of 28 adults diagnosed with ASD Level 1 and 25 controls was submitted to a cartoon-like task with the instruction to describe social scenes and to Navon letter stimuli. Both quantitative measures and qualitative (thematic content analysis) procedures were used to assess performance. Heatmap and fixation preferences according to the stimuli quadrants were used to investigate eye-tracking patterns. A tendency to local processing, independently of the stimuli type, in the ASD participants was observed. Data from visual tracking by quadrants and from verbal reports suggest loss of social cues important for understanding context. Their reaction time and response duration were increased in relation to controls. The findings corroborate the idea that weak central coherence may be part of the cognitive phenotype in ASD. LAY ABSTRACT Autistic adults often report difficulties in interpreting social situations. These difficulties are commonly associated with a tendency to visually focus on specific parts of the situation (known as local processing) to the detriment of the whole situation. This way of looking at things has been given the name \\\\u201cweak central coherence,\\\\u201d and may result in difficulties in understanding a situation or other people\\\\u2019s behaviors. A group of ASD and controls were asked to describe two different types of image, one showing a common social situation, the other Navon figure. Eye-tracking technology was used to analyze how the participants looked at the images (which part of the image and for how long) and asked about what they had seen. The results showed that in the group of autistic participants there was a tendency to focus on the details in both types of images. The analysis of the verbal reports revealed that the interpretation of the social contexts by those with ASD was not what was expected, associated with a specific focus on details. These findings may be useful for a better understanding of some difficulties experienced by ASD in social contexts and contribute to therapeutic treatments.\"\\n      },\\n      {\\n        \"paperId\": \"fff1e9eb32f8591aea3f2db54035465093381b89\",\\n        \"url\": \"https://www.semanticscholar.org/paper/fff1e9eb32f8591aea3f2db54035465093381b89\",\\n        \"title\": \"Selective Prompting Tuning for Personalized Conversations with LLMs\",\\n        \"citationCount\": 13,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2406.18187, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work initializes a set of soft prompts and uses a trainable dense retriever to adaptively select suitable soft prompts for LLMs according to different input contexts, where the prompt retriever is dynamically updated through feedback from the LLMs.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-06-26\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2274015208\",\\n            \"name\": \"Qiushi Huang\"\\n          },\\n          {\\n            \"authorId\": \"2110814131\",\\n            \"name\": \"Xubo Liu\"\\n          },\\n          {\\n            \"authorId\": \"2273320275\",\\n            \"name\": \"Tom Ko\"\\n          },\\n          {\\n            \"authorId\": \"49814531\",\\n            \"name\": \"Boyong Wu\"\\n          },\\n          {\\n            \"authorId\": \"2239051433\",\\n            \"name\": \"Wenwu Wang\"\\n          },\\n          {\\n            \"authorId\": \"2273525536\",\\n            \"name\": \"Yu Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2189113746\",\\n            \"name\": \"Lilian Tang\"\\n          }\\n        ],\\n        \"abstract\": \"In conversational AI, personalizing dialogues with persona profiles and contextual understanding is essential. Despite large language models\\' (LLMs) improved response coherence, effective persona integration remains a challenge. In this work, we first study two common approaches for personalizing LLMs: textual prompting and direct fine-tuning. We observed that textual prompting often struggles to yield responses that are similar to the ground truths in datasets, while direct fine-tuning tends to produce repetitive or overly generic replies. To alleviate those issues, we propose \\\\\\\\textbf{S}elective \\\\\\\\textbf{P}rompt \\\\\\\\textbf{T}uning (SPT), which softly prompts LLMs for personalized conversations in a selective way. Concretely, SPT initializes a set of soft prompts and uses a trainable dense retriever to adaptively select suitable soft prompts for LLMs according to different input contexts, where the prompt retriever is dynamically updated through feedback from the LLMs. Additionally, we propose context-prompt contrastive learning and prompt fusion learning to encourage the SPT to enhance the diversity of personalized conversations. Experiments on the CONVAI2 dataset demonstrate that SPT significantly enhances response diversity by up to 90\\\\\\\\%, along with improvements in other critical performance indicators. Those results highlight the efficacy of SPT in fostering engaging and personalized dialogue generation. The SPT model code (https://github.com/hqsiswiliam/SPT) is publicly available for further exploration.\"\\n      },\\n      {\\n        \"paperId\": \"71e294c3b503f141b11cad8e3ebc033a667d2b3b\",\\n        \"url\": \"https://www.semanticscholar.org/paper/71e294c3b503f141b11cad8e3ebc033a667d2b3b\",\\n        \"title\": \"Exploring Learner Prompting Behavior and Its Effect on ChatGPT-Assisted English Writing Revision\",\\n        \"citationCount\": 12,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {\\'abstract\\'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1007/s40299-024-00930-6?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1007/s40299-024-00930-6, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"Findings underscore the necessity of incorporating prompt-writing instruction into English writing pedagogy to enhance learners\\\\u2019 ability to craft specific, goal-aligned prompts, leading to more productive feedback from AI tools like ChatGPT and facilitating meaningful improvements in higher-order writing skills.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-10-21\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"116938583\",\\n            \"name\": \"M. Hwang\"\\n          },\\n          {\\n            \"authorId\": \"121076013\",\\n            \"name\": \"Robert D. Jeens\"\\n          },\\n          {\\n            \"authorId\": \"2314224171\",\\n            \"name\": \"Hee-Kyung Lee\"\\n          }\\n        ],\\n        \"abstract\": null\\n      },\\n      {\\n        \"paperId\": \"b908824639d18f11883abcab21efeb22e315ab9c\",\\n        \"url\": \"https://www.semanticscholar.org/paper/b908824639d18f11883abcab21efeb22e315ab9c\",\\n        \"title\": \"Multimodal Procedural Planning via Dual Text-Image Prompting\",\\n        \"citationCount\": 51,\\n        \"openAccessPdf\": {\\n          \"url\": \"http://arxiv.org/pdf/2305.01795\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2305.01795, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"Text-Image Prompting (TIP) is proposed, a dual-modality prompting method that jointly leverages zero-shot reasoning ability in large language models (LLMs) and compelling text-to-image generation ability from diffusion-based models to tackle the key challenges of MPP.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2023-05-02\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"47006228\",\\n            \"name\": \"Yujie Lu\"\\n          },\\n          {\\n            \"authorId\": \"2887562\",\\n            \"name\": \"Pan Lu\"\\n          },\\n          {\\n            \"authorId\": \"2142370346\",\\n            \"name\": \"Zhiyu Chen\"\\n          },\\n          {\\n            \"authorId\": \"51439692\",\\n            \"name\": \"Wanrong Zhu\"\\n          },\\n          {\\n            \"authorId\": \"47120131\",\\n            \"name\": \"X. Wang\"\\n          },\\n          {\\n            \"authorId\": \"1682479\",\\n            \"name\": \"William Yang Wang\"\\n          }\\n        ],\\n        \"abstract\": \"Embodied agents have achieved prominent performance in following human instructions to complete tasks. However, the potential of providing instructions informed by texts and images to assist humans in completing tasks remains underexplored. To uncover this capability, we present the multimodal procedural planning (MPP) task, in which models are given a high-level goal and generate plans of paired text-image steps, providing more complementary and informative guidance than unimodal plans. The key challenges of MPP are to ensure the informativeness, temporal coherence,and accuracy of plans across modalities. To tackle this, we propose Text-Image Prompting (TIP), a dual-modality prompting method that jointly leverages zero-shot reasoning ability in large language models (LLMs) and compelling text-to-image generation ability from diffusion-based models. TIP improves the interaction in the dual modalities using Text-to-Image Bridge and Image-to-Text Bridge, allowing LLMs to guide the textual-grounded image plan generation and leveraging the descriptions of image plans to ground the textual plan reversely. To address the lack of relevant datasets, we collect WIKIPLAN and RECIPEPLAN as a testbed for MPP. Our results show compelling human preferences and automatic scores against unimodal and multimodal baselines on WIKIPLAN and RECIPEPLAN in terms of informativeness, temporal coherence, and plan accuracy. Our code and data: https://github.com/YujieLu10/MPP.\"\\n      },\\n      {\\n        \"paperId\": \"5e490270c0fd49418e6bc5732c375867f253a20f\",\\n        \"url\": \"https://www.semanticscholar.org/paper/5e490270c0fd49418e6bc5732c375867f253a20f\",\\n        \"title\": \"Fine-Grained Modeling of Narrative Context: A Coherence Perspective via Retrospective Questions\",\\n        \"citationCount\": 10,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://arxiv.org/pdf/2402.13551\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2402.13551, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work introduces an original and practical paradigm for narrative comprehension, stemming from the characteristics that individual passages within narratives tend to be more cohesively related than isolated, by formulating a graph dubbed NarCo, which explicitly depicts task-agnostic coherence dependencies that are ready to be consumed by various downstream tasks.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-02-21\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2276317005\",\\n            \"name\": \"Liyan Xu\"\\n          },\\n          {\\n            \"authorId\": \"153154545\",\\n            \"name\": \"JiangNan Li\"\\n          },\\n          {\\n            \"authorId\": \"2265525656\",\\n            \"name\": \"Mo Yu\"\\n          },\\n          {\\n            \"authorId\": \"2283871195\",\\n            \"name\": \"Jie Zhou\"\\n          }\\n        ],\\n        \"abstract\": \"This work introduces an original and practical paradigm for narrative comprehension, stemming from the characteristics that individual passages within narratives tend to be more cohesively related than isolated. Complementary to the common end-to-end paradigm, we propose a fine-grained modeling of narrative context, by formulating a graph dubbed NarCo, which explicitly depicts task-agnostic coherence dependencies that are ready to be consumed by various downstream tasks. In particular, edges in NarCo encompass free-form retrospective questions between context snippets, inspired by human cognitive perception that constantly reinstates relevant events from prior context. Importantly, our graph formalism is practically instantiated by LLMs without human annotations, through our designed two-stage prompting scheme. To examine the graph properties and its utility, we conduct three studies in narratives, each from a unique angle: edge relation efficacy, local context enrichment, and broader application in QA. All tasks could benefit from the explicit coherence captured by NarCo.\"\\n      },\\n      {\\n        \"paperId\": \"e47bce9eaa7d0cce48c64d614f11105d48fb3881\",\\n        \"url\": \"https://www.semanticscholar.org/paper/e47bce9eaa7d0cce48c64d614f11105d48fb3881\",\\n        \"title\": \"Conceptual and design thinking for thematic analysis\",\\n        \"citationCount\": 2681,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {\\'abstract\\'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1037/QUP0000196?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1037/QUP0000196, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": null,\\n        \"publicationTypes\": null,\\n        \"publicationDate\": \"2021-02-22\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"40465786\",\\n            \"name\": \"Virginia Braun\"\\n          },\\n          {\\n            \"authorId\": \"145881640\",\\n            \"name\": \"Victoria Clarke\"\\n          }\\n        ],\\n        \"abstract\": null\\n      },\\n      {\\n        \"paperId\": \"fb449d114d7d3550fb2fff424be6b5d8434d7924\",\\n        \"url\": \"https://www.semanticscholar.org/paper/fb449d114d7d3550fb2fff424be6b5d8434d7924\",\\n        \"title\": \"Prompting Large Language Models for Topic Modeling\",\\n        \"citationCount\": 45,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2312.09693, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper proposes PromptTopic, a novel topic modeling approach that harnesses the advanced language understanding of large language models (LLMs) to address challenges of short text datasets that lack co-occurring words.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2023-12-15\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2262111802\",\\n            \"name\": \"Han Wang\"\\n          },\\n          {\\n            \"authorId\": \"2218633063\",\\n            \"name\": \"Nirmalendu Prakash\"\\n          },\\n          {\\n            \"authorId\": \"2261746844\",\\n            \"name\": \"N. Hoang\"\\n          },\\n          {\\n            \"authorId\": \"72043108\",\\n            \"name\": \"Ming Shan Hee\"\\n          },\\n          {\\n            \"authorId\": \"2339777541\",\\n            \"name\": \"Usman Naseem\"\\n          },\\n          {\\n            \"authorId\": \"2266006388\",\\n            \"name\": \"Roy Ka-Wei Lee\"\\n          }\\n        ],\\n        \"abstract\": \"Topic modeling is a widely used technique for revealing underlying thematic structures within textual data. However, existing models have certain limitations, particularly when dealing with short text datasets that lack co-occurring words. Moreover, these models often neglect sentence-level semantics, focusing primarily on token-level semantics. In this paper, we propose PromptTopic, a novel topic modeling approach that harnesses the advanced language understanding of large language models (LLMs) to address these challenges. It involves extracting topics at the sentence level from individual documents, then aggregating and condensing these topics into a predefined quantity, ultimately providing coherent topics for texts of varying lengths. This approach eliminates the need for manual parameter tuning and improves the quality of extracted topics. We benchmark PromptTopic against the state-of-the-art baselines on three vastly diverse datasets, establishing its proficiency in discovering meaningful topics. Furthermore, qualitative analysis showcases PromptTopic\\\\u2019s ability to uncover relevant topics in multiple datasets.\"\\n      },\\n      {\\n        \"paperId\": \"3f5bc00fb6d1de56e3a4902df74ef305adfe4dcd\",\\n        \"url\": \"https://www.semanticscholar.org/paper/3f5bc00fb6d1de56e3a4902df74ef305adfe4dcd\",\\n        \"title\": \"Improved prompting and process for writing user personas with LLMs, using qualitative interviews: Capturing behaviour and personality traits of users\",\\n        \"citationCount\": 13,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://arxiv.org/pdf/2310.06391\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2310.06391, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This draft paper presents a workflow for creating User Personas with Large Language Models, using the results of a Thematic Analysis of qualitative interviews, using a recently released LLM which allows the processing of 16 thousand tokens and the possibility to offer a refined prompting for the creation of Personas.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2023-10-10\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"1774708\",\\n            \"name\": \"S. Paoli\"\\n          }\\n        ],\\n        \"abstract\": \"This draft paper presents a workflow for creating User Personas with Large Language Models, using the results of a Thematic Analysis of qualitative interviews. The proposed workflow uses improved prompting and a larger pool of Themes, compared to previous work conducted by the author for the same task. This is possible due to the capabilities of a recently released LLM which allows the processing of 16 thousand tokens (GPT3.5-Turbo-16k) and also due to the possibility to offer a refined prompting for the creation of Personas. The paper offers details of performing Phase 2 and 3 of Thematic Analysis, and then discusses the improved workflow for creating Personas. The paper also offers some reflections on the relationship between the proposed process and existing approaches to Personas such as the data-driven and qualitative Personas. Moreover, the paper offers reflections on the capacity of LLMs to capture user behaviours and personality traits, from the underlying dataset of qualitative interviews used for the analysis.\"\\n      },\\n      {\\n        \"paperId\": \"234e31a541d5d4d1f64497cdd98dfde5131a342a\",\\n        \"url\": \"https://www.semanticscholar.org/paper/234e31a541d5d4d1f64497cdd98dfde5131a342a\",\\n        \"title\": \"BERTopic Modeling of Natural Language Processing Abstracts: Thematic Structure and Trajectory\",\\n        \"citationCount\": 16,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://doi.org/10.30865/mib.v7i3.6426\",\\n          \"status\": \"GOLD\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.30865/mib.v7i3.6426?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.30865/mib.v7i3.6426, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This research aimed to apply unsupervised clustering techniques to 13,027 Scopus abstracts to uncover structure and themes in natural language processing (NLP) publications to uncover structure and themes in research trends.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2023-07-31\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2091271718\",\\n            \"name\": \"S. Samsir\"\\n          },\\n          {\\n            \"authorId\": \"2313547470\",\\n            \"name\": \"Reagan Surbakti Saragih\"\\n          },\\n          {\\n            \"authorId\": \"2313548926\",\\n            \"name\": \"S. Subagio\"\\n          },\\n          {\\n            \"authorId\": \"2200502353\",\\n            \"name\": \"Rahmad Aditiya\"\\n          },\\n          {\\n            \"authorId\": \"2091271983\",\\n            \"name\": \"Ronal Watrianthos\"\\n          }\\n        ],\\n        \"abstract\": \"The rapid growth in the academic literature presents challenges in identifying relevant studies. This research aimed to apply unsupervised clustering techniques to 13,027 Scopus abstracts to uncover structure and themes in natural language processing (NLP) publications. Abstracts were pre-processed with tokenization, lemmatization, and vectorization. The BERTopic algorithm was used for clustering, using the MiniLM-L6-v2 embedding model and a minimum topic size of 50. Quantitative analysis revealed eight main topics, with sizes ranging from 205 to 4089 abstracts per topic. The language models topic was most prominent with 4089 abstracts. The topics were evaluated using coherence scores between 0.42 and 0.58, indicating meaningful themes. Keywords and sample documents provided interpretable topic representations. The results showcase the ability to produce coherent topics and capture connections between NLP studies. Clustering supports focused browsing and identification of relevant literature. Unlike human-curated classifications, the unsupervised data-driven approach prevents bias. Given the need to understand research trends, clustering abstracts enables efficient knowledge discovery from scientific corpora. This methodology can be applied to various datasets and fields to uncover overlooked patterns. The ability to adjust parameters allows for customized analysis. In general, unsupervised clustering provides a versatile framework for navigating, summarizing, and analyzing academic literature as volumes expand exponentially.\"\\n      },\\n      {\\n        \"paperId\": \"baeb8b5869eb94a2e43ea8cd0df3b8077ec8e91c\",\\n        \"url\": \"https://www.semanticscholar.org/paper/baeb8b5869eb94a2e43ea8cd0df3b8077ec8e91c\",\\n        \"title\": \"SAM-OCTA: Prompting Segment-Anything for OCTA Image Segmentation\",\\n        \"citationCount\": 10,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://arxiv.org/pdf/2310.07183\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2310.07183, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"From comprehensive experimental results with the OCTA-500 dataset, the SAM-OCTA method has achieved state-of-the-art performance in common OCTA segmentation tasks related to RV and FAZ, and it also performs accurate segmentation of artery-vein and local vessels.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2023-10-11\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2243298334\",\\n            \"name\": \"Xinrun Chen\"\\n          },\\n          {\\n            \"authorId\": \"2243437545\",\\n            \"name\": \"Chengliang Wang\"\\n          },\\n          {\\n            \"authorId\": \"2216447644\",\\n            \"name\": \"Haojian Ning\"\\n          },\\n          {\\n            \"authorId\": \"2257137905\",\\n            \"name\": \"Shiying Li\"\\n          }\\n        ],\\n        \"abstract\": \"Segmenting specific targets or biomarkers is necessary to analyze optical coherence tomography angiography (OCTA) images. Previous methods typically segment all the targets in an OCTA sample, such as retinal vessels (RVs). Although these methods perform well in accuracy and precision, OCTA analyses often focusing local information within the images which has not been fulfilled. In this paper, we propose a method called SAM-OCTA for local segmentation in OCTA images. The method fine-tunes a pre-trained segment anything model (SAM) using low-rank adaptation (LoRA) and utilizes prompt points for local RVs, arteries, and veins segmentation in OCTA. To explore the effect and mechanism of prompt points, we set up global and local segmentation modes with two prompt point generation strategies, namely random selection and special annotation. Considering practical usage, we conducted extended experiments with different model scales and analyzed the model performance before and after fine-tuning besides the general segmentation task. From comprehensive experimental results with the OCTA-500 dataset, our SAM-OCTA method has achieved state-of-the-art performance in common OCTA segmentation tasks related to RV and FAZ, and it also performs accurate segmentation of artery-vein and local vessels. The code is available at https://github.com/ShellRedia/SAM-OCTA-extend.\"\\n      },\\n      {\\n        \"paperId\": \"78164b272db4afa960e3e0e62e3331dc36a48cd2\",\\n        \"url\": \"https://www.semanticscholar.org/paper/78164b272db4afa960e3e0e62e3331dc36a48cd2\",\\n        \"title\": \"Analyzable Chain-of-Musical-Thought Prompting for High-Fidelity Music Generation\",\\n        \"citationCount\": 12,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2503.19611, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"MusiCoT empowers the AR model to first outline an overall music structure before generating audio tokens, thereby enhancing the coherence and creativity of the resulting compositions, producing music quality that rivals state-of-the-art generation models.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2025-03-25\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2316946541\",\\n            \"name\": \"Max W. Y. Lam\"\\n          },\\n          {\\n            \"authorId\": \"2358193518\",\\n            \"name\": \"Yijin Xing\"\\n          },\\n          {\\n            \"authorId\": \"2352015696\",\\n            \"name\": \"Weiya You\"\\n          },\\n          {\\n            \"authorId\": \"2287919309\",\\n            \"name\": \"Jingcheng Wu\"\\n          },\\n          {\\n            \"authorId\": \"2118932090\",\\n            \"name\": \"Zongyu Yin\"\\n          },\\n          {\\n            \"authorId\": \"2352404752\",\\n            \"name\": \"Fuqiang Jiang\"\\n          },\\n          {\\n            \"authorId\": \"2319220700\",\\n            \"name\": \"Hangyu Liu\"\\n          },\\n          {\\n            \"authorId\": \"2319238631\",\\n            \"name\": \"Feng Liu\"\\n          },\\n          {\\n            \"authorId\": \"2292777550\",\\n            \"name\": \"Xingda Li\"\\n          },\\n          {\\n            \"authorId\": \"2352448900\",\\n            \"name\": \"Wei-Tsung Lu\"\\n          },\\n          {\\n            \"authorId\": \"2352227661\",\\n            \"name\": \"Hanyu Chen\"\\n          },\\n          {\\n            \"authorId\": \"2352042214\",\\n            \"name\": \"Tong Feng\"\\n          },\\n          {\\n            \"authorId\": \"2352908525\",\\n            \"name\": \"Tianwei Zhao\"\\n          },\\n          {\\n            \"authorId\": \"2352289761\",\\n            \"name\": \"Chien-Hung Liu\"\\n          },\\n          {\\n            \"authorId\": \"2300130251\",\\n            \"name\": \"Xuchen Song\"\\n          },\\n          {\\n            \"authorId\": \"2353891852\",\\n            \"name\": \"Yang Li\"\\n          },\\n          {\\n            \"authorId\": \"2353423689\",\\n            \"name\": \"Yahui Zhou\"\\n          }\\n        ],\\n        \"abstract\": \"Autoregressive (AR) models have demonstrated impressive capabilities in generating high-fidelity music. However, the conventional next-token prediction paradigm in AR models does not align with the human creative process in music composition, potentially compromising the musicality of generated samples. To overcome this limitation, we introduce MusiCoT, a novel chain-of-thought (CoT) prompting technique tailored for music generation. MusiCoT empowers the AR model to first outline an overall music structure before generating audio tokens, thereby enhancing the coherence and creativity of the resulting compositions. By leveraging the contrastive language-audio pretraining (CLAP) model, we establish a chain of\\\\\"musical thoughts\\\\\", making MusiCoT scalable and independent of human-labeled data, in contrast to conventional CoT methods. Moreover, MusiCoT allows for in-depth analysis of music structure, such as instrumental arrangements, and supports music referencing -- accepting variable-length audio inputs as optional style references. This innovative approach effectively addresses copying issues, positioning MusiCoT as a vital practical method for music prompting. Our experimental results indicate that MusiCoT consistently achieves superior performance across both objective and subjective metrics, producing music quality that rivals state-of-the-art generation models. Our samples are available at https://MusiCoT.github.io/.\"\\n      },\\n      {\\n        \"paperId\": \"8d7971734b904ece00f8c830b3ddd684deca50a3\",\\n        \"url\": \"https://www.semanticscholar.org/paper/8d7971734b904ece00f8c830b3ddd684deca50a3\",\\n        \"title\": \"ChatGPT in thematic analysis: Can AI become a research assistant in qualitative research?\",\\n        \"citationCount\": 18,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {\\'abstract\\'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1007/s11135-025-02165-z?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1007/s11135-025-02165-z, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This article evaluates how GenAI can support thematic analysis using a publicly available interview dataset from Lumivero and introduces Guided AI Thematic Analysis (GAITA), an adaptation of King et al.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2025-06-02\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"1450826804\",\\n            \"name\": \"Kien Nguyen-Trung\"\\n          }\\n        ],\\n        \"abstract\": null\\n      },\\n      {\\n        \"paperId\": \"94617fc3593e87c4ff72ea0a3f5456fde5896801\",\\n        \"url\": \"https://www.semanticscholar.org/paper/94617fc3593e87c4ff72ea0a3f5456fde5896801\",\\n        \"title\": \"Deductive Closure Training of Language Models for Coherence, Accuracy, and Updatability\",\\n        \"citationCount\": 21,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2401.08574, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"DCT shows that LMs\\' reasoning capabilities during inference can be leveraged during training to improve their reliability, and shows that LMs\\' reasoning capabilities during inference can be leveraged during training to improve their reliability.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2024-01-16\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"1754469865\",\\n            \"name\": \"Afra Feyza Aky\\\\u00fcrek\"\\n          },\\n          {\\n            \"authorId\": \"1992708068\",\\n            \"name\": \"Ekin Aky\\\\u00fcrek\"\\n          },\\n          {\\n            \"authorId\": \"41019330\",\\n            \"name\": \"Leshem Choshen\"\\n          },\\n          {\\n            \"authorId\": \"2129412\",\\n            \"name\": \"Derry Tanti Wijaya\"\\n          },\\n          {\\n            \"authorId\": \"2257268279\",\\n            \"name\": \"Jacob Andreas\"\\n          }\\n        ],\\n        \"abstract\": \"While language models (LMs) can sometimes generate factually correct text and estimate truth values of individual claims, these generally do not reflect a globally coherent, manipulable model of the world. As a consequence, current LMs also generate incorrect or nonsensical content, and are difficult to edit and bring up to date. We present a method called Deductive Closure Training (DCT) that uses LMs themselves to identify implications of (and contradictions within) the text that they generate, yielding an efficient self-supervised procedure for improving LM factuality. Given a collection of seed documents, DCT prompts LMs to generate additional text implied by these documents, reason globally about the correctness of this generated text, and finally fine-tune on text inferred to be correct. Given seed documents from a trusted source, DCT provides a tool for supervised model updating; if seed documents are sampled from the LM itself, DCT enables fully unsupervised fine-tuning for improved coherence and accuracy. Across the CREAK, MQUaKE, and Reversal Curse datasets, supervised DCT improves LM fact verification and text generation accuracy by 3-26%; on CREAK fully unsupervised DCT improves verification accuracy by 12%. These results show that LMs\\' reasoning capabilities during inference can be leveraged during training to improve their reliability.\"\\n      },\\n      {\\n        \"paperId\": \"e3f68363ec2c5b1472dad31d8551dd516861c134\",\\n        \"url\": \"https://www.semanticscholar.org/paper/e3f68363ec2c5b1472dad31d8551dd516861c134\",\\n        \"title\": \"Performance of Artificial Intelligence in Detecting Diabetic Macular Edema From Fundus Photography and Optical Coherence Tomography Images: A Systematic Review and Meta-analysis.\",\\n        \"citationCount\": 20,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://diabetesjournals.figshare.com/articles/figure/Performance_of_Artificial_Intelligence_in_Detecting_Diabetic_Macular_Edema_from_Fundus_Photographs_and_Optical_Coherence_Tomography_Images_A_Systematic_Review_and_Meta-analysis/24518287/1/files/43067464.pdf\",\\n          \"status\": \"GREEN\",\\n          \"license\": \"CCBYNCSA\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.2337/dc23-0993?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.2337/dc23-0993, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This meta-analysis demonstrates satisfactory performance of AI in detecting DME from FP or OCT images and indicates external validation is warranted for future studies to evaluate model generalizability.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Review\"\\n        ],\\n        \"publicationDate\": \"2024-01-19\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2279977400\",\\n            \"name\": \"Ching Lam\"\\n          },\\n          {\\n            \"authorId\": \"2279974752\",\\n            \"name\": \"Yiu Lun Wong\"\\n          },\\n          {\\n            \"authorId\": \"1758777204\",\\n            \"name\": \"Ziqi Tang\"\\n          },\\n          {\\n            \"authorId\": \"2191364808\",\\n            \"name\": \"Xiaoyan Hu\"\\n          },\\n          {\\n            \"authorId\": \"2191377385\",\\n            \"name\": \"Truong X. Nguyen\"\\n          },\\n          {\\n            \"authorId\": \"2277830890\",\\n            \"name\": \"Dawei Yang\"\\n          },\\n          {\\n            \"authorId\": \"2280039410\",\\n            \"name\": \"Shuyi Zhang\"\\n          },\\n          {\\n            \"authorId\": \"2280058989\",\\n            \"name\": \"J. Ding\"\\n          },\\n          {\\n            \"authorId\": \"34785499\",\\n            \"name\": \"Simon K H Szeto\"\\n          },\\n          {\\n            \"authorId\": \"108011531\",\\n            \"name\": \"A. Ran\"\\n          },\\n          {\\n            \"authorId\": \"2249532719\",\\n            \"name\": \"C. Y. Cheung\"\\n          }\\n        ],\\n        \"abstract\": \"BACKGROUND\\\\nDiabetic macular edema (DME) is the leading cause of vision loss in people with diabetes. Application of artificial intelligence (AI) in interpreting fundus photography (FP) and optical coherence tomography (OCT) images allows prompt detection and intervention.\\\\n\\\\n\\\\nPURPOSE\\\\nTo evaluate the performance of AI in detecting DME from FP or OCT images and identify potential factors affecting model performances.\\\\n\\\\n\\\\nDATA SOURCES\\\\nWe searched seven electronic libraries up to 12 February 2023.\\\\n\\\\n\\\\nSTUDY SELECTION\\\\nWe included studies using AI to detect DME from FP or OCT images.\\\\n\\\\n\\\\nDATA EXTRACTION\\\\nWe extracted study characteristics and performance parameters.\\\\n\\\\n\\\\nDATA SYNTHESIS\\\\nFifty-three studies were included in the meta-analysis. FP-based algorithms of 25 studies yielded pooled area under the receiver operating characteristic curve (AUROC), sensitivity, and specificity of 0.964, 92.6%, and 91.1%, respectively. OCT-based algorithms of 28 studies yielded pooled AUROC, sensitivity, and specificity of 0.985, 95.9%, and 97.9%, respectively. Potential factors improving model performance included deep learning techniques, larger size, and more diversity in training data sets. Models demonstrated better performance when validated internally than externally, and those trained with multiple data sets showed better results upon external validation.\\\\n\\\\n\\\\nLIMITATIONS\\\\nAnalyses were limited by unstandardized algorithm outcomes and insufficient data in patient demographics, OCT volumetric scans, and external validation.\\\\n\\\\n\\\\nCONCLUSIONS\\\\nThis meta-analysis demonstrates satisfactory performance of AI in detecting DME from FP or OCT images. External validation is warranted for future studies to evaluate model generalizability. Further investigations may estimate optimal sample size, effect of class balance, patient demographics, and additional benefits of OCT volumetric scans.\"\\n      },\\n      {\\n        \"paperId\": \"9aa5a10147b893f13f55f7500c89ba0a5e5d065b\",\\n        \"url\": \"https://www.semanticscholar.org/paper/9aa5a10147b893f13f55f7500c89ba0a5e5d065b\",\\n        \"title\": \"Vision-Language Models for Feature Detection of Macular Diseases on Optical Coherence Tomography.\",\\n        \"citationCount\": 19,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1001/jamaophthalmol.2024.1165?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1001/jamaophthalmol.2024.1165, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"In this study, a generalist VLM demonstrated limited vision capabilities for feature detection and management of macular disease, however, it showed low self-contradiction, suggesting strong language capabilities.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-05-02\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"41132090\",\\n            \"name\": \"F. Antaki\"\\n          },\\n          {\\n            \"authorId\": \"49334294\",\\n            \"name\": \"Reena Chopra\"\\n          },\\n          {\\n            \"authorId\": \"5638585\",\\n            \"name\": \"P. Keane\"\\n          }\\n        ],\\n        \"abstract\": \"Importance\\\\nVision-language models (VLMs) are a novel artificial intelligence technology capable of processing image and text inputs. While demonstrating strong generalist capabilities, their performance in ophthalmology has not been extensively studied.\\\\n\\\\n\\\\nObjective\\\\nTo assess the performance of the Gemini Pro VLM in expert-level tasks for macular diseases from optical coherence tomography (OCT) scans.\\\\n\\\\n\\\\nDesign, Setting, and Participants\\\\nThis was a cross-sectional diagnostic accuracy study evaluating a generalist VLM on ophthalmology-specific tasks using the open-source Optical Coherence Tomography Image Database. The dataset included OCT B-scans from 50 unique patients: healthy individuals and those with macular hole, diabetic macular edema, central serous chorioretinopathy, and age-related macular degeneration. Each OCT scan was labeled for 10 key pathological features, referral recommendations, and treatments. The images were captured using a Cirrus high definition OCT machine (Carl Zeiss Meditec) at Sankara Nethralaya Eye Hospital, Chennai, India, and the dataset was published in December 2018. Image acquisition dates were not specified.\\\\n\\\\n\\\\nExposures\\\\nGemini Pro, using a standard prompt to extract structured responses on December 15, 2023.\\\\n\\\\n\\\\nMain Outcomes and Measures\\\\nThe primary outcome was model responses compared against expert labels, calculating F1 scores for each pathological feature. Secondary outcomes included accuracy in diagnosis, referral urgency, and treatment recommendation. The model\\'s internal concordance was evaluated by measuring the alignment between referral and treatment recommendations, independent of diagnostic accuracy.\\\\n\\\\n\\\\nResults\\\\nThe mean F1 score was 10.7% (95% CI, 2.4-19.2). Measurable F1 scores were obtained for macular hole (36.4%; 95% CI, 0-71.4), pigment epithelial detachment (26.1%; 95% CI, 0-46.2), subretinal hyperreflective material (24.0%; 95% CI, 0-45.2), and subretinal fluid (20.0%; 95% CI, 0-45.5). A correct diagnosis was achieved in 17 of 50 cases (34%; 95% CI, 22-48). Referral recommendations varied: 28 of 50 were correct (56%; 95% CI, 42-70), 10 of 50 were overcautious (20%; 95% CI, 10-32), and 12 of 50 were undercautious (24%; 95% CI, 12-36). Referral and treatment concordance were very high, with 48 of 50 (96%; 95 % CI, 90-100) and 48 of 49 (98%; 95% CI, 94-100) correct answers, respectively.\\\\n\\\\n\\\\nConclusions and Relevance\\\\nIn this study, a generalist VLM demonstrated limited vision capabilities for feature detection and management of macular disease. However, it showed low self-contradiction, suggesting strong language capabilities. As VLMs continue to improve, validating their performance on large benchmarking datasets will help ascertain their potential in ophthalmology.\"\\n      },\\n      {\\n        \"paperId\": \"4bd41e401be5ca91d3302c2a888f565e8fc13749\",\\n        \"url\": \"https://www.semanticscholar.org/paper/4bd41e401be5ca91d3302c2a888f565e8fc13749\",\\n        \"title\": \"Large Language Models Can Enable Inductive Thematic Analysis of a Social Media Corpus in a Single Prompt: Human Validation Study\",\\n        \"citationCount\": 21,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://s3.ca-central-1.amazonaws.com/assets.jmir.org/assets/preprints/preprint-59641-accepted.pdf\",\\n          \"status\": \"GOLD\",\\n          \"license\": \"CCBY\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC11393503, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"LLMs can effectively and efficiently process large social media\\\\u2013based health-related data sets and extract themes from such data that human subject matter experts deem reasonable, but it is unable to show that the LLMs tested can replicate the depth of analysis from human subject matter experts by consistently extracting the same themes from the same data.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-04-24\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"6436816\",\\n            \"name\": \"Michael S. Deiner\"\\n          },\\n          {\\n            \"authorId\": \"2211301918\",\\n            \"name\": \"Vlad Honcharov\"\\n          },\\n          {\\n            \"authorId\": \"1492112680\",\\n            \"name\": \"Jiawei Li\"\\n          },\\n          {\\n            \"authorId\": \"2237755335\",\\n            \"name\": \"T. Mackey\"\\n          },\\n          {\\n            \"authorId\": \"2274513993\",\\n            \"name\": \"Travis C Porco\"\\n          },\\n          {\\n            \"authorId\": \"2261922685\",\\n            \"name\": \"Urmimala Sarkar\"\\n          }\\n        ],\\n        \"abstract\": \"Background Manually analyzing public health\\\\u2013related content from social media provides valuable insights into the beliefs, attitudes, and behaviors of individuals, shedding light on trends and patterns that can inform public understanding, policy decisions, targeted interventions, and communication strategies. Unfortunately, the time and effort needed from well-trained human subject matter experts makes extensive manual social media listening unfeasible. Generative large language models (LLMs) can potentially summarize and interpret large amounts of text, but it is unclear to what extent LLMs can glean subtle health-related meanings in large sets of social media posts and reasonably report health-related themes. Objective We aimed to assess the feasibility of using LLMs for topic model selection or inductive thematic analysis of large contents of social media posts by attempting to answer the following question: Can LLMs conduct topic model selection and inductive thematic analysis as effectively as humans did in a prior manual study, or at least reasonably, as judged by subject matter experts? Methods We asked the same research question and used the same set of social media content for both the LLM selection of relevant topics and the LLM analysis of themes as was conducted manually in a published study about vaccine rhetoric. We used the results from that study as background for this LLM experiment by comparing the results from the prior manual human analyses with the analyses from 3 LLMs: GPT4-32K, Claude-instant-100K, and Claude-2-100K. We also assessed if multiple LLMs had equivalent ability and assessed the consistency of repeated analysis from each LLM. Results The LLMs generally gave high rankings to the topics chosen previously by humans as most relevant. We reject a null hypothesis (P<.001, overall comparison) and conclude that these LLMs are more likely to include the human-rated top 5 content areas in their top rankings than would occur by chance. Regarding theme identification, LLMs identified several themes similar to those identified by humans, with very low hallucination rates. Variability occurred between LLMs and between test runs of an individual LLM. Despite not consistently matching the human-generated themes, subject matter experts found themes generated by the LLMs were still reasonable and relevant. Conclusions LLMs can effectively and efficiently process large social media\\\\u2013based health-related data sets. LLMs can extract themes from such data that human subject matter experts deem reasonable. However, we were unable to show that the LLMs we tested can replicate the depth of analysis from human subject matter experts by consistently extracting the same themes from the same data. There is vast potential, once better validated, for automated LLM-based real-time social listening for common and rare health conditions, informing public health understanding of the public\\\\u2019s interests and concerns and determining the public\\\\u2019s ideas to address them.\"\\n      },\\n      {\\n        \"paperId\": \"4913529041ac69912680d1f8c1b1e4229a68b61b\",\\n        \"url\": \"https://www.semanticscholar.org/paper/4913529041ac69912680d1f8c1b1e4229a68b61b\",\\n        \"title\": \"The Intellectual Evolution of Educational Leadership Research: A Combined Bibliometric and Thematic Analysis Using SciMAT\",\\n        \"citationCount\": 15,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://www.mdpi.com/2227-7102/14/4/429/pdf?version=1713520131\",\\n          \"status\": \"GOLD\",\\n          \"license\": \"CCBY\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.3390/educsci14040429?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.3390/educsci14040429, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": null,\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-04-19\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"66769167\",\\n            \"name\": \"Turgut Karakose\"\\n          },\\n          {\\n            \"authorId\": \"114837130\",\\n            \"name\": \"K. Leithwood\"\\n          },\\n          {\\n            \"authorId\": \"3224712\",\\n            \"name\": \"Tijen T\\\\u00fcl\\\\u00fcba\\\\u015f\"\\n          }\\n        ],\\n        \"abstract\": \"This study aims to describe the century-long trajectory of educational leadership research (ELR), including changes over time in its main and subsidiary themes, as well as its most influential authors, papers, and journals. The study combines the bibliometric performance and science mapping analysis of 7282 articles retrieved from the Scopus and WoS databases. SciMAT software (version 1.1.04) was used to analyze changes over four sequential time periods and to exhibit the thematic evolution of the field\\\\u2014Period 1 (1907 to 2004), Period 2 (2005 to 2012), Period 3 (2013 to 2019), and Period 4 (2020\\\\u20132023). Research during Period 1 focused on principals and included efforts to distinguish between their administrative functions and forms of \\\\u2018strong\\\\u2019 leadership contributing to school improvement. Period 2 included research aimed at understanding what strong principal leadership entailed, including the development and testing of more coherent models of such leadership. While instructional and transformational leadership models were prominent during Periods 1 and 2, Period 3 research invested heavily in conceptions of leadership distribution. Early research about \\\\u2018social justice leadership\\\\u2019 appeared during this period and eventually flourished during Period 4. While principals were an active focus through all Periods, the leadership of others gradually dominated ELR and accounted for the broader leadership theme found in all four periods. The results point to the evolutionary nature of ELR development, which eventually produced a relatively robust knowledge base. Experiences with the COVID-19 pandemic suggest that crises such as this might prompt more revolutionary orientations in the ELR field.\"\\n      },\\n      {\\n        \"paperId\": \"587b50d31e99f37489e1a2b338d8ddb07026da0b\",\\n        \"url\": \"https://www.semanticscholar.org/paper/587b50d31e99f37489e1a2b338d8ddb07026da0b\",\\n        \"title\": \"Exploring Students\\\\u2019 Perceptions of ChatGPT: Thematic Analysis and Follow-Up Survey\",\\n        \"citationCount\": 271,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://doi.org/10.1109/access.2023.3268224\",\\n          \"status\": \"GOLD\",\\n          \"license\": \"CCBYNCND\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ACCESS.2023.3268224?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ACCESS.2023.3268224, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"It is concluded that ChatGPT can and should be used for learning, however, students should be aware of its limitations.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Review\"\\n        ],\\n        \"publicationDate\": null,\\n        \"authors\": [\\n          {\\n            \"authorId\": \"1754800\",\\n            \"name\": \"Abdulhadi Shoufan\"\\n          }\\n        ],\\n        \"abstract\": \"ChatGPT has sparked both excitement and skepticism in education. To analyze its impact on teaching and learning it is crucial to understand how students perceive ChatGPT and assess its potential and challenges. Toward this, we conducted a two-stage study with senior students in a computer engineering program ( $n=56$ ). In the first stage, we asked the students to evaluate ChatGPT using their own words after they used it to complete one learning activity. The returned responses (3136 words) were analyzed by coding and theme building (36 codes and 15 themes). In the second stage, we used the derived codes and themes to create a 27-item questionnaire. The students responded to this questionnaire three weeks later after completing other activities with the help of ChatGPT. The results show that the students admire the capabilities of ChatGPT and find it interesting, motivating, and helpful for study and work. They find it easy to use and appreciate its human-like interface that provides well-structured responses and good explanations. However, many students feel that ChatGPT\\\\u2019s answers are not always accurate and most of them believe that it requires good background knowledge to work with since it does not replace human intelligence. So, most students think that ChatGPT needs to be improved but are optimistic that this will happen soon. When it comes to the negative impact of ChatGPT on learning, academic integrity, jobs, and life, the students are divided. We conclude that ChatGPT can and should be used for learning. However, students should be aware of its limitations. Educators should try using ChatGPT and guide students on effective prompting techniques and how to assess generated responses. The developers should improve their models to enhance the accuracy of given answers. The study provides insights into the capabilities and limitations of ChatGPT in education and informs future research and development.\"\\n      },\\n      {\\n        \"paperId\": \"a01a9c4a114fbf201540268f928ccf77bc3f9357\",\\n        \"url\": \"https://www.semanticscholar.org/paper/a01a9c4a114fbf201540268f928ccf77bc3f9357\",\\n        \"title\": \"Fine-Grained Visual Prompting\",\\n        \"citationCount\": 90,\\n        \"openAccessPdf\": {\\n          \"url\": \"http://arxiv.org/pdf/2306.04356\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2306.04356, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper introduces a new zero-shot framework that leverages pixel-level annotations acquired from a generalist segmentation model for fine-grained visual prompting and reveals that a straightforward application of blur outside the target mask, referred to as the Blur Reverse Mask, exhibits exceptional effectiveness.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2023-06-07\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2146416206\",\\n            \"name\": \"Lingfeng Yang\"\\n          },\\n          {\\n            \"authorId\": \"2217456303\",\\n            \"name\": \"Yueze Wang\"\\n          },\\n          {\\n            \"authorId\": \"2144439048\",\\n            \"name\": \"Xiang Li\"\\n          },\\n          {\\n            \"authorId\": \"51316629\",\\n            \"name\": \"Xinlong Wang\"\\n          },\\n          {\\n            \"authorId\": \"2146236917\",\\n            \"name\": \"Jian Yang\"\\n          }\\n        ],\\n        \"abstract\": \"Vision-Language Models (VLMs), such as CLIP, have demonstrated impressive zero-shot transfer capabilities in image-level visual perception. However, these models have shown limited performance in instance-level tasks that demand precise localization and recognition. Previous works have suggested that incorporating visual prompts, such as colorful boxes or circles, can improve the ability of models to recognize objects of interest. Nonetheless, compared to language prompting, visual prompting designs are rarely explored. Existing approaches, which employ coarse visual cues such as colorful boxes or circles, often result in sub-optimal performance due to the inclusion of irrelevant and noisy pixels. In this paper, we carefully study the visual prompting designs by exploring more fine-grained markings, such as segmentation masks and their variations. In addition, we introduce a new zero-shot framework that leverages pixel-level annotations acquired from a generalist segmentation model for fine-grained visual prompting. Consequently, our investigation reveals that a straightforward application of blur outside the target mask, referred to as the Blur Reverse Mask, exhibits exceptional effectiveness. This proposed prompting strategy leverages the precise mask annotations to reduce focus on weakly related regions while retaining spatial coherence between the target and the surrounding background. Our Fine-Grained Visual Prompting (FGVP) demonstrates superior performance in zero-shot comprehension of referring expressions on the RefCOCO, RefCOCO+, and RefCOCOg benchmarks. It outperforms prior methods by an average margin of 3.0% to 4.6%, with a maximum improvement of 12.5% on the RefCOCO+ testA subset. Code is available at https://github.com/ylingfeng/FGVP.\"\\n      },\\n      {\\n        \"paperId\": \"e1770838ec0667cad48729a81764ed9964d6a8e6\",\\n        \"url\": \"https://www.semanticscholar.org/paper/e1770838ec0667cad48729a81764ed9964d6a8e6\",\\n        \"title\": \"LLM-in-the-loop: Leveraging Large Language Model for Thematic Analysis\",\\n        \"citationCount\": 108,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2310.15100, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This work proposes a human-LLM collaboration framework (i.e., LLM-in-the-loop) to conduct TA with in-context learning (ICL), which yields similar coding quality to that of human coders but reduces TA\\'s labor and time demands.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\",\\n          \"Review\"\\n        ],\\n        \"publicationDate\": \"2023-10-23\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"3400291\",\\n            \"name\": \"Shih-Chieh Dai\"\\n          },\\n          {\\n            \"authorId\": \"2261362789\",\\n            \"name\": \"Aiping Xiong\"\\n          },\\n          {\\n            \"authorId\": \"1746959\",\\n            \"name\": \"Lun-Wei Ku\"\\n          }\\n        ],\\n        \"abstract\": \"Thematic analysis (TA) has been widely used for analyzing qualitative data in many disciplines and fields. To ensure reliable analysis, the same piece of data is typically assigned to at least two human coders. Moreover, to produce meaningful and useful analysis, human coders develop and deepen their data interpretation and coding over multiple iterations, making TA labor-intensive and time-consuming. Recently the emerging field of large language models (LLMs) research has shown that LLMs have the potential replicate human-like behavior in various tasks: in particular, LLMs outperform crowd workers on text-annotation tasks, suggesting an opportunity to leverage LLMs on TA. We propose a human-LLM collaboration framework (i.e., LLM-in-the-loop) to conduct TA with in-context learning (ICL). This framework provides the prompt to frame discussions with a LLM (e.g., GPT-3.5) to generate the final codebook for TA. We demonstrate the utility of this framework using survey datasets on the aspects of the music listening experience and the usage of a password manager. Results of the two case studies show that the proposed framework yields similar coding quality to that of human coders but reduces TA\\'s labor and time demands.\"\\n      },\\n      {\\n        \"paperId\": \"b54398b8ccba47ecab43a5311e5b2a6c61461936\",\\n        \"url\": \"https://www.semanticscholar.org/paper/b54398b8ccba47ecab43a5311e5b2a6c61461936\",\\n        \"title\": \"Who Evaluates the Evaluations? Objectively Scoring Text-to-Image Prompt Coherence Metrics with T2IScoreScore (TS2)\",\\n        \"citationCount\": 12,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2404.04251, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"T2IScoreScore is introduced, a curated set of semantic error graphs containing a prompt and a set of increasingly erroneous images to rigorously judge whether a given prompt faithfulness metric can correctly order images with respect to their objective error count and significantly discriminate between different error nodes, using meta-metric scores derived from established statistical tests.\"\\n        },\\n        \"publicationTypes\": null,\\n        \"publicationDate\": \"2024-04-05\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"48227633\",\\n            \"name\": \"Michael Stephen Saxon\"\\n          },\\n          {\\n            \"authorId\": \"2069493939\",\\n            \"name\": \"Fatima Jahara\"\\n          },\\n          {\\n            \"authorId\": \"2302799897\",\\n            \"name\": \"Mahsa Khoshnoodi\"\\n          },\\n          {\\n            \"authorId\": \"2257339858\",\\n            \"name\": \"Yujie Lu\"\\n          },\\n          {\\n            \"authorId\": \"2309678102\",\\n            \"name\": \"Aditya Sharma\"\\n          },\\n          {\\n            \"authorId\": \"2257130316\",\\n            \"name\": \"W. Wang\"\\n          }\\n        ],\\n        \"abstract\": \"With advances in the quality of text-to-image (T2I) models has come interest in benchmarking their prompt faithfulness -- the semantic coherence of generated images to the prompts they were conditioned on. A variety of T2I faithfulness metrics have been proposed, leveraging advances in cross-modal embeddings and vision-language models (VLMs). However, these metrics are not rigorously compared and benchmarked, instead presented with correlation to human Likert scores over a set of easy-to-discriminate images against seemingly weak baselines. We introduce T2IScoreScore, a curated set of semantic error graphs containing a prompt and a set of increasingly erroneous images. These allow us to rigorously judge whether a given prompt faithfulness metric can correctly order images with respect to their objective error count and significantly discriminate between different error nodes, using meta-metric scores derived from established statistical tests. Surprisingly, we find that the state-of-the-art VLM-based metrics (e.g., TIFA, DSG, LLMScore, VIEScore) we tested fail to significantly outperform simple (and supposedly worse) feature-based metrics like CLIPScore, particularly on a hard subset of naturally-occurring T2I model errors. TS2 will enable the development of better T2I prompt faithfulness metrics through more rigorous comparison of their conformity to expected orderings and separations under objective criteria.\"\\n      },\\n      {\\n        \"paperId\": \"5121e40b45f33c2fe08087173cfb8ddfc2bf4fa2\",\\n        \"url\": \"https://www.semanticscholar.org/paper/5121e40b45f33c2fe08087173cfb8ddfc2bf4fa2\",\\n        \"title\": \"Super-Resolution Cloth Animation with Spatial and Temporal Coherence\",\\n        \"citationCount\": 10,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1145/3658143?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/3658143, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper decomposes garment meshes into overlapping patches for adaptability to various styles and geometric continuity and achieves an 8\\\\u00d7 improvement in resolution for cloth animations, leveraging two core modules.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-07-19\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2312152389\",\\n            \"name\": \"Jiawang Yu\"\\n          },\\n          {\\n            \"authorId\": \"2312169459\",\\n            \"name\": \"Zhendong Wang\"\\n          }\\n        ],\\n        \"abstract\": \"Creating super-resolution cloth animations, which refine coarse cloth meshes with fine wrinkle details, faces challenges in preserving spatial consistency and temporal coherence across frames. In this paper, we introduce a general framework to address these issues, leveraging two core modules. The first module interleaves a simulator and a corrector. The simulator handles cloth dynamics, while the corrector rectifies differences in low-frequency features across various resolutions. This interleaving ensures prompt correction of spatial errors from the coarse simulation, effectively preventing their temporal propagation. The second module performs mesh-based super-resolution for detailed wrinkle enhancements. We decompose garment meshes into overlapping patches for adaptability to various styles and geometric continuity. Our method achieves an 8\\\\u00d7 improvement in resolution for cloth animations. We showcase the effectiveness of our method through diverse animation examples, including simple cloth pieces and intricate garments.\"\\n      },\\n      {\\n        \"paperId\": \"65319931eb2af60a8b922bb5f9ce1194f300e6df\",\\n        \"url\": \"https://www.semanticscholar.org/paper/65319931eb2af60a8b922bb5f9ce1194f300e6df\",\\n        \"title\": \"\\\\\"We Are Visual Thinkers, Not Verbal Thinkers!\\\\\": A Thematic Analysis of How Professional Designers Use Generative AI Image Generation Tools\",\\n        \"citationCount\": 12,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": \"CLOSED\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1145/3679318.3685370?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/3679318.3685370, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A qualitative study involving 16 professional designers from the automotive industry identified their challenges with existing GenAI image generation tools in daily design practices, and revealed the need for visual input-centric multi-modal interfaces that extend beyond textual prompts.\"\\n        },\\n        \"publicationTypes\": [\\n          \"Book\",\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-10-13\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2264963307\",\\n            \"name\": \"Hyerim Park\"\\n          },\\n          {\\n            \"authorId\": \"29415466\",\\n            \"name\": \"Joscha Eirich\"\\n          },\\n          {\\n            \"authorId\": \"2292312388\",\\n            \"name\": \"Andr\\\\u00e9 Luckow\"\\n          },\\n          {\\n            \"authorId\": \"2243254992\",\\n            \"name\": \"Michael Sedlmair\"\\n          }\\n        ],\\n        \"abstract\": \"Generative artificial intelligence (GenAI) has become increasingly popular, influencing various creative domains. However, while broader societal perspectives have been analyzed, specific examinations of how practitioners utilize GenAI tools to enhance their current workflows remain limited. To address this gap, we conducted a qualitative study involving 16 professional designers from the automotive industry. We aimed to identify their challenges with existing GenAI image generation tools in daily design practices. Thematic analysis revealed four key themes: (1) the need for visual input-centric multi-modal interfaces that extend beyond textual prompts, (2) the lack of support for the iterative nature of design processes in GenAI tools, (3) difficulties in controlling prompts to achieve desired outputs, and (4) the significance of incorporating human experiences and emotions into design. Based on our findings, we propose and discuss potential design considerations for enhancing future GenAI image generation tool interfaces.\"\\n      },\\n      {\\n        \"paperId\": \"22438bcb9ef4fb5be5ea9ea8307d4691a3209f4d\",\\n        \"url\": \"https://www.semanticscholar.org/paper/22438bcb9ef4fb5be5ea9ea8307d4691a3209f4d\",\\n        \"title\": \"Quantitative and qualitative assessment of anterior segment optical coherence tomography capture of disease state in childhood anterior uveitis\",\\n        \"citationCount\": 18,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://discovery.ucl.ac.uk/10144499/3/Solebo_AS_OCT_clean_230122.pdf\",\\n          \"status\": \"GREEN\",\\n          \"license\": \"other-oa\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1136/bjophthalmol-2021-320448?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1136/bjophthalmol-2021-320448, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"The findings suggest that OCT-based imaging holds the promise of deeper understanding of disease, improved patient experience and more granular monitoring of activity with resultant improved outcomes, but further work is needed to refine acquisition and analysis protocols.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2022-02-23\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"1967704903\",\\n            \"name\": \"Katie Etherton\"\\n          },\\n          {\\n            \"authorId\": \"145884297\",\\n            \"name\": \"J. Rahi\"\\n          },\\n          {\\n            \"authorId\": \"3971827\",\\n            \"name\": \"H. Petrushkin\"\\n          },\\n          {\\n            \"authorId\": \"2090508795\",\\n            \"name\": \"A. Dick\"\\n          },\\n          {\\n            \"authorId\": \"1573631346\",\\n            \"name\": \"S. Akbarali\"\\n          },\\n          {\\n            \"authorId\": \"15498366\",\\n            \"name\": \"R. Pattani\"\\n          },\\n          {\\n            \"authorId\": \"4326096\",\\n            \"name\": \"S. Hau\"\\n          },\\n          {\\n            \"authorId\": \"46521197\",\\n            \"name\": \"S. Lacassagne\"\\n          },\\n          {\\n            \"authorId\": \"46521856\",\\n            \"name\": \"Xiaoxuan Liu\"\\n          },\\n          {\\n            \"authorId\": \"145661041\",\\n            \"name\": \"A. Denniston\"\\n          },\\n          {\\n            \"authorId\": \"8489719\",\\n            \"name\": \"A. Solebo\"\\n          }\\n        ],\\n        \"abstract\": \"Background/aims Anterior segment optical coherence tomography (AS-OCT) assessment of anterior chamber inflammation is an emerging tool. We describe the performance of AS-OCT in a paediatric population. Methods A mixed-methods prospective study, using routine clinical assessment as reference standard, and AS-OCT, with Tomey CASIA2 or Heidelberg Spectralis HS1, as index test, with data collected on patient perceptions of imaging. Repeatability, diagnostic indices, responsiveness to clinical change and clinical correlations of imaging-based metrics (image cell count, size, density and brightness) were assessed, with construction of receiver operated characteristic curves. Exploratory thematic analysis of responses from families was undertaken. Results A total of 90 children (180 eyes) underwent imaging. Bland Altman limits of agreement for CASIA2 repeatability ranged from +17 cells (95%\\\\u2009CI 13.6 to 21.1) to \\\\u221219 cells (95%\\\\u2009CI \\\\u221215.6 to \\\\u221223.2) and HS1 from +1 (95% CI 0.9 to 1.2) to \\\\u22121.0 (\\\\u22121.2 to \\\\u22120.8) cells. CASIA2 imaging had higher sensitivity of 0.92 (95% CI 0.78 to 0.97) vs HS1 imaging 0.17 (95% CI 0.07 to 0.34), with positive correlation between clinical grade and CASIA2 cell count (coefficient 12.8, p=0.02, 95%\\\\u2009CI 2.2 to 23.4). Change in clinical grade at follow-up examinations correlated with change in image based \\\\u2018cell\\\\u2019 count (r2=0.79, p<0.001). Patients reported a potential positive impact of seeing their disease activity. Conclusion Our findings suggest that OCT-based imaging holds the promise of deeper understanding of disease, improved patient experience and more granular monitoring of activity with resultant improved outcomes, but further work is needed to refine acquisition and analysis protocols.\"\\n      },\\n      {\\n        \"paperId\": \"a0968924129aee94ca2070004e657df3d34a41dd\",\\n        \"url\": \"https://www.semanticscholar.org/paper/a0968924129aee94ca2070004e657df3d34a41dd\",\\n        \"title\": \"Academic publisher guidelines on AI usage: A ChatGPT supported thematic analysis\",\\n        \"citationCount\": 67,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://f1000research.com/articles/12-1398/pdf\",\\n          \"status\": \"GOLD\",\\n          \"license\": \"CCBY\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC10844801, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A novel analysis supported by GenAI tools is used to identify themes emerging in the policy landscape, underscoring the need for an informed, flexible approach to policy formulation that can adapt to the rapidly evolving landscape of AI technologies.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Review\"\\n        ],\\n        \"publicationDate\": \"2023-10-23\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"48765116\",\\n            \"name\": \"Mike Perkins\"\\n          },\\n          {\\n            \"authorId\": \"114397354\",\\n            \"name\": \"Jasper Roe\"\\n          }\\n        ],\\n        \"abstract\": \"Background As Artificial Intelligence (AI) technologies such as Generative AI (GenAI) have become more common in academic settings, it is necessary to examine how these tools interact with issues of authorship, academic integrity, and research methodologies. The current landscape lacks cohesive policies and guidelines for regulating AI\\\\u2019s role in academic research which has prompted discussions among publishers, authors, and institutions. Methods This study employs inductive thematic analysis to explore publisher policies regarding AI-assisted authorship and academic work. Our methods involved a two-fold analysis using both AI-assisted and traditional unassisted techniques to examine the available policies from leading academic publishers and other publishing or academic entities. The framework was designed to offer multiple perspectives, harnessing the strengths of AI for pattern recognition while leveraging human expertise for nuanced interpretation. The results of these two analyses are combined to form the final themes. Results Our findings indicate six overall themes, three of which were independently identified in both the AI-assisted and unassisted, manual analysis using common software tools. A broad consensus appears among publishers that human authorship remains paramount and that the use of GenAI tools is permissible but must be disclosed. However, GenAI tools are increasingly acknowledged for their supportive roles, including text generation and data analysis. The study also discusses the inherent limitations and biases of AI-assisted analysis, necessitating rigorous scrutiny by authors, reviewers, and editors. Conclusions There is a growing recognition of AI\\\\u2019s role as a valuable auxiliary tool in academic research, but one that comes with caveats pertaining to integrity, accountability, and interpretive limitations. This study used a novel analysis supported by GenAI tools to identify themes emerging in the policy landscape, underscoring the need for an informed, flexible approach to policy formulation that can adapt to the rapidly evolving landscape of AI technologies.\"\\n      },\\n      {\\n        \"paperId\": \"5c8ad3edc40efc0da8937fdafae7796d487efdf1\",\\n        \"url\": \"https://www.semanticscholar.org/paper/5c8ad3edc40efc0da8937fdafae7796d487efdf1\",\\n        \"title\": \"Pain experiences during intrauterine device procedures: a thematic analysis of tweets\",\\n        \"citationCount\": 14,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://srh.bmj.com/content/familyplanning/early/2024/06/11/bmjsrh-2023-202011.full.pdf\",\\n          \"status\": \"HYBRID\",\\n          \"license\": \"CCBYNC\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC11503099, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"The findings attest the need for strategies to improve the patient experience for those opting for IUD as a clinical priority and further research should explore IUD users\\' experiences, expectations and wishes around pain management.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2024-06-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"6247626\",\\n            \"name\": \"Neda Taghinejadi\"\\n          },\\n          {\\n            \"authorId\": \"6056729\",\\n            \"name\": \"H. van der Westhuizen\"\\n          },\\n          {\\n            \"authorId\": \"1723406956\",\\n            \"name\": \"F. I. Ayomoh\"\\n          },\\n          {\\n            \"authorId\": \"47340073\",\\n            \"name\": \"Wasim Ahmed\"\\n          },\\n          {\\n            \"authorId\": \"2300379872\",\\n            \"name\": \"Trisha Greenhalgh\"\\n          },\\n          {\\n            \"authorId\": \"144511766\",\\n            \"name\": \"A. Boylan\"\\n          }\\n        ],\\n        \"abstract\": \"Introduction In June 2021, high-profile testimonials in the media about pain during intrauterine device (IUD) procedures in the UK prompted significant discussion across platforms including Twitter (subsequently renamed X). We examined a sample of Twitter postings (tweets) to gain insight into public perspectives and experiences. Methods We harvested tweets posted or retweeted on 21\\\\u201322 June 2021 which contained the search terms coil, intrauterine system, IUD or intrauterine. We analysed the dataset thematically and selected illustrative tweets with the authors\\\\u2019 consent for publication. Results Following deduplication and screening, we included 1431 tweets in our analysis. We identified testimonials with descriptions of varied pain experiences. Twitter users reported that clinicians had not warned them that pain could be severe or explained the options for pain relief. Some raised concerns about pain being minimised or dismissed and linked this to the management of women\\\\u2019s pain in medicine more broadly. Twitter users described connecting to an online community with shared experiences as validating and used this as a springboard for collective action. Conclusions While we acknowledge the limitations of our sample, this study highlights important perspectives and accounts relating to pain during IUD procedures. Our findings attest to the need for strategies to improve the patient experience for those opting for IUD as a clinical priority. Further research should explore IUD users\\' experiences, expectations and wishes around pain management.\"\\n      },\\n      {\\n        \"paperId\": \"b09ab1ca37173fbea6c24cda2bbda0c93cea6c91\",\\n        \"url\": \"https://www.semanticscholar.org/paper/b09ab1ca37173fbea6c24cda2bbda0c93cea6c91\",\\n        \"title\": \"Singing off the same hymn sheet? Examining coherence in a talent development pathway (part 2)\",\\n        \"citationCount\": 12,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1080/02640414.2021.2021702?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1080/02640414.2021.2021702, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"Results suggest that the talent development environment provides a long-term development experience supplemented with individual athlete attention at international level, but there is a need for improved direction from the National Governing Body if systematic coherence towards talent development is to be achieved.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2022-01-05\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"122738315\",\\n            \"name\": \"O. Curran\"\\n          },\\n          {\\n            \"authorId\": \"91859067\",\\n            \"name\": \"D. Passmore\"\\n          },\\n          {\\n            \"authorId\": \"6973495\",\\n            \"name\": \"\\\\u00c1. Macnamara\"\\n          }\\n        ],\\n        \"abstract\": \"ABSTRACT Existing literature highlights the common characteristics of successful talent development environments, notably the need for long-term development, individual athlete attention, communication, alignment, and psycho-behavioural development. Little is known however about the complex talent development environment of an international sport organisation where multiple contexts and various stakeholders exist. Considering the lack of research relating to females in talent development, we examined a female national hockey talent development environment and more specifically the level of coherence that existed within the talent development environment from different stakeholder perspectives. Twenty-seven international female hockey players and fourteen pathway staff members from across the talent development pathway participated in semi-structured focus groups. An inductive\\\\u2013deductive thematic analysis was conducted. Results suggest that the talent development environment provides a long-term development experience supplemented with individual athlete attention at international level. However, a general lack of coherence and systematic development was evident across the talent development environment contexts with varying levels of coherence found within the higher-order themes of appropriate development, not early success, individualised and ongoing development, and wide-ranging coherent messages and support. This highlights a need for improved direction from the National Governing Body if systematic coherence towards talent development is to be achieved.\"\\n      },\\n      {\\n        \"paperId\": \"c8e142c6ee5b8a044e406670e01d98f97a5d26bf\",\\n        \"url\": \"https://www.semanticscholar.org/paper/c8e142c6ee5b8a044e406670e01d98f97a5d26bf\",\\n        \"title\": \"Comparing the Efficacy and Efficiency of Human and Generative AI: Qualitative Thematic Analyses\",\\n        \"citationCount\": 32,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://doi.org/10.2196/54482\",\\n          \"status\": \"GOLD\",\\n          \"license\": \"CCBY\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC11329846, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"The promising consistency in the themes generated by human coders and GenAI suggests that these technologies hold promise in reducing the resource intensiveness of qualitative thematic analysis; however, the relatively lower reliability in coding between them suggests that hybrid approaches are necessary.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2023-11-11\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"34245122\",\\n            \"name\": \"Maximo R. Prescott\"\\n          },\\n          {\\n            \"authorId\": \"2305191451\",\\n            \"name\": \"S. Yeager\"\\n          },\\n          {\\n            \"authorId\": \"2275218415\",\\n            \"name\": \"Lillian Ham\"\\n          },\\n          {\\n            \"authorId\": \"2051442834\",\\n            \"name\": \"C. R. Rivera Saldana\"\\n          },\\n          {\\n            \"authorId\": \"2305195892\",\\n            \"name\": \"Vanessa Serrano\"\\n          },\\n          {\\n            \"authorId\": \"2305192478\",\\n            \"name\": \"J. Narez\"\\n          },\\n          {\\n            \"authorId\": \"84640204\",\\n            \"name\": \"D. Paltin\"\\n          },\\n          {\\n            \"authorId\": \"2262498185\",\\n            \"name\": \"Jorge Delgado\"\\n          },\\n          {\\n            \"authorId\": \"2300816756\",\\n            \"name\": \"David J Moore\"\\n          },\\n          {\\n            \"authorId\": \"2301912568\",\\n            \"name\": \"Jessica Montoya\"\\n          }\\n        ],\\n        \"abstract\": \"Background Qualitative methods are incredibly beneficial to the dissemination and implementation of new digital health interventions; however, these methods can be time intensive and slow down dissemination when timely knowledge from the data sources is needed in ever-changing health systems. Recent advancements in generative artificial intelligence (GenAI) and their underlying large language models (LLMs) may provide a promising opportunity to expedite the qualitative analysis of textual data, but their efficacy and reliability remain unknown. Objective The primary objectives of our study were to evaluate the consistency in themes, reliability of coding, and time needed for inductive and deductive thematic analyses between GenAI (ie, ChatGPT and Bard) and human coders. Methods The qualitative data for this study consisted of 40 brief SMS text message reminder prompts used in a digital health intervention for promoting antiretroviral medication adherence among people with HIV who use methamphetamine. Inductive and deductive thematic analyses of these SMS text messages were conducted by 2 independent teams of human coders. An independent human analyst conducted analyses following both approaches using ChatGPT and Bard. The consistency in themes (or the extent to which the themes were the same) and reliability (or agreement in coding of themes) between methods were compared. Results The themes generated by GenAI (both ChatGPT and Bard) were consistent with 71% (5/7) of the themes identified by human analysts following inductive thematic analysis. The consistency in themes was lower between humans and GenAI following a deductive thematic analysis procedure (ChatGPT: 6/12, 50%; Bard: 7/12, 58%). The percentage agreement (or intercoder reliability) for these congruent themes between human coders and GenAI ranged from fair to moderate (ChatGPT, inductive: 31/66, 47%; ChatGPT, deductive: 22/59, 37%; Bard, inductive: 20/54, 37%; Bard, deductive: 21/58, 36%). In general, ChatGPT and Bard performed similarly to each other across both types of qualitative analyses in terms of consistency of themes (inductive: 6/6, 100%; deductive: 5/6, 83%) and reliability of coding (inductive: 23/62, 37%; deductive: 22/47, 47%). On average, GenAI required significantly less overall time than human coders when conducting qualitative analysis (20, SD 3.5 min vs 567, SD 106.5 min). Conclusions The promising consistency in the themes generated by human coders and GenAI suggests that these technologies hold promise in reducing the resource intensiveness of qualitative thematic analysis; however, the relatively lower reliability in coding between them suggests that hybrid approaches are necessary. Human coders appeared to be better than GenAI at identifying nuanced and interpretative themes. Future studies should consider how these powerful technologies can be best used in collaboration with human coders to improve the efficiency of qualitative research in hybrid approaches while also mitigating potential ethical risks that they may pose.\"\\n      },\\n      {\\n        \"paperId\": \"0edcd6dd2e952523c35e77d0f9cce5927b97d63e\",\\n        \"url\": \"https://www.semanticscholar.org/paper/0edcd6dd2e952523c35e77d0f9cce5927b97d63e\",\\n        \"title\": \"CoAScore: Chain-of-Aspects Prompting for NLG Evaluation\",\\n        \"citationCount\": 15,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2312.10355, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"The experimental findings highlight that, in comparison to individual aspect evaluation, CoAScore exhibits a higher correlation with human judgments, which significantly outperforms existing unsupervised evaluation metrics, whether for assessing overall quality or other aspects.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2023-12-16\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2275055956\",\\n            \"name\": \"Peiyuan Gong\"\\n          },\\n          {\\n            \"authorId\": \"2275056887\",\\n            \"name\": \"Jiaxin Mao\"\\n          }\\n        ],\\n        \"abstract\": \"Recently, natural language generation (NLG) evaluation has shifted from a single-aspect to a multi-aspect paradigm, allowing for a more accurate assessment. Large language models (LLMs) achieve superior performance on various NLG evaluation tasks. However, current work often employs the LLM to independently evaluate different aspects, which largely ignores the rich correlation between various aspects. To fill this research gap, in this work, we propose an NLG evaluation metric called CoAScore. Powered by LLMs, the CoAScore utilizes multi-aspect knowledge through a CoA (\\\\\\\\textbf{C}hain-\\\\\\\\textbf{o}f-\\\\\\\\textbf{A}spects) prompting framework when assessing the quality of a certain aspect. Specifically, for a given aspect to evaluate, we first prompt the LLM to generate a chain of aspects that are relevant to the target aspect and could be useful for the evaluation. We then collect evaluation scores for each generated aspect, and finally, leverage the knowledge of these aspects to improve the evaluation of the target aspect. We evaluate CoAScore across five NLG evaluation tasks (e.g., summarization, dialog response generation, etc) and nine aspects (e.g., overall quality, relevance, coherence, etc). Our experimental findings highlight that, in comparison to individual aspect evaluation, CoAScore exhibits a higher correlation with human judgments. This improvement significantly outperforms existing unsupervised evaluation metrics, whether for assessing overall quality or other aspects. We also conducted extensive ablation studies to validate the effectiveness of the three stages within the CoAScore framework and conducted case studies to show how the LLM performs in these stages. Our code and scripts are available.\"\\n      },\\n      {\\n        \"paperId\": \"0cc88cef19caa4a22639e2393407258489aa61fb\",\\n        \"url\": \"https://www.semanticscholar.org/paper/0cc88cef19caa4a22639e2393407258489aa61fb\",\\n        \"title\": \"Thematic Analysis and Artificial Intelligence: A Step-by-Step Process for Using ChatGPT in Thematic Analysis\",\\n        \"citationCount\": 20,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1177/16094069251333886?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1177/16094069251333886, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": null\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2025-04-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2356693836\",\\n            \"name\": \"Muhammad Naeem\"\\n          },\\n          {\\n            \"authorId\": \"2357287042\",\\n            \"name\": \"Tracy Smith\"\\n          },\\n          {\\n            \"authorId\": \"2357851988\",\\n            \"name\": \"Lorna Thomas\"\\n          }\\n        ],\\n        \"abstract\": \"This study sets out how to use generative artificial intelligence (AI) in the six steps of systematic thematic analysis. It leverages AI to address the limitations of traditional thematic analysis. This paper developed prompts (inputs) for ChatGPT (a generative AI chatbot based on a large language model) that are based on many researchers\\\\u2019 discussions and criticisms of qualitative data analysis. The contributions of this paper are twofold. First, it addresses a critical research gap by showcasing ChatGPT prompts for each step of the six steps of systematic thematic analysis, which also addresses researcher training in thematic analysis. Second, it contributes to the development of input to train AI in thematic analysis, including a description of how to familiarize an AI system with the context of a research study and the researcher\\\\u2019s methodological and theoretical considerations; this approach helps to reduce human bias and improves accountability and transparency in thematic analysis.\"\\n      },\\n      {\\n        \"paperId\": \"87fd1c20910a10f4c98e5da9c5144d7231de7901\",\\n        \"url\": \"https://www.semanticscholar.org/paper/87fd1c20910a10f4c98e5da9c5144d7231de7901\",\\n        \"title\": \"Thematic analysis of interview data with ChatGPT: designing and testing a reliable research protocol for qualitative research\",\\n        \"citationCount\": 12,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {\\'abstract\\'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1007/s11135-025-02199-3?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1007/s11135-025-02199-3, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"The ChatGPT model in its current state is unable to substitute the contextual insights and subtle metaphorical nuances associated with human qualitative analysis, interpretation and reflexivity, and the protocol design is able to reliably identify different thematic patterns emerging from the text.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Review\"\\n        ],\\n        \"publicationDate\": \"2025-06-20\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2365893719\",\\n            \"name\": \"Manuel Goyanes\"\\n          },\\n          {\\n            \"authorId\": \"2279054737\",\\n            \"name\": \"Carlos Lopezosa\"\\n          },\\n          {\\n            \"authorId\": \"2142957088\",\\n            \"name\": \"Beatriz Jord\\\\u00e1\"\\n          }\\n        ],\\n        \"abstract\": null\\n      },\\n      {\\n        \"paperId\": \"973b86e015b48e1f4a77f8dacfe4364e3959acd5\",\\n        \"url\": \"https://www.semanticscholar.org/paper/973b86e015b48e1f4a77f8dacfe4364e3959acd5\",\\n        \"title\": \"From Overdiagnosis to Overtreatment of Low-Risk Thyroid Cancer: A Thematic Analysis of Attitudes and Beliefs of Endocrinologists, Surgeons, and Patients\",\\n        \"citationCount\": 69,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7232663\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1089/thy.2019.0587?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1089/thy.2019.0587, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"Surgeons\\' and endocrinologists\\' attitudes and beliefs about overtreatment focus on the automaticity of overdiagnosis, and both patients and providers are cognizant of the cascade of clinical events that propel patients from incidental discovery of a thyroid nodule to surgery.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2020-01-07\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"1481094858\",\\n            \"name\": \"Catherine B. Jensen\"\\n          },\\n          {\\n            \"authorId\": \"17311230\",\\n            \"name\": \"Megan C. Saucke\"\\n          },\\n          {\\n            \"authorId\": \"4717672\",\\n            \"name\": \"D. Francis\"\\n          },\\n          {\\n            \"authorId\": \"113479047\",\\n            \"name\": \"C. Voils\"\\n          },\\n          {\\n            \"authorId\": \"3861366\",\\n            \"name\": \"Susan C. Pitt\"\\n          }\\n        ],\\n        \"abstract\": \"Introduction: The optimal management for patients with small, low-risk thyroid cancer is often debated. We aimed to characterize the attitudes and beliefs of providers and patients about management of small, low-risk thyroid cancer and how they relate to overtreatment. Methods: We conducted 34 semi-structured interviews with surgeons (n\\\\u2009=\\\\u200912), endocrinologists (n\\\\u2009=\\\\u200912), and patients with <1.5\\\\u2009cm papillary thyroid cancer (n\\\\u2009=\\\\u200910). Interviews probed about diagnosis and treatment decision-making, including nonoperative options. We used thematic analysis to identify themes related to overtreatment and created concept diagrams to map observed relationships between themes. Results: When providers discussed management of small, low-risk thyroid cancer, most felt that overtreatment was a problem, and some brought it up without prompting. Providers often believed that overtreatment results from overdiagnosis and relayed how the process commonly starts with incidental discovery of a thyroid nodule on imaging. Providers viewed biopsy of the nodule as a reflexive or habitual action. They ascribed inappropriate biopsy to lack of adherence to or knowledge of guidelines, radiologist recommendations, and the desire of patients and physicians to minimize diagnostic uncertainty. Providers described subsequent cancer diagnosis as an event that \\\\u201copens Pandora\\'s box\\\\u201d and often provokes a strong instinctive, culturally rooted need to proceed with surgery\\\\u2014specifically total thyroidectomy. Consequently, most providers felt that it is easier to prevent overdiagnosis than overtreatment and recommended strategies such as improving guideline adherence, resetting patients\\' expectations, and engaging the media. In contrast, patients did not bring up or openly discuss overtreatment or overdiagnosis. Some patients described the seemingly automatic process from an incidental finding to surgery. Their statements confirmed that the \\\\u201cneed to know\\\\u201d was a major motivation for biopsying their nodule. Patients felt that once they had a cancer diagnosis, surgery was a foregone conclusion. Patients admitted their knowledge about thyroid nodules and cancer was low, leaving room for education about the need for biopsy and less extensive treatment options. Conclusions: Surgeons\\' and endocrinologists\\' attitudes and beliefs about overtreatment focus on the automaticity of overdiagnosis. Both patients and providers are cognizant of the cascade of clinical events that propel patients from incidental discovery of a thyroid nodule to surgery.\"\\n      },\\n      {\\n        \"paperId\": \"2f7899c5c8908c4696fa94d9601f44f019f1a68c\",\\n        \"url\": \"https://www.semanticscholar.org/paper/2f7899c5c8908c4696fa94d9601f44f019f1a68c\",\\n        \"title\": \"Optical coherence tomography confirms non\\\\u2010malignant pigmented lesions in phacomatosis pigmentokeratotica using a support vector machine learning algorithm\",\\n        \"citationCount\": 12,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://onlinelibrary.wiley.com/doi/pdfdirect/10.1111/srt.13377\",\\n          \"status\": \"HYBRID\",\\n          \"license\": \"CCBY\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC10228288, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": null\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2023-05-30\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2213561309\",\\n            \"name\": \"Jenna Lee\"\\n          },\\n          {\\n            \"authorId\": \"2086942038\",\\n            \"name\": \"M. Beirami\"\\n          },\\n          {\\n            \"authorId\": \"1707975\",\\n            \"name\": \"R. Ebrahimpour\"\\n          },\\n          {\\n            \"authorId\": \"51195617\",\\n            \"name\": \"Carolina Puyana\"\\n          },\\n          {\\n            \"authorId\": \"5516425\",\\n            \"name\": \"Maria Tsoukas\"\\n          },\\n          {\\n            \"authorId\": \"150169927\",\\n            \"name\": \"K. Avanaki\"\\n          }\\n        ],\\n        \"abstract\": \"Phacomatosis pigmentokeratotica (PPK), an epidermal nevus syndrome, is characterized by the coexistence of nevus spilus and nevus sebaceus. Within the nevus spilus, an extensive range of atypical nevi of different morphologies may manifest. Pigmented lesions may fulfill the ABCDE criteria for melanoma, which may prompt a physician to perform a full\\\\u2010thickness biopsy.\"\\n      },\\n      {\\n        \"paperId\": \"847ee2ae8b588e94fcc3ff33be0601cd354df75b\",\\n        \"url\": \"https://www.semanticscholar.org/paper/847ee2ae8b588e94fcc3ff33be0601cd354df75b\",\\n        \"title\": \"Professional noticing coherence: exploring relationships between component processes\",\\n        \"citationCount\": 15,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1080/10986065.2021.1977086?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1080/10986065.2021.1977086, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": null,\\n        \"publicationTypes\": null,\\n        \"publicationDate\": \"2021-09-22\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2118528180\",\\n            \"name\": \"Jonathan Thomas\"\\n          },\\n          {\\n            \"authorId\": \"66391853\",\\n            \"name\": \"David M. Dueber\"\\n          },\\n          {\\n            \"authorId\": \"50589798\",\\n            \"name\": \"Molly H. Fisher\"\\n          },\\n          {\\n            \"authorId\": \"71615154\",\\n            \"name\": \"C. Jong\"\\n          },\\n          {\\n            \"authorId\": \"73308684\",\\n            \"name\": \"E. Schack\"\\n          }\\n        ],\\n        \"abstract\": \"ABSTRACT Teacher noticing and related variants have ascended in prominence among the mathematics education research community. While the component processes of such noticing (e.g., attending, interpreting and deciding) have been cast as interrelated, capturing the relationships amongst the components has been more elusive. We focused on the component processes of teacher noticing with particular attention given to interrelatedness. Specifically, we were interested in how and the extent to which the component processes of professional noticing (attending, interpreting, deciding) are thematically connected when preservice elementary teachers are engaged in an assessment approximating professional noticing. We refer to this thematic linkage in this paper as coherence. Our findings suggest a complex interplay between the creation and continuation of themes when enacting professional noticing, and the quality of such noticing.\"\\n      },\\n      {\\n        \"paperId\": \"6312d430e9fd98c23af1198ea0f57e3f6e59d542\",\\n        \"url\": \"https://www.semanticscholar.org/paper/6312d430e9fd98c23af1198ea0f57e3f6e59d542\",\\n        \"title\": \"An Investigation of the Coherence of Oral Narratives: Associations With Mental Health, Social Support and the Coherence of Written Narratives\",\\n        \"citationCount\": 11,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://www.frontiersin.org/articles/10.3389/fpsyg.2020.602725/pdf\",\\n          \"status\": \"GOLD\",\\n          \"license\": \"CCBY\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC7838430, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"The ability to elaborate emotionally and make meaning out of important life events in oral narratives is, to a certain extent, related to better mental health and more social support and thematic coherence may be a relatively stable feature of individuals\\\\u2019 narrative styles that is reflected in narratives of different modalities.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2021-01-13\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"84155118\",\\n            \"name\": \"L. Vanaken\"\\n          },\\n          {\\n            \"authorId\": \"4062885\",\\n            \"name\": \"P. Bijttebier\"\\n          },\\n          {\\n            \"authorId\": \"145065355\",\\n            \"name\": \"D. Hermans\"\\n          }\\n        ],\\n        \"abstract\": \"Research Questions In a first research question, we examined whether the relations that are generally observed between the coherence of written autobiographical narratives and outcomes of mental health and social support, can be replicated for the coherence of oral narratives. Second, we studied whether the coherence of oral narratives is related to the coherence of written narratives. Methods Pearson correlations and t-tests were calculated on data of two separate studies to examine the research questions. Results First, only thematic coherence of oral narratives was significantly, although moderately, negatively associated to symptoms of depression, anxiety and negative social interactions. Second, the coherence of oral narratives was higher than the coherence of written narratives. Only the thematic coherence of oral narratives was positively associated with thematic and total coherence of written narratives. Furthermore, correlations between written and oral narratives were stronger for negative narratives as compared to positive narratives. Discussion The ability to elaborate emotionally and make meaning out of important life events in oral narratives is, to a certain extent, related to better mental health and more social support. Furthermore, thematic coherence may be a relatively stable feature of individuals\\\\u2019 narrative styles that is reflected in narratives of different modalities. Nonetheless, these topics need to be further researched to overcome present limitations.\"\\n      },\\n      {\\n        \"paperId\": \"5442ac92fbdbe515cbb71cd03ac5063ebb1d5e08\",\\n        \"url\": \"https://www.semanticscholar.org/paper/5442ac92fbdbe515cbb71cd03ac5063ebb1d5e08\",\\n        \"title\": \"Deep Learning Model Based on 3D Optical Coherence Tomography Images for the Automated Detection of Pathologic Myopia\",\\n        \"citationCount\": 30,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://www.mdpi.com/2075-4418/12/3/742/pdf?version=1647601223\",\\n          \"status\": \"GOLD\",\\n          \"license\": \"CCBY\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC8947335, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This study aimed to develop an algorithm that uses 3D optical coherence tomography volumetric images (C-scan) to automatically diagnose patients with pathologic myopia and found the model based on EfficientNetB4 showed the best performance.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2022-03-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2115235853\",\\n            \"name\": \"S. Park\"\\n          },\\n          {\\n            \"authorId\": \"2380782\",\\n            \"name\": \"T. Ko\"\\n          },\\n          {\\n            \"authorId\": \"8593047\",\\n            \"name\": \"Chan-Kee Park\"\\n          },\\n          {\\n            \"authorId\": \"2124919953\",\\n            \"name\": \"Yong-Chan Kim\"\\n          },\\n          {\\n            \"authorId\": \"2159622586\",\\n            \"name\": \"In-Young Choi\"\\n          }\\n        ],\\n        \"abstract\": \"Pathologic myopia causes vision impairment and blindness, and therefore, necessitates a prompt diagnosis. However, there is no standardized definition of pathologic myopia, and its interpretation by 3D optical coherence tomography images is subjective, requiring considerable time and money. Therefore, there is a need for a diagnostic tool that can automatically and quickly diagnose pathologic myopia in patients. This study aimed to develop an algorithm that uses 3D optical coherence tomography volumetric images (C-scan) to automatically diagnose patients with pathologic myopia. The study was conducted using 367 eyes of patients who underwent optical coherence tomography tests at the Ophthalmology Department of Incheon St. Mary\\\\u2019s Hospital and Seoul St. Mary\\\\u2019s Hospital from January 2012 to May 2020. To automatically diagnose pathologic myopia, a deep learning model was developed using 3D optical coherence tomography images. The model was developed using transfer learning based on four pre-trained convolutional neural networks (ResNet18, ResNext50, EfficientNetB0, EfficientNetB4). Grad-CAM was used to visualize features affecting the detection of pathologic myopia. The performance of each model was evaluated and compared based on accuracy, sensitivity, specificity, and area under the receiver operating characteristic curve (AUROC). The model based on EfficientNetB4 showed the best performance (95% accuracy, 93% sensitivity, 96% specificity, and 98% AUROC) in identifying pathologic myopia.\"\\n      },\\n      {\\n        \"paperId\": \"1157efb8ddfbfdbe3e23078ae823507e3b8971ea\",\\n        \"url\": \"https://www.semanticscholar.org/paper/1157efb8ddfbfdbe3e23078ae823507e3b8971ea\",\\n        \"title\": \"Discursive Fields and the Diversity-Coherence Paradox: An Ecological Perspective on the Blockchain Community Discourse\",\\n        \"citationCount\": 23,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.25300/misq/2022/15736?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.25300/misq/2022/15736, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": null,\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2022-09-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2088902\",\\n            \"name\": \"S. Miranda\"\\n          },\\n          {\\n            \"authorId\": \"2111270075\",\\n            \"name\": \"D. Wang\"\\n          },\\n          {\\n            \"authorId\": \"2159799633\",\\n            \"name\": \"Chuan (Annie) Tian\"\\n          }\\n        ],\\n        \"abstract\": \"Innovation breakthroughs prompt sensemaking discourses that promote community learning and socially construct the innovation. Through this discourse, interested actors advance diverse frames, appealing to consumers with disparate preferences but raising concerns for the coherence of that discourse. We unpack this diversity-coherence paradox by recasting coherence as the relatedness of innovation frames and spotlighting the role of discursive fields that circumscribe meaning. Our empirical context is the first six years of blockchain discourse across seven discursive fields. Our research offers three insights in furtherance of an ecological perspective on innovation discourse. First, framing diversity emanates from discursive fields rather than from actors. Second, fields play differentiated roles in the framing process. Enactment fields comprised of actors with direct experience with the technology limit diversity. They do so by erecting walls that circumscribe discourse through imprinting on their original frame and retracting from or abandoning frames learned from other fields. In contrast, mediated fields, in which actors lack direct experience with the technology, enhance diversity. They do so by imitating or learning from other fields and foreshadowing or anticipating the frames used by other fields, thereby building bridges. Third, rather than opposing each other, diversity and coherence coevolve as the diversity induced by mediated fields increases framing redundancies, synthesizing frames into a coherent community understanding of the innovation. Our research signals to the actors who serve as innovation ambassadors and gatekeepers that diverse views of an innovation are not only inevitable, given the many discourse fields in which those views are formulated, but can also be coherent and desirable.\"\\n      },\\n      {\\n        \"paperId\": \"5fe7d2d670adf17a2ae2a34d76129e43fb7c2d9f\",\\n        \"url\": \"https://www.semanticscholar.org/paper/5fe7d2d670adf17a2ae2a34d76129e43fb7c2d9f\",\\n        \"title\": \"The Irish Football Player Pathway: Examining Stakeholder Coherence Throughout and Across the Player Development System\",\\n        \"citationCount\": 20,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://www.frontiersin.org/articles/10.3389/fspor.2022.834633/pdf\",\\n          \"status\": \"GOLD\",\\n          \"license\": \"CCBY\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC8884116, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"A lack of stakeholder coherence is highlighted across the Irish player pathway to maximize long-term player development in the future, with findings highlighting the need for organizational intervention and structural change across the pathway.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2022-02-14\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"119033786\",\\n            \"name\": \"Liam Sweeney\"\\n          },\\n          {\\n            \"authorId\": \"6973495\",\\n            \"name\": \"\\\\u00c1. Macnamara\"\\n          },\\n          {\\n            \"authorId\": \"103263632\",\\n            \"name\": \"Daniel M. Horan\"\\n          }\\n        ],\\n        \"abstract\": \"Maximizing the efficiency of the player development system is a strategic priority for any professional football club or association. However, the successful development of a young footballer is largely dependent upon the roles and relationships of the different stakeholders invested in the developmental process. This study examined the level of horizontal (i.e., extent to which stakeholders across a pathway stage work with players in an agreed fashion to optimize their experience) and vertical (i.e., extent to which multiple stages of the pathway are coordinated and build chronologically from previous involvement toward long-term needs) stakeholder coherence throughout the Irish football player pathway following a restructuring of development policies and the implementation of a nationwide academy system between 2016 and 2020 under the Football Association of Ireland\\'s (FAI) Player Development Plan. As a second aim, we explored each of the key stakeholders\\' alignment to academic talent development principles in order to provide practical recommendations for future player and coach development policies. Accordingly, a series of interviews were conducted with 31 key stakeholders currently engaged in the player pathway. These key stakeholders consisted of parents, coaches and members of the FAI as the National Governing Body for football in Ireland. Data were analyzed using Reflexive Thematic Analysis, with findings highlighting a lack of stakeholder coherence across the pathway, both vertically and horizontally. Stakeholders displayed inconsistency in their understanding of the purpose of the player pathway and its long-term strategic aims, as well as demonstrating poor and incohesive relationships with each of the different stakeholders. Moreover, talent development principles between the different stakeholders appeared well-understood overall, although the practical implementation of several of these principles in applied practice did not appear to exist. Results highlight the need for organizational intervention and structural change across the Irish player pathway to maximize long-term player development in the future. Practical implications for the FAI are discussed and recommendations are made to support optimal player development policies moving forward.\"\\n      },\\n      {\\n        \"paperId\": \"e55ed5e7e6f998874e4edeed7f257ec8d148fc11\",\\n        \"url\": \"https://www.semanticscholar.org/paper/e55ed5e7e6f998874e4edeed7f257ec8d148fc11\",\\n        \"title\": \"An Analysis of Cohesion and Coherence of Descriptive Texts Written by Junior High School Students\",\\n        \"citationCount\": 10,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://www.atlantis-press.com/article/125956016.pdf\",\\n          \"status\": \"GOLD\",\\n          \"license\": \"CCBYNC\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.2991/ASSEHR.K.210427.030?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.2991/ASSEHR.K.210427.030, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": null,\\n        \"publicationTypes\": null,\\n        \"publicationDate\": \"2021-04-28\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2121899244\",\\n            \"name\": \"Galis Muthia Zahra\"\\n          },\\n          {\\n            \"authorId\": \"2345026648\",\\n            \"name\": \"Emi Emilia\"\\n          },\\n          {\\n            \"authorId\": \"77566729\",\\n            \"name\": \"Iyen Nurlaelawati\"\\n          }\\n        ],\\n        \"abstract\": \"The study aims to investigate the cohesion and coherence of descriptive texts written by seventh grade junior high school students. This study was conducted in the even semester of the 2019/2020 academic year during the Covid-19 pandemic. This study used a qualitative case study design, and the data were obtained from six texts representing high, middle, and low achiever students. To identify the texts\\\\u2019 cohesion and coherence, the grammar of textual metafunction from systemic functional linguistics (SFL), that is the theme system was used. The findings show that all students had the ability to make descriptive texts in terms of cohesion and coherence. All texts successfully used different types of themes, including topical and textual themes, and thematic progression, including the zigzag and reiteration patterns to create coherence especially at the clause level. Various cohesive devices such as reference, conjunction, lexical, and ellipsis were also used to create a cohesive text. It was also found that the texts written by high achiever students were more coherent than the texts written by middle and low achiever students due to several aspects such as the more diverse pattern and the more frequent use of pattern. In addition, the high and middle achiever texts seemed more cohesive than the low achiever texts due to the high number of cohesive devices used in the middle achiever texts, and high number of conjunctions used in the middle and high achiever texts. Based on the findings, more support is needed from teachers when teaching descriptive text to middle and low achiever students, especially in a pandemic era.\"\\n      },\\n      {\\n        \"paperId\": \"46812cd7024c70dadaaecba7d854252f27b582ec\",\\n        \"url\": \"https://www.semanticscholar.org/paper/46812cd7024c70dadaaecba7d854252f27b582ec\",\\n        \"title\": \"Molecular Contrast Optical Coherence Tomography and Its Applications in Medicine\",\\n        \"citationCount\": 13,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://www.mdpi.com/1422-0067/23/6/3038/pdf?version=1647248458\",\\n          \"status\": \"GOLD\",\\n          \"license\": \"CCBY\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC8949853, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"The recent advances of molecular contrast OCT imaging techniques, the corresponding contrast agents, especially the nanoparticle-based ones, and their applications are reviewed and the properties, design criteria, merit, and demerit of those contrast agents are summarized.\"\\n        },\\n        \"publicationTypes\": [\\n          \"Review\",\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2022-03-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"35035162\",\\n            \"name\": \"Ancong Wang\"\\n          },\\n          {\\n            \"authorId\": \"2105875970\",\\n            \"name\": \"Wenliu Qi\"\\n          },\\n          {\\n            \"authorId\": \"4707531\",\\n            \"name\": \"Tianxin Gao\"\\n          },\\n          {\\n            \"authorId\": \"2118488394\",\\n            \"name\": \"Xiaoying Tang\"\\n          }\\n        ],\\n        \"abstract\": \"The growing need to understand the molecular mechanisms of diseases has prompted the revolution in molecular imaging techniques along with nanomedicine development. Conventional optical coherence tomography (OCT) is a low-cost in vivo imaging modality that provides unique high spatial and temporal resolution anatomic images but little molecular information. However, given the widespread adoption of OCT in research and clinical practice, its robust molecular imaging extensions are strongly desired to combine with anatomical images. A range of relevant approaches has been reported already. In this article, we review the recent advances of molecular contrast OCT imaging techniques, the corresponding contrast agents, especially the nanoparticle-based ones, and their applications. We also summarize the properties, design criteria, merit, and demerit of those contrast agents. In the end, the prospects and challenges for further research and development in this field are outlined.\"\\n      },\\n      {\\n        \"paperId\": \"5b906cd1a282df558db06fb94741c66294c77a43\",\\n        \"url\": \"https://www.semanticscholar.org/paper/5b906cd1a282df558db06fb94741c66294c77a43\",\\n        \"title\": \"The Textual-Visual Thematic Analysis: A Framework to Analyze the Conjunction and Interaction of Visual and Textual Data\",\\n        \"citationCount\": 25,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://nsuworks.nova.edu/cgi/viewcontent.cgi?article=5456&context=tqr\",\\n          \"status\": \"GOLD\",\\n          \"license\": \"CCBYNCSA\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.46743/2160-3715/2022.5456?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.46743/2160-3715/2022.5456, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": null,\\n        \"publicationTypes\": null,\\n        \"publicationDate\": null,\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2158241407\",\\n            \"name\": \"Gabriela Trombeta\"\\n          },\\n          {\\n            \"authorId\": \"120232113\",\\n            \"name\": \"S. Cox\"\\n          }\\n        ],\\n        \"abstract\": \"Visual methods offer an innovative approach to qualitative research through their potential to prompt dialogue, enrich verbal and textual data, and enable participants to communicate about difficult topics. However, the use of visual methods requires that researchers rethink methodological aspects of data generation and analysis, especially when working with participant-generated images. Although there are now many analytical frameworks and guidebooks providing instructions on the analysis of textual and visual materials, detailed descriptions of how these elements are brought together are often missing from research reports, precluding novice and other researchers from understanding how findings were attained. Our aim in this article is to describe and illustrate the Textual-Visual Thematic Analysis (TVTA), a framework we developed to collaboratively analyze the conjunction and interaction of textual and visual data in a photo-elicitation study. Given that the ethical and methodological aspects are deeply entwined, we begin the article by contextualizing the data obtained from the photo-elicitation study and then consider confidentiality and approaches to valuing participants\\' voices. Next, we share the TVTA framework, its procedural implementation, and insights derived from evolving our data analysis approach. We conclude by offering reflections on the limitations and possibilities for future research.\"\\n      },\\n      {\\n        \"paperId\": \"b2f8cbaa22e4a8aac716a6f035f324fd667d7e8f\",\\n        \"url\": \"https://www.semanticscholar.org/paper/b2f8cbaa22e4a8aac716a6f035f324fd667d7e8f\",\\n        \"title\": \"Assessment of COVID-19 pandemic responses in African countries: thematic synthesis of WHO intra-action review reports\",\\n        \"citationCount\": 27,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://bmjopen.bmj.com/content/bmjopen/12/5/e056896.full.pdf\",\\n          \"status\": \"GOLD\",\\n          \"license\": \"CCBY\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC9062458, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"It is suggested that African countries\\\\u2019 response to the COVID-19 pandemic was prompt and may have contributed to the lower cases and deaths in the region compared with countries in other regions.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Review\"\\n        ],\\n        \"publicationDate\": \"2022-05-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"6312621\",\\n            \"name\": \"A. Talisuna\"\\n          },\\n          {\\n            \"authorId\": \"8406874\",\\n            \"name\": \"C. Iwu\"\\n          },\\n          {\\n            \"authorId\": \"6612736\",\\n            \"name\": \"J. Okeibunor\"\\n          },\\n          {\\n            \"authorId\": \"31474564\",\\n            \"name\": \"Mary Stephen\"\\n          },\\n          {\\n            \"authorId\": \"144645159\",\\n            \"name\": \"E. Musa\"\\n          },\\n          {\\n            \"authorId\": \"32861033\",\\n            \"name\": \"B. Herring\"\\n          },\\n          {\\n            \"authorId\": \"6327315\",\\n            \"name\": \"Otim Patrick Cossy Ramadan\"\\n          },\\n          {\\n            \"authorId\": \"15071000\",\\n            \"name\": \"Daniel Yota\"\\n          },\\n          {\\n            \"authorId\": \"6230122\",\\n            \"name\": \"M. Nanyunja\"\\n          },\\n          {\\n            \"authorId\": \"6894832\",\\n            \"name\": \"A. Mpairwe\"\\n          },\\n          {\\n            \"authorId\": \"1442059469\",\\n            \"name\": \"F. Banza\"\\n          },\\n          {\\n            \"authorId\": \"153908087\",\\n            \"name\": \"A. Diallo\"\\n          },\\n          {\\n            \"authorId\": \"1442059350\",\\n            \"name\": \"Roland Kimbi Wango\"\\n          },\\n          {\\n            \"authorId\": \"2129120286\",\\n            \"name\": \"Christian Massidi\"\\n          },\\n          {\\n            \"authorId\": \"134071938\",\\n            \"name\": \"Hilary K. Njenge\"\\n          },\\n          {\\n            \"authorId\": \"2128400616\",\\n            \"name\": \"M. Traore\"\\n          },\\n          {\\n            \"authorId\": \"1442061510\",\\n            \"name\": \"Antonio Oke\"\\n          },\\n          {\\n            \"authorId\": \"1483613748\",\\n            \"name\": \"Boukare Bonkoungou\"\\n          },\\n          {\\n            \"authorId\": \"1381443808\",\\n            \"name\": \"Landry Ndriko Mayigane\"\\n          },\\n          {\\n            \"authorId\": \"6166357\",\\n            \"name\": \"I. Conteh\"\\n          },\\n          {\\n            \"authorId\": \"2129119453\",\\n            \"name\": \"Fekadu Senait\"\\n          },\\n          {\\n            \"authorId\": \"6150958\",\\n            \"name\": \"S. Chungong\"\\n          },\\n          {\\n            \"authorId\": \"10314719\",\\n            \"name\": \"B. Impouma\"\\n          },\\n          {\\n            \"authorId\": \"2121935091\",\\n            \"name\": \"Nsenga Ngoy\"\\n          },\\n          {\\n            \"authorId\": \"6979323\",\\n            \"name\": \"C. Wiysonge\"\\n          },\\n          {\\n            \"authorId\": \"4284508\",\\n            \"name\": \"Z. Yoti\"\\n          },\\n          {\\n            \"authorId\": \"2969537\",\\n            \"name\": \"A. Gueye\"\\n          }\\n        ],\\n        \"abstract\": \"Objectives We conducted a review of intra-action review (IAR) reports of the national response to the COVID-19 pandemic in Africa. We highlight best practices and challenges and offer perspectives for the future. Design A thematic analysis across 10 preparedness and response domains, namely, governance, leadership, and coordination; planning and monitoring; risk communication and community engagement; surveillance, rapid response, and case investigation; infection prevention and control; case management; screening and monitoring at points of entry; national laboratory system; logistics and supply chain management; and maintaining essential health services during the COVID-19 pandemic. Setting All countries in the WHO African Region were eligible for inclusion in the study. National IAR reports submitted by March 2021 were analysed. Results We retrieved IAR reports from 18 African countries. The COVID-19 pandemic response in African countries has relied on many existing response systems such as laboratory systems, surveillance systems for previous outbreaks of highly infectious diseases and a logistics management information system. These best practices were backed by strong political will. The key challenges included low public confidence in governments, inadequate adherence to infection prevention and control measures, shortages of personal protective equipment, inadequate laboratory capacity, inadequate contact tracing, poor supply chain and logistics management systems, and lack of training of key personnel at national and subnational levels. Conclusion These findings suggest that African countries\\\\u2019 response to the COVID-19 pandemic was prompt and may have contributed to the lower cases and deaths in the region compared with countries in other regions. The IARs demonstrate that many technical areas still require immediate improvement to guide decisions in subsequent waves or future outbreaks.\"\\n      },\\n      {\\n        \"paperId\": \"7b61104ddb7302f51dc30a77cfc766b483db2fdc\",\\n        \"url\": \"https://www.semanticscholar.org/paper/7b61104ddb7302f51dc30a77cfc766b483db2fdc\",\\n        \"title\": \"Multi-Temporal Sentinel-1 Backscatter and Coherence for Rainforest Mapping\",\\n        \"citationCount\": 33,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://www.mdpi.com/2072-4292/12/5/847/pdf?version=1583481683\",\\n          \"status\": \"GOLD\",\\n          \"license\": \"CCBY\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.3390/rs12050847?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.3390/rs12050847, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"This paper presents how the interferometric capabilities of the Sentinel-1 satellites constellation can be exploited for the monthly mapping of the Amazon rainforest and an analysis on the benefits of the use of textural information, derived from Sentinel-2 backscatter, in order to enhance the classification accuracy.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2020-03-06\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"2049260\",\\n            \"name\": \"A. Pulella\"\\n          },\\n          {\\n            \"authorId\": \"1617617187\",\\n            \"name\": \"Rodrigo Arag\\\\u00e3o Santos\"\\n          },\\n          {\\n            \"authorId\": \"51243828\",\\n            \"name\": \"F. Sica\"\\n          },\\n          {\\n            \"authorId\": \"70278054\",\\n            \"name\": \"Philipp Posovszky\"\\n          },\\n          {\\n            \"authorId\": \"2261116\",\\n            \"name\": \"P. Rizzoli\"\\n          }\\n        ],\\n        \"abstract\": \"This paper reports recent advancements in the field of Synthetic Aperture Radar (SAR) for forest mapping by using interferometric short-time-series. In particular, we first present how the interferometric capabilities of the Sentinel-1 satellites constellation can be exploited for the monthly mapping of the Amazon rainforest. Indeed, the evolution in time of the interferometric coherence can be properly modeled as an exponential decay and the retrieved interferometric parameters can be used, together with the backscatter, as input features to the machine learning Random Forests classifier. Furthermore, we present an analysis on the benefits of the use of textural information, derived from Sentinel-1 backscatter, in order to enhance the classification accuracy. These textures are computed through the Sum And Difference Histograms methodology and the final classification accuracy, resulting by adding them to the aforementioned features, is a thematic map that exceeds an overall agreement of 85%, when validated using the optical external reference Finer Resolution Observation and Monitoring of Global Land Cover (FROM-GLC) map. The experiments presented in the final part of the paper are enriched with a further analysis and discussion on the selected scenes using updated multispectral Sentinel-2 acquisitions.\"\\n      },\\n      {\\n        \"paperId\": \"a37bd8ad2c2356a39fa0271479c6b46e8b108d2b\",\\n        \"url\": \"https://www.semanticscholar.org/paper/a37bd8ad2c2356a39fa0271479c6b46e8b108d2b\",\\n        \"title\": \"\\\\u2018This bloody rona!\\\\u2019: using the digital story completion method and thematic analysis to explore the mental health impacts of COVID-19 in Australia\",\\n        \"citationCount\": 15,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://bmjopen.bmj.com/content/bmjopen/12/1/e057393.full.pdf\",\\n          \"status\": \"GOLD\",\\n          \"license\": \"CCBYNC\",\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC8764712, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"It is suggested that points of convergence across stories indicate a level of shared experience among participants relating to COVID-19 in Australia, due to intensive media coverage of the pandemic, persistent public health messaging, engagement with social media and instant messaging technologies, and extended lockdowns that impacted the mental health of vast numbers of Australians.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2022-01-01\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"153603815\",\\n            \"name\": \"Priya Vaughan\"\\n          },\\n          {\\n            \"authorId\": \"40481393\",\\n            \"name\": \"Caroline Lenette\"\\n          },\\n          {\\n            \"authorId\": \"5498463\",\\n            \"name\": \"K. Boydell\"\\n          }\\n        ],\\n        \"abstract\": \"Objective To use the digital story completion method to prompt participants to describe thoughts, fears and mental health experiences in response to a story stem about COVID-19, to capture a specific sociohistoric moment. Design We used digital story completion, a qualitative research method, to gather narratives from Australians coping with physical distancing and social restriction measures. Our reflexive thematic analysis of the data was underpinned by a constructionist approach to reflect the importance of social context in understanding health experiences. Setting Australia. Participants 52 people living in Australia (aged 18 years and over). Results Four meta-themes were prevalent across 52 stories submitted: (1) expressions of mental distress linked to COVID-19; (2) various coping strategies offered by characters in stories; (3) narratives outlining social support offered to alleviate distress; and (4) specialised COVID-19 vocabulary. Conclusion We cautiously propose that points of convergence across stories indicate a level of shared experience among participants relating to COVID-19 in Australia. We suggest this is due to intensive media coverage of the pandemic, persistent public health messaging, engagement with social media and instant messaging technologies, and extended lockdowns that impacted the mental health of vast numbers of Australians.\"\\n      },\\n      {\\n        \"paperId\": \"b3535815a1cd7ed648ea64c591802301822e576a\",\\n        \"url\": \"https://www.semanticscholar.org/paper/b3535815a1cd7ed648ea64c591802301822e576a\",\\n        \"title\": \"Achieving healthy ageing through the perspective of sense of coherence among senior-only households: a qualitative study\",\\n        \"citationCount\": 20,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1080/13607863.2020.1725805?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1080/13607863.2020.1725805, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"It is suggested that sense of coherence towards the pursuit of healthy ageing can be addressed by reducing the unpredictability of ageing-related processes and vulnerabilities (comprehensibility), supporting active adoption of actions which promotes physical, mental and social health (manageability) and individual reflection in making sense of old age to seek motivation in living each day purposefully (meaningfulness).\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2020-02-18\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"14137710\",\\n            \"name\": \"Betsy Seah\"\\n          },\\n          {\\n            \"authorId\": \"6102067\",\\n            \"name\": \"G. Espnes\"\\n          },\\n          {\\n            \"authorId\": \"1870959\",\\n            \"name\": \"E. Ang\"\\n          },\\n          {\\n            \"authorId\": \"71772268\",\\n            \"name\": \"Jian Yang Lim\"\\n          },\\n          {\\n            \"authorId\": \"6631742\",\\n            \"name\": \"Y. Kowitlawakul\"\\n          },\\n          {\\n            \"authorId\": \"2395324152\",\\n            \"name\": \"Wenru Wang\"\\n          }\\n        ],\\n        \"abstract\": \"Abstract Objectives: Explore perceptions towards healthy ageing through the perspective of sense of coherence among older adults residing in senior-only households. Methods: A qualitative study using focus group interviews was conducted and appreciative inquiry was adopted as a strengths-based interviewing approach. 27 older adults who either live alone or with their spouses only were involved in six focus group discussions at a community centre in Singapore. Data saturation was achieved and thematic analysis was performed to analyse the data. Results: The four emerging themes were (1) contending evolving vulnerabilities, (2) intrinsic value of health, (3) taking care of oneself is a personal responsibility, and (4) taking one day at a time: outlook towards later part of life. Older adults\\\\u2019 underlying pathogenic orientation towards health contributed to their perceived unpredictable confrontations with vicissitudes including illness and death. This played a part to their short outlook towards old age. Consequently, this could limit their will and abilities to seek meaningful pursuits or valued aspirations and movement towards the salutogenic health pole. Conclusion: By reframing the definition of health to pursuing and fulfilling valued accomplishments, optimal health can be achieved regardless of physical health state. This study suggested that sense of coherence towards the pursuit of healthy ageing can be addressed by reducing the unpredictability of ageing-related processes and vulnerabilities (comprehensibility), supporting active adoption of actions which promotes physical, mental and social health (manageability) and individual reflection in making sense of old age to seek motivation in living each day purposefully (meaningfulness).\"\\n      },\\n      {\\n        \"paperId\": \"dd80b198ab64a4c04cde06ed5cf31662270b86af\",\\n        \"url\": \"https://www.semanticscholar.org/paper/dd80b198ab64a4c04cde06ed5cf31662270b86af\",\\n        \"title\": \"The experience of using prompting technology from the perspective of people with Dementia and their primary carers\",\\n        \"citationCount\": 26,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1080/13607863.2020.1745145?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1080/13607863.2020.1745145, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"Carers and participants were influenced by their approaches to technology and determined the usefulness of the prompter according to whether it worked for them and fitted into their routines, consistent with the Technology Acceptance Model.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2020-03-30\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"145617572\",\\n            \"name\": \"N. Evans\"\\n          },\\n          {\\n            \"authorId\": \"48063215\",\\n            \"name\": \"H. Boyd\"\\n          },\\n          {\\n            \"authorId\": \"48688534\",\\n            \"name\": \"N. Harris\"\\n          },\\n          {\\n            \"authorId\": \"38349961\",\\n            \"name\": \"K. Noonan\"\\n          },\\n          {\\n            \"authorId\": \"40227345\",\\n            \"name\": \"T. Ingram\"\\n          },\\n          {\\n            \"authorId\": \"48756330\",\\n            \"name\": \"A. Jarvis\"\\n          },\\n          {\\n            \"authorId\": \"1613011708\",\\n            \"name\": \"J. Ridgers\"\\n          },\\n          {\\n            \"authorId\": \"144757940\",\\n            \"name\": \"R. Cheston\"\\n          }\\n        ],\\n        \"abstract\": \"Abstract Objectives People who are living with dementia typically experience difficulties in completing multi-step, everyday tasks. However, digital technology such as touchscreen tablets provide a means of delivering concise personalised prompts that combine audio, text and pictures. This study was one component of a broader, mixed methods study that tested how an application (app) \\\\u2013based prompter running on a touchscreen tablet computer could support everyday activities in individuals with mild to moderate dementia. In this study we set out to understand the experiences of people living with dementia and their primary carer in using the prompter over a four-week period. Method We collected qualitative data using semi-structured interviews from 26 dyads, composed of a person living with dementia and their carer. Dyads were interviewed at the start and end of this period. Transcripts were then analysed using thematic analysis. Results The study identified three overarching themes related to: participants\\\\u2019 attitudes towards the technology; their judgements about how useful the prompter would be; and the emotional impact of using it. Conclusion Consistent with the Technology Acceptance Model, carers and participants were influenced by their approaches to technology and determined the usefulness of the prompter according to whether it worked for them and fitted into their routines. In addition, participants\\\\u2019 decisions about using the prompter were also determined by the extent to which doing so would impact on their self-identity.\"\\n      },\\n      {\\n        \"paperId\": \"fb1d85fe28b5e92e22d084eca674d4a2b48cdc5a\",\\n        \"url\": \"https://www.semanticscholar.org/paper/fb1d85fe28b5e92e22d084eca674d4a2b48cdc5a\",\\n        \"title\": \"Prompting to Distill: Boosting Data-Free Knowledge Distillation via Reinforced Prompt\",\\n        \"citationCount\": 12,\\n        \"openAccessPdf\": {\\n          \"url\": \"https://arxiv.org/pdf/2205.07523\",\\n          \"status\": \"GREEN\",\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2205.07523, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"PromptDFD leverages a pre-trained generative model to provide language priors and introduces a reinforced topic prompter to control data synthesis, making the generated samples thematically relevant and semantically plausible, and thus friendly to downstream tasks.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\",\\n          \"Conference\"\\n        ],\\n        \"publicationDate\": \"2022-05-16\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"15532066\",\\n            \"name\": \"Xinyin Ma\"\\n          },\\n          {\\n            \"authorId\": \"48631088\",\\n            \"name\": \"Xinchao Wang\"\\n          },\\n          {\\n            \"authorId\": \"150110431\",\\n            \"name\": \"Gongfan Fang\"\\n          },\\n          {\\n            \"authorId\": \"1471660296\",\\n            \"name\": \"Yongliang Shen\"\\n          },\\n          {\\n            \"authorId\": \"1776903\",\\n            \"name\": \"Weiming Lu\"\\n          }\\n        ],\\n        \"abstract\": \"Data-free knowledge distillation (DFKD) conducts knowledge distillation via eliminating the dependence of original training data, and has recently achieved impressive results in accelerating pre-trained language models. At the heart of DFKD is to reconstruct a synthetic dataset by inverting the parameters of the uncompressed model. Prior DFKD approaches, however, have largely relied on hand-crafted priors of the target data distribution for the reconstruction, which can be inevitably biased and often incompetent to capture the intrinsic distributions. To address this problem, we propose a prompt-based method, termed as PromptDFD, that allows us to take advantage of learned language priors, which effectively harmonizes the synthetic sentences to be semantically and grammatically correct. Specifically, PromptDFD leverages a pre-trained generative model to provide language priors and introduces a reinforced topic prompter to control data synthesis, making the generated samples thematically relevant and semantically plausible, and thus friendly to downstream tasks. As shown in our experiments, the proposed method substantially improves the synthesis quality and achieves considerable improvements on distillation performance. In some cases, PromptDFD even gives rise to results on par with those from the data-driven knowledge distillation with access to the original training data.\"\\n      },\\n      {\\n        \"paperId\": \"89bbc885e69ff2098b478893cf2e17d8a13464ff\",\\n        \"url\": \"https://www.semanticscholar.org/paper/89bbc885e69ff2098b478893cf2e17d8a13464ff\",\\n        \"title\": \"Rethinking sense of coherence: Perceptions of comprehensibility, manageability, and meaningfulness in a group of Palestinian health care providers operating in the West Bank and Israel\",\\n        \"citationCount\": 12,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1177/1363461520941386?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1177/1363461520941386, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": {\\n          \"model\": \"tldr@v2.0.0\",\\n          \"text\": \"It is found that context-specific features of SOC can mobilize generalized resistance resources for coping with traumatic and stressful experiences, even in an environment characterized by political instability, military violence, and social trauma.\"\\n        },\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": \"2020-08-26\",\\n        \"authors\": [\\n          {\\n            \"authorId\": \"35149021\",\\n            \"name\": \"G. Veronese\"\\n          },\\n          {\\n            \"authorId\": \"1912267580\",\\n            \"name\": \"Yamina Dhaouadi\"\\n          },\\n          {\\n            \"authorId\": \"51039166\",\\n            \"name\": \"Abdelhamid Afana\"\\n          }\\n        ],\\n        \"abstract\": \"Drawing on a salutogenic perspective, we explored sense of coherence (SOC) in a group of Palestinian mental health care providers living and working in Israel and the occupied Palestinian territories (West Bank). Specifically, we conducted a qualitative exploration of the cultural characteristics of SOC and its components (comprehensibility, manageability, and meaningfulness) in two groups of Palestinian Muslim helpers. We found that context-specific features of SOC can mobilize generalized resistance resources for coping with traumatic and stressful experiences, even in an environment characterized by political instability, military violence, and social trauma. Ten main themes emerged from the thematic content analysis: acceptance, reacting to adversity, acknowledging human insecurity (comprehensibility), self-control, talking to family, education as a resource for survival, connecting to the severity of the event, responsibility as a source of control (manageability), religiosity, and sense of belonging (meaningfulness). The Islamic faith, as expressed through the concepts of Sumud and Taslim, seemed to permeate individuals\\\\u2019 ability to attribute meaning to historical and transgenerational trauma, as well as to their ongoing traumatic conditions, thus acting as their ultimate source of health and wellbeing. A holistic, spiritual, and collectivist outlook helped respondents to approach their lives with optimism. We discuss the implications for mental health care providers and future research directions.\"\\n      },\\n      {\\n        \"paperId\": \"9adfdd1b97b98638b23dc1e08842143136dea497\",\\n        \"url\": \"https://www.semanticscholar.org/paper/9adfdd1b97b98638b23dc1e08842143136dea497\",\\n        \"title\": \"Joint Segmentation and Quantification of Chorioretinal Biomarkers in Optical Coherence Tomography Scans: A Deep Learning Approach\",\\n        \"citationCount\": 44,\\n        \"openAccessPdf\": {\\n          \"url\": \"\",\\n          \"status\": null,\\n          \"license\": null,\\n          \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {\\'tldr\\'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/TIM.2021.3077988?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/TIM.2021.3077988, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"\\n        },\\n        \"tldr\": null,\\n        \"publicationTypes\": [\\n          \"JournalArticle\"\\n        ],\\n        \"publicationDate\": null,\\n        \"authors\": [\\n          {\\n            \"authorId\": \"15356244\",\\n            \"name\": \"Bilal Hassan\"\\n          },\\n          {\\n            \"authorId\": \"121165014\",\\n            \"name\": \"S. Qin\"\\n          },\\n          {\\n            \"authorId\": \"38509414\",\\n            \"name\": \"Taimur Hassan\"\\n          },\\n          {\\n            \"authorId\": \"40988624\",\\n            \"name\": \"Ramsha Ahmed\"\\n          },\\n          {\\n            \"authorId\": \"1802072\",\\n            \"name\": \"N. Werghi\"\\n          }\\n        ],\\n        \"abstract\": \"In ophthalmology, chorioretinal biomarkers (CRBMs) play a significant role in detecting, quantifying, and ameliorating the treatment of chronic eye conditions. Optical coherence tomography (OCT) imaging is primarily used for investigating various CRBMs and prompt intervention of retinal conditions. However, with extensive clinical applications and increasing prevalence of ocular diseases, the number of OCT scans obtained globally exceeds ophthalmologists\\\\u2019 capacity to examine these in a meaningful manner. Instead, the emergence of deep learning provides a cost-effective and reliable alternative for automated analysis of scans, assisting ophthalmologists in clinical routines and research. This article presents a residual learning-based framework (RASP-Net) that integrates atrous spatial pyramid pooling, coherent preprocessing, and postprocessing mechanisms to achieve joint segmentation and quantification of 11 CRBMs. We used a total of 7000 annotated scans for training, validation, and testing purposes of RASP-Net. Moreover, a novel algorithm for 3-D macular profiles reconstruction is presented to give a more intuitive way for characterizing the CRBMs based on coarse contouring and quantification. The proposed framework is evaluated through several experiments using different performance metrics. The results presented in this study validate the optimal performance of RASP-Net in precise detection and segmentation of CRBMs, with mean balanced accuracy, intersection over union, and dice score values of 0.916, 0.634, and 0.776 respectively. The proposed RASP-Net model characterizes a wide range of CRBMs with fine-grained pixelwise segmentation, extraction, and quantification in the context of retinal pathologies. This proposed system can allow retina experts to monitor the improvement and deterioration of the underlying ocular conditions.\"\\n      }\\n    ]\\n  }\\n}', additional_kwargs={}, response_metadata={}, id='72bc78d3-e637-430e-b36d-02b9a73baa04'), AIMessage(content='{\\n  \"papers\": []\\n}', additional_kwargs={}, response_metadata={}, id='bc1324e1-1ae2-466e-9e21-dec6d67b8895')]}\n"
     ]
    }
   ],
   "source": [
    "graph = workflow.compile()\n",
    "graph.get_graph().draw_mermaid_png(output_file_path='story.png')\n",
    "\n",
    "result_llm = graph.invoke(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            (\n",
    "                \"user\",\n",
    "                \"\"\"Dynamic Prompt Adaptation:\n",
    "                Problem: Large Language Models (LLMs) often struggle with maintaining coherence over extended interactions or creative tasks, leading to thematic inconsistencies and reduced reader engagement.\n",
    "                Existing Methods: Current methods often use fixed prompts or few-shot examples, which may not adapt to the evolving context of a conversation or creative narrative. Techniques such as Chain-of-Thought prompting are utilized, but they do not inherently address continuity and adaptability across interactions.\n",
    "                Motivation: Human creative writing often involves iterative dialogue and adaptation to the flow of discussion. The style and context of interactions can shift dynamically based on prior exchanges. Dynamic adaptation mirrors how authors and conversationalists adjust based on audience feedback and shifting themes.\n",
    "                Proposed Method: We propose Dynamic Prompt Adaptation, involving three phases: (1) Contextual Analysis - Analyze previous outputs and user prompts to extract key themes and tonal shifts, applying a prompting structure like 'Reflect on the previous topic of [theme] and build on it.' (2) Adaptive Prompt Generation - Using insights from the analysis, generate updated prompts that introduce new elements or clarify past responses, e.g., 'Continuing from your last thought on [theme], can you expand on how this might be represented in [new context]?' (3) Iterative Context Update - As the dialogue progresses, generate a synthesis of all prior interactions to maintain thematic coherence, prompting with 'Summarize the key points discussed so far to keep track of our narrative.'\n",
    "                Experiment Plan: Test against static prompting strategies by evaluating engagement scores, coherence assessments, and user satisfaction in storytelling scenarios using standard text generation metrics such as BLEU and ROUGE. Incorporate user feedback on naturalness and adaptability during the interaction. Datasets could include the 'Story Cloze Test' dataset and user-generated dialogue interactions scraped from platforms like Reddit to assess conversational engagement.\"\"\"\n",
    "            )\n",
    "        ]\n",
    "    },\n",
    "    debug=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "3d43f51b",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_papers = json.loads(result_llm[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "d1b4c8de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>methodology_overlap</th>\n",
       "      <th>problem_overlap</th>\n",
       "      <th>domain_overlap</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.85</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.84</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.83</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.70</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.78</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.72</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.80</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.72</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.66</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.68</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.81</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.73</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.77</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.80</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.72</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.70</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.64</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.66</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.58</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.68</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.72</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    methodology_overlap  problem_overlap  domain_overlap\n",
       "0                  0.85             0.85            0.70\n",
       "1                  0.84             0.82            0.78\n",
       "2                  0.83             0.82            0.80\n",
       "3                  0.70             0.70            0.75\n",
       "4                  0.78             0.75            0.76\n",
       "5                  0.72             0.68            0.65\n",
       "6                  0.80             0.80            0.80\n",
       "7                  0.72             0.70            0.70\n",
       "8                  0.66             0.66            0.70\n",
       "9                  0.68             0.70            0.72\n",
       "10                 0.81             0.82            0.79\n",
       "11                 0.73             0.74            0.72\n",
       "12                 0.77             0.76            0.75\n",
       "13                 0.80             0.78            0.79\n",
       "14                 0.72             0.74            0.72\n",
       "15                 0.70             0.70            0.75\n",
       "16                 0.64             0.66            0.68\n",
       "17                 0.66             0.66            0.70\n",
       "18                 0.58             0.62            0.60\n",
       "19                 0.68             0.70            0.72"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# Parse the results\n",
    "result_content = json.loads(result_llm[\"messages\"][-1].content)\n",
    "\n",
    "# Create DataFrame\n",
    "papers_df = pd.DataFrame(result_content['papers'])\n",
    "\n",
    "papers_df[[\"methodology_overlap\",\"problem_overlap\",\"domain_overlap\"]]\n",
    "\n",
    "\n",
    "## NOTE\n",
    "## Wrong overlap calculation\n",
    "\n",
    "### Development Note\n",
    "## might be interesting to make the output of this papers analysis passed into \n",
    "## a GAN-like architecture (So not using overlapping score like the current one)\n",
    "## where one agent is argumenting why the idea is novel\n",
    "## and another agent is criticising why the idea is not novel, both based on the prior work\n",
    "## and then another agent act as a judge to decide which argument is stronger"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c48acd",
   "metadata": {},
   "source": [
    "## Evaluation Program"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f8839bf",
   "metadata": {},
   "source": [
    "Notes\n",
    "\n",
    "-> since everything here will be based on the retrieved papers, we need to make sure that the papers are retrieved correctly.\n",
    "\n",
    "\n",
    "-> and also we need to limit the number of papers not to be too many that would make the system too complex."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ead707",
   "metadata": {},
   "source": [
    "### Adversarial GAN-like Evaluation of Research Ideas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a00ae5a3",
   "metadata": {},
   "source": [
    "#### Advocate Agent\n",
    "The goal of the advocate agent is to defend the research idea, grounded based on the retrieved papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4ba0bc31",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdversarialAgent(BaseModel):\n",
    "    \"\"\"\n",
    "    Base class for Adversarial Agents\n",
    "    All agents must provide evidence-based arguments citing retrieved papers.\n",
    "    \"\"\"\n",
    "    argument: str = Field(\n",
    "        description=\"The main argument presented by the agent\"\n",
    "    )\n",
    "    \n",
    "    supporting_papers: List[str] = Field(\n",
    "        description=\"List of paper IDs cited to support this argument\",\n",
    "        default_factory=list\n",
    "    )\n",
    "    \n",
    "    key_points: List[str] = Field(\n",
    "        description=\"Main points made in this argument\",\n",
    "        default_factory=list\n",
    "    )\n",
    "\n",
    "    def to_dict(self) -> Dict:\n",
    "        return self.model_dump()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1be492cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "advocate_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\n",
    "        \"system\",\n",
    "        \"\"\"You are the ADVOCATE for the proposed research idea.\n",
    "Your goal is to defend the idea, highlight its novelty and feasibility.\n",
    "Use the provided retrieved papers to support your arguments.\n",
    "\n",
    "Focus on:\n",
    "1. Unique contributions\n",
    "2. How it improves upon existing methods (cite paper IDs)\n",
    "3. Why the potential impact outweighs risks\n",
    "\n",
    "Research Idea:\n",
    "{research_idea}\n",
    "\n",
    "Retrieved Papers:\n",
    "{retrieved_papers}\n",
    "\n",
    "Provide a strong, evidence-based argument citing specific papers.\n",
    "\"\"\"\n",
    "    ),\n",
    "    MessagesPlaceholder(variable_name=\"history\"),  # Automatically includes message history\n",
    "])\n",
    "\n",
    "# Create advocate agent that returns structured output\n",
    "class AdvocateResponse(AdversarialAgent):\n",
    "    \"\"\"Advocate's argument response\"\"\"\n",
    "    pass\n",
    "\n",
    "advocate_agent = advocate_prompt | llm.with_structured_output(AdvocateResponse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1797683b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_advocate_agent(state: DebateState):\n",
    "    research_idea = state['research_idea']\n",
    "    retrieved_papers = state['retrieved_papers']\n",
    "    messages = state['messages']\n",
    "    \n",
    "    response = advocate_agent.invoke({\n",
    "        \"research_idea\": research_idea,\n",
    "        \"retrieved_papers\": retrieved_papers,\n",
    "        \"history\": messages  # Pass the messages list directly\n",
    "    })\n",
    "    \n",
    "    return {\n",
    "        \"messages\": [AIMessage(content=response.argument)]\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ad10e3",
   "metadata": {},
   "source": [
    "#### Critics Agent\n",
    "The goal of the critics agent is to challenge the idea and the advocate's argument by finding the gap or weakness in the idea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3a22f829",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "skeptic_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\n",
    "        \"system\",\n",
    "        \"\"\"You are the SKEPTIC of the proposed research idea.\n",
    "Your goal is to critique the idea, point out flaws, and question its novelty.\n",
    "Use the provided retrieved papers to show similarity to prior work or identify weaknesses.\n",
    "\n",
    "Focus on:\n",
    "1. Overlaps with existing work (cite specific papers).\n",
    "2. Potential technical challenges or flaws.\n",
    "3. Why the idea might not be as novel or impactful as claimed.\n",
    "\n",
    "Research Idea:\n",
    "{research_idea}\n",
    "\n",
    "Retrieved Papers:\n",
    "{retrieved_papers}\n",
    "\n",
    "Provide a critical, evidence-based counter-argument.\n",
    "\"\"\"\n",
    "    ),\n",
    "    MessagesPlaceholder(variable_name=\"history\"),  # Automatically includes message history\n",
    "])\n",
    "\n",
    "# Create skeptic agent that returns structured output\n",
    "class SkepticResponse(AdversarialAgent):\n",
    "    \"\"\"Skeptic's argument response\"\"\"\n",
    "    pass\n",
    "\n",
    "skeptic_agent = skeptic_prompt | llm.with_structured_output(SkepticResponse)\n",
    "\n",
    "def call_skeptic_agent(state: DebateState):\n",
    "    research_idea = state['research_idea']\n",
    "    retrieved_papers = state['retrieved_papers']\n",
    "    messages = state['messages']\n",
    "    \n",
    "    response = skeptic_agent.invoke({\n",
    "        \"research_idea\": research_idea,\n",
    "        \"retrieved_papers\": retrieved_papers,\n",
    "        \"history\": messages  # Pass the messages list directly\n",
    "    })\n",
    "    \n",
    "    return {\n",
    "        \"messages\": [AIMessage(content=response.argument)]\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f307bae",
   "metadata": {},
   "source": [
    "#### Judge Agent\n",
    "The goal of the judge agent is to be a neutral and objective evaluator between the advocate and critic agents, and to find the final verdict of the research idea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "3beea497",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "moderator_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\n",
    "        \"system\",\n",
    "        \"\"\"You are an EXPERT guiding a debate between an Advocate and a Skeptic about a research idea.\n",
    "Your goal is to synthesize the arguments, ask probing questions to clarify the idea, and ensure the discussion remains grounded in the literature.\n",
    "\n",
    "Research Idea:\n",
    "{research_idea}\n",
    "\n",
    "Retrieved Papers:\n",
    "{retrieved_papers}\n",
    "\n",
    "Current Iteration: {iteration} / {max_iterations}\n",
    "\n",
    "Task:\n",
    "1. Summarize the key points made by both sides so far.\n",
    "2. If the maximum iterations have been reached or if the discussion has converged, provide a FINAL VERDICT on the idea's novelty and feasibility. Start your response with \"VERDICT:\".\n",
    "3. If the discussion should continue, ask a specific, probing question to guide the next round of debate.\n",
    "\n",
    "OUTPUT (valid JSON only):\n",
    "\n",
    "\"\"\"\n",
    "    ),\n",
    "    MessagesPlaceholder(variable_name=\"history\"),  # Automatically includes message history\n",
    "])\n",
    "\n",
    "# Create moderator response that returns structured output\n",
    "class ModeratorResponse(AdversarialAgent):\n",
    "    \"\"\"Moderator's response\"\"\"\n",
    "    verdict: str = Field(\n",
    "        description=\"Final verdict if debate concluded (starts with 'VERDICT:')\",\n",
    "        default=None\n",
    "    )\n",
    "    next_question: Optional[str] = Field(\n",
    "        description=\"Question for next round if continuing\",\n",
    "        default=None\n",
    "    )\n",
    "    \n",
    "    def to_dict(self) -> Dict:\n",
    "        return self.model_dump()\n",
    "\n",
    "moderator_agent = moderator_prompt | llm.with_structured_output(ModeratorResponse)\n",
    "\n",
    "\n",
    "\n",
    "# def call_moderator_agent(state: DebateState):\n",
    "#     research_idea = state['research_idea']\n",
    "#     retrieved_papers = state['retrieved_papers']\n",
    "#     messages = state['messages']\n",
    "#     iteration = state.get('iteration', 0)\n",
    "#     max_iterations = state.get('max_iterations', 3)\n",
    "    \n",
    "#     response = moderator_agent.invoke({\n",
    "#         \"research_idea\": research_idea,\n",
    "#         \"retrieved_papers\": retrieved_papers,\n",
    "#         \"iteration\": iteration,\n",
    "#         \"max_iterations\": max_iterations,\n",
    "#         \"history\": messages  # Pass the messages list directly\n",
    "#     })\n",
    "    \n",
    "#     # Force verdict on final iteration\n",
    "#     if iteration + 1 >= max_iterations:\n",
    "#         # Prepend VERDICT: if not already present\n",
    "#         argument = response.argument\n",
    "#         if not argument.startswith(\"VERDICT:\"):\n",
    "#             argument = f\"VERDICT: {argument}\"\n",
    "#         return {\n",
    "#             \"messages\": [AIMessage(content=argument)],\n",
    "#             \"iteration\": iteration + 1\n",
    "#         }\n",
    "    \n",
    "#     return {\n",
    "#         \"messages\": [AIMessage(content=response.argument)],\n",
    "#         \"iteration\": iteration + 1\n",
    "#     }\n",
    "\n",
    "class VerdictResponse(BaseModel):\n",
    "    \"\"\"Final verdict structure\"\"\"\n",
    "    novelty_score: int = Field(description=\"Score 1-10 for novelty\", ge=1, le=10)\n",
    "    feasibility_score: int = Field(description=\"Score 1-10 for feasibility\", ge=1, le=10)\n",
    "    summary: str = Field(description=\"Summary of the debate\")\n",
    "    strengths: List[str] = Field(description=\"Key strengths identified\")\n",
    "    weaknesses: List[str] = Field(description=\"Key weaknesses identified\")\n",
    "    recommendation: str = Field(description=\"Accept/Revise/Reject with justification\")\n",
    "    \n",
    "    def to_verdict_text(self) -> str:\n",
    "        return f\"\"\"VERDICT: Research Idea Evaluation\n",
    "\n",
    "SCORES:\n",
    "- Novelty: {self.novelty_score}/10\n",
    "- Feasibility: {self.feasibility_score}/10\n",
    "\n",
    "SUMMARY:\n",
    "{self.summary}\n",
    "\n",
    "STRENGTHS:\n",
    "{chr(10).join(f'- {s}' for s in self.strengths)}\n",
    "\n",
    "WEAKNESSES:\n",
    "{chr(10).join(f'- {w}' for w in self.weaknesses)}\n",
    "\n",
    "RECOMMENDATION: {self.recommendation}\n",
    "\"\"\"\n",
    "\n",
    "# Update moderator to use VerdictResponse on final iteration\n",
    "def call_moderator_agent(state: DebateState):\n",
    "    research_idea = state['research_idea']\n",
    "    retrieved_papers = state['retrieved_papers']\n",
    "    messages = state['messages']\n",
    "    iteration = state.get('iteration', 0)\n",
    "    max_iterations = state.get('max_iterations', 3)\n",
    "    \n",
    "    \n",
    "    is_final_iteration = (iteration +1 >= max_iterations)\n",
    "    \n",
    "    if is_final_iteration:\n",
    "        # Create a verdict-specific prompt\n",
    "        verdict_prompt = ChatPromptTemplate.from_messages([\n",
    "            (\n",
    "                \"system\",\n",
    "                \"\"\"You are the EXPERT providing the FINAL VERDICT for a research idea debate.\n",
    "\n",
    "Research Idea:\n",
    "{research_idea}\n",
    "\n",
    "Retrieved Papers:\n",
    "{retrieved_papers}\n",
    "\n",
    "Debate History:\n",
    "The Advocate and Skeptic have exchanged arguments over {iteration} rounds.\n",
    "\n",
    "YOUR TASK:\n",
    "Provide a comprehensive final verdict with structured scores.\n",
    "\n",
    "You MUST return a structured response with:\n",
    "- novelty_score: integer 1-10\n",
    "- feasibility_score: integer 1-10  \n",
    "- summary: string summarizing the debate\n",
    "- strengths: list of strings\n",
    "- weaknesses: list of strings\n",
    "- recommendation: \"Accept\" or \"Revise\" or \"Reject\" with justification\n",
    "\n",
    "Be decisive and evidence-based. \n",
    "START your response with \"VERDICT:\".\n",
    "\"\"\"\n",
    "            ),\n",
    "            MessagesPlaceholder(variable_name=\"history\"),\n",
    "        ])\n",
    "        \n",
    "        # Use verdict-specific prompt and schema\n",
    "        verdict_agent = verdict_prompt | llm.with_structured_output(VerdictResponse)\n",
    "        response = verdict_agent.invoke({\n",
    "            \"research_idea\": research_idea,\n",
    "            \"retrieved_papers\": retrieved_papers,\n",
    "            \"iteration\": iteration,\n",
    "            \"max_iterations\": max_iterations,\n",
    "            \"history\": messages\n",
    "        })\n",
    "        \n",
    "        return {\n",
    "            \"messages\": [AIMessage(content=response.to_verdict_text())],\n",
    "            \"iteration\": iteration + 1\n",
    "        }\n",
    "    else:\n",
    "        # Regular moderator response\n",
    "        response = moderator_agent.invoke({\n",
    "            \"research_idea\": research_idea,\n",
    "            \"retrieved_papers\": retrieved_papers,\n",
    "            \"iteration\": iteration,\n",
    "            \"max_iterations\": max_iterations,\n",
    "            \"history\": messages\n",
    "        })\n",
    "        \n",
    "        return {\n",
    "            \"messages\": [AIMessage(content=response.argument)],\n",
    "            \"iteration\": iteration + 1\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "359a6653",
   "metadata": {},
   "source": [
    "#### Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "ecc8511a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict, Annotated\n",
    "from langgraph.graph import StateGraph, END, START\n",
    "from langchain_core.messages import BaseMessage\n",
    "import operator\n",
    "\n",
    "# Define the state for the adversarial debate\n",
    "class DebateState(TypedDict):\n",
    "    \"\"\"State for the adversarial debate graph\"\"\"\n",
    "    research_idea: str\n",
    "    retrieved_papers: str\n",
    "    messages: Annotated[list[BaseMessage], operator.add]\n",
    "    iteration: int\n",
    "    max_iterations: int\n",
    "    debate_concluded: bool\n",
    "\n",
    "# Define routing logic\n",
    "def should_continue(state: DebateState) -> str:\n",
    "    \"\"\"Determine if debate should continue or end\"\"\"\n",
    "    # Check if we've reached max iterations\n",
    "    if state['iteration'] >= state['max_iterations']:\n",
    "        return \"end\"\n",
    "    \n",
    "    # Check if moderator issued a verdict\n",
    "    last_message = state['messages'][-1].content\n",
    "    if isinstance(last_message, str) and last_message.startswith(\"VERDICT:\"):\n",
    "        return \"end\"\n",
    "    \n",
    "    # Continue the debate\n",
    "    return \"continue\"\n",
    "\n",
    "def route_after_moderator(state: DebateState) -> str:\n",
    "    \"\"\"Route after moderator's turn\"\"\"\n",
    "    result = should_continue(state)\n",
    "    if result == \"end\":\n",
    "        return END\n",
    "    return \"advocate\"\n",
    "\n",
    "# Build the debate workflow\n",
    "debate_workflow = StateGraph(DebateState)\n",
    "\n",
    "# Add nodes\n",
    "debate_workflow.add_node(\"advocate\", call_advocate_agent)\n",
    "debate_workflow.add_node(\"skeptic\", call_skeptic_agent)\n",
    "debate_workflow.add_node(\"moderator\", call_moderator_agent)\n",
    "\n",
    "# Add edges\n",
    "debate_workflow.add_edge(START, \"advocate\")\n",
    "debate_workflow.add_edge(\"advocate\", \"skeptic\")\n",
    "debate_workflow.add_edge(\"skeptic\", \"moderator\")\n",
    "\n",
    "# Add conditional edge from moderator\n",
    "debate_workflow.add_conditional_edges(\n",
    "    \"moderator\",\n",
    "    route_after_moderator,\n",
    "    {\n",
    "        \"advocate\": \"advocate\",\n",
    "        END: END\n",
    "    }\n",
    ")\n",
    "\n",
    "# Compile the graph\n",
    "debate_graph = debate_workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a4cb2f19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "# Visualize the debate graph\n",
    "display(Image(debate_graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "9255129e",
   "metadata": {},
   "outputs": [
    {
     "ename": "APIConnectionError",
     "evalue": "Connection error.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mConnectError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/idea_evaluation/lib/python3.14/site-packages/httpx/_transports/default.py:101\u001b[39m, in \u001b[36mmap_httpcore_exceptions\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    100\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m101\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[32m    102\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/idea_evaluation/lib/python3.14/site-packages/httpx/_transports/default.py:250\u001b[39m, in \u001b[36mHTTPTransport.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m     resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_pool\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp.stream, typing.Iterable)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/idea_evaluation/lib/python3.14/site-packages/httpcore/_sync/connection_pool.py:256\u001b[39m, in \u001b[36mConnectionPool.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    255\u001b[39m     \u001b[38;5;28mself\u001b[39m._close_connections(closing)\n\u001b[32m--> \u001b[39m\u001b[32m256\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    258\u001b[39m \u001b[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[32m    259\u001b[39m \u001b[38;5;66;03m# the point at which the response is closed.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/idea_evaluation/lib/python3.14/site-packages/httpcore/_sync/connection_pool.py:236\u001b[39m, in \u001b[36mConnectionPool.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    234\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    235\u001b[39m     \u001b[38;5;66;03m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m236\u001b[39m     response = \u001b[43mconnection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    237\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpool_request\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\n\u001b[32m    238\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    239\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[32m    240\u001b[39m     \u001b[38;5;66;03m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[32m    241\u001b[39m     \u001b[38;5;66;03m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[32m    242\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    243\u001b[39m     \u001b[38;5;66;03m# In this case we clear the connection and try again.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/idea_evaluation/lib/python3.14/site-packages/httpcore/_sync/connection.py:101\u001b[39m, in \u001b[36mHTTPConnection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    100\u001b[39m     \u001b[38;5;28mself\u001b[39m._connect_failed = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m101\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[32m    103\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._connection.handle_request(request)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/idea_evaluation/lib/python3.14/site-packages/httpcore/_sync/connection.py:78\u001b[39m, in \u001b[36mHTTPConnection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m     77\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m78\u001b[39m     stream = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_connect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     80\u001b[39m     ssl_object = stream.get_extra_info(\u001b[33m\"\u001b[39m\u001b[33mssl_object\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/idea_evaluation/lib/python3.14/site-packages/httpcore/_sync/connection.py:124\u001b[39m, in \u001b[36mHTTPConnection._connect\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    123\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[33m\"\u001b[39m\u001b[33mconnect_tcp\u001b[39m\u001b[33m\"\u001b[39m, logger, request, kwargs) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[32m--> \u001b[39m\u001b[32m124\u001b[39m     stream = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_network_backend\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnect_tcp\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    125\u001b[39m     trace.return_value = stream\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/idea_evaluation/lib/python3.14/site-packages/httpcore/_backends/sync.py:207\u001b[39m, in \u001b[36mSyncBackend.connect_tcp\u001b[39m\u001b[34m(self, host, port, timeout, local_address, socket_options)\u001b[39m\n\u001b[32m    202\u001b[39m exc_map: ExceptionMapping = {\n\u001b[32m    203\u001b[39m     socket.timeout: ConnectTimeout,\n\u001b[32m    204\u001b[39m     \u001b[38;5;167;01mOSError\u001b[39;00m: ConnectError,\n\u001b[32m    205\u001b[39m }\n\u001b[32m--> \u001b[39m\u001b[32m207\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[32m    208\u001b[39m     sock = socket.create_connection(\n\u001b[32m    209\u001b[39m         address,\n\u001b[32m    210\u001b[39m         timeout,\n\u001b[32m    211\u001b[39m         source_address=source_address,\n\u001b[32m    212\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/idea_evaluation/lib/python3.14/contextlib.py:162\u001b[39m, in \u001b[36m_GeneratorContextManager.__exit__\u001b[39m\u001b[34m(self, typ, value, traceback)\u001b[39m\n\u001b[32m    161\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m162\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgen\u001b[49m\u001b[43m.\u001b[49m\u001b[43mthrow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    163\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    164\u001b[39m     \u001b[38;5;66;03m# Suppress StopIteration *unless* it's the same exception that\u001b[39;00m\n\u001b[32m    165\u001b[39m     \u001b[38;5;66;03m# was passed to throw().  This prevents a StopIteration\u001b[39;00m\n\u001b[32m    166\u001b[39m     \u001b[38;5;66;03m# raised inside the \"with\" statement from being suppressed.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/idea_evaluation/lib/python3.14/site-packages/httpcore/_exceptions.py:14\u001b[39m, in \u001b[36mmap_exceptions\u001b[39m\u001b[34m(map)\u001b[39m\n\u001b[32m     13\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(exc, from_exc):\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m to_exc(exc) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mexc\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[31mConnectError\u001b[39m: [Errno 8] nodename nor servname provided, or not known",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mConnectError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/idea_evaluation/lib/python3.14/site-packages/openai/_base_client.py:982\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m    981\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m982\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    983\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    984\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_should_stream_response_body\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    985\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    986\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    987\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m httpx.TimeoutException \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/idea_evaluation/lib/python3.14/site-packages/httpx/_client.py:914\u001b[39m, in \u001b[36mClient.send\u001b[39m\u001b[34m(self, request, stream, auth, follow_redirects)\u001b[39m\n\u001b[32m    912\u001b[39m auth = \u001b[38;5;28mself\u001b[39m._build_request_auth(request, auth)\n\u001b[32m--> \u001b[39m\u001b[32m914\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_handling_auth\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    915\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    916\u001b[39m \u001b[43m    \u001b[49m\u001b[43mauth\u001b[49m\u001b[43m=\u001b[49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    917\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    918\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    919\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    920\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/idea_evaluation/lib/python3.14/site-packages/httpx/_client.py:942\u001b[39m, in \u001b[36mClient._send_handling_auth\u001b[39m\u001b[34m(self, request, auth, follow_redirects, history)\u001b[39m\n\u001b[32m    941\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m942\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_handling_redirects\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    943\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    944\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    945\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    946\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    947\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/idea_evaluation/lib/python3.14/site-packages/httpx/_client.py:979\u001b[39m, in \u001b[36mClient._send_handling_redirects\u001b[39m\u001b[34m(self, request, follow_redirects, history)\u001b[39m\n\u001b[32m    977\u001b[39m     hook(request)\n\u001b[32m--> \u001b[39m\u001b[32m979\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_single_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    980\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/idea_evaluation/lib/python3.14/site-packages/httpx/_client.py:1014\u001b[39m, in \u001b[36mClient._send_single_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m   1013\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request=request):\n\u001b[32m-> \u001b[39m\u001b[32m1014\u001b[39m     response = \u001b[43mtransport\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1016\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response.stream, SyncByteStream)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/idea_evaluation/lib/python3.14/site-packages/httpx/_transports/default.py:249\u001b[39m, in \u001b[36mHTTPTransport.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    237\u001b[39m req = httpcore.Request(\n\u001b[32m    238\u001b[39m     method=request.method,\n\u001b[32m    239\u001b[39m     url=httpcore.URL(\n\u001b[32m   (...)\u001b[39m\u001b[32m    247\u001b[39m     extensions=request.extensions,\n\u001b[32m    248\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m249\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[32m    250\u001b[39m     resp = \u001b[38;5;28mself\u001b[39m._pool.handle_request(req)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/idea_evaluation/lib/python3.14/contextlib.py:162\u001b[39m, in \u001b[36m_GeneratorContextManager.__exit__\u001b[39m\u001b[34m(self, typ, value, traceback)\u001b[39m\n\u001b[32m    161\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m162\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgen\u001b[49m\u001b[43m.\u001b[49m\u001b[43mthrow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    163\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    164\u001b[39m     \u001b[38;5;66;03m# Suppress StopIteration *unless* it's the same exception that\u001b[39;00m\n\u001b[32m    165\u001b[39m     \u001b[38;5;66;03m# was passed to throw().  This prevents a StopIteration\u001b[39;00m\n\u001b[32m    166\u001b[39m     \u001b[38;5;66;03m# raised inside the \"with\" statement from being suppressed.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/idea_evaluation/lib/python3.14/site-packages/httpx/_transports/default.py:118\u001b[39m, in \u001b[36mmap_httpcore_exceptions\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    117\u001b[39m message = \u001b[38;5;28mstr\u001b[39m(exc)\n\u001b[32m--> \u001b[39m\u001b[32m118\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m mapped_exc(message) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mexc\u001b[39;00m\n",
      "\u001b[31mConnectError\u001b[39m: [Errno 8] nodename nor servname provided, or not known",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mAPIConnectionError\u001b[39m                        Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[108]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      6\u001b[39m retrieved_papers_text = getReferencePaper.prepare_papers_for_llm(papers_json)\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# Run the adversarial debate\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m debate_result = \u001b[43mdebate_graph\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mresearch_idea\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresearch_idea_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mretrieved_papers\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mretrieved_papers_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43miteration\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_iterations\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdebate_concluded\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[32m     16\u001b[39m \u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# Display the debate messages\u001b[39;00m\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m msg \u001b[38;5;129;01min\u001b[39;00m debate_result[\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/idea_evaluation/lib/python3.14/site-packages/langgraph/pregel/main.py:3050\u001b[39m, in \u001b[36mPregel.invoke\u001b[39m\u001b[34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, **kwargs)\u001b[39m\n\u001b[32m   3047\u001b[39m chunks: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any] | Any] = []\n\u001b[32m   3048\u001b[39m interrupts: \u001b[38;5;28mlist\u001b[39m[Interrupt] = []\n\u001b[32m-> \u001b[39m\u001b[32m3050\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3051\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   3052\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3053\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3054\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mupdates\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalues\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m   3055\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalues\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m   3056\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3057\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprint_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprint_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3058\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3059\u001b[39m \u001b[43m    \u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[43m=\u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3060\u001b[39m \u001b[43m    \u001b[49m\u001b[43minterrupt_after\u001b[49m\u001b[43m=\u001b[49m\u001b[43minterrupt_after\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3061\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdurability\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdurability\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3062\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3063\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   3064\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalues\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\n\u001b[32m   3065\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/idea_evaluation/lib/python3.14/site-packages/langgraph/pregel/main.py:2633\u001b[39m, in \u001b[36mPregel.stream\u001b[39m\u001b[34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, subgraphs, debug, **kwargs)\u001b[39m\n\u001b[32m   2631\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m task \u001b[38;5;129;01min\u001b[39;00m loop.match_cached_writes():\n\u001b[32m   2632\u001b[39m     loop.output_writes(task.id, task.writes, cached=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m-> \u001b[39m\u001b[32m2633\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrunner\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtick\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2634\u001b[39m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtasks\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrites\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2635\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstep_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2636\u001b[39m \u001b[43m    \u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2637\u001b[39m \u001b[43m    \u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m=\u001b[49m\u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43maccept_push\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2638\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   2639\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# emit output\u001b[39;49;00m\n\u001b[32m   2640\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01myield from\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_output\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2641\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprint_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubgraphs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mqueue\u001b[49m\u001b[43m.\u001b[49m\u001b[43mEmpty\u001b[49m\n\u001b[32m   2642\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2643\u001b[39m loop.after_tick()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/idea_evaluation/lib/python3.14/site-packages/langgraph/pregel/_runner.py:167\u001b[39m, in \u001b[36mPregelRunner.tick\u001b[39m\u001b[34m(self, tasks, reraise, timeout, retry_policy, get_waiter, schedule_task)\u001b[39m\n\u001b[32m    165\u001b[39m t = tasks[\u001b[32m0\u001b[39m]\n\u001b[32m    166\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m167\u001b[39m     \u001b[43mrun_with_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    168\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    169\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    170\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfigurable\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    171\u001b[39m \u001b[43m            \u001b[49m\u001b[43mCONFIG_KEY_CALL\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    172\u001b[39m \u001b[43m                \u001b[49m\u001b[43m_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    173\u001b[39m \u001b[43m                \u001b[49m\u001b[43mweakref\u001b[49m\u001b[43m.\u001b[49m\u001b[43mref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    174\u001b[39m \u001b[43m                \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    175\u001b[39m \u001b[43m                \u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweakref\u001b[49m\u001b[43m.\u001b[49m\u001b[43mref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    176\u001b[39m \u001b[43m                \u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m=\u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    177\u001b[39m \u001b[43m                \u001b[49m\u001b[43msubmit\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msubmit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    178\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    179\u001b[39m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    180\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    181\u001b[39m     \u001b[38;5;28mself\u001b[39m.commit(t, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    182\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/idea_evaluation/lib/python3.14/site-packages/langgraph/pregel/_retry.py:42\u001b[39m, in \u001b[36mrun_with_retry\u001b[39m\u001b[34m(task, retry_policy, configurable)\u001b[39m\n\u001b[32m     40\u001b[39m     task.writes.clear()\n\u001b[32m     41\u001b[39m     \u001b[38;5;66;03m# run the task\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtask\u001b[49m\u001b[43m.\u001b[49m\u001b[43mproc\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m.\u001b[49m\u001b[43minput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     43\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ParentCommand \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m     44\u001b[39m     ns: \u001b[38;5;28mstr\u001b[39m = config[CONF][CONFIG_KEY_CHECKPOINT_NS]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/idea_evaluation/lib/python3.14/site-packages/langgraph/_internal/_runnable.py:656\u001b[39m, in \u001b[36mRunnableSeq.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    654\u001b[39m     \u001b[38;5;66;03m# run in context\u001b[39;00m\n\u001b[32m    655\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(config, run) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m--> \u001b[39m\u001b[32m656\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    657\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    658\u001b[39m     \u001b[38;5;28minput\u001b[39m = step.invoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/idea_evaluation/lib/python3.14/site-packages/langgraph/_internal/_runnable.py:400\u001b[39m, in \u001b[36mRunnableCallable.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    398\u001b[39m         run_manager.on_chain_end(ret)\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m400\u001b[39m     ret = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    401\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.recurse \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Runnable):\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ret.invoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[53]\u001b[39m\u001b[32m, line 6\u001b[39m, in \u001b[36mcall_advocate_agent\u001b[39m\u001b[34m(state)\u001b[39m\n\u001b[32m      3\u001b[39m retrieved_papers = state[\u001b[33m'\u001b[39m\u001b[33mretrieved_papers\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m      4\u001b[39m messages = state[\u001b[33m'\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m response = \u001b[43madvocate_agent\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mresearch_idea\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresearch_idea\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mretrieved_papers\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mretrieved_papers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mhistory\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Pass the messages list directly\u001b[39;49;00m\n\u001b[32m     10\u001b[39m \u001b[43m\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[32m     13\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m: [AIMessage(content=response.argument)]\n\u001b[32m     14\u001b[39m }\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/idea_evaluation/lib/python3.14/site-packages/langchain_core/runnables/base.py:3129\u001b[39m, in \u001b[36mRunnableSequence.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   3127\u001b[39m                 input_ = context.run(step.invoke, input_, config, **kwargs)\n\u001b[32m   3128\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3129\u001b[39m                 input_ = \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3130\u001b[39m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[32m   3131\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/idea_evaluation/lib/python3.14/site-packages/langchain_core/runnables/base.py:5534\u001b[39m, in \u001b[36mRunnableBindingBase.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   5527\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m   5528\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m   5529\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   5532\u001b[39m     **kwargs: Any | \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   5533\u001b[39m ) -> Output:\n\u001b[32m-> \u001b[39m\u001b[32m5534\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbound\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   5535\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   5536\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_merge_configs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5537\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43m{\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5538\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/idea_evaluation/lib/python3.14/site-packages/langchain_core/language_models/chat_models.py:398\u001b[39m, in \u001b[36mBaseChatModel.invoke\u001b[39m\u001b[34m(self, input, config, stop, **kwargs)\u001b[39m\n\u001b[32m    384\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    385\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m    386\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    391\u001b[39m     **kwargs: Any,\n\u001b[32m    392\u001b[39m ) -> AIMessage:\n\u001b[32m    393\u001b[39m     config = ensure_config(config)\n\u001b[32m    394\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[32m    395\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAIMessage\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    396\u001b[39m         cast(\n\u001b[32m    397\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mChatGeneration\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m--> \u001b[39m\u001b[32m398\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    399\u001b[39m \u001b[43m                \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    400\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    401\u001b[39m \u001b[43m                \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcallbacks\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    402\u001b[39m \u001b[43m                \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtags\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    403\u001b[39m \u001b[43m                \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    404\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_name\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    405\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    406\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    407\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m.generations[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m],\n\u001b[32m    408\u001b[39m         ).message,\n\u001b[32m    409\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/idea_evaluation/lib/python3.14/site-packages/langchain_core/language_models/chat_models.py:1117\u001b[39m, in \u001b[36mBaseChatModel.generate_prompt\u001b[39m\u001b[34m(self, prompts, stop, callbacks, **kwargs)\u001b[39m\n\u001b[32m   1108\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m   1109\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate_prompt\u001b[39m(\n\u001b[32m   1110\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1114\u001b[39m     **kwargs: Any,\n\u001b[32m   1115\u001b[39m ) -> LLMResult:\n\u001b[32m   1116\u001b[39m     prompt_messages = [p.to_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[32m-> \u001b[39m\u001b[32m1117\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/idea_evaluation/lib/python3.14/site-packages/langchain_core/language_models/chat_models.py:927\u001b[39m, in \u001b[36mBaseChatModel.generate\u001b[39m\u001b[34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[39m\n\u001b[32m    924\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(input_messages):\n\u001b[32m    925\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    926\u001b[39m         results.append(\n\u001b[32m--> \u001b[39m\u001b[32m927\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    928\u001b[39m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    929\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    930\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    931\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    932\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    933\u001b[39m         )\n\u001b[32m    934\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    935\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/idea_evaluation/lib/python3.14/site-packages/langchain_core/language_models/chat_models.py:1221\u001b[39m, in \u001b[36mBaseChatModel._generate_with_cache\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1219\u001b[39m     result = generate_from_stream(\u001b[38;5;28miter\u001b[39m(chunks))\n\u001b[32m   1220\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m inspect.signature(\u001b[38;5;28mself\u001b[39m._generate).parameters.get(\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1221\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1222\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m   1223\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1224\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1225\u001b[39m     result = \u001b[38;5;28mself\u001b[39m._generate(messages, stop=stop, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/idea_evaluation/lib/python3.14/site-packages/langchain_openai/chat_models/base.py:1356\u001b[39m, in \u001b[36mBaseChatOpenAI._generate\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1354\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m raw_response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(raw_response, \u001b[33m\"\u001b[39m\u001b[33mhttp_response\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m   1355\u001b[39m         e.response = raw_response.http_response  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1356\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m   1357\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   1358\u001b[39m     \u001b[38;5;28mself\u001b[39m.include_response_headers\n\u001b[32m   1359\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m raw_response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1360\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(raw_response, \u001b[33m\"\u001b[39m\u001b[33mheaders\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1361\u001b[39m ):\n\u001b[32m   1362\u001b[39m     generation_info = {\u001b[33m\"\u001b[39m\u001b[33mheaders\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mdict\u001b[39m(raw_response.headers)}\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/idea_evaluation/lib/python3.14/site-packages/langchain_openai/chat_models/base.py:1324\u001b[39m, in \u001b[36mBaseChatOpenAI._generate\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1321\u001b[39m payload.pop(\u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1322\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1323\u001b[39m     raw_response = (\n\u001b[32m-> \u001b[39m\u001b[32m1324\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mroot_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mchat\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompletions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwith_raw_response\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1325\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mpayload\u001b[49m\n\u001b[32m   1326\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1327\u001b[39m     )\n\u001b[32m   1328\u001b[39m     response = raw_response.parse()\n\u001b[32m   1329\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m openai.BadRequestError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/idea_evaluation/lib/python3.14/site-packages/openai/_legacy_response.py:364\u001b[39m, in \u001b[36mto_raw_response_wrapper.<locals>.wrapped\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    360\u001b[39m extra_headers[RAW_RESPONSE_HEADER] = \u001b[33m\"\u001b[39m\u001b[33mtrue\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    362\u001b[39m kwargs[\u001b[33m\"\u001b[39m\u001b[33mextra_headers\u001b[39m\u001b[33m\"\u001b[39m] = extra_headers\n\u001b[32m--> \u001b[39m\u001b[32m364\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m cast(LegacyAPIResponse[R], \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/idea_evaluation/lib/python3.14/site-packages/openai/resources/chat/completions/completions.py:184\u001b[39m, in \u001b[36mCompletions.parse\u001b[39m\u001b[34m(self, messages, model, audio, response_format, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, prompt_cache_key, prompt_cache_retention, reasoning_effort, safety_identifier, seed, service_tier, stop, store, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, verbosity, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m    177\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mparser\u001b[39m(raw_completion: ChatCompletion) -> ParsedChatCompletion[ResponseFormatT]:\n\u001b[32m    178\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _parse_chat_completion(\n\u001b[32m    179\u001b[39m         response_format=response_format,\n\u001b[32m    180\u001b[39m         chat_completion=raw_completion,\n\u001b[32m    181\u001b[39m         input_tools=chat_completion_tools,\n\u001b[32m    182\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m184\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    185\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/chat/completions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    186\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    187\u001b[39m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    188\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    189\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    190\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43maudio\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    191\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfrequency_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    192\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunction_call\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    193\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunctions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    194\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogit_bias\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    195\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    196\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_completion_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    197\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    198\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    199\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodalities\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodalities\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    200\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mn\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    201\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mparallel_tool_calls\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    202\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprediction\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    203\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpresence_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    204\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprompt_cache_key\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt_cache_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    205\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprompt_cache_retention\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt_cache_retention\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    206\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mreasoning_effort\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_effort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    207\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mresponse_format\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m_type_to_response_format\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    208\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msafety_identifier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msafety_identifier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    209\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mseed\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    210\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mservice_tier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    211\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstop\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    212\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstore\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    213\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    214\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    215\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtemperature\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    216\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtool_choice\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    217\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtools\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    218\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_logprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_p\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    220\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    221\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mverbosity\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbosity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    222\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mweb_search_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mweb_search_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    223\u001b[39m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    224\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    225\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    226\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    227\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    228\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    229\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    230\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    231\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpost_parser\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    232\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    233\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# we turn the `ChatCompletion` instance into a `ParsedChatCompletion`\u001b[39;49;00m\n\u001b[32m    234\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# in the `parser` function above\u001b[39;49;00m\n\u001b[32m    235\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mType\u001b[49m\u001b[43m[\u001b[49m\u001b[43mParsedChatCompletion\u001b[49m\u001b[43m[\u001b[49m\u001b[43mResponseFormatT\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mChatCompletion\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    236\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    237\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/idea_evaluation/lib/python3.14/site-packages/openai/_base_client.py:1259\u001b[39m, in \u001b[36mSyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[39m\n\u001b[32m   1245\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1246\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1247\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1254\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1255\u001b[39m ) -> ResponseT | _StreamT:\n\u001b[32m   1256\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1257\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001b[32m   1258\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1259\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/idea_evaluation/lib/python3.14/site-packages/openai/_base_client.py:1014\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1011\u001b[39m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m   1013\u001b[39m     log.debug(\u001b[33m\"\u001b[39m\u001b[33mRaising connection error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1014\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m APIConnectionError(request=request) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m   1016\u001b[39m log.debug(\n\u001b[32m   1017\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mHTTP Response: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m%i\u001b[39;00m\u001b[33m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m'\u001b[39m,\n\u001b[32m   1018\u001b[39m     request.method,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     response.headers,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m log.debug(\u001b[33m\"\u001b[39m\u001b[33mrequest_id: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m, response.headers.get(\u001b[33m\"\u001b[39m\u001b[33mx-request-id\u001b[39m\u001b[33m\"\u001b[39m))\n",
      "\u001b[31mAPIConnectionError\u001b[39m: Connection error.",
      "During task with name 'advocate' and id '08f38a9c-5c80-ce6b-5861-ad1e0f43b3d2'"
     ]
    }
   ],
   "source": [
    "# Extract research idea from initial user input\n",
    "research_idea_text = result_llm[\"messages\"][0].content\n",
    "\n",
    "# Extract and format retrieved papers\n",
    "papers_json = json.loads(result_llm[\"messages\"][-2].content)  # -2 because -1 is the analysis\n",
    "retrieved_papers_text = getReferencePaper.prepare_papers_for_llm(papers_json)\n",
    "\n",
    "# Run the adversarial debate\n",
    "debate_result = debate_graph.invoke({\n",
    "    \"research_idea\": research_idea_text,\n",
    "    \"retrieved_papers\": retrieved_papers_text,\n",
    "    \"messages\": [],\n",
    "    \"iteration\": 0,\n",
    "    \"max_iterations\": 4,\n",
    "    \"debate_concluded\": False\n",
    "})\n",
    "\n",
    "# Display the debate messages\n",
    "for msg in debate_result[\"messages\"]:\n",
    "    print(f\"\\n{'-'*80}\\n{msg.content}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c561162f",
   "metadata": {},
   "source": [
    "##### Example Result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "092a8720",
   "metadata": {},
   "source": [
    "gemini\n",
    "\n",
    "--------------------------------------------------------------------------------\n",
    "Dynamic Prompt Adaptation (DPA) operationalizes a three-phase, closed-loop prompting paradigm to directly address coherence and adaptability in long-form generation and multi-turn dialogue. Unlike fixed prompts or static few-shot prompts, DPA explicitly analyzes the evolving interaction history to extract themes, tone shifts, and narrative goals (Contextual Analysis), generates updated prompts that steer subsequent outputs toward new elements or clarified past responses (Adaptive Prompt Generation), and continually synthesizes the aggregate of prior exchanges to preserve global coherence (Iterative Context Update). This creates a lightweight, memory-efficient alternative to full fine-tuning while delivering sustained prompt-driven alignment across long sessions, multi-turn narratives, and dynamic topics. The approach is novel in its explicit three-phase cycle tailored for ongoing interactions, rather than single-shot adaptation, and it targets the core bottleneck of LLM coherence over time rather than surface-level accuracy alone. In addition, the framework is designed to be modular and cross-domain: it can pair with parameter-efficient adapters (e.g., DynaLoRA-style dynamics) to strike a favorable accuracy/compute trade-off as shown by dynamic-adapter+prompt-tuning work (Dynamic Adapter Meets Prompt Tuning) and Time-LlaMA-style dynamic adaptation, ensuring feasibility for real-world deployment with modest compute overhead. The plan also outlines concrete, ecologically valid experiments (storytelling and dynamic dialogue) using established coherence and engagement metrics, leveraging CTTA-inspired prompts to handle domain shifts and evolving contexts. This positions DPA as a practical, scalable solution with immediate impact for education, entertainment, and humanAI collaboration in creative writing and extended conversations.\n",
    "\n",
    "\n",
    "--------------------------------------------------------------------------------\n",
    "Dynamic Prompt Adaptation (DPA) proposes a three-phase loop (Contextual Analysis, Adaptive Prompt Generation, Iterative Context Update) to maintain coherence and adaptability in extended LLM interactions. While appealing, the idea risks being incremental rather than novel: it essentially concatenates existing dynamic-prompt and prompt-tuning ideas (analysis-driven prompt updates, per-step prompt adaptation, and history-aware synthesis) that are already explored in the literature. Moreover, there are significant practical and evaluative risks (unstable prompts, drift over many turns, poor correlation of BLEU/ROUGE with narrative quality, data issues) that are not adequately addressed. The claimed noveltythree phases tailored for ongoing interactionsoverlaps substantially with prior dynamic-prompting and test-time adaptation work, and the experimental plan relies on metrics and datasets that have well-known limitations for evaluating long-form coherence and engagement. Without stronger theoretical grounding, rigorous baselines, and robust, human-centered evaluation, the proposal may yield modest, domain-limited impact despite substantial engineering effort.\n",
    "\n",
    "\n",
    "--------------------------------------------------------------------------------\n",
    "Advocate argues that Dynamic Prompt Adaptation (DPA) offers a novel three-phase loop (Contextual Analysis, Adaptive Prompt Generation, Iterative Context Update) to sustain coherence and adaptability in long-form LLM interactions. It emphasizes explicit, history-aware prompting as a lightweight alternative to full fine-tuning, with modular compatibility alongside parameter-efficient methods (e.g., dynamic adapters, LoRA). Proponents claim novelty lies in the explicit three-phase cycle tailored for ongoing interactions and the potential for cross-domain impact in creative writing and extended conversations, supported by a concrete experiment plan and engagement metrics. Skeptic counterpoints stress that the core ideas resemble existing dynamic prompting and test-time adaptation paradigms, risk being incremental rather than groundbreaking, and rely on evaluation metrics (e.g., BLEU/ROUGE) and datasets (e.g., Reddit) that have known limitations for measuring long-range coherence and narrative quality. They caution about prompt drift, instability over many turns, lack of rigorous theoretical grounding, and potential biases in data and baselines. Overall, the debate centers on whether the proposed three-phase loop genuinely introduces a new methodological paradigm or re-packages established dynamic prompting concepts with potentially modest novelty and impact without stronger theoretical guarantees or robust human-centered evaluation.\n",
    "\n",
    "\n",
    "--------------------------------------------------------------------------------\n",
    "Dynamic Prompt Adaptation (DPA) introduces a principled three-phase loopContextual Analysis, Adaptive Prompt Generation, and Iterative Context Updatethat actively maintains coherence and adaptability in long-form LLM interactions. This goes beyond fixed prompts, fixed few-shots, or single-turn CoT-style prompting by building a lightweight, memory-efficient mechanism that continually reasons about the evolving dialogue and fabricates targeted prompt updates to steer future generations. The novelty lies not just in prompting, but in the explicit three-phase orchestration that (i) extracts themes and tonal shifts from prior exchanges, (ii) generates updated prompts that integrate new elements or reframe past responses, and (iii) synthesizes the entire dialogue history to preserve global coherence over many turns. This closes the loop between memory-like context and prompt-driven control, enabling sustained narrative quality in story-telling, tutoring, conversational agents, and interactive media. Importantly, DPA remains highly feasible in practice by leveraging established parameter-efficient dynamics (e.g., dynamic adapters, LoRA-style modules, and memory-efficient prompt mechanisms) as shown in related work, ensuring deployment on commodity hardware without full fine-tuning.\n",
    "\n",
    "Key contributions and novelty beyond prior work:\n",
    "- Explicit three-phase loop tailored for ongoing interactions, not just one-shot adaptation or static prompting (Contextual Analysis, Adaptive Prompt Generation, Iterative Context Update). This targets long-range coherence and dynamic thematic evolution in dialogue and narrative tasks.\n",
    "- Modular, plugandplay design that can be paired with parameter-efficient adaptation techniques (e.g., Dynamic Adapters + Prompt Tuning; Time-LlaMAs dynamic LoRA selection) to balance accuracy and compute (see Dynamic Adapter Meets Prompt Tuning; Time-LlaMA) [papers cited].\n",
    "- Compatibility with multi-modal and cross-domain prompts via domain-aware prompting and meta-relabling strategies, enabling robust generalization across tasks and domains (DAPrompt and RADA-prompt style ideas) [papers cited].\n",
    "- A concrete experimental path that uses established storytelling and dialogue benchmarks (e.g., Story Cloze-like tasks and user-dialogue datasets) with multi-faceted metrics (engagement, coherence, user satisfaction, BLEU/ROUGE) to capture long-range narrative quality, moving beyond surface metrics.\n",
    "- A risk-mitigated deployment plan that embraces ensemble and adaptive prompting strategies to curb drift and overfitting, inspired by robust test-time prompting approaches.\n",
    "\n",
    "How DPA improves upon existing methods, with supporting evidence from the literature:\n",
    "- Dynamic prompt cores/CTTA-style adaptation: DPCore shows that dynamic prompt coresets, dedicated visual prompts, and a dynamic update mechanism can achieve robust continual test-time adaptation while drastically reducing trainable parameters and compute, illustrating the practicality and efficiency of dynamic prompting in changing contexts (DPCore; 246482d9758e93d0b349e2926996d887417174d8).\n",
    "- Parameter-efficient adaptation with prompts: Dynamic Adapter + Prompt Tuning (DAPT) demonstrates that freezing base models and adding dynamic adapters alongside internal prompts yields superior performance with dramatic reductions in trainable parameters and memory (6b533de65b3d2cee190415f5a2a2e6afe2ac7c78).\n",
    "- Dynamic, task-aware adaptation without full fine-tuning: Time-LlaMA introduces dynamic low-rank adaptation that selects LoRA modules per input to balance performance and inference efficiency, supporting the feasibility of input-dependent, prompt-related adaptation in large models (650a24da1702beca7eb70011a26f1f3238efad4b).\n",
    "- Prompt-based test-time adaptation and robust prompting: DAPrompt and related prompt-learning schemes show that ensembling and meta-prompting can mitigate biases and improve robustness during test-time adaptation, aligning with DPAs emphasis on robust, adaptive prompting during ongoing interactions (5db3cfc974c42bfa2d9518a8910762790; 759b5f58e58a76f79a7d845acd3169dc899d0ac2).\n",
    "- Retrieval-augmented and iterative prompting for long-horizon tasks: RAT and Iter-RetGen illustrate how iterative retrieval-generation loops can enhance reasoning and grounding in long-horizon tasks, informing DPAs design where prompts are updated based on retrieved dialogue history and prior prompts (dc256e179d4e8eff48879a40ddc414b15b0b2300; 8051818817a9a3815be6623a679d4a7f5a7b7964).\n",
    "- Encouraging coherent multi-turn generation with structured prompting: Works on narrative generation and multi-turn interactions (e.g., DialogGen, Agents Room, SCENECRAFT) demonstrate the value of structured, multi-agent or multi-stage prompting and planning to achieve coherence and narrative alignment; DPA operationalizes this mindset into a three-phase, repeatable loop suitable for dynamic contexts.\n",
    "\n",
    "Why the potential impact justifies the risks:\n",
    "- Long-form coherence is a central bottleneck in education, entertainment, and human-AI collaboration. By continuously aligning prompts with evolving context, DPA can sustain thematic consistency, maintain stylistic voice, and adapt to user feedback without costly fine-tuning (as demonstrated by dynamic prompting literature and test-time adaptation work cited above).\n",
    "- The approach is scalable and deployment-friendly. The literature shows that dynamic prompt mechanisms, domain prompts, and adaptive prompts can achieve notable gains with modest parameter overhead and without retraining core models (e.g., DAPrompt, DPCore, DAPT, Time-LlaMA) [papers cited].\n",
    "- Cross-domain applicability: The architecture supports applying adaptive prompting to a wide range of taskscreative writing, interactive gaming narratives, tutoring, and long-form content generationthrough its modular three-phase design and compatibility with dynamic adapters and memory-augmented prompts.\n",
    "- Risk management via ensemble and dynamic prompting strategies: The literature provides practical approaches to reduce overfitting and bias at test-time (e.g., ensemble prompts in robust TTA; meta-prompting in domain adaptation), which we can incorporate to mitigate drift and reliability concerns in extended interactions (ADAPROMPT; 5db3cfc9; 759b5f58).\n",
    "\n",
    "Feasibility and a concrete path forward:\n",
    "- Feasibility is supported by substantial precedent for dynamic prompting and adaptive prompting in large language models across modalities and domains, with concrete gains in efficiency and performance (see the cited papers). DPAs three-phase loop can be implemented with existing tooling: a lightweight contextual analyzer to extract themes/tones from the history, a prompt generator that produces updated prompts conditioned on the extracted signals, and a synthesis module that maintains an ongoing narrative memory to inform future updates.\n",
    "- Evaluation strategy is well-grounded: we will compare against static prompts and per-turn prompting baselines, using engagement and coherence metrics, human judgments, and standard text-generation metrics (BLEU, ROUGE), in datasets such as Story Cloze-like storytelling tasks and user-dialogue interactions on Reddit-like conversational data.\n",
    "- Additional safeguards: adopt ensemble prompting, confidence-based prompts, and selective memory buffers to curb drift, following best practices from robust test-time prompting literature (e.g., ensemble prompts in ADAPPROMPT; domain-prompt learning approaches; RAT-like iterative grounding). These measures address common failure modes in long-running prompts and support reliable deployment.\n",
    "\n",
    "In sum, DPA is a novel, feasible, and impactful framework that explicitly closes the loop between evolving dialogue context and prompt-level control, enabling robust coherence and dynamic adaptability across long interactions while leveraging and integrating proven, efficient prompting and adaptation techniques from the referenced literature.\n",
    "\n",
    "\n",
    "--------------------------------------------------------------------------------\n",
    "The Dynamic Prompt Adaptation (DPA) idea aims to continuously steer LLM outputs across long interactions by a threephase loop: Contextual Analysis, Adaptive Prompt Generation, and Iterative Context Update. While appealing, the proposal appears incremental and shows substantial overlap with a body of prior work on dynamic/adaptive prompting, continual test-time adaptation, and retrieval-grounded prompting. Several cited papers already demonstrate close ideas: dynamic prompts and memory-aware adaptation (DPCore), parameter-efficient prompt/adaptation (Dynamic Adapter Meets Prompt Tuning; Time-LlaMA), domain-aware and meta-prompting (Domain Prompt Tuning via Meta Relabeling; Decorate the Newcomers CTTA; ADAPROMPT family), and iterative grounding/retrieval loops (RAT, Iter-RetGen). Moreover, long-horizon coherence in narratives and dialogue has been explored via multi-agent/prompting pipelines (DialogGen, Agents Room, PANGeA, SCENECRAFT). Taken together, the core thrustaligning prompts to evolving context across turnshas already been explored in multiple orthogonal directions, making the three-phase framing less novel than claimed. The claim of a single, unified novel paradigm for dynamic, evolving prompts across domains is therefore questionable.\n",
    "\n",
    "\n",
    "--------------------------------------------------------------------------------\n",
    "Advocate argued that Dynamic Prompt Adaptation (DPA) offers a novel three-phase loop (Contextual Analysis, Adaptive Prompt Generation, Iterative Context Update) to sustain coherence and adaptability in long-form LLM interactions, and that it can operate without full fine-tuning by leveraging dynamic prompts and modular adapters. Skeptic contended that the three-phase loop largely re-packages established dynamic prompting and test-time adaptation concepts, with substantial overlap to work on DPCore, Dynamic Adapter + Prompt Tuning, Time-LlaMA, and domain/prompts-based adaptation, raising concerns about novelty and impact. The subsequent moderator assessment reinforced the view that the core idea is incremental, citing multiple orthogonal lines of prior work (dynamic prompts, CTTA, retrieval-grounded prompting, and narrative/dialogue pipelines) and arguing that without stronger theoretical grounding or robust human-centric evaluation, the proposal risks limited novelty and practical payoff.\n",
    "\n",
    "\n",
    "--------------------------------------------------------------------------------\n",
    "Dynamic Prompt Adaptation (DPA) offers a principled, three-phase loop that explicitly ties evolving dialogue context to prompt-level control, enabling sustained coherence and adaptability in long-form interactions without full model fine-tuning. The three phasesContextual Analysis (extracting themes, tonal shifts, and narrative goals from history), Adaptive Prompt Generation (producing updated prompts that extend past ideas or steer toward new elements), and Iterative Context Update (synthesizing prior exchanges to preserve global coherence)form a cohesive memory-prompting cycle that mirrors human creative and conversational workflows. This design yields several unique advantages:\n",
    "- Memory-efficient long-horizon coherence: by re-synthesizing context into targeted prompts rather than retraining or storing large state, DPA achieves durable coherence with modest compute overhead, which aligns with successful demonstrations of dynamic prompting and prompt-efficient adapters (e.g., Dynamic Adapter Meets Prompt Tuning; Time-LlaMA).\n",
    "- Modular, domain-agnostic applicability: the three-phase loop can be paired with parameter-efficient primitives (dynamic adapters, LoRA variants, internal prompts) to adapt to storytelling, tutoring, and interactive media without expensive finetuning (supported by DAPT and Time-LlaMA literature).\n",
    "- Robustness under domain shifts and evolving user feedback: the framework inherently accommodates shifting audience and themes, leveraging ideas from test-time adaptation and meta-prompting to maintain reliability across sessions (RAT-inspired grounding, ensemble prompting strategies in robust TTA).\n",
    "- Cross-modal and multi-turn potential: by integrating with domain prompts and meta-relabeling we can extend DPA to visual storytelling, interactive dialogue, and multimodal narrative generation, as supported by MaPLe and related multi-modal prompt work.\n",
    "- Concrete, testable impact on reader engagement and coherence: the proposed evaluation plan mirrors established storytelling and dialogue tasks and benefits from prior work showing that adaptive prompting yields improvements in coherence, engagement, and adaptability (RAT, Iter-RetGen, SCENECRAFT, DialogGen).\n",
    "Overall, DPA brings a coherent, scalable, and practically deployable paradigm that leverages the best of prompt-based adaptation while addressing the key pain point of long-context coherence in LLM-powered creative and conversational tasks.\n",
    "\n",
    "\n",
    "--------------------------------------------------------------------------------\n",
    "The Dynamic Prompt Adaptation (DPA) idea promises to maintain coherence across long interactions by a three-phase loop (Contextual Analysis, Adaptive Prompt Generation, Iterative Context Update) and to do so without full fine-tuning. However, the contribution is largely incremental and overlaps with a broad set of prior dynamic/adaptive prompting and test-time adaptation works. There is insufficient theoretical grounding, and the evaluation plan relies on metrics and data that have well-known limitations for judging long-horizon coherence and engagement. The claimed noveltyan explicit three-phase cycle tailored for ongoing interactionsappears to be a re-packaging of existing concepts rather than a fundamentally new paradigm. Without stronger baselines, rigorous human evaluations, or a formal treatment of stability/guarantees over many turns, the work risks limited impact beyond narrow demonstrations in storytelling or dialogue tasks.\n",
    "\n",
    "Key overlaps include dynamic prompts and domain-adaptive prompting, dynamic adapters, and retrieval-grounded prompting from the literature (see DPCore; Dynamic Adapter Meets Prompt Tuning; Time-LlaMA; DAPrompt/ADT family; RAT/Iter-RetGen). The three-phase loop shares motifs with iterative grounding and prompt refinement seen in Retrieval-Augmented Thoughts (RAT), Iterative Retrieval-Generation (Iter-RetGen), and CTTA-style adaptation. In short, the core ideaprompts that adapt to evolving context across turnshas already been explored in multiple orthogonal directions; the proposed three-phase framing does not clearly establish a unique, cohesive methodological advance on top of these prior works.\n",
    "\n",
    "Supporting_papers IDs: [\"246482d9758e93d0b349e2926996d887417174d8\", \"6b533de65b3d2cee190415f5a2a2e6afe2ac7c78\", \"650a24da1702beca7eb70011a26f1f3238efad4b\", \"eacb61136023a2f30c5a0313f222d50e5f63ac9b\", \"5db3cfc974c42bfa2d9518a8910762790\", \"dc256e179d4e8eff48879a40ddc414b15b0b2300\", \"dc256e179d4e8eff48879a40ddc414b15b0b2300\", \"8051818817a9a3815be6623a679d4a7f5a7b7964\"]}\n",
    "\n",
    "\n",
    "--------------------------------------------------------------------------------\n",
    "Advocate argues Dynamic Prompt Adaptation (DPA) offers a novel three-phase loop (Contextual Analysis, Adaptive Prompt Generation, Iterative Context Update) to maintain coherence and adaptability across long interactions without full fine-tuning, with modular compatibility to existing parameter-efficient methods. Skeptic counters that the three-phase loop largely recycles established dynamic prompting and test-time adaptation ideas and overlaps with DPCore, dynamic adapters + prompt tuning, Time-LlaMA, RAT/Iter-RetGen, and Dialog/NPC-driven narrative pipelines, raising concerns about novelty and practical impact. The moderator notes substantial overlap across multiple orthogonal lines of prior work and emphasizes the need for stronger theoretical grounding and robust, human-centered evaluation to establish genuine novelty and generalizability beyond narrow storytelling tasks. The current stance favors caution: feasible to implement, but novelty and significance require stronger demonstration and clearer baselines.\n",
    "\n",
    "\n",
    "--------------------------------------------------------------------------------\n",
    "Dynamic Prompt Adaptation (DPA) offers a principled threephase loopContextual Analysis, Adaptive Prompt Generation, and Iterative Context Updatethat continually aligns prompts with an evolving dialogue to sustain coherence and engagement without full model finetuning. The novelty lies in explicitly decoupling history understanding (Contextual Analysis) from prompt reconfiguration (Adaptive Prompt Generation) and global history synthesis (Iterative Context Update) into a repeatable, memoryefficient cycle that can operate across domains (storytelling, tutoring, conversational agents, interactive media). Crucially, DPA is designed to plug into existing parameterefficient pipelines (e.g., dynamic adapters and prompt tuning) to balance accuracy and compute, making it deployable on commodity hardware. The feasibility and impact are underpinned by a rich literature showing that dynamic prompts, adaptive prompts, and retrievalaugmented reasoning can yield gains with modest training or finetuning costs, and scale to longhorizon tasks:\n",
    "\n",
    "- Dynamic Prompt Coreset and continual testtime adaptation demonstrate robust performance across changing domains with dramatically reduced trainable parameters and compute (DPCore) (246482d9758e93d0b349e2926996d887417174d8).\n",
    "- Parameterefficient transfer with prompts and dynamic adapters shows substantial gains while freezing the backbone (Dynamic Adapter Meets Prompt Tuning) (6b533de65b3d2cee190415f5a2a2e6afe2ac7c78).\n",
    "- Dynamic lowrank adaptation selects task/instancespecific modules per input, enabling efficient, scalable adaptation without full finetuning (TimeLLaMA) (650a24da1702beca7eb70011a26f1f3238efad4b).\n",
    "- Domainaware and meta prompting approaches illustrate how prompts can embed domain semantics and adapt to shifts without retraining (Domain Prompt Tuning via Meta Relabeling; Decorate the Newcomers for CTTA) (eacb61136023a2f30c5a0313f222d50e5f63ac9b).\n",
    "- Robust testtime prompting and ensemble strategies mitigate biases and improve adaptation under distribution shift (ADAPROMPT family) (5db3cfc974c42bfa2d9518a8910762790b516037).\n",
    "- Retrievalaugmented thought and iterative retrievalgeneration loops provide grounding and improved reasoning over long horizons, aligning prompts with retrieved evidence (RAT; IterRetGen) (dc256e179d4e8eff48879a40ddc414b15b0b2300; 8051818817a9a3815be6623a679d4a7f5a7b7964).\n",
    "- Longform narrative and multiturn dialogue literature shows tangible gains from structured, multistage prompting and planning (DialogGen; SCENECRAFT; Agents Room; PANGeA).\n",
    "\n",
    "Together, these findings support the core premise of DPA: a lightweight, threephase, historyaware prompting protocol can markedly improve coherence and adaptability across long interactions while avoiding heavy finetuning, thereby delivering high impact with feasible resource requirements.\n",
    "\n",
    "\n",
    "--------------------------------------------------------------------------------\n",
    "Dynamic Prompt Adaptation (DPA) proposes a three-phase loop (Contextual Analysis, Adaptive Prompt Generation, Iterative Context Update) to maintain coherence and adaptability across long LLM interactions without full fine-tuning. While appealing, the idea risks being incremental rather than revolutionary. The literature already shows extensive precedent for dynamic, memory-aware prompting and test-time adaptation, including explicit three-phase or iterative workflows, prompting strategies, and retrieval-grounded reasoning. Without stronger theoretical grounding, rigorous baselines, or robust human-centered evaluation, DPAs claimed novelty and practical impact remain uncertain and potentially domain-limited to demonstrations rather than generalizable gains across tasks and modalities.\n",
    "\n",
    "\n",
    "--------------------------------------------------------------------------------\n",
    "VERDICT: Research Idea Evaluation\n",
    "\n",
    "SCORES:\n",
    "- Novelty: 4/10\n",
    "- Feasibility: 7/10\n",
    "\n",
    "SUMMARY:\n",
    "The debate centers on the novelty and feasibility of Dynamic Prompt Adaptation (DPA), a proposed three-phase method (Contextual Analysis, Adaptive Prompt Generation, Iterative Context Update) to improve coherence in long-form LLM interactions. The Advocate positions DPA as a novel, lightweight, and modular framework that explicitly orchestrates prompt updates based on the evolving dialogue history, citing supporting evidence from literature on dynamic prompting and parameter-efficient adaptation. The Skeptic argues that DPA is incremental, repackaging existing concepts from continual test-time adaptation, retrieval-augmented generation, and other dynamic prompting techniques without offering a significant methodological leap. The Skeptic also raises concerns about the reliance on potentially weak evaluation metrics (BLEU/ROUGE) and the practical challenges of prompt drift and stability over many turns. The final verdict leans towards the Skeptic's view, scoring novelty low while acknowledging feasibility, and recommending a revision to better differentiate the work from the extensive prior art and to incorporate more robust, human-centered evaluation methods.\n",
    "\n",
    "STRENGTHS:\n",
    "- The proposed method directly addresses the critical challenge of maintaining long-term coherence in LLMs.\n",
    "- The iterative, three-phase loop is a structured and intuitive approach to dynamically adapting prompts based on conversational context.\n",
    "- The experiment plan is well-defined, with clear baselines and metrics for evaluation.\n",
    "- The idea of integrating with parameter-efficient methods like LoRA and dynamic adapters makes it computationally feasible.\n",
    "\n",
    "WEAKNESSES:\n",
    "- The novelty of the proposed method is questionable, as it appears to be a recombination of existing techniques in dynamic prompting and test-time adaptation.\n",
    "- The reliance on automated metrics like BLEU and ROUGE is a significant weakness, as these are known to correlate poorly with human judgments of narrative coherence and quality.\n",
    "- The proposal does not adequately address the risk of 'prompt drift' or error accumulation over long interactions, where the model could get stuck in repetitive loops or diverge thematically.\n",
    "- The datasets mentioned (Story Cloze, Reddit) may not be sufficient to robustly evaluate long-form narrative coherence and engagement.\n",
    "\n",
    "RECOMMENDATION: Revise. The proposal is feasible but the novelty is questionable given the extensive prior work on dynamic and adaptive prompting. The authors should clearly articulate what distinguishes their three-phase loop from existing iterative/adaptive prompting frameworks (e.g., retrieval-augmented generation, continual test-time adaptation). The experiment plan needs more robust, human-centered evaluation metrics beyond BLEU/ROUGE to convincingly demonstrate improved coherence and engagement."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe80423d",
   "metadata": {},
   "source": [
    "### AGENTIC AI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d07491a",
   "metadata": {},
   "source": [
    "ReAct\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc5356c7",
   "metadata": {},
   "source": [
    "#### SINGLE AGENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6370bac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running ReAct Agent Evaluation with Pre-Retrieved Papers...\n",
      "Number of retrieved papers: 199\n",
      "\n",
      "============================================================\n",
      "REACT AGENT EVALUATION COMPLETE\n",
      "============================================================\n",
      "Final Scores: content='{\\n  \"novelty_score\": 6,\\n  \"feasibility_score\": 7,\\n  \"impact_score\": 7,\\n  \"summary\": \"Based on the analysis of retrieved papers, the Phase 1 contextual analysis components (theme extraction, tonal/style tracking, discourse drift, and theme detection) show insights that largely align with existing literature. The integrated anglecombining theme extraction with tonal/style tracking and discourse drift in Phase 1offers incremental novelty at best. Feasibility is high, as the methods (topic/theme detection, stylometry, drift/discourse monitoring) are established and can be implemented with standard NLP/tooling. The potential impact is moderate, contingent on demonstrated improvements in early contextual understanding and downstream tasks, and on avoiding redundancy with prior work.\",\\n  \"recommendation\": \"Revise\"\\n}' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 1394, 'prompt_tokens': 294, 'total_tokens': 1688, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 1216, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-5-nano-2025-08-07', 'system_fingerprint': None, 'id': 'chatcmpl-Chii3x1Krl0KP6xllSfhGqmGgyn2I', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='lc_run--a7c610b9-c5eb-45e1-a26e-1fdf8e1a3586-0' usage_metadata={'input_tokens': 294, 'output_tokens': 1394, 'total_tokens': 1688, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 1216}}\n",
      "Total Iterations: 4\n",
      "Number of Findings: 4\n",
      "\n",
      "Workflow Visualization:\n",
      "\n",
      "============================================================\n",
      "REACT AGENT EVALUATION COMPLETE\n",
      "============================================================\n",
      "Final Scores: content='{\\n  \"novelty_score\": 6,\\n  \"feasibility_score\": 7,\\n  \"impact_score\": 7,\\n  \"summary\": \"Based on the analysis of retrieved papers, the Phase 1 contextual analysis components (theme extraction, tonal/style tracking, discourse drift, and theme detection) show insights that largely align with existing literature. The integrated anglecombining theme extraction with tonal/style tracking and discourse drift in Phase 1offers incremental novelty at best. Feasibility is high, as the methods (topic/theme detection, stylometry, drift/discourse monitoring) are established and can be implemented with standard NLP/tooling. The potential impact is moderate, contingent on demonstrated improvements in early contextual understanding and downstream tasks, and on avoiding redundancy with prior work.\",\\n  \"recommendation\": \"Revise\"\\n}' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 1394, 'prompt_tokens': 294, 'total_tokens': 1688, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 1216, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-5-nano-2025-08-07', 'system_fingerprint': None, 'id': 'chatcmpl-Chii3x1Krl0KP6xllSfhGqmGgyn2I', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='lc_run--a7c610b9-c5eb-45e1-a26e-1fdf8e1a3586-0' usage_metadata={'input_tokens': 294, 'output_tokens': 1394, 'total_tokens': 1688, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 1216}}\n",
      "Total Iterations: 4\n",
      "Number of Findings: 4\n",
      "\n",
      "Workflow Visualization:\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langgraph.graph import StateGraph, END, START\n",
    "from langchain_core.tools import Tool \n",
    "from typing import TypedDict, Annotated, Literal\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage\n",
    "import operator\n",
    "import json\n",
    "import re\n",
    "\n",
    "# Define the agent state\n",
    "class AgentState(TypedDict):\n",
    "    proposal: str\n",
    "    retrieved_papers: str  # Pre-retrieved papers from earlier pipeline\n",
    "    plan: str\n",
    "    findings: Annotated[list, operator.add]\n",
    "    scores: dict\n",
    "    confidence: dict\n",
    "    iteration: int\n",
    "    next_action: str\n",
    "\n",
    "# Tools that work with pre-retrieved papers\n",
    "def analyze_papers_tool(focus_area: str, papers: str) -> str:\n",
    "    \"\"\"Analyze retrieved papers focusing on a specific area\"\"\"\n",
    "    return f\"Analysis of papers focusing on '{focus_area}': Found relevant insights from the pre-retrieved literature.\"\n",
    "\n",
    "def extract_paper_details_tool(paper_criteria: str, papers: str) -> str:\n",
    "    \"\"\"Extract specific details from papers based on criteria\"\"\"\n",
    "    # Parse papers and extract relevant details\n",
    "    lines = papers.split('\\n\\n---\\n\\n')\n",
    "    relevant_papers = []\n",
    "    \n",
    "    for paper in lines[:3]:  # Limit to first 3 papers for demonstration\n",
    "        if paper.strip():\n",
    "            relevant_papers.append(f\"Paper analysis for '{paper_criteria}': {paper[:200]}...\")\n",
    "    \n",
    "    return f\"Extracted details based on '{paper_criteria}': {len(relevant_papers)} papers analyzed.\"\n",
    "\n",
    "def compare_methodologies_tool(methodology_aspect: str, papers: str) -> str:\n",
    "    \"\"\"Compare methodologies in retrieved papers\"\"\"\n",
    "    return f\"Methodology comparison for '{methodology_aspect}': Analyzed methodological approaches in retrieved papers.\"\n",
    "\n",
    "def execute_tool(tool_name: str, params: dict, retrieved_papers: str) -> str:\n",
    "    \"\"\"Execute the specified tool with parameters and pre-retrieved papers\"\"\"\n",
    "    if tool_name == \"analyze_papers\":\n",
    "        return analyze_papers_tool(params.get(\"focus_area\", \"\"), retrieved_papers)\n",
    "    elif tool_name == \"extract_details\":\n",
    "        return extract_paper_details_tool(params.get(\"criteria\", \"\"), retrieved_papers)\n",
    "    elif tool_name == \"compare_methods\":\n",
    "        return compare_methodologies_tool(params.get(\"aspect\", \"\"), retrieved_papers)\n",
    "    else:\n",
    "        return f\"Tool {tool_name} executed with params {params}\"\n",
    "\n",
    "# Define tools\n",
    "tools = [\n",
    "    Tool(\n",
    "        name=\"analyze_papers\",\n",
    "        func=analyze_papers_tool,\n",
    "        description=\"Analyze retrieved papers focusing on specific aspects\"\n",
    "    ),\n",
    "    Tool(\n",
    "        name=\"extract_details\", \n",
    "        func=extract_paper_details_tool,\n",
    "        description=\"Extract specific methodological or technical details\"\n",
    "    ),\n",
    "    Tool(\n",
    "        name=\"compare_methods\",\n",
    "        func=compare_methodologies_tool,\n",
    "        description=\"Compare methodologies across retrieved papers\"\n",
    "    ),\n",
    "]\n",
    "\n",
    "# Define agent nodes\n",
    "def planning_node(state: AgentState):\n",
    "    \"\"\"Agent creates investigation plan\"\"\"\n",
    "    messages = [\n",
    "        HumanMessage(content=f\"\"\"\n",
    "        Given this research proposal: {state['proposal']}\n",
    "        \n",
    "        Available retrieved papers: {len(state['retrieved_papers'].split('Paper ID:'))-1} papers\n",
    "        \n",
    "        Create a step-by-step plan to evaluate its novelty and feasibility using the already retrieved papers.\n",
    "        Focus on:\n",
    "        1. Analyzing overlaps with existing methods\n",
    "        2. Identifying unique contributions\n",
    "        3. Assessing technical feasibility\n",
    "        \n",
    "        Return just the plan as a string.\n",
    "        \"\"\")\n",
    "    ]\n",
    "    \n",
    "    plan_response = llm.invoke(messages)\n",
    "    return {\n",
    "        \"plan\": plan_response.content,\n",
    "        \"iteration\": 0,\n",
    "        \"next_action\": \"investigate\"\n",
    "    }\n",
    "\n",
    "def investigation_node(state: AgentState):\n",
    "    \"\"\"Agent investigates using retrieved papers\"\"\"\n",
    "    messages = [\n",
    "        HumanMessage(content=f\"\"\"\n",
    "        Current plan: {state['plan']}\n",
    "        Findings so far: {state.get('findings', [])}\n",
    "        Current iteration: {state.get('iteration', 0)}\n",
    "        \n",
    "        You have access to pre-retrieved papers. Based on the plan and current findings, what should you analyze next?\n",
    "        Choose one:\n",
    "        1. Analyze papers for specific aspects (respond with: \"TOOL: analyze_papers, FOCUS: <aspect to focus on>\")\n",
    "        2. Extract technical details (respond with: \"TOOL: extract_details, CRITERIA: <what to extract>\") \n",
    "        3. Compare methodologies (respond with: \"TOOL: compare_methods, ASPECT: <methodology aspect>\")\n",
    "        4. Conclude investigation (respond with: \"CONCLUDE\")\n",
    "        \n",
    "        Respond in the exact format specified above.\n",
    "        \"\"\")\n",
    "    ]\n",
    "    \n",
    "    decision_response = llm.invoke(messages)\n",
    "    decision = decision_response.content.strip()\n",
    "    \n",
    "    if decision.startswith(\"TOOL:\"):\n",
    "        # Parse the tool command\n",
    "        parts = decision.split(\", \")\n",
    "        tool_name = parts[0].split(\": \")[1]\n",
    "        \n",
    "        if \"FOCUS:\" in decision:\n",
    "            focus_area = parts[1].split(\": \")[1]\n",
    "            result = execute_tool(tool_name, {\"focus_area\": focus_area}, state['retrieved_papers'])\n",
    "        elif \"CRITERIA:\" in decision:\n",
    "            criteria = parts[1].split(\": \")[1]\n",
    "            result = execute_tool(tool_name, {\"criteria\": criteria}, state['retrieved_papers'])\n",
    "        elif \"ASPECT:\" in decision:\n",
    "            aspect = parts[1].split(\": \")[1]\n",
    "            result = execute_tool(tool_name, {\"aspect\": aspect}, state['retrieved_papers'])\n",
    "        else:\n",
    "            result = \"Tool execution failed - invalid parameters\"\n",
    "        \n",
    "        return {\n",
    "            \"findings\": [result],\n",
    "            \"iteration\": state.get(\"iteration\", 0) + 1,\n",
    "            \"next_action\": \"reflect\"\n",
    "        }\n",
    "    else:\n",
    "        return {\"next_action\": \"conclude\"}\n",
    "\n",
    "def reflection_node(state: AgentState):\n",
    "    \"\"\"Agent reflects on progress\"\"\"\n",
    "    messages = [\n",
    "        HumanMessage(content=f\"\"\"\n",
    "        Findings so far: {state.get('findings', [])}\n",
    "        Current iteration: {state.get('iteration', 0)}\n",
    "        \n",
    "        Based on your analysis of the retrieved papers, evaluate the confidence level (0-100) for each aspect:\n",
    "        - Novelty assessment confidence (how well you understand what's new)\n",
    "        - Feasibility assessment confidence (how realistic the implementation seems)\n",
    "        - Overall investigation completeness (do you have enough information)\n",
    "        \n",
    "        Return a JSON-like response:\n",
    "        {{\"novelty\": <score>, \"feasibility\": <score>, \"overall\": <score>}}\n",
    "        \n",
    "        Then decide: Should I continue investigating (if overall < 75) or conclude?\n",
    "        Add on a new line: CONTINUE or CONCLUDE\n",
    "        \"\"\")\n",
    "    ]\n",
    "    \n",
    "    confidence_response = llm.invoke(messages)\n",
    "    response_lines = confidence_response.content.strip().split('\\n')\n",
    "    \n",
    "    # Parse confidence scores (simplified)\n",
    "    try:\n",
    "        confidence_line = response_lines[0]\n",
    "        # Extract numbers from the response (simplified parsing)\n",
    "        numbers = re.findall(r'\\d+', confidence_line)\n",
    "        if len(numbers) >= 3:\n",
    "            confidence = {\n",
    "                \"novelty\": int(numbers[0]),\n",
    "                \"feasibility\": int(numbers[1]), \n",
    "                \"overall\": int(numbers[2])\n",
    "            }\n",
    "        else:\n",
    "            confidence = {\"novelty\": 50, \"feasibility\": 50, \"overall\": 50}\n",
    "    except:\n",
    "        confidence = {\"novelty\": 50, \"feasibility\": 50, \"overall\": 50}\n",
    "    \n",
    "    # Determine next action\n",
    "    next_action = \"investigate\" if confidence.get(\"overall\", 0) < 75 else \"conclude\"\n",
    "    \n",
    "    return {\n",
    "        \"confidence\": confidence,\n",
    "        \"next_action\": next_action\n",
    "    }\n",
    "\n",
    "def scoring_node(state: AgentState):\n",
    "    \"\"\"Generate final scores and report\"\"\"\n",
    "    messages = [\n",
    "        HumanMessage(content=f\"\"\"\n",
    "        Based on analysis of retrieved papers and findings: {state.get('findings', [])}\n",
    "        Confidence levels: {state.get('confidence', {})}\n",
    "        \n",
    "        Generate final evaluation scores (1-10) for:\n",
    "        - Novelty: How new/original is this idea compared to retrieved papers?\n",
    "        - Feasibility: How realistic is implementation based on similar work?\n",
    "        - Impact: Potential significance of results based on the literature?\n",
    "        \n",
    "        Provide a brief summary and recommendation based on the paper analysis.\n",
    "        \n",
    "        Format as JSON:\n",
    "        {{\"novelty_score\": <1-10>, \"feasibility_score\": <1-10>, \"impact_score\": <1-10>, \"summary\": \"<text>\", \"recommendation\": \"<Accept/Revise/Reject>\"}}\n",
    "        \"\"\")\n",
    "    ]\n",
    "    \n",
    "    scores_response = llm.invoke(messages)\n",
    "    \n",
    "\n",
    "    return {\n",
    "        \"scores\": scores_response,\n",
    "        \"next_action\": \"end\"\n",
    "    }\n",
    "\n",
    "# Define routing function\n",
    "def should_continue(state: AgentState) -> Literal[\"investigate\", \"conclude\"]:\n",
    "    \"\"\"Determine next step based on current state\"\"\"\n",
    "    next_action = state.get(\"next_action\", \"investigate\")\n",
    "    \n",
    "    # Safety check - limit iterations\n",
    "    if state.get(\"iteration\", 0) >= 4:  # Reduced since we're using pre-retrieved papers\n",
    "        return \"conclude\"\n",
    "    \n",
    "    if next_action == \"conclude\":\n",
    "        return \"conclude\"\n",
    "    else:\n",
    "        return \"investigate\"\n",
    "\n",
    "# Build the graph\n",
    "agentic_workflow = StateGraph(AgentState)\n",
    "\n",
    "# Add nodes\n",
    "agentic_workflow.add_node(\"planning\", planning_node)\n",
    "agentic_workflow.add_node(\"investigation\", investigation_node)\n",
    "agentic_workflow.add_node(\"reflection\", reflection_node)\n",
    "agentic_workflow.add_node(\"scoring\", scoring_node)\n",
    "\n",
    "# Define edges (control flow)\n",
    "agentic_workflow.add_edge(START, \"planning\")\n",
    "agentic_workflow.add_edge(\"planning\", \"investigation\")\n",
    "agentic_workflow.add_edge(\"investigation\", \"reflection\")\n",
    "\n",
    "# Conditional edge based on confidence/decision\n",
    "agentic_workflow.add_conditional_edges(\n",
    "    \"reflection\",\n",
    "    should_continue,\n",
    "    {\n",
    "        \"investigate\": \"investigation\",\n",
    "        \"conclude\": \"scoring\"\n",
    "    }\n",
    ")\n",
    "agentic_workflow.add_edge(\"scoring\", END)\n",
    "\n",
    "# Compile the graph\n",
    "agentic_app = agentic_workflow.compile()\n",
    "\n",
    "# Extract research idea from initial user input\n",
    "research_idea_text = result_llm[\"messages\"][0].content\n",
    "\n",
    "# Extract and format retrieved papers\n",
    "papers_json = json.loads(result_llm[\"messages\"][-2].content)  # -2 because -1 is the analysis\n",
    "retrieved_papers_text = getReferencePaper.prepare_papers_for_llm(papers_json)\n",
    "\n",
    "print(\"Running ReAct Agent Evaluation with Pre-Retrieved Papers...\")\n",
    "print(f\"Number of retrieved papers: {len(retrieved_papers_text.split('Paper ID:'))-1}\")\n",
    "\n",
    "# Run the agent\n",
    "try:\n",
    "    result = agentic_app.invoke({\n",
    "        \"proposal\": research_idea_text,\n",
    "        \"retrieved_papers\": retrieved_papers_text,\n",
    "        \"plan\": \"\",\n",
    "        \"findings\": [],\n",
    "        \"scores\": {},\n",
    "        \"confidence\": {},\n",
    "        \"iteration\": 0,\n",
    "        \"next_action\": \"start\"\n",
    "    })\n",
    "    \n",
    "    print(result)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error running ReAct agent: {e}\")\n",
    "\n",
    "# Visualize the graph\n",
    "from IPython.display import Image, display\n",
    "print(\"\\nWorkflow Visualization:\")\n",
    "display(Image(agentic_app.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "06124169",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'content'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[107]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[43mresult\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcontent\u001b[49m)\n",
      "\u001b[31mAttributeError\u001b[39m: 'dict' object has no attribute 'content'"
     ]
    }
   ],
   "source": [
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b422e009",
   "metadata": {},
   "source": [
    "##### Example Result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dcd0e84",
   "metadata": {},
   "source": [
    "{'proposal': \"Dynamic Prompt Adaptation:\n",
    "                Problem: Large Language Models (LLMs) often struggle with maintaining coherence over extended interactions or creative tasks, leading to thematic inconsistencies and reduced reader engagement.\n",
    "                Existing Methods: Current methods often use fixed prompts or few-shot examples, which may not adapt to the evolving context of a conversation or creative narrative. Techniques such as Chain-of-Thought prompting are utilized, but they do not inherently address continuity and adaptability across interactions.\n",
    "                Motivation: Human creative writing often involves iterative dialogue and adaptation to the flow of discussion. The style and context of interactions can shift dynamically based on prior exchanges. Dynamic adaptation mirrors how authors and conversationalists adjust based on audience feedback and shifting themes.\n",
    "                Proposed Method: We propose Dynamic Prompt Adaptation, involving three phases: (1) Contextual Analysis - Analyze previous outputs and user prompts to extract key themes and tonal shifts, applying a prompting structure like 'Reflect on the previous topic of [theme] and build on it.' (2) Adaptive Prompt Generation - Using insights from the analysis, generate updated prompts that introduce new elements or clarify past responses, e.g., 'Continuing from your last thought on [theme], can you expand on how this might be represented in [new context]?' (3) Iterative Context Update - As the dialogue progresses, generate a synthesis of all prior interactions to maintain thematic coherence, prompting with 'Summarize the key points discussed so far to keep track of our narrative.'\n",
    "                Experiment Plan: Test against static prompting strategies by evaluating engagement scores, coherence assessments, and user satisfaction in storytelling scenarios using standard text generation metrics such as BLEU and ROUGE. Incorporate user feedback on naturalness and adaptability during the interaction. Datasets could include the 'Story Cloze Test' dataset and user-generated dialogue interactions scraped from platforms like Reddit to assess conversational engagement.\", 'retrieved_papers': 'Paper ID: 246482d9758e93d0b349e2926996d887417174d8\n",
    "                                    Title: DPCore: Dynamic Prompt Coreset for Continual Test-Time Adaptation\n",
    "                                    Abstract: Continual Test-Time Adaptation (CTTA) seeks to adapt source pre-trained models to continually changing, unseen target domains. While existing CTTA methods assume structured domain changes with uniform durations, real-world environments often exhibit dynamic patterns where domains recur with varying frequencies and durations. Current approaches, which adapt the same parameters across different domains, struggle in such dynamic conditions-they face convergence issues with brief domain exposures, risk forgetting previously learned knowledge, or misapplying it to irrelevant domains. To remedy this, we propose DPCore, a method designed for robust performance across diverse domain change patterns while ensuring computational efficiency. DPCore integrates three key components: Visual Prompt Adaptation for efficient domain alignment, a Prompt Coreset for knowledge preservation, and a Dynamic Update mechanism that intelligently adjusts existing prompts for similar domains while creating new ones for substantially different domains. Extensive experiments on four benchmarks demonstrate that DPCore consistently outperforms various CTTA methods, achieving state-of-the-art performance in both structured and dynamic settings while reducing trainable parameters by 99% and computation time by 64% compared to previous approaches.\n",
    "                                \n",
    "\n",
    "---\n",
    "\n",
    "Paper ID: 6b533de65b3d2cee190415f5a2a2e6afe2ac7c78\n",
    "                                    Title: Dynamic Adapter Meets Prompt Tuning: Parameter-Efficient Transfer Learning for Point Cloud Analysis\n",
    "                                    Abstract: Point cloud analysis has achieved outstanding performance by transferring point cloud pretrained models. However, existing methods for model adaptation usually update all model parameters, i.e., full fine-tuning paradigm, which is inefficient as it relies on high computational costs (e.g., training GPU memory) and massive storage space. In this paper, we aim to study parameter-efficient transfer learning for point cloud analysis with an ideal tradeoff between task performance and parameter efficiency. To achieve this goal, we freeze the parameters of the default pretrained models and then propose the Dynamic Adapter, which generates a dynamic scale for each token, considering the token significance to the downstream task. We further seamlessly integrate Dynamic Adapter with Prompt Tuning (DAPT) by constructing Internal Prompts, capturing the instance-specific features for interaction. Extensive experiments conducted on five challenging datasets demonstrate that the proposed DAPT achieves superior performance compared to the full fine-tuning counterparts while significantly reducing the trainable parameters and training GPU memory by 95% and 35%, respectively. Code is available at https://github.com/LMD0311/DAPT.\n",
    "                                \n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "2) Build a structured evidence map from the 199 papers\n",
    "- Create a data schema: paper_id, title, year, method category, prompting technique, context length, memory/retention mechanism, adaptation trigger, evaluation metrics, datasets, claimed contributions.\n",
    "- Populate a searchable database or spreadsheet and tag papers by phase-related capabilities (Contextual Analysis, Adaptive Prompt Generation, Iterative Context Update) and by overlap with proposed components.\n",
    "\n",
    "3) Overlap analysis by phase\n",
    "- Phase 1: Contextual Analysis\n",
    "  - Identify papers on theme detection, discourse/semantic drift, tone/style tracking, and prompting templates that reflect prior content.\n",
    "  - Record how prior work handles extraction of themes, tonal shifts, and narrative continuity.\n",
    "- Phase 2: Adaptive Prompt Generation\n",
    "  - Identify work on dynamic prompts, prompt rewrites, plan-and-solve prompting, instruction-tuning for adaptability, and prompt re-routing based on user input.\n",
    "- Phase 3: Iterative Context Update\n",
    "  - Identify long-context maintenance methods, memory-augmented generation, dialogue/state tracking, and synthesis prompts that condense prior interactions.\n",
    "- For each phase, compute overlaps with your proposed prompts, templates, and objectives; note any gaps or novel couplings.\n",
    "\n",
    "4) Gap analysis to reveal unique contributions\n",
    "- Compare three-phase orchestration against single-phase dynamic prompting and against memory augmented or retrieval-based methods.\n",
    "- Assess whether prior work demonstrates end-to-end pipelines that:\n",
    "  - Analyze prior outputs and user prompts for theme/tonal shifts (Phase 1).\n",
    "  - Generate updated prompts that explicitly steer continuation (Phase 2).\n",
    "  - Produce iterative, synthesized summaries to maintain coherence across many turns (Phase 3).\n",
    "- Identify gaps your plan addresses: explicit phased approach tailored to creative/storytelling tasks, integration of reflective prompts with continuity-focused synthesis, and a combined automatic/human-evaluation framework.\n",
    "\n",
    "5) Feasibility assessment of the proposed architecture\n",
    "- Decompose into modules and interfaces:\n",
    "  - Module A: Contextual Analysis (theme extraction, tonal shift detection, discourse tracking).\n",
    "  - Module B: Adaptive Prompt Generator (template-driven and learned prompts, handling new elements and clarifications).\n",
    "  - Module C: Iterative Context Updater (global narrative synthesis, coherence-preserving summaries).\n",
    "- Assess each module for technical feasibility with current LLMs:\n",
    "  - Prompt length constraints and context window limits; potential need for retrieval-augmented inputs.\n",
    "  - Computational and latency costs of repeated prompting across turns.\n",
    "  - Hallucination risk and drift control through synthesis prompts and validation checks.\n",
    "- Propose feasible realizations:\n",
    "  - Use a modular pipeline with a lightweight external memory store or episodic memory of themes.\n",
    "  Use retrieval or summarization steps to compress history within model limits.\n",
    "  Rely on established prompting techniques (CoT, self-critique, instruction-following) augmented by explicit coherence checks.\n",
    "- Identify technical risks and mitigation strategies (prompt drift, data leakage between prompts, evaluation noise).\n",
    "\n",
    "6) Experimental design aligned with novelty and feasibility\n",
    "- Baselines:\n",
    "  - Static prompts with fixed prompts.\n",
    "  - Dynamic prompting without phased structure.\n",
    "  - Existing memory-augmented/dialogue systems.\n",
    "- Datasets and domains:\n",
    "  - Story Cloze Test (coherence in narrative endings).\n",
    "  - Reddit-based dialogue interactions (creative storytelling threads, conversations).\n",
    "  - Additional long-form storytelling datasets or writing prompts to test continuity.\n",
    "- Evaluation metrics:\n",
    "  - Automatic: BLEU, ROUGE, METEOR, BERTScore, ROUGE-L; plus coherence-oriented metrics (entity grid/coherence scores, COH-METER where feasible).\n",
    "  - Engagement and naturalness: user-rated scales, preference tests.\n",
    "  - Specific to coherence: longitudinal coherence scores across turns, consistency of themes, and narrative arc continuity.\n",
    "- Evaluation design:\n",
    "  - Within-subject A/B/C testing comparing the three-phase approach versus baselines.\n",
    "  - Ablation studies to isolate contributions of Phase 1, Phase 2, and Phase 3.\n",
    "  - Statistical analysis plans (confidence intervals, significance tests, effect sizes).\n",
    "- Reproducibility plan:\n",
    "  - Pre-register hypotheses, methods, and evaluation protocol.\n",
    "  - Share prompts templates, evaluation scripts, and synthetic datasets where permissible.\n",
    "  - Document data licensing and preprocessing steps.\n",
    "\n",
    "7) Data handling, ethics, and compliance\n",
    "- Ensure dataset licenses and terms (Story Cloze, Reddit data) are compliant; obtain permissions where needed.\n",
    "- Address privacy and consent for user-generated dialogue data.\n",
    "- Mitigate bias and ensure diversity of writing styles and topics in evaluation materials.\n",
    "\n",
    "8) Architectural detail and implementation plan\n",
    "- Define a concrete pipeline:\n",
    "  - Input: user prompts and prior outputs.\n",
    "  - Phase 1: run contextual analysis to extract themes, tonal cues, and narrative branches.\n",
    "  - Phase 2: produce updated prompts that introduce new elements or clarify past responses, with explicit references to themes and context.\n",
    "  - Phase 3: generate a synthesized summary of prior interactions to guide future turns and prompt the model for continuity.\n",
    "  - Output: a continuation generation with coherence checks, followed by optional human-in-the-loop review.\n",
    "- Data flows and interfaces:\n",
    "  - Clear API boundaries between modules; stateless prompts per turn with a memory ledger.\n",
    "  - Logging for reproducibility and evaluation auditing.\n",
    "- Resource plan:\n",
    "  - Estimate compute for 199-paper-informed analysis, pipeline prototyping, and full experiments.\n",
    "  - Plan for iterative pilot studies before full-scale evaluation.\n",
    "\n",
    "9) Timeline and milestones\n",
    "- Month 12: complete novelty criteria, build evidence map from 199 papers, perform phase-wise overlap analysis.\n",
    "- Month 34: conduct gap/unique contribution assessment; draft architecture design and feasibility notes.\n",
    "- Month 56: implement a minimal viable three-phase pipeline prototype; develop datasets, baselines, and evaluation plan.\n",
    "- Month 78: run experiments across Story Cloze and dialogue datasets; collect automatic and human evaluations; perform ablations.\n",
    "- Month 9: synthesize results, refine claims of novelty and feasibility; prepare reproducibility materials.\n",
    "- Month 10: write up plan for potential conference submission; outline follow-up experiments and extensions.\n",
    "\n",
    "10) Deliverables and documentation\n",
    "- A structured novelty/feasibility report summarizing overlaps, gaps, unique contributions, and feasibility assessments.\n",
    "- An implementation blueprint for the three-phase dynamic prompt adaptation pipeline.\n",
    "- A detailed experimental protocol, including baselines, datasets, metrics, and statistical analysis plan.\n",
    "- Reproducibility package plan (prompts templates, evaluation scripts, data handling notes) with licensing considerations.\n",
    "\n",
    "Note: The plan emphasizes using the 199 retrieved papers to systematically map overlaps, identify true innovations, and assess the practical viability of the proposed three-phase dynamic prompt adaptation approach for long-horizon creative writing tasks.', 'findings': [\"Analysis of papers focusing on 'Phase 1 Contextual Analysis (theme extraction': Found relevant insights from the pre-retrieved literature.\", \"Analysis of papers focusing on 'Theme extraction and tonal/style tracking in Phase 1 Contextual Analysis': Found relevant insights from the pre-retrieved literature.\", \"Analysis of papers focusing on 'Theme extraction and tonal/style tracking in Phase 1 Contextual Analysis (discourse drift': Found relevant insights from the pre-retrieved literature.\", \"Analysis of papers focusing on 'Phase 1 Contextual Analysis (theme detection': Found relevant insights from the pre-retrieved literature.\"], 'scores': AIMessage(content='{\n",
    "  \"novelty_score\": 6,\n",
    "  \"feasibility_score\": 7,\n",
    "  \"impact_score\": 7,\n",
    "  \"summary\": \"Based on the analysis of retrieved papers, the Phase 1 contextual analysis components (theme extraction, tonal/style tracking, discourse drift, and theme detection) show insights that largely align with existing literature. The integrated anglecombining theme extraction with tonal/style tracking and discourse drift in Phase 1offers incremental novelty at best. Feasibility is high, as the methods (topic/theme detection, stylometry, drift/discourse monitoring) are established and can be implemented with standard NLP/tooling. The potential impact is moderate, contingent on demonstrated improvements in early contextual understanding and downstream tasks, and on avoiding redundancy with prior work.\",\n",
    "  \"recommendation\": \"Revise\"\n",
    "}', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 1394, 'prompt_tokens': 294, 'total_tokens': 1688, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 1216, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-5-nano-2025-08-07', 'system_fingerprint': None, 'id': 'chatcmpl-Chii3x1Krl0KP6xllSfhGqmGgyn2I', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--a7c610b9-c5eb-45e1-a26e-1fdf8e1a3586-0', usage_metadata={'input_tokens': 294, 'output_tokens': 1394, 'total_tokens': 1688, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 1216}}), 'confidence': {'novelty': 64, 'feasibility': 69, 'overall': 66}, 'iteration': 4, 'next_action': 'end'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c40d9cc",
   "metadata": {},
   "source": [
    "#### MULTIPLE AGENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97bf45b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "idea_evaluation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
